{"2023-12-12T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2312.07540v1","updated":"2023-12-12T18:59:30Z","published":"2023-12-12T18:59:30Z","title":"diff History for Long-Context Language Agents","summary":"  Language Models (LMs) offer an exciting solution for general-purpose embodied\ncontrol. However, a key technical issue arises when using an LM-based\ncontroller: environment observations must be converted to text, which coupled\nwith history, leads to prohibitively large textual prompts. As a result, prior\nwork in LM agents is limited to restricted domains with either small\nobservation size or minimal needs for interaction history. In this paper, we\nintroduce a simple and highly effective solution to these issues. We exploit\nthe fact that consecutive text observations have high similarity and propose to\ncompress them via the Unix diff command. We demonstrate our approach in\nNetHack, a complex rogue-like video game, that requires long-horizon reasoning\nfor decision-making and is far from solved, particularly for neural agents.\nDiff history offers an average of 4x increase in the length of the text-based\ninteraction history available to the LM. This observational compression along\nwith the benefits of abstraction yields a 7x improvement in game score on\nheld-out environment instances over state-of-the-art baselines. It also\noutperforms prior agents that use visual observations by over 40%.\n","authors":["Ulyana Piterbarg","Lerrel Pinto","Rob Fergus"],"pdf_url":"https://arxiv.org/pdf/2312.07540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07532v1","updated":"2023-12-12T18:58:02Z","published":"2023-12-12T18:58:02Z","title":"Interfacing Foundation Models' Embeddings","summary":"  We present FIND, a generalized interface for aligning foundation models'\nembeddings. As shown in teaser figure, a lightweight transformer interface\nwithout tuning any foundation model weights is enough for a unified image\n(segmentation) and dataset-level (retrieval) understanding. The proposed\ninterface has the following favorable attributes: (1) Generalizable. It applies\nto various tasks spanning retrieval, segmentation, \\textit{etc.}, under the\nsame architecture and weights. (2) Prototypable. Different tasks are able to be\nimplemented through prototyping attention masks and embedding types. (3)\nExtendable. The proposed interface is adaptive to new tasks, and new models.\n(4) Interleavable. With the benefit of multi-task multi-modal training, the\nproposed interface creates an interleaved shared embedding space. In light of\nthe interleaved embedding space, we introduce the FIND-Bench, which introduces\nnew training and evaluation annotations to the COCO dataset for interleave\nsegmentation and retrieval. Our approach achieves state-of-the-art performance\non FIND-Bench and competitive performance on standard retrieval and\nsegmentation settings. The training, evaluation, and demo code as well as the\ndataset have been released at https://github.com/UX-Decoder/FIND.\n","authors":["Xueyan Zou","Linjie Li","Jianfeng Wang","Jianwei Yang","Mingyu Ding","Zhengyuan Yang","Feng Li","Hao Zhang","Shilong Liu","Arul Aravinthan","Yong Jae Lee","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2312.07532v1.pdf","comment":"CODE: https://github.com/UX-Decoder/FIND"},{"id":"http://arxiv.org/abs/2312.07527v1","updated":"2023-12-12T18:55:43Z","published":"2023-12-12T18:55:43Z","title":"BaRDa: A Belief and Reasoning Dataset that Separates Factual Accuracy\n  and Reasoning Ability","summary":"  While there are numerous benchmarks comparing the performance of modern\nlanguage models (LMs), end-task evaluations often conflate notions of *factual\naccuracy* (\"truth\") and *reasoning ability* (\"rationality\", or \"honesty\" in the\nsense of correctly reporting implications of beliefs). Our goal is a dataset\nthat clearly distinguishes these two notions. Our approach is to leverage and\nextend a collection of human-annotated *entailment trees*, engineered to\nexpress both good and bad chains of reasoning, and using a mixture of true and\nfalse facts, in particular including counterfactual examples, to avoid belief\nbias (also known as the \"content effect\"). The resulting dataset, called BaRDa,\ncontains 3000 entailments (1787 valid, 1213 invalid), using 6681 true and 2319\nfalse statements. Testing on four GPT-series models,\nGPT3(curie)/GPT3(davinici)/3.5/4, we find factual accuracy (truth) scores of\n74.1/80.6/82.6/87.1 and reasoning accuracy scores of 63.1/78.0/71.8/79.2. This\nshows the clear progression of models towards improved factual accuracy and\nentailment reasoning, and the dataset provides a new benchmark that more\ncleanly separates and quantifies these two notions.\n","authors":["Peter Clark","Bhavana Dalvi Mishra","Oyvind Tafjord"],"pdf_url":"https://arxiv.org/pdf/2312.07527v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05488v2","updated":"2023-12-12T18:32:39Z","published":"2023-12-09T07:33:26Z","title":"Can Large Language Models Serve as Rational Players in Game Theory? A\n  Systematic Analysis","summary":"  Game theory, as an analytical tool, is frequently utilized to analyze human\nbehavior in social science research. With the high alignment between the\nbehavior of Large Language Models (LLMs) and humans, a promising research\ndirection is to employ LLMs as substitutes for humans in game experiments,\nenabling social science research. However, despite numerous empirical\nresearches on the combination of LLMs and game theory, the capability\nboundaries of LLMs in game theory remain unclear. In this research, we endeavor\nto systematically analyze LLMs in the context of game theory. Specifically,\nrationality, as the fundamental principle of game theory, serves as the metric\nfor evaluating players' behavior -- building a clear desire, refining belief\nabout uncertainty, and taking optimal actions. Accordingly, we select three\nclassical games (dictator game, Rock-Paper-Scissors, and ring-network game) to\nanalyze to what extent LLMs can achieve rationality in these three aspects. The\nexperimental results indicate that even the current state-of-the-art LLM\n(GPT-4) exhibits substantial disparities compared to humans in game theory. For\ninstance, LLMs struggle to build desires based on uncommon preferences, fail to\nrefine belief from many simple patterns, and may overlook or modify refined\nbelief when taking actions. Therefore, we consider that introducing LLMs into\ngame experiments in the field of social science should be approached with\ngreater caution.\n","authors":["Caoyun Fan","Jindou Chen","Yaohui Jin","Hao He"],"pdf_url":"https://arxiv.org/pdf/2312.05488v2.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2312.07492v1","updated":"2023-12-12T18:27:44Z","published":"2023-12-12T18:27:44Z","title":"SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in\n  Generative Language Models","summary":"  Current datasets for unwanted social bias auditing are limited to studying\nprotected demographic features such as race and gender. In this work, we\nintroduce a comprehensive benchmark that is meant to capture the amplification\nof social bias, via stigmas, in generative language models. We start with a\ncomprehensive list of 93 stigmas documented in social science literature and\ncurate a question-answering (QA) dataset which involves simple social\nsituations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts, with a\nvariety of prompt styles, carefully constructed to systematically test for both\nsocial bias and model robustness. We present results for SocialStigmaQA with\ntwo widely used open source generative language models and we demonstrate that\nthe output generated by these models considerably amplifies existing social\nbias against stigmatized groups. Specifically, we find that the proportion of\nsocially biased output ranges from 45% to 59% across a variety of decoding\nstrategies and prompting styles. We discover that the deliberate design of the\ntemplates in our benchmark (e.g., by adding biasing text to the prompt or\nvarying the answer that indicates bias) impact the model tendencies to generate\nsocially biased output. Additionally, we report on patterns in the generated\nchain-of-thought output, finding a variety of problems from subtle bias to\nevidence of a lack of reasoning.\n  Warning: This paper contains examples of text which is toxic, biased, and\nharmful.\n","authors":["Manish Nagireddy","Lamogha Chiazor","Moninder Singh","Ioana Baldini"],"pdf_url":"https://arxiv.org/pdf/2312.07492v1.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2312.07476v1","updated":"2023-12-12T18:05:46Z","published":"2023-12-12T18:05:46Z","title":"Comparable Demonstrations are Important in In-Context Learning: A Novel\n  Perspective on Demonstration Selection","summary":"  In-Context Learning (ICL) is an important paradigm for adapting Large\nLanguage Models (LLMs) to downstream tasks through a few demonstrations.\nDespite the great success of ICL, the limitation of the demonstration number\nmay lead to demonstration bias, i.e. the input-label mapping induced by LLMs\nmisunderstands the task's essence. Inspired by human experience, we attempt to\nmitigate such bias through the perspective of the inter-demonstration\nrelationship. Specifically, we construct Comparable Demonstrations (CDs) by\nminimally editing the texts to flip the corresponding labels, in order to\nhighlight the task's essence and eliminate potential spurious correlations\nthrough the inter-demonstration comparison. Through a series of experiments on\nCDs, we find that (1) demonstration bias does exist in LLMs, and CDs can\nsignificantly reduce such bias; (2) CDs exhibit good performance in ICL,\nespecially in out-of-distribution scenarios. In summary, this study explores\nthe ICL mechanisms from a novel perspective, providing a deeper insight into\nthe demonstration selection strategy for ICL.\n","authors":["Caoyun Fan","Jidong Tian","Yitian Li","Hao He","Yaohui Jin"],"pdf_url":"https://arxiv.org/pdf/2312.07476v1.pdf","comment":"ICASSP 2024"},{"id":"http://arxiv.org/abs/2309.13788v2","updated":"2023-12-12T17:35:23Z","published":"2023-09-25T00:45:07Z","title":"Can LLM-Generated Misinformation Be Detected?","summary":"  The advent of Large Language Models (LLMs) has made a transformative impact.\nHowever, the potential that LLMs such as ChatGPT can be exploited to generate\nmisinformation has posed a serious concern to online safety and public trust. A\nfundamental research question is: will LLM-generated misinformation cause more\nharm than human-written misinformation? We propose to tackle this question from\nthe perspective of detection difficulty. We first build a taxonomy of\nLLM-generated misinformation. Then we categorize and validate the potential\nreal-world methods for generating misinformation with LLMs. Then, through\nextensive empirical investigation, we discover that LLM-generated\nmisinformation can be harder to detect for humans and detectors compared to\nhuman-written misinformation with the same semantics, which suggests it can\nhave more deceptive styles and potentially cause more harm. We also discuss the\nimplications of our discovery on combating misinformation in the age of LLMs\nand the countermeasures.\n","authors":["Canyu Chen","Kai Shu"],"pdf_url":"https://arxiv.org/pdf/2309.13788v2.pdf","comment":"The code, dataset and more resources on LLMs and misinformation will\n  be released on the project website: https://llm-misinformation.github.io/"},{"id":"http://arxiv.org/abs/2212.13201v3","updated":"2023-12-12T17:18:48Z","published":"2022-12-26T16:13:57Z","title":"Highlighting Named Entities in Input for Auto-Formulation of\n  Optimization Problems","summary":"  Operations research deals with modeling and solving real-world problems as\nmathematical optimization problems. While solving mathematical systems is\naccomplished by analytical software, formulating a problem as a set of\nmathematical operations has been typically done manually by domain experts.\nRecent machine learning methods have shown promise in converting textual\nproblem descriptions to corresponding mathematical formulations. This paper\npresents an approach that converts linear programming word problems into\nmathematical formulations. We leverage the named entities in the input and\naugment the input to highlight these entities. Our approach achieves the\nhighest accuracy among all submissions to the NL4Opt Competition, securing\nfirst place in the generation track.\n","authors":["Neeraj Gangwar","Nickvash Kani"],"pdf_url":"https://arxiv.org/pdf/2212.13201v3.pdf","comment":"Published in CICM 2023"},{"id":"http://arxiv.org/abs/2312.07435v1","updated":"2023-12-12T17:00:46Z","published":"2023-12-12T17:00:46Z","title":"Cross-modal Contrastive Learning with Asymmetric Co-attention Network\n  for Video Moment Retrieval","summary":"  Video moment retrieval is a challenging task requiring fine-grained\ninteractions between video and text modalities. Recent work in image-text\npretraining has demonstrated that most existing pretrained models suffer from\ninformation asymmetry due to the difference in length between visual and\ntextual sequences. We question whether the same problem also exists in the\nvideo-text domain with an auxiliary need to preserve both spatial and temporal\ninformation. Thus, we evaluate a recently proposed solution involving the\naddition of an asymmetric co-attention network for video grounding tasks.\nAdditionally, we incorporate momentum contrastive loss for robust,\ndiscriminative representation learning in both modalities. We note that the\nintegration of these supplementary modules yields better performance compared\nto state-of-the-art models on the TACoS dataset and comparable results on\nActivityNet Captions, all while utilizing significantly fewer parameters with\nrespect to baseline.\n","authors":["Love Panta","Prashant Shrestha","Brabeem Sapkota","Amrita Bhattarai","Suresh Manandhar","Anand Kumar Sah"],"pdf_url":"https://arxiv.org/pdf/2312.07435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07419v1","updated":"2023-12-12T16:41:29Z","published":"2023-12-12T16:41:29Z","title":"Towards Faster k-Nearest-Neighbor Machine Translation","summary":"  Recent works have proven the effectiveness of k-nearest-neighbor machine\ntranslation(a.k.a kNN-MT) approaches to produce remarkable improvement in\ncross-domain translations. However, these models suffer from heavy retrieve\noverhead on the entire datastore when decoding each token. We observe that\nduring the decoding phase, about 67% to 84% of tokens are unvaried after\nsearching over the corpus datastore, which means most of the tokens cause\nfutile retrievals and introduce unnecessary computational costs by initiating\nk-nearest-neighbor searches. We consider this phenomenon is explainable in\nlinguistics and propose a simple yet effective multi-layer perceptron (MLP)\nnetwork to predict whether a token should be translated jointly by the neural\nmachine translation model and probabilities produced by the kNN or just by the\nneural model. The results show that our method succeeds in reducing redundant\nretrieval operations and significantly reduces the overhead of kNN retrievals\nby up to 53% at the expense of a slight decline in translation quality.\nMoreover, our method could work together with all existing kNN-MT systems.\n","authors":["Xiangyu Shi","Yunlong Liang","Jinan Xu","Yufeng Chen"],"pdf_url":"https://arxiv.org/pdf/2312.07419v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2308.06077v2","updated":"2023-12-12T16:39:26Z","published":"2023-08-11T11:29:51Z","title":"Fly-Swat or Cannon? Cost-Effective Language Model Choice via\n  Meta-Modeling","summary":"  Generative language models (LMs) have become omnipresent across data science.\nFor a wide variety of tasks, inputs can be phrased as natural language prompts\nfor an LM, from whose output the solution can then be extracted. LM performance\nhas consistently been increasing with model size - but so has the monetary cost\nof querying the ever larger models. Importantly, however, not all inputs are\nequally hard: some require larger LMs for obtaining a satisfactory solution,\nwhereas for others smaller LMs suffice. Based on this fact, we design a\nframework for Cost-Effective Language Model Choice (CELMOC). Given a set of\ninputs and a set of candidate LMs, CELMOC judiciously assigns each input to an\nLM predicted to do well on the input according to a so-called meta-model,\naiming to achieve high overall performance at low cost. The cost-performance\ntrade-off can be flexibly tuned by the user. Options include, among others,\nmaximizing total expected performance (or the number of processed inputs) while\nstaying within a given cost budget, or minimizing total cost while processing\nall inputs. We evaluate CELMOC on 14 datasets covering five natural language\ntasks, using four candidate LMs of vastly different size and cost. With CELMOC,\nwe match the performance of the largest available LM while achieving a cost\nreduction of 63%. Via our publicly available library, researchers as well as\npractitioners can thus save large amounts of money without sacrificing\nperformance.\n","authors":["Marija Šakota","Maxime Peyrard","Robert West"],"pdf_url":"https://arxiv.org/pdf/2308.06077v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07405v1","updated":"2023-12-12T16:25:05Z","published":"2023-12-12T16:25:05Z","title":"ICL Markup: Structuring In-Context Learning using Soft-Token Tags","summary":"  Large pretrained language models (LLMs) can be rapidly adapted to a wide\nvariety of tasks via a text-to-text approach, where the instruction and input\nare fed to the model in natural language. Combined with in-context learning\n(ICL), this paradigm is impressively flexible and powerful. However, it also\nburdens users with an overwhelming number of choices, many of them arbitrary.\nInspired by markup languages like HTML, we contribute a method of using\nsoft-token tags to compose prompt templates. This approach reduces arbitrary\ndecisions and streamlines the application of ICL. Our method is a form of\nmeta-learning for ICL; it learns these tags in advance during a\nparameter-efficient fine-tuning ``warm-up'' process. The tags can subsequently\nbe used in templates for ICL on new, unseen tasks without any additional\nfine-tuning. Our experiments with this approach yield promising initial\nresults, improving LLM performance on important enterprise applications such as\nfew-shot and open-world intent detection, as well as text classification in\nnews and legal domains.\n","authors":["Marc-Etienne Brunet","Ashton Anderson","Richard Zemel"],"pdf_url":"https://arxiv.org/pdf/2312.07405v1.pdf","comment":"R0-FoMo: Workshop on Robustness of Few-shot and Zero-shot Learning in\n  Foundation Models at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2312.07399v1","updated":"2023-12-12T16:14:45Z","published":"2023-12-12T16:14:45Z","title":"Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis\n  Framework with Prompt-Generated Rationales","summary":"  Machine reasoning has made great progress in recent years owing to large\nlanguage models (LLMs). In the clinical domain, however, most NLP-driven\nprojects mainly focus on clinical classification or reading comprehension, and\nunder-explore clinical reasoning for disease diagnosis due to the expensive\nrationale annotation with clinicians. In this work, we present a\n``reasoning-aware'' diagnosis framework that rationalizes the diagnostic\nprocess via prompt-based learning in a time- and labor-efficient manner, and\nlearns to reason over the prompt-generated rationales. Specifically, we address\nthe clinical reasoning for disease diagnosis, where the LLM generates\ndiagnostic rationales providing its insight on presented patient data and the\nreasoning path towards the diagnosis, namely Clinical Chain-of-Thought\n(Clinical CoT). We empirically demonstrate LLMs/LMs' ability of clinical\nreasoning via extensive experiments and analyses on both rationale generation\nand disease diagnosis in various settings. We further propose a novel set of\ncriteria for evaluating machine-generated rationales' potential for real-world\nclinical settings, facilitating and benefiting future research in this area.\n","authors":["Taeyoon Kwon","Kai Tzu-iunn Ong","Dongjin Kang","Seungjun Moon","Jeong Ryong Lee","Dosik Hwang","Yongsik Sim","Beomseok Sohn","Dongha Lee","Jinyoung Yeo"],"pdf_url":"https://arxiv.org/pdf/2312.07399v1.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2312.07398v1","updated":"2023-12-12T16:14:43Z","published":"2023-12-12T16:14:43Z","title":"LLMEval: A Preliminary Study on How to Evaluate Large Language Models","summary":"  Recently, the evaluation of Large Language Models has emerged as a popular\narea of research. The three crucial questions for LLM evaluation are ``what,\nwhere, and how to evaluate''. However, the existing research mainly focuses on\nthe first two questions, which are basically what tasks to give the LLM during\ntesting and what kind of knowledge it should deal with. As for the third\nquestion, which is about what standards to use, the types of evaluators, how to\nscore, and how to rank, there hasn't been much discussion. In this paper, we\nanalyze evaluation methods by comparing various criteria with both manual and\nautomatic evaluation, utilizing onsite, crowd-sourcing, public annotators and\nGPT-4, with different scoring methods and ranking systems. We propose a new\ndataset, LLMEval and conduct evaluations on 20 LLMs. A total of 2,186\nindividuals participated, leading to the generation of 243,337 manual\nannotations and 57,511 automatic evaluation results. We perform comparisons and\nanalyses of different settings and conduct 10 conclusions that can provide some\ninsights for evaluating LLM in the future. The dataset and the results are\npublicly available at https://github.com/llmeval .\n","authors":["Yue Zhang","Ming Zhang","Haipeng Yuan","Shichun Liu","Yongyao Shi","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2312.07398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07395v1","updated":"2023-12-12T16:10:19Z","published":"2023-12-12T16:10:19Z","title":"A Simple Recipe for Contrastively Pre-training Video-First Encoders\n  Beyond 16 Frames","summary":"  Understanding long, real-world videos requires modeling of long-range visual\ndependencies. To this end, we explore video-first architectures, building on\nthe common paradigm of transferring large-scale, image--text models to video\nvia shallow temporal fusion. However, we expose two limitations to the\napproach: (1) decreased spatial capabilities, likely due to poor\nvideo--language alignment in standard video datasets, and (2) higher memory\nconsumption, bottlenecking the number of frames that can be processed. To\nmitigate the memory bottleneck, we systematically analyze the memory/accuracy\ntrade-off of various efficient methods: factorized attention,\nparameter-efficient image-to-video adaptation, input masking, and\nmulti-resolution patchification. Surprisingly, simply masking large portions of\nthe video (up to 75%) during contrastive pre-training proves to be one of the\nmost robust ways to scale encoders to videos up to 4.3 minutes at 1 FPS. Our\nsimple approach for training long video-to-text models, which scales to 1B\nparameters, does not add new architectural complexity and is able to outperform\nthe popular paradigm of using much larger LLMs as an information aggregator\nover segment-based information on benchmarks with long-range temporal\ndependencies (YouCook2, EgoSchema).\n","authors":["Pinelopi Papalampidi","Skanda Koppula","Shreya Pathak","Justin Chiu","Joe Heyward","Viorica Patraucean","Jiajun Shen","Antoine Miech","Andrew Zisserman","Aida Nematzdeh"],"pdf_url":"https://arxiv.org/pdf/2312.07395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.12424v3","updated":"2023-12-12T16:08:18Z","published":"2023-06-21T17:59:51Z","title":"VisoGender: A dataset for benchmarking gender bias in image-text pronoun\n  resolution","summary":"  We introduce VisoGender, a novel dataset for benchmarking gender bias in\nvision-language models. We focus on occupation-related biases within a\nhegemonic system of binary gender, inspired by Winograd and Winogender schemas,\nwhere each image is associated with a caption containing a pronoun relationship\nof subjects and objects in the scene. VisoGender is balanced by gender\nrepresentation in professional roles, supporting bias evaluation in two ways:\ni) resolution bias, where we evaluate the difference between pronoun resolution\naccuracies for image subjects with gender presentations perceived as masculine\nversus feminine by human annotators and ii) retrieval bias, where we compare\nratios of professionals perceived to have masculine and feminine gender\npresentations retrieved for a gender-neutral search query. We benchmark several\nstate-of-the-art vision-language models and find that they demonstrate bias in\nresolving binary gender in complex scenes. While the direction and magnitude of\ngender bias depends on the task and the model being evaluated, captioning\nmodels are generally less biased than Vision-Language Encoders. Dataset and\ncode are available at https://github.com/oxai/visogender\n","authors":["Siobhan Mackenzie Hall","Fernanda Gonçalves Abrantes","Hanwen Zhu","Grace Sodunke","Aleksandar Shtedritski","Hannah Rose Kirk"],"pdf_url":"https://arxiv.org/pdf/2306.12424v3.pdf","comment":"NeurIPS Datasets and Benchmarks 2023. Data and code available at\n  https://github.com/oxai/visogender"},{"id":"http://arxiv.org/abs/2312.05180v2","updated":"2023-12-12T16:06:32Z","published":"2023-12-08T17:05:47Z","title":"PathFinder: Guided Search over Multi-Step Reasoning Paths","summary":"  With recent advancements in large language models, methods like\nchain-of-thought prompting to elicit reasoning chains have been shown to\nimprove results on reasoning tasks. However, tasks that require multiple steps\nof reasoning still pose significant challenges to state-of-the-art models.\nDrawing inspiration from the beam search algorithm, we propose PathFinder, a\ntree-search-based reasoning path generation approach. It enhances diverse\nbranching and multi-hop reasoning through the integration of dynamic decoding,\nenabled by varying sampling methods and parameters. Using constrained\nreasoning, PathFinder integrates novel quality constraints, pruning, and\nexploration methods to enhance the efficiency and the quality of generation.\nMoreover, it includes scoring and ranking features to improve candidate\nselection. Our approach outperforms competitive baselines on three complex\narithmetic and commonsense reasoning tasks by 6% on average. Our model\ngeneralizes well to longer, unseen reasoning chains, reflecting similar\ncomplexities to beam search with large branching factors.\n","authors":["Olga Golovneva","Sean O'Brien","Ramakanth Pasunuru","Tianlu Wang","Luke Zettlemoyer","Maryam Fazel-Zarandi","Asli Celikyilmaz"],"pdf_url":"https://arxiv.org/pdf/2312.05180v2.pdf","comment":"NeurIPS 2023 R0-FoMo Workshop"},{"id":"http://arxiv.org/abs/2312.07338v1","updated":"2023-12-12T14:58:08Z","published":"2023-12-12T14:58:08Z","title":"Self-supervised Adaptive Pre-training of Multilingual Speech Models for\n  Language and Dialect Identification","summary":"  Pre-trained Transformer-based speech models have shown striking performance\nwhen fine-tuned on various downstream tasks such as automatic speech\nrecognition and spoken language identification (SLID). However, the problem of\ndomain mismatch remains a challenge in this area, where the domain of the\npre-training data might differ from that of the downstream labeled data used\nfor fine-tuning. In multilingual tasks such as SLID, the pre-trained speech\nmodel may not support all the languages in the downstream task. To address this\nchallenge, we propose self-supervised adaptive pre-training (SAPT) to adapt the\npre-trained model to the target domain and languages of the downstream task. We\napply SAPT to the XLSR-128 model and investigate the effectiveness of this\napproach for the SLID task. First, we demonstrate that SAPT improves XLSR\nperformance on the FLEURS benchmark with substantial gains up to 40.1% for\nunder-represented languages. Second, we apply SAPT on four different datasets\nin a few-shot learning setting, showing that our approach improves the sample\nefficiency of XLSR during fine-tuning. Our experiments provide strong empirical\nevidence that continual adaptation via self-supervision improves downstream\nperformance for multilingual speech models.\n","authors":["Mohammed Maqsood Shaik","Dietrich Klakow","Badr M. Abdullah"],"pdf_url":"https://arxiv.org/pdf/2312.07338v1.pdf","comment":"Submitted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.07305v1","updated":"2023-12-12T14:24:54Z","published":"2023-12-12T14:24:54Z","title":"SCCA: Shifted Cross Chunk Attention for long contextual semantic\n  expansion","summary":"  Sparse attention as a efficient method can significantly decrease the\ncomputation cost, but current sparse attention tend to rely on window self\nattention which block the global information flow. For this problem, we present\nShifted Cross Chunk Attention (SCCA), using different KV shifting strategy to\nextend respective field in each attention layer. Except, we combine Dilated\nAttention(DA) and Dilated Neighborhood Attention(DNA) to present Shifted\nDilated Attention(SDA). Both SCCA and SDA can accumulate attention results in\nmulti head attention to obtain approximate respective field in full attention.\nIn this paper, we conduct language modeling experiments using different pattern\nof SCCA and combination of SCCA and SDA. The proposed shifted cross chunk\nattention (SCCA) can effectively extend large language models (LLMs) to longer\ncontext combined with Positional interpolation(PI) and LoRA than current sparse\nattention. Notably, SCCA adopts LLaMA2 7B from 4k context to 8k in single V100.\nThis attention pattern can provide a Plug-and-play fine-tuning method to extend\nmodel context while retaining their original architectures, and is compatible\nwith most existing techniques.\n","authors":["Yuxiang Guo"],"pdf_url":"https://arxiv.org/pdf/2312.07305v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2305.07402v3","updated":"2023-12-12T14:04:34Z","published":"2023-05-12T11:58:15Z","title":"Synergistic Interplay between Search and Large Language Models for\n  Information Retrieval","summary":"  Information retrieval (IR) plays a crucial role in locating relevant\nresources from vast amounts of data, and its applications have evolved from\ntraditional knowledge bases to modern retrieval models (RMs). The emergence of\nlarge language models (LLMs) has further revolutionized the IR field by\nenabling users to interact with search systems in natural languages. In this\npaper, we explore the advantages and disadvantages of LLMs and RMs,\nhighlighting their respective strengths in understanding user-issued queries\nand retrieving up-to-date information. To leverage the benefits of both\nparadigms while circumventing their limitations, we propose InteR, a novel\nframework that facilitates information refinement through synergy between RMs\nand LLMs. InteR allows RMs to expand knowledge in queries using LLM-generated\nknowledge collections and enables LLMs to enhance prompt formulation using\nretrieved documents. This iterative refinement process augments the inputs of\nRMs and LLMs, leading to more accurate retrieval. Experiments on large-scale\nretrieval benchmarks involving web search and low-resource retrieval tasks\ndemonstrate that InteR achieves overall superior zero-shot retrieval\nperformance compared to state-of-the-art methods, even those using relevance\njudgment. Source code is available at https://github.com/Cyril-JZ/InteR\n","authors":["Jiazhan Feng","Chongyang Tao","Xiubo Geng","Tao Shen","Can Xu","Guodong Long","Dongyan Zhao","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2305.07402v3.pdf","comment":"Pre-print. Work in progress"},{"id":"http://arxiv.org/abs/2312.07280v1","updated":"2023-12-12T13:57:57Z","published":"2023-12-12T13:57:57Z","title":"Towards Equipping Transformer with the Ability of Systematic\n  Compositionality","summary":"  One of the key factors in language productivity and human cognition is the\nability of systematic compositionality, which refers to understanding composed\nunseen examples of seen primitives. However, recent evidence reveals that the\nTransformers have difficulty generalizing the composed context based on the\nseen primitives. To this end, we take the first step to propose a\ncompositionality-aware Transformer called CAT and two novel pre-training tasks\nto facilitate systematic compositionality. We tentatively provide a successful\nimplementation of a multi-layer CAT on the basis of the especially popular\nBERT. The experimental results demonstrate that CAT outperforms baselines on\ncompositionality-aware tasks with minimal impact on the effectiveness on\nstandardized language understanding tasks.\n","authors":["Chen Huang","Peixin Qin","Wenqiang Lei","Jiancheng Lv"],"pdf_url":"https://arxiv.org/pdf/2312.07280v1.pdf","comment":"Accepted to AAAI 2024. Paper with appendix"},{"id":"http://arxiv.org/abs/2312.07255v1","updated":"2023-12-12T13:35:41Z","published":"2023-12-12T13:35:41Z","title":"GIST: Improving Parameter Efficient Fine Tuning via Knowledge\n  Interaction","summary":"  The Parameter-Efficient Fine-Tuning (PEFT) method, which adjusts or\nintroduces fewer trainable parameters to calibrate pre-trained models on\ndownstream tasks, has become a recent research interest. However, existing PEFT\nmethods within the traditional fine-tiuning framework have two main\nshortcomings: 1) They overlook the explicit association between trainable\nparameters and downstream task knowledge. 2) They neglect the interaction\nbetween the intrinsic task-agnostic knowledge of pre-trained models and the\ntask-specific knowledge in downstream tasks. To address this gap, we propose a\nnovel fine-tuning framework, named GIST, in a plug-and-play manner.\nSpecifically, our framework first introduces a trainable token, called the Gist\ntoken, when applying PEFT methods on downstream tasks. This token serves as an\naggregator of the task-specific knowledge learned by the PEFT methods and forms\nan explicit association with downstream knowledge. Furthermore, to facilitate\nexplicit interaction between task-agnostic and task-specific knowledge, we\nintroduce the concept of Knowledge Interaction via a Bidirectional\nKullback-Leibler Divergence objective. As a result, PEFT methods within our\nframework can make the pre-trained model understand downstream tasks more\ncomprehensively by leveraging the knowledge interaction. Extensive experiments\ndemonstrate the universality and scalability of our framework. Notably, on the\nVTAB-1K benchmark, we employ the Adapter (a prevalent PEFT method) within our\nGIST framework and achieve a performance boost of 2.25%, with an increase of\nonly 0.8K parameters. The Code will be released.\n","authors":["Jiacheng Ruan","Jingsheng Gao","Mingye Xie","Suncheng Xiang","Zefang Yu","Ting Liu","Yuzhuo Fu"],"pdf_url":"https://arxiv.org/pdf/2312.07255v1.pdf","comment":"17pages, 8 figures, 22 tables, Work in progress"},{"id":"http://arxiv.org/abs/2312.07254v1","updated":"2023-12-12T13:35:33Z","published":"2023-12-12T13:35:33Z","title":"The GUA-Speech System Description for CNVSRC Challenge 2023","summary":"  This study describes our system for Task 1 Single-speaker Visual Speech\nRecognition (VSR) fixed track in the Chinese Continuous Visual Speech\nRecognition Challenge (CNVSRC) 2023. Specifically, we use intermediate\nconnectionist temporal classification (Inter CTC) residual modules to relax the\nconditional independence assumption of CTC in our model. Then we use a\nbi-transformer decoder to enable the model to capture both past and future\ncontextual information. In addition, we use Chinese characters as the modeling\nunits to improve the recognition accuracy of our model. Finally, we use a\nrecurrent neural network language model (RNNLM) for shallow fusion in the\ninference stage. Experiments show that our system achieves a character error\nrate (CER) of 38.09% on the Eval set which reaches a relative CER reduction of\n21.63% over the official baseline, and obtains a second place in the challenge.\n","authors":["Shengqiang Li","Chao Lei","Baozhong Ma","Binbin Zhang","Fuping Pan"],"pdf_url":"https://arxiv.org/pdf/2312.07254v1.pdf","comment":"CNVSRC 2023 Challenge"},{"id":"http://arxiv.org/abs/2310.14505v3","updated":"2023-12-12T13:28:35Z","published":"2023-10-23T02:32:30Z","title":"Sentiment analysis with adaptive multi-head attention in Transformer","summary":"  We propose a novel framework based on the attention mechanism to identify the\nsentiment of a movie review document. Previous efforts on deep neural networks\nwith attention mechanisms focus on encoder and decoder with fixed numbers of\nmulti-head attention. Therefore, we need a mechanism to stop the attention\nprocess automatically if no more useful information can be read from the\nmemory.In this paper, we propose an adaptive multi-head attention architecture\n(AdaptAttn) which varies the number of attention heads based on length of\nsentences. AdaptAttn has a data preprocessing step where each document is\nclassified into any one of the three bins small, medium or large based on\nlength of the sentence. The document classified as small goes through two heads\nin each layer, the medium group passes four heads and the large group is\nprocessed by eight heads. We examine the merit of our model on the Stanford\nlarge movie review dataset. The experimental results show that the F1 score\nfrom our model is on par with the baseline model.\n","authors":["Fanfei Meng","David Demeter"],"pdf_url":"https://arxiv.org/pdf/2310.14505v3.pdf","comment":"Accepted by the 4th International Conference on Signal Processing and\n  Machine Learning"},{"id":"http://arxiv.org/abs/2312.07250v1","updated":"2023-12-12T13:26:42Z","published":"2023-12-12T13:26:42Z","title":"Neural Machine Translation of Clinical Text: An Empirical Investigation\n  into Multilingual Pre-Trained Language Models and Transfer-Learning","summary":"  We conduct investigations on clinical text machine translation by examining\nmultilingual neural network models using deep learning such as Transformer\nbased structures. Furthermore, to address the language resource imbalance\nissue, we also carry out experiments using a transfer learning methodology\nbased on massive multilingual pre-trained language models (MMPLMs). The\nexperimental results on three subtasks including 1) clinical case (CC), 2)\nclinical terminology (CT), and 3) ontological concept (OC) show that our models\nachieved top-level performances in the ClinSpEn-2022 shared task on\nEnglish-Spanish clinical domain data. Furthermore, our expert-based human\nevaluations demonstrate that the small-sized pre-trained language model (PLM)\nwon over the other two extra-large language models by a large margin, in the\nclinical domain fine-tuning, which finding was never reported in the field.\nFinally, the transfer learning method works well in our experimental setting\nusing the WMT21fb model to accommodate a new language space Spanish that was\nnot seen at the pre-training stage within WMT21fb itself, which deserves more\nexploitation for clinical knowledge transformation, e.g. to investigate into\nmore languages. These research findings can shed some light on domain-specific\nmachine translation development, especially in clinical and healthcare fields.\nFurther research projects can be carried out based on our work to improve\nhealthcare text analytics and knowledge transformation.\n","authors":["Lifeng Han","Serge Gladkoff","Gleb Erofeev","Irina Sorokina","Betty Galiano","Goran Nenadic"],"pdf_url":"https://arxiv.org/pdf/2312.07250v1.pdf","comment":"Submitted to Frontiers in Digital Health - Health Informatics"},{"id":"http://arxiv.org/abs/2312.05662v2","updated":"2023-12-12T12:51:52Z","published":"2023-12-09T20:04:20Z","title":"Understanding the Effect of Model Compression on Social Bias in Large\n  Language Models","summary":"  Large Language Models (LLMs) trained with self-supervision on vast corpora of\nweb text fit to the social biases of that text. Without intervention, these\nsocial biases persist in the model's predictions in downstream tasks, leading\nto representational harm. Many strategies have been proposed to mitigate the\neffects of inappropriate social biases learned during pretraining.\nSimultaneously, methods for model compression have become increasingly popular\nto reduce the computational burden of LLMs. Despite the popularity and need for\nboth approaches, little work has been done to explore the interplay between\nthese two. We perform a carefully controlled study of the impact of model\ncompression via quantization and knowledge distillation on measures of social\nbias in LLMs. Longer pretraining and larger models led to higher social bias,\nand quantization showed a regularizer effect with its best trade-off around 20%\nof the original pretraining time.\n","authors":["Gustavo Gonçalves","Emma Strubell"],"pdf_url":"https://arxiv.org/pdf/2312.05662v2.pdf","comment":"EMNLP 2023 Main"},{"id":"http://arxiv.org/abs/2302.11091v2","updated":"2023-12-12T12:46:39Z","published":"2023-02-22T01:57:42Z","title":"GTRL: An Entity Group-Aware Temporal Knowledge Graph Representation\n  Learning Method","summary":"  Temporal Knowledge Graph (TKG) representation learning embeds entities and\nevent types into a continuous low-dimensional vector space by integrating the\ntemporal information, which is essential for downstream tasks, e.g., event\nprediction and question answering. Existing methods stack multiple graph\nconvolution layers to model the influence of distant entities, leading to the\nover-smoothing problem. To alleviate the problem, recent studies infuse\nreinforcement learning to obtain paths that contribute to modeling the\ninfluence of distant entities. However, due to the limited number of hops,\nthese studies fail to capture the correlation between entities that are far\napart and even unreachable. To this end, we propose GTRL, an entity Group-aware\nTemporal knowledge graph Representation Learning method. GTRL is the first work\nthat incorporates the entity group modeling to capture the correlation between\nentities by stacking only a finite number of layers. Specifically, the entity\ngroup mapper is proposed to generate entity groups from entities in a learning\nway. Based on entity groups, the implicit correlation encoder is introduced to\ncapture implicit correlations between any pairwise entity groups. In addition,\nthe hierarchical GCNs are exploited to accomplish the message aggregation and\nrepresentation updating on the entity group graph and the entity graph.\nFinally, GRUs are employed to capture the temporal dependency in TKGs.\nExtensive experiments on three real-world datasets demonstrate that GTRL\nachieves the state-of-the-art performances on the event prediction task,\noutperforming the best baseline by an average of 13.44%, 9.65%, 12.15%, and\n15.12% in MRR, Hits@1, Hits@3, and Hits@10, respectively.\n","authors":["Xing Tang","Ling Chen"],"pdf_url":"https://arxiv.org/pdf/2302.11091v2.pdf","comment":"Accepted by TKDE, 16 pages, and 9 figures"},{"id":"http://arxiv.org/abs/2312.07228v1","updated":"2023-12-12T12:43:01Z","published":"2023-12-12T12:43:01Z","title":"Toxic language detection: a systematic survey of Arabic datasets","summary":"  This paper offers a comprehensive survey of Arabic datasets focused on online\ntoxic language. We systematically gathered a total of 49 available datasets and\ntheir corresponding papers and conducted a thorough analysis, considering 16\ncriteria across three primary dimensions: content, annotation process, and\nreusability. This analysis enabled us to identify existing gaps and make\nrecommendations for future research works.\n","authors":["Imene Bensalem","Paolo Rosso","Hanane Zitouni"],"pdf_url":"https://arxiv.org/pdf/2312.07228v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07194v1","updated":"2023-12-12T12:00:04Z","published":"2023-12-12T12:00:04Z","title":"Verbreitungsmechanismen schädigender Sprache im Netz: Anatomie zweier\n  Shitstorms","summary":"  In this working paper, we turn our attention to two exemplary, cross-media\nshitstorms directed against well-known individuals from the business world.\nBoth have in common, first, the trigger, a controversial statement by the\nperson who thereby becomes the target of the shitstorm, and second, the\nidentity of this target as relatively privileged: cis-male, white, successful.\nWe examine the spread of the outrage wave across two media at a time and test\nthe applicability of computational linguistic methods for analyzing its time\ncourse. Assuming that harmful language spreads like a virus in digital space,\nwe are primarily interested in the events and constellations that lead to the\nuse of harmful language, and whether and how a linguistic formation of \"tribes\"\noccurs. Our research therefore focuses, first, on the distribution of\nlinguistic features within the overall shitstorm: are individual words or\nphrases increasingly used after their introduction, and through which pathways\nthey spread. Second, we ask whether \"tribes,\" for example, one group of\nsupporters and one of opponents of the target, have a distinguished linguistic\nform. Our hypothesis is that supporters remain equally active over time, while\nthe dynamic \"ripple\" effect of the shitstorm is based on the varying\nparticipation of opponents.\n","authors":["Tatjana Scheffler","Veronika Solopova","Mihaela Popa-Wyatt"],"pdf_url":"https://arxiv.org/pdf/2312.07194v1.pdf","comment":"in German language"},{"id":"http://arxiv.org/abs/2312.07182v1","updated":"2023-12-12T11:38:09Z","published":"2023-12-12T11:38:09Z","title":"Classifying complex documents: comparing bespoke solutions to large\n  language models","summary":"  Here we search for the best automated classification approach for a set of\ncomplex legal documents. Our classification task is not trivial: our aim is to\nclassify ca 30,000 public courthouse records from 12 states and 267 counties at\ntwo different levels using nine sub-categories. Specifically, we investigated\nwhether a fine-tuned large language model (LLM) can achieve the accuracy of a\nbespoke custom-trained model, and what is the amount of fine-tuning necessary.\n","authors":["Glen Hopkins","Kristjan Kalm"],"pdf_url":"https://arxiv.org/pdf/2312.07182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07141v1","updated":"2023-12-12T10:24:17Z","published":"2023-12-12T10:24:17Z","title":"Multilingual large language models leak human stereotypes across\n  language boundaries","summary":"  Multilingual large language models have been increasingly popular for their\nproficiency in comprehending and generating text across various languages.\nPrevious research has shown that the presence of stereotypes and biases in\nmonolingual large language models can be attributed to the nature of their\ntraining data, which is collected from humans and reflects societal biases.\nMultilingual language models undergo the same training procedure as monolingual\nones, albeit with training data sourced from various languages. This raises the\nquestion: do stereotypes present in one social context leak across languages\nwithin the model? In our work, we first define the term ``stereotype leakage''\nand propose a framework for its measurement. With this framework, we\ninvestigate how stereotypical associations leak across four languages: English,\nRussian, Chinese, and Hindi. To quantify the stereotype leakage, we employ an\napproach from social psychology, measuring stereotypes via group-trait\nassociations. We evaluate human stereotypes and stereotypical associations\nmanifested in multilingual large language models such as mBERT, mT5, and\nChatGPT. Our findings show a noticeable leakage of positive, negative, and\nnon-polar associations across all languages. Notably, Hindi within multilingual\nmodels appears to be the most susceptible to influence from other languages,\nwhile Chinese is the least. Additionally, ChatGPT exhibits a better alignment\nwith human scores than other models.\n","authors":["Yang Trista Cao","Anna Sotnikova","Jieyu Zhao","Linda X. Zou","Rachel Rudinger","Hal Daume III"],"pdf_url":"https://arxiv.org/pdf/2312.07141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07110v1","updated":"2023-12-12T09:39:03Z","published":"2023-12-12T09:39:03Z","title":"LLMs Perform Poorly at Concept Extraction in Cyber-security Research\n  Literature","summary":"  The cybersecurity landscape evolves rapidly and poses threats to\norganizations. To enhance resilience, one needs to track the latest\ndevelopments and trends in the domain. It has been demonstrated that standard\nbibliometrics approaches show their limits in such a fast-evolving domain. For\nthis purpose, we use large language models (LLMs) to extract relevant knowledge\nentities from cybersecurity-related texts. We use a subset of arXiv preprints\non cybersecurity as our data and compare different LLMs in terms of entity\nrecognition (ER) and relevance. The results suggest that LLMs do not produce\ngood knowledge entities that reflect the cybersecurity context, but our results\nshow some potential for noun extractors. For this reason, we developed a noun\nextractor boosted with some statistical analysis to extract specific and\nrelevant compound nouns from the domain. Later, we tested our model to identify\ntrends in the LLM domain. We observe some limitations, but it offers promising\nresults to monitor the evolution of emergent trends.\n","authors":["Maxime Würsch","Andrei Kucharavy","Dimitri Percia David","Alain Mermoud"],"pdf_url":"https://arxiv.org/pdf/2312.07110v1.pdf","comment":"24 pages, 9 figures"},{"id":"http://arxiv.org/abs/2312.07088v1","updated":"2023-12-12T09:14:55Z","published":"2023-12-12T09:14:55Z","title":"BED: Bi-Encoder-Decoder Model for Canonical Relation Extraction","summary":"  Canonical relation extraction aims to extract relational triples from\nsentences, where the triple elements (entity pairs and their relationship) are\nmapped to the knowledge base. Recently, methods based on the encoder-decoder\narchitecture are proposed and achieve promising results. However, these methods\ncannot well utilize the entity information, which is merely used as augmented\ntraining data. Moreover, they are incapable of representing novel entities,\nsince no embeddings have been learned for them. In this paper, we propose a\nnovel framework, Bi-Encoder-Decoder (BED), to solve the above issues.\nSpecifically, to fully utilize entity information, we employ an encoder to\nencode semantics of this information, leading to high-quality entity\nrepresentations. For novel entities, given a trained entity encoder, their\nrepresentations can be easily generated. Experimental results on two datasets\nshow that, our method achieves a significant performance improvement over the\nprevious state-of-the-art and handle novel entities well without retraining.\n","authors":["Nantao Zheng","Siyu Long","Xinyu Dai"],"pdf_url":"https://arxiv.org/pdf/2312.07088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.15057v2","updated":"2023-12-12T08:46:11Z","published":"2023-05-24T11:47:35Z","title":"Linear-Time Modeling of Linguistic Structure: An Order-Theoretic\n  Perspective","summary":"  Tasks that model the relation between pairs of tokens in a string are a vital\npart of understanding natural language. Such tasks, in general, require\nexhaustive pair-wise comparisons of tokens, thus having a quadratic runtime\ncomplexity in the length of the string. We show that these exhaustive\ncomparisons can be avoided, and, moreover, the complexity of such tasks can be\nreduced to linear by casting the relation between tokens as a partial order\nover the string. Our method predicts real numbers for each token in a string in\nparallel and sorts the tokens accordingly, resulting in total orders of the\ntokens in the string. Each total order implies a set of arcs oriented from\nsmaller to greater tokens, sorted by their predicted numbers. The intersection\nof total orders results in a partial order over the set of tokens in the\nstring, which is then decoded into a directed graph representing the desired\nlinguistic structure. Our experiments on dependency parsing and coreference\nresolution show that our method achieves state-of-the-art or comparable\nperformance. Moreover, the linear complexity and parallelism of our method\ndouble the speed of graph-based coreference resolution models, and bring a\n10-times speed-up over graph-based dependency parsers.\n","authors":["Tianyu Liu","Afra Amini","Mrinmaya Sachan","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2305.15057v2.pdf","comment":"EMNLP 2023, 23 pages"},{"id":"http://arxiv.org/abs/2312.07069v1","updated":"2023-12-12T08:43:20Z","published":"2023-12-12T08:43:20Z","title":"Context Matter: Data-Efficient Augmentation of Large Language Models for\n  Scientific Applications","summary":"  In this paper, we explore the challenges inherent to Large Language Models\n(LLMs) like GPT-4, particularly their propensity for hallucinations, logic\nmistakes, and incorrect conclusions when tasked with answering complex\nquestions. The capacity of LLMs to present erroneous answers in a coherent and\nsemantically rigorous manner further complicates the detection of factual\ninaccuracies. This issue is especially pronounced in fields that require\nspecialized expertise. Our work delves into these challenges, aiming to enhance\nthe understanding and mitigation of such errors, thereby contributing to the\nimprovement of LLM accuracy and reliability in scientific and other specialized\ndomains. Our findings reveal a non-linear relationship between the context's\nrelevancy and the answers' measured quality. In addition, we demonstrate that\nwith the correct calibration, it is possible to automate the grading procedure\n-- a finding suggesting that, at least to some degree, the LLMs can be used to\nself-examine the quality of their own performance. Finally, we describe an\nexperimental platform that can be seen as a proof-of-concept of the techniques\ndescribed in this work.\n","authors":["Xiang Li","Haoran Tang","Siyu Chen","Ziwei Wang","Anurag Maravi","Marcin Abram"],"pdf_url":"https://arxiv.org/pdf/2312.07069v1.pdf","comment":"11 pages, 6 figures, 4 tables, 3 pages of supplementary material"},{"id":"http://arxiv.org/abs/2312.07066v1","updated":"2023-12-12T08:40:38Z","published":"2023-12-12T08:40:38Z","title":"DiffuVST: Narrating Fictional Scenes with Global-History-Guided\n  Denoising Models","summary":"  Recent advances in image and video creation, especially AI-based image\nsynthesis, have led to the production of numerous visual scenes that exhibit a\nhigh level of abstractness and diversity. Consequently, Visual Storytelling\n(VST), a task that involves generating meaningful and coherent narratives from\na collection of images, has become even more challenging and is increasingly\ndesired beyond real-world imagery. While existing VST techniques, which\ntypically use autoregressive decoders, have made significant progress, they\nsuffer from low inference speed and are not well-suited for synthetic scenes.\nTo this end, we propose a novel diffusion-based system DiffuVST, which models\nthe generation of a series of visual descriptions as a single conditional\ndenoising process. The stochastic and non-autoregressive nature of DiffuVST at\ninference time allows it to generate highly diverse narratives more\nefficiently. In addition, DiffuVST features a unique design with bi-directional\ntext history guidance and multimodal adapter modules, which effectively improve\ninter-sentence coherence and image-to-text fidelity. Extensive experiments on\nthe story generation task covering four fictional visual-story datasets\ndemonstrate the superiority of DiffuVST over traditional autoregressive models\nin terms of both text quality and inference speed.\n","authors":["Shengguang Wu","Mei Yuan","Qi Su"],"pdf_url":"https://arxiv.org/pdf/2312.07066v1.pdf","comment":"EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2312.05603v2","updated":"2023-12-12T08:04:30Z","published":"2023-12-09T16:10:23Z","title":"Sim-GPT: Text Similarity via GPT Annotated Data","summary":"  Due to the lack of a large collection of high-quality labeled sentence pairs\nwith textual similarity scores, existing approaches for Semantic Textual\nSimilarity (STS) mostly rely on unsupervised techniques or training signals\nthat are only partially correlated with textual similarity, e.g., NLI-based\ndatasets. To tackle this issue, in this paper, we propose the strategy of\nmeasuring text similarity via GPT annotated data (Sim-GPT for short). The core\nidea of Sim-GPT is to generate data with STS labels using GPT-4, based on which\nan STS model is trained. Sim-GPT framework utilizes LLMs to provide a\nsubstantial amount of reliable annotated data filling the gap of the lack of\ntraining signals for STS. Sim-GPT is trained on a one-time generated dataset\nusing BERT or RoBERTa as the backbone, which offers long-term savings in cost\nand speed compared to repeatedly invoking LLMs for each sentence pair. Trained\non the examples from GPT-4 (371K), Sim-GPT yields SOTA performances on the\nwidely-used seven STS benchmarks: +0.99 over supervised-SimCSE, and +0.42 over\nthe current SOTA PromCSE model. To encourage further advancements of the field,\nwe release both models and the 371K annotated examples from GPT-4. Code, models\nand annotated data are available at: https://github.com/ShuheWang1998/Sim-GPT.\n","authors":["Shuhe Wang","Beiming Cao","Shengyu Zhang","Xiaoya Li","Jiwei Li","Fei Wu","Guoyin Wang","Eduard Hovy"],"pdf_url":"https://arxiv.org/pdf/2312.05603v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07049v1","updated":"2023-12-12T08:02:06Z","published":"2023-12-12T08:02:06Z","title":"Improving Factual Error Correction by Learning to Inject Factual Errors","summary":"  Factual error correction (FEC) aims to revise factual errors in false claims\nwith minimal editing, making them faithful to the provided evidence. This task\nis crucial for alleviating the hallucination problem encountered by large\nlanguage models. Given the lack of paired data (i.e., false claims and their\ncorresponding correct claims), existing methods typically adopt the\nmask-then-correct paradigm. This paradigm relies solely on unpaired false\nclaims and correct claims, thus being referred to as distantly supervised\nmethods. These methods require a masker to explicitly identify factual errors\nwithin false claims before revising with a corrector. However, the absence of\npaired data to train the masker makes accurately pinpointing factual errors\nwithin claims challenging. To mitigate this, we propose to improve FEC by\nLearning to Inject Factual Errors (LIFE), a three-step distantly supervised\nmethod: mask-corrupt-correct. Specifically, we first train a corruptor using\nthe mask-then-corrupt procedure, allowing it to deliberately introduce factual\nerrors into correct text. The corruptor is then applied to correct claims,\ngenerating a substantial amount of paired data. After that, we filter out\nlow-quality data, and use the remaining data to train a corrector. Notably, our\ncorrector does not require a masker, thus circumventing the bottleneck\nassociated with explicit factual error identification. Our experiments on a\npublic dataset verify the effectiveness of LIFE in two key aspects: Firstly, it\noutperforms the previous best-performing distantly supervised method by a\nnotable margin of 10.59 points in SARI Final (19.3% improvement). Secondly,\neven compared to ChatGPT prompted with in-context examples, LIFE achieves a\nsuperiority of 7.16 points in SARI Final.\n","authors":["Xingwei He","Qianru Zhang","A-Long Jin","Jun Ma","Yuan Yuan","Siu Ming Yiu"],"pdf_url":"https://arxiv.org/pdf/2312.07049v1.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2312.07046v1","updated":"2023-12-12T07:56:57Z","published":"2023-12-12T07:56:57Z","title":"Rethinking Compression: Reduced Order Modelling of Latent Features in\n  Large Language Models","summary":"  Due to the substantial scale of Large Language Models (LLMs), the direct\napplication of conventional compression methodologies proves impractical. The\ncomputational demands associated with even minimal gradient updates present\nchallenges, particularly on consumer-grade hardware. This paper introduces an\ninnovative approach for the parametric and practical compression of LLMs based\non reduced order modelling, which entails low-rank decomposition within the\nfeature space and re-parameterization in the weight space. Notably, this\ncompression technique operates in a layer-wise manner, obviating the need for a\nGPU device and enabling the compression of billion-scale models within\nstringent constraints of both memory and time. Our method represents a\nsignificant advancement in model compression by leveraging matrix\ndecomposition, demonstrating superior efficacy compared to the prevailing\nstate-of-the-art structured pruning method.\n","authors":["Arnav Chavan","Nahush Lele","Deepak Gupta"],"pdf_url":"https://arxiv.org/pdf/2312.07046v1.pdf","comment":"Brief technical report; Code will be made available at\n  https://github.com/transmuteAI/trailmet/tree/main/trailmet/algorithms/llm-rom"},{"id":"http://arxiv.org/abs/2210.09932v2","updated":"2023-12-12T07:39:55Z","published":"2022-10-18T15:28:30Z","title":"Making Science Simple: Corpora for the Lay Summarisation of Scientific\n  Literature","summary":"  Lay summarisation aims to jointly summarise and simplify a given text, thus\nmaking its content more comprehensible to non-experts. Automatic approaches for\nlay summarisation can provide significant value in broadening access to\nscientific literature, enabling a greater degree of both interdisciplinary\nknowledge sharing and public understanding when it comes to research findings.\nHowever, current corpora for this task are limited in their size and scope,\nhindering the development of broadly applicable data-driven approaches. Aiming\nto rectify these issues, we present two novel lay summarisation datasets, PLOS\n(large-scale) and eLife (medium-scale), each of which contains biomedical\njournal articles alongside expert-written lay summaries. We provide a thorough\ncharacterisation of our lay summaries, highlighting differing levels of\nreadability and abstractiveness between datasets that can be leveraged to\nsupport the needs of different applications. Finally, we benchmark our datasets\nusing mainstream summarisation approaches and perform a manual evaluation with\ndomain experts, demonstrating their utility and casting light on the key\nchallenges of this task.\n","authors":["Tomas Goldsack","Zhihao Zhang","Chenghua Lin","Carolina Scarton"],"pdf_url":"https://arxiv.org/pdf/2210.09932v2.pdf","comment":"16 pages, 9 figures. Accepted to EMNLP 2022"},{"id":"http://arxiv.org/abs/2312.07028v1","updated":"2023-12-12T07:26:36Z","published":"2023-12-12T07:26:36Z","title":"Dynamic Corrective Self-Distillation for Better Fine-Tuning of\n  Pretrained Models","summary":"  We tackle the challenging issue of aggressive fine-tuning encountered during\nthe process of transfer learning of pre-trained language models (PLMs) with\nlimited labeled downstream data. This problem primarily results in a decline in\nperformance on the subsequent task. Inspired by the adaptive boosting method in\ntraditional machine learning, we present an effective dynamic corrective\nself-distillation (DCS) approach to improve the fine-tuning of the PLMs. Our\ntechnique involves performing a self-distillation mechanism where, at each\niteration, the student model actively adapts and corrects itself by dynamically\nadjusting the weights assigned to individual data points. This iterative\nself-correcting process significantly enhances the overall fine-tuning\ncapability of PLMs, leading to improved performance and robustness. We\nconducted comprehensive evaluations using the GLUE benchmark demonstrating the\nefficacy of our method in enhancing the fine-tuning process for various PLMs\nacross diverse downstream tasks.\n","authors":["Ibtihel Amara","Vinija Jain","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2312.07028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06363v2","updated":"2023-12-12T06:53:27Z","published":"2023-12-11T13:11:04Z","title":"MMICT: Boosting Multi-Modal Fine-Tuning with In-Context Examples","summary":"  Although In-Context Learning (ICL) brings remarkable performance gains to\nLarge Language Models (LLMs), the improvements remain lower than fine-tuning on\ndownstream tasks. This paper introduces Multi-Modal In-Context Tuning (MMICT),\na novel multi-modal fine-tuning paradigm that boosts multi-modal fine-tuning by\nfully leveraging the promising ICL capability of multi-modal LLMs (MM-LLMs). We\npropose the Multi-Modal Hub (M-Hub), a unified module that captures various\nmulti-modal features according to different inputs and objectives. Based on\nM-Hub, MMICT enables MM-LLMs to learn from in-context visual-guided textual\nfeatures and subsequently generate outputs conditioned on the textual-guided\nvisual features. Moreover, leveraging the flexibility of M-Hub, we design a\nvariety of in-context demonstrations. Extensive experiments on a diverse range\nof downstream multi-modal tasks demonstrate that MMICT significantly\noutperforms traditional fine-tuning strategy and the vanilla ICT method that\ndirectly takes the concatenation of all information from different modalities\nas input.\n","authors":["Tao Chen","Enwei Zhang","Yuting Gao","Ke Li","Xing Sun","Yan Zhang","Hui Li"],"pdf_url":"https://arxiv.org/pdf/2312.06363v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04344v2","updated":"2023-12-12T06:37:53Z","published":"2023-12-07T15:05:59Z","title":"Enhancing Medical Task Performance in GPT-4V: A Comprehensive Study on\n  Prompt Engineering Strategies","summary":"  OpenAI's latest large vision-language model (LVLM), GPT-4V(ision), has piqued\nconsiderable interest for its potential in medical applications. Despite its\npromise, recent studies and internal reviews highlight its underperformance in\nspecialized medical tasks. This paper explores the boundary of GPT-4V's\ncapabilities in medicine, particularly in processing complex imaging data from\nendoscopies, CT scans, and MRIs etc. Leveraging open-source datasets, we\nassessed its foundational competencies, identifying substantial areas for\nenhancement. Our research emphasizes prompt engineering, an often-underutilized\nstrategy for improving AI responsiveness. Through iterative testing, we refined\nthe model's prompts, significantly improving its interpretative accuracy and\nrelevance in medical imaging. From our comprehensive evaluations, we distilled\n10 effective prompt engineering techniques, each fortifying GPT-4V's medical\nacumen. These methodical enhancements facilitate more reliable, precise, and\nclinically valuable insights from GPT-4V, advancing its operability in critical\nhealthcare environments. Our findings are pivotal for those employing AI in\nmedicine, providing clear, actionable guidance on harnessing GPT-4V's full\ndiagnostic potential.\n","authors":["Pengcheng Chen","Ziyan Huang","Zhongying Deng","Tianbin Li","Yanzhou Su","Haoyu Wang","Jin Ye","Yu Qiao","Junjun He"],"pdf_url":"https://arxiv.org/pdf/2312.04344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07000v1","updated":"2023-12-12T06:10:42Z","published":"2023-12-12T06:10:42Z","title":"Alignment for Honesty","summary":"  Recent research has made significant strides in applying alignment techniques\nto enhance the helpfulness and harmlessness of large language models (LLMs) in\naccordance with human intentions. In this paper, we argue for the importance of\nalignment for honesty, ensuring that LLMs proactively refuse to answer\nquestions when they lack knowledge, while still not being overly conservative.\nHowever, a pivotal aspect of alignment for honesty involves discerning the\nlimits of an LLM's knowledge, which is far from straightforward. This challenge\ndemands comprehensive solutions in terms of metric development, benchmark\ncreation, and training methodologies. In this paper, we address these\nchallenges by first establishing a precise problem definition and defining\n``honesty'' inspired by the Analects of Confucius. This serves as a cornerstone\nfor developing metrics that effectively measure an LLM's honesty by quantifying\nits progress post-alignment. Furthermore, we introduce a flexible training\nframework which is further instantiated by several efficient fine-tuning\ntechniques that emphasize honesty without sacrificing performance on other\ntasks. Our extensive experiments reveal that these aligned models show a marked\nincrease in honesty, as indicated by our proposed metrics. We open-source a\nwealth of resources to facilitate future research at\nhttps://github.com/GAIR-NLP/alignment-for-honesty, including honesty-aligned\nmodels, training and evaluation datasets for honesty alignment, concept\nglossary, as well as all relevant source code.\n","authors":["Yuqing Yang","Ethan Chern","Xipeng Qiu","Graham Neubig","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2312.07000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06635v2","updated":"2023-12-12T06:04:14Z","published":"2023-12-11T18:51:59Z","title":"Gated Linear Attention Transformers with Hardware-Efficient Training","summary":"  Transformers with linear attention allow for efficient parallel training but\ncan simultaneously be formulated as an RNN with 2D (matrix-valued) hidden\nstates, thus enjoying linear (with respect to output length) inference\ncomplexity. Recent works such as RetNet (Sun et al., 2023) and TransNormerLLM\n(Qin et al., 2023a) observe that adding a global decay term to the additive RNN\nupdate rule greatly improves performance, sometimes outperforming standard\nTransformers with softmax attention when trained at scale. In this work we show\nthat adding a data-dependent gating mechanism further improves performance. We\nderive a parallel form of this gated linear attention layer that enables\nefficient training. However, a straightforward, numerically stable\nimplementation of this parallel form requires generalized matrix\nmultiplications in log-space for numerical stability, and thus cannot take\nadvantage of tensor cores on modern GPUs which are optimized for standard\nmatrix multiplications. We develop a hardware-efficient version of the parallel\nform that can still make use of tensor cores through block-parallel\ncomputations over sequence chunks. Experiments on moderate-scale language\nmodeling (340M-parameter models trained on 15B tokens, 1.3B-parameter models\ntrained on 100B tokens) show that gated linear attention (GLA) Transformers\nperform competitively against a strong LLaMA-architecture Transformer baseline\n(Touvron et al., 2023) as well as Mamba (Gu & Dao, 2023), a recently introduced\nstate-space model with a data-dependent state transition mechanism. For\ntraining speed, our Triton-based implementation performs comparably to\nCUDA-optimized FlashAttention-2 (Dao, 2023) under the regular 2048 training\nlength setting, while outperforming FlashAttention-2 when training on longer\nsequences beyond 4096.\n","authors":["Songlin Yang","Bailin Wang","Yikang Shen","Rameswar Panda","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2312.06635v2.pdf","comment":"fix code link"},{"id":"http://arxiv.org/abs/2312.04837v2","updated":"2023-12-12T05:48:14Z","published":"2023-12-08T05:23:50Z","title":"Localized Symbolic Knowledge Distillation for Visual Commonsense Models","summary":"  Instruction following vision-language (VL) models offer a flexible interface\nthat supports a broad range of multimodal tasks in a zero-shot fashion.\nHowever, interfaces that operate on full images do not directly enable the user\nto \"point to\" and access specific regions within images. This capability is\nimportant not only to support reference-grounded VL benchmarks, but also, for\npractical applications that require precise within-image reasoning. We build\nLocalized Visual Commonsense models, which allow users to specify (multiple)\nregions as input. We train our model by sampling localized commonsense\nknowledge from a large language model (LLM): specifically, we prompt an LLM to\ncollect commonsense knowledge given a global literal image description and a\nlocal literal region description automatically generated by a set of VL models.\nWith a separately trained critic model that selects high-quality examples, we\nfind that training on the localized commonsense corpus can successfully distill\nexisting VL models to support a reference-as-input interface. Empirical results\nand human evaluations in a zero-shot setup demonstrate that our distillation\nmethod results in more precise VL models of reasoning compared to a baseline of\npassing a generated referring expression to an LLM.\n","authors":["Jae Sung Park","Jack Hessel","Khyathi Raghavi Chandu","Paul Pu Liang","Ximing Lu","Peter West","Youngjae Yu","Qiuyuan Huang","Jianfeng Gao","Ali Farhadi","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2312.04837v2.pdf","comment":"Neurips 2023"},{"id":"http://arxiv.org/abs/2309.09357v4","updated":"2023-12-12T05:08:51Z","published":"2023-09-17T19:46:03Z","title":"Talk2Care: Facilitating Asynchronous Patient-Provider Communication with\n  Large-Language-Model","summary":"  Despite the plethora of telehealth applications to assist home-based older\nadults and healthcare providers, basic messaging and phone calls are still the\nmost common communication methods, which suffer from limited availability,\ninformation loss, and process inefficiencies. One promising solution to\nfacilitate patient-provider communication is to leverage large language models\n(LLMs) with their powerful natural conversation and summarization capability.\nHowever, there is a limited understanding of LLMs' role during the\ncommunication. We first conducted two interview studies with both older adults\n(N=10) and healthcare providers (N=9) to understand their needs and\nopportunities for LLMs in patient-provider asynchronous communication. Based on\nthe insights, we built an LLM-powered communication system, Talk2Care, and\ndesigned interactive components for both groups: (1) For older adults, we\nleveraged the convenience and accessibility of voice assistants (VAs) and built\nan LLM-powered VA interface for effective information collection. (2) For\nhealth providers, we built an LLM-based dashboard to summarize and present\nimportant health information based on older adults' conversations with the VA.\nWe further conducted two user studies with older adults and providers to\nevaluate the usability of the system. The results showed that Talk2Care could\nfacilitate the communication process, enrich the health information collected\nfrom older adults, and considerably save providers' efforts and time. We\nenvision our work as an initial exploration of LLMs' capability in the\nintersection of healthcare and interpersonal communication.\n","authors":["Ziqi Yang","Xuhai Xu","Bingsheng Yao","Shao Zhang","Ethan Rogers","Stephen Intille","Nawar Shara","Guodong Gordon Gao","Dakuo Wang"],"pdf_url":"https://arxiv.org/pdf/2309.09357v4.pdf","comment":"Under submission to IMWUT'23, 26 pages"},{"id":"http://arxiv.org/abs/2309.07870v3","updated":"2023-12-12T04:47:21Z","published":"2023-09-14T17:18:25Z","title":"Agents: An Open-source Framework for Autonomous Language Agents","summary":"  Recent advances on large language models (LLMs) enable researchers and\ndevelopers to build autonomous language agents that can automatically solve\nvarious tasks and interact with environments, humans, and other agents using\nnatural language interfaces. We consider language agents as a promising\ndirection towards artificial general intelligence and release Agents, an\nopen-source library with the goal of opening up these advances to a wider\nnon-specialist audience. Agents is carefully engineered to support important\nfeatures including planning, memory, tool usage, multi-agent communication, and\nfine-grained symbolic control. Agents is user-friendly as it enables\nnon-specialists to build, customize, test, tune, and deploy state-of-the-art\nautonomous language agents without much coding. The library is also\nresearch-friendly as its modularized design makes it easily extensible for\nresearchers. Agents is available at https://github.com/aiwaves-cn/agents.\n","authors":["Wangchunshu Zhou","Yuchen Eleanor Jiang","Long Li","Jialong Wu","Tiannan Wang","Shi Qiu","Jintian Zhang","Jing Chen","Ruipu Wu","Shuai Wang","Shiding Zhu","Jiyu Chen","Wentao Zhang","Xiangru Tang","Ningyu Zhang","Huajun Chen","Peng Cui","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2309.07870v3.pdf","comment":"Code available at https://github.com/aiwaves-cn/agents"},{"id":"http://arxiv.org/abs/2312.06974v1","updated":"2023-12-12T04:25:26Z","published":"2023-12-12T04:25:26Z","title":"SM70: A Large Language Model for Medical Devices","summary":"  We are introducing SM70, a 70 billion-parameter Large Language Model that is\nspecifically designed for SpassMed's medical devices under the brand name\n'JEE1' (pronounced as G1 and means 'Life'). This large language model provides\nmore accurate and safe responses to medical-domain questions. To fine-tune\nSM70, we used around 800K data entries from the publicly available dataset\nMedAlpaca. The Llama2 70B open-sourced model served as the foundation for SM70,\nand we employed the QLoRA technique for fine-tuning. The evaluation is\nconducted across three benchmark datasets - MEDQA - USMLE, PUBMEDQA, and USMLE\n- each representing a unique aspect of medical knowledge and reasoning. The\nperformance of SM70 is contrasted with other notable LLMs, including Llama2\n70B, Clinical Camel 70 (CC70), GPT 3.5, GPT 4, and Med-Palm, to provide a\ncomparative understanding of its capabilities within the medical domain. Our\nresults indicate that SM70 outperforms several established models in these\ndatasets, showcasing its proficiency in handling a range of medical queries,\nfrom fact-based questions derived from PubMed abstracts to complex clinical\ndecision-making scenarios. The robust performance of SM70, particularly in the\nUSMLE and PUBMEDQA datasets, suggests its potential as an effective tool in\nclinical decision support and medical information retrieval. Despite its\npromising results, the paper also acknowledges the areas where SM70 lags behind\nthe most advanced model, GPT 4, thereby highlighting the need for further\ndevelopment, especially in tasks demanding extensive medical knowledge and\nintricate reasoning.\n","authors":["Anubhav Bhatti","Surajsinh Parmar","San Lee"],"pdf_url":"https://arxiv.org/pdf/2312.06974v1.pdf","comment":"5 Pages, Technical Report"},{"id":"http://arxiv.org/abs/2311.06062v2","updated":"2023-12-12T03:44:04Z","published":"2023-11-10T13:55:05Z","title":"Practical Membership Inference Attacks against Fine-tuned Large Language\n  Models via Self-prompt Calibration","summary":"  Membership Inference Attacks (MIA) aim to infer whether a target data record\nhas been utilized for model training or not. Prior attempts have quantified the\nprivacy risks of language models (LMs) via MIAs, but there is still no\nconsensus on whether existing MIA algorithms can cause remarkable privacy\nleakage on practical Large Language Models (LLMs). Existing MIAs designed for\nLMs can be classified into two categories: reference-free and reference-based\nattacks. They are both based on the hypothesis that training records\nconsistently strike a higher probability of being sampled. Nevertheless, this\nhypothesis heavily relies on the overfitting of target models, which will be\nmitigated by multiple regularization methods and the generalization of LLMs.\nThe reference-based attack seems to achieve promising effectiveness in LLMs,\nwhich measures a more reliable membership signal by comparing the probability\ndiscrepancy between the target model and the reference model. However, the\nperformance of reference-based attack is highly dependent on a reference\ndataset that closely resembles the training dataset, which is usually\ninaccessible in the practical scenario. Overall, existing MIAs are unable to\neffectively unveil privacy leakage over practical fine-tuned LLMs that are\noverfitting-free and private. We propose a Membership Inference Attack based on\nSelf-calibrated Probabilistic Variation (SPV-MIA). Specifically, since\nmemorization in LLMs is inevitable during the training process and occurs\nbefore overfitting, we introduce a more reliable membership signal,\nprobabilistic variation, which is based on memorization rather than\noverfitting. Furthermore, we introduce a self-prompt approach, which constructs\nthe dataset to fine-tune the reference model by prompting the target LLM\nitself. In this manner, the adversary can collect a dataset with a similar\ndistribution from public APIs.\n","authors":["Wenjie Fu","Huandong Wang","Chen Gao","Guanghua Liu","Yong Li","Tao Jiang"],"pdf_url":"https://arxiv.org/pdf/2311.06062v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06648v2","updated":"2023-12-12T03:37:59Z","published":"2023-12-11T18:57:35Z","title":"Dense X Retrieval: What Retrieval Granularity Should We Use?","summary":"  Dense retrieval has become a prominent method to obtain relevant context or\nworld knowledge in open-domain NLP tasks. When we use a learned dense retriever\non a retrieval corpus at inference time, an often-overlooked design choice is\nthe retrieval unit in which the corpus is indexed, e.g. document, passage, or\nsentence. We discover that the retrieval unit choice significantly impacts the\nperformance of both retrieval and downstream tasks. Distinct from the typical\napproach of using passages or sentences, we introduce a novel retrieval unit,\nproposition, for dense retrieval. Propositions are defined as atomic\nexpressions within text, each encapsulating a distinct factoid and presented in\na concise, self-contained natural language format. We conduct an empirical\ncomparison of different retrieval granularity. Our results reveal that\nproposition-based retrieval significantly outperforms traditional passage or\nsentence-based methods in dense retrieval. Moreover, retrieval by proposition\nalso enhances the performance of downstream QA tasks, since the retrieved texts\nare more condensed with question-relevant information, reducing the need for\nlengthy input tokens and minimizing the inclusion of extraneous, irrelevant\ninformation.\n","authors":["Tong Chen","Hongwei Wang","Sihao Chen","Wenhao Yu","Kaixin Ma","Xinran Zhao","Hongming Zhang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2312.06648v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05497v2","updated":"2023-12-12T03:22:32Z","published":"2023-12-09T07:51:56Z","title":"History Matters: Temporal Knowledge Editing in Large Language Model","summary":"  The imperative task of revising or updating the knowledge stored within large\nlanguage models arises from two distinct sources: intrinsic errors inherent in\nthe model which should be corrected and outdated knowledge due to external\nshifts in the real world which should be updated. Prevailing efforts in model\nediting conflate these two distinct categories of edits arising from distinct\nreasons and directly modify the original knowledge in models into new\nknowledge. However, we argue that preserving the model's original knowledge\nremains pertinent. Specifically, if a model's knowledge becomes outdated due to\nevolving worldly dynamics, it should retain recollection of the historical\nknowledge while integrating the newfound knowledge. In this work, we introduce\nthe task of Temporal Knowledge Editing (TKE) and establish a benchmark AToKe\n(Assessment of TempOral Knowledge Editing) to evaluate current model editing\nmethods. We find that while existing model editing methods are effective at\nmaking models remember new knowledge, the edited model catastrophically forgets\nhistorical knowledge. To address this gap, we propose a simple and general\nframework termed Multi-Editing with Time Objective (METO) for enhancing\nexisting editing models, which edits both historical and new knowledge\nconcurrently and optimizes the model's prediction for the time of each fact.\nOur assessments demonstrate that while AToKe is still difficult, METO maintains\nthe effectiveness of learning new knowledge and meanwhile substantially\nimproves the performance of edited models on utilizing historical knowledge.\n","authors":["Xunjian Yin","Jin Jiang","Liming Yang","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2312.05497v2.pdf","comment":"AAAI 2024. 14 pages, 6 figures"},{"id":"http://arxiv.org/abs/2312.04691v2","updated":"2023-12-12T03:17:29Z","published":"2023-12-07T20:42:05Z","title":"Simul-LLM: A Framework for Exploring High-Quality Simultaneous\n  Translation with Large Language Models","summary":"  Large language models (LLMs) with billions of parameters and pretrained on\nmassive amounts of data are now capable of near or better than state-of-the-art\nperformance in a variety of downstream natural language processing tasks.\nNeural machine translation (NMT) is one such task that LLMs have been applied\nto with great success. However, little research has focused on applying LLMs to\nthe more difficult subset of NMT called simultaneous translation (SimulMT),\nwhere translation begins before the entire source context is available to the\nmodel. In this paper, we address key challenges facing LLMs fine-tuned for\nSimulMT, validate classical SimulMT concepts and practices in the context of\nLLMs, explore adapting LLMs that are fine-tuned for NMT to the task of SimulMT,\nand introduce Simul-LLM, the first open-source fine-tuning and evaluation\npipeline development framework for LLMs focused on SimulMT.\n","authors":["Victor Agostinelli","Max Wild","Matthew Raffel","Kazi Ahmed Asif Fuad","Lizhong Chen"],"pdf_url":"https://arxiv.org/pdf/2312.04691v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06950v1","updated":"2023-12-12T03:09:30Z","published":"2023-12-12T03:09:30Z","title":"READ-PVLA: Recurrent Adapter with Partial Video-Language Alignment for\n  Parameter-Efficient Transfer Learning in Low-Resource Video-Language Modeling","summary":"  Fully fine-tuning pretrained large-scale transformer models has become a\npopular paradigm for video-language modeling tasks, such as temporal language\ngrounding and video-language summarization. With a growing number of tasks and\nlimited training data, such full fine-tuning approach leads to costly model\nstorage and unstable training. To overcome these shortcomings, we introduce\nlightweight adapters to the pre-trained model and only update them at\nfine-tuning time. However, existing adapters fail to capture intrinsic temporal\nrelations among video frames or textual words. Moreover, they neglect the\npreservation of critical task-related information that flows from the raw\nvideo-language input into the adapter's low-dimensional space. To address these\nissues, we first propose a novel REcurrent ADapter (READ) that employs\nrecurrent computation to enable temporal modeling capability. Second, we\npropose Partial Video-Language Alignment (PVLA) objective via the use of\npartial optimal transport to maintain task-related information flowing into our\nREAD modules. We validate our READ-PVLA framework through extensive experiments\nwhere READ-PVLA significantly outperforms all existing fine-tuning strategies\non multiple low-resource temporal language grounding and video-language\nsummarization benchmarks.\n","authors":["Thong Nguyen","Xiaobao Wu","Xinshuai Dong","Khoi Le","Zhiyuan Hu","Cong-Duy Nguyen","See-Kiong Ng","Luu Anh Tuan"],"pdf_url":"https://arxiv.org/pdf/2312.06950v1.pdf","comment":"Accepted at AAAI 2024"},{"id":"http://arxiv.org/abs/2312.06926v1","updated":"2023-12-12T01:42:41Z","published":"2023-12-12T01:42:41Z","title":"Content-Localization based Neural Machine Translation for Informal\n  Dialectal Arabic: Spanish/French to Levantine/Gulf Arabic","summary":"  Resources in high-resource languages have not been efficiently exploited in\nlow-resource languages to solve language-dependent research problems. Spanish\nand French are considered high resource languages in which an adequate level of\ndata resources for informal online social behavior modeling, is observed.\nHowever, a machine translation system to access those data resources and\ntransfer their context and tone to a low-resource language like dialectal\nArabic, does not exist. In response, we propose a framework that localizes\ncontents of high-resource languages to a low-resource language/dialects by\nutilizing AI power. To the best of our knowledge, we are the first work to\nprovide a parallel translation dataset from/to informal Spanish and French\nto/from informal Arabic dialects. Using this, we aim to enrich the\nunder-resource-status dialectal Arabic and fast-track the research of diverse\nonline social behaviors within and across smart cities in different\ngeo-regions. The experimental results have illustrated the capability of our\nproposed solution in exploiting the resources between high and low resource\nlanguages and dialects. Not only this, but it has also been proven that\nignoring dialects within the same language could lead to misleading analysis of\nonline social behavior.\n","authors":["Fatimah Alzamzami","Abdulmotaleb El Saddik"],"pdf_url":"https://arxiv.org/pdf/2312.06926v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2312.03727"},{"id":"http://arxiv.org/abs/2312.06924v1","updated":"2023-12-12T01:39:29Z","published":"2023-12-12T01:39:29Z","title":"Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an\n  In-Context Attack","summary":"  Recent developments in balancing the usefulness and safety of Large Language\nModels (LLMs) have raised a critical question: Are mainstream NLP tasks\nadequately aligned with safety consideration? Our study, focusing on\nsafety-sensitive documents obtained through adversarial attacks, reveals\nsignificant disparities in the safety alignment of various NLP tasks. For\ninstance, LLMs can effectively summarize malicious long documents but often\nrefuse to translate them. This discrepancy highlights a previously unidentified\nvulnerability: attacks exploiting tasks with weaker safety alignment, like\nsummarization, can potentially compromise the integraty of tasks traditionally\ndeemed more robust, such as translation and question-answering (QA). Moreover,\nthe concurrent use of multiple NLP tasks with lesser safety alignment increases\nthe risk of LLMs inadvertently processing harmful content. We demonstrate these\nvulnerabilities in various safety-aligned LLMs, particularly Llama2 models and\nGPT-4, indicating an urgent need for strengthening safety alignments across a\nbroad spectrum of NLP tasks.\n","authors":["Yu Fu","Yufei Li","Wen Xiao","Cong Liu","Yue Dong"],"pdf_url":"https://arxiv.org/pdf/2312.06924v1.pdf","comment":"17 pages,10 figures"},{"id":"http://arxiv.org/abs/2309.17453v3","updated":"2023-12-12T00:33:07Z","published":"2023-09-29T17:59:56Z","title":"Efficient Streaming Language Models with Attention Sinks","summary":"  Deploying Large Language Models (LLMs) in streaming applications such as\nmulti-round dialogue, where long interactions are expected, is urgently needed\nbut poses two major challenges. Firstly, during the decoding stage, caching\nprevious tokens' Key and Value states (KV) consumes extensive memory. Secondly,\npopular LLMs cannot generalize to longer texts than the training sequence\nlength. Window attention, where only the most recent KVs are cached, is a\nnatural approach -- but we show that it fails when the text length surpasses\nthe cache size. We observe an interesting phenomenon, namely attention sink,\nthat keeping the KV of initial tokens will largely recover the performance of\nwindow attention. In this paper, we first demonstrate that the emergence of\nattention sink is due to the strong attention scores towards initial tokens as\na ``sink'' even if they are not semantically important. Based on the above\nanalysis, we introduce StreamingLLM, an efficient framework that enables LLMs\ntrained with a finite length attention window to generalize to infinite\nsequence lengths without any fine-tuning. We show that StreamingLLM can enable\nLlama-2, MPT, Falcon, and Pythia to perform stable and efficient language\nmodeling with up to 4 million tokens and more. In addition, we discover that\nadding a placeholder token as a dedicated attention sink during pre-training\ncan further improve streaming deployment. In streaming settings, StreamingLLM\noutperforms the sliding window recomputation baseline by up to 22.2x speedup.\nCode and datasets are provided at https://github.com/mit-han-lab/streaming-llm.\n","authors":["Guangxuan Xiao","Yuandong Tian","Beidi Chen","Song Han","Mike Lewis"],"pdf_url":"https://arxiv.org/pdf/2309.17453v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07797v1","updated":"2023-12-12T23:23:23Z","published":"2023-12-12T23:23:23Z","title":"Sentiment analysis in Tourism: Fine-tuning BERT or sentence embeddings\n  concatenation?","summary":"  Undoubtedly that the Bidirectional Encoder representations from Transformers\nis the most powerful technique in making Natural Language Processing tasks such\nas Named Entity Recognition, Question & Answers or Sentiment Analysis, however,\nthe use of traditional techniques remains a major potential for the improvement\nof recent models, in particular word tokenization techniques and embeddings,\nbut also the improvement of neural network architectures which are now the core\nof each architecture. recent. In this paper, we conduct a comparative study\nbetween Fine-Tuning the Bidirectional Encoder Representations from Transformers\nand a method of concatenating two embeddings to boost the performance of a\nstacked Bidirectional Long Short-Term Memory-Bidirectional Gated Recurrent\nUnits model; these two approaches are applied in the context of sentiment\nanalysis of shopping places in Morocco. A search for the best learning rate was\nmade at the level of the two approaches, and a comparison of the best\noptimizers was made for each sentence embedding combination with regard to the\nsecond approach.\n","authors":["Ibrahim Bouabdallaoui","Fatima Guerouate","Samya Bouhaddour","Chaimae Saadi","Mohammed Sbihi"],"pdf_url":"https://arxiv.org/pdf/2312.07797v1.pdf","comment":"Accepted manuscript at ICMECE 2022 Conference (Barcelona, Spain)"},{"id":"http://arxiv.org/abs/2312.07796v1","updated":"2023-12-12T23:22:57Z","published":"2023-12-12T23:22:57Z","title":"Harnessing Retrieval-Augmented Generation (RAG) for Uncovering Knowledge\n  Gaps","summary":"  The paper presents a methodology for uncovering knowledge gaps on the\ninternet using the Retrieval Augmented Generation (RAG) model. By simulating\nuser search behaviour, the RAG system identifies and addresses gaps in\ninformation retrieval systems. The study demonstrates the effectiveness of the\nRAG system in generating relevant suggestions with a consistent accuracy of\n93%. The methodology can be applied in various fields such as scientific\ndiscovery, educational enhancement, research development, market analysis,\nsearch engine optimisation, and content development. The results highlight the\nvalue of identifying and understanding knowledge gaps to guide future\nendeavours.\n","authors":["Joan Figuerola Hurtado"],"pdf_url":"https://arxiv.org/pdf/2312.07796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07763v1","updated":"2023-12-12T22:11:17Z","published":"2023-12-12T22:11:17Z","title":"Can LLM find the green circle? Investigation and Human-guided tool\n  manipulation for compositional generalization","summary":"  The meaning of complex phrases in natural language is composed of their\nindividual components. The task of compositional generalization evaluates a\nmodel's ability to understand new combinations of components. Previous studies\ntrained smaller, task-specific models, which exhibited poor generalization.\nWhile large language models (LLMs) exhibit impressive generalization abilities\non many tasks through in-context learning (ICL), their potential for\ncompositional generalization remains unexplored. In this paper, we first\nempirically investigate prevailing ICL methods in compositional generalization.\nWe find that they struggle with complex compositional questions due to\ncumulative errors in long reasoning steps and intricate logic required for\ntool-making. Consequently, we propose a human-guided tool manipulation\nframework (HTM) that generates tools for sub-questions and integrates multiple\ntools. Our method enhances the effectiveness of tool creation and usage with\nminimal human effort. Experiments show that our method achieves\nstate-of-the-art performance on two compositional generalization benchmarks and\noutperforms existing methods on the most challenging test split by 70%.\n","authors":["Min Zhang","Jianfeng He","Shuo Lei","Murong Yue","Linhang Wang","Chang-Tien Lu"],"pdf_url":"https://arxiv.org/pdf/2312.07763v1.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.07743v1","updated":"2023-12-12T21:22:07Z","published":"2023-12-12T21:22:07Z","title":"FULL-W2V: Fully Exploiting Data Reuse for W2V on GPU-Accelerated Systems","summary":"  Word2Vec remains one of the highly-impactful innovations in the field of\nNatural Language Processing (NLP) that represents latent grammatical and\nsyntactical information in human text with dense vectors in a low dimension.\nWord2Vec has high computational cost due to the algorithm's inherent\nsequentiality, intensive memory accesses, and the large vocabularies it\nrepresents. While prior studies have investigated technologies to explore\nparallelism and improve memory system performance, they struggle to effectively\ngain throughput on powerful GPUs.\n  We identify memory data access and latency as the primary bottleneck in prior\nworks on GPUs, which prevents highly optimized kernels from attaining the\narchitecture's peak performance. We present a novel algorithm, FULL-W2V, which\nmaximally exploits the opportunities for data reuse in the W2V algorithm and\nleverages GPU architecture and resources to reduce access to low memory levels\nand improve temporal locality. FULL-W2V is capable of reducing accesses to GPU\nglobal memory significantly, e.g., by more than 89\\%, compared to prior\nstate-of-the-art GPU implementations, resulting in significant performance\nimprovement that scales across successive hardware generations. Our prototype\nimplementation achieves 2.97X speedup when ported from Nvidia Pascal P100 to\nVolta V100 cards, and outperforms the state-of-the-art by 5.72X on V100 cards\nwith the same embedding quality. In-depth analysis indicates that the reduction\nof memory accesses through register and shared memory caching and\nhigh-throughput shared memory reduction leads to a significantly improved\narithmetic intensity. FULL-W2V can potentially benefit many applications in NLP\nand other domains.\n","authors":["Thomas Randall","Tyler Allen","Rong Ge"],"pdf_url":"https://arxiv.org/pdf/2312.07743v1.pdf","comment":"12 pages, 7 figures, 7 tables, the definitive version of this work is\n  published in the Proceedings of the ACM International Conference on\n  Supercomputing 2021, available at https://doi.org/10.1145/3447818.3460373"},{"id":"http://arxiv.org/abs/2310.08320v2","updated":"2023-12-12T19:54:02Z","published":"2023-10-12T13:33:04Z","title":"Defending Our Privacy With Backdoors","summary":"  The proliferation of large AI models trained on uncurated, often sensitive\nweb-scraped data has raised significant privacy concerns. One of the concerns\nis that adversaries can extract information about the training data using\nprivacy attacks. Unfortunately, the task of removing specific information from\nthe models without sacrificing performance is not straightforward and has\nproven to be challenging. We propose a rather easy yet effective defense based\non backdoor attacks to remove private information such as names of individuals\nfrom models, and focus in this work on text encoders. Specifically, through\nstrategic insertion of backdoors, we align the embeddings of sensitive phrases\nwith those of neutral terms-\"a person\" instead of the person's name. Our\nempirical results demonstrate the effectiveness of our backdoor-based defense\non CLIP by assessing its performance using a specialized privacy attack for\nzero-shot classifiers. Our approach provides not only a new \"dual-use\"\nperspective on backdoor attacks, but also presents a promising avenue to\nenhance the privacy of individuals within models trained on uncurated\nweb-scraped data.\n","authors":["Dominik Hintersdorf","Lukas Struppek","Daniel Neider","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2310.08320v2.pdf","comment":"14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2312.07661v1","updated":"2023-12-12T19:00:04Z","published":"2023-12-12T19:00:04Z","title":"CLIP as RNN: Segment Countless Visual Concepts without Training Endeavor","summary":"  Existing open-vocabulary image segmentation methods require a fine-tuning\nstep on mask annotations and/or image-text datasets. Mask labels are\nlabor-intensive, which limits the number of categories in segmentation\ndatasets. As a result, the open-vocabulary capacity of pre-trained VLMs is\nseverely reduced after fine-tuning. However, without fine-tuning, VLMs trained\nunder weak image-text supervision tend to make suboptimal mask predictions when\nthere are text queries referring to non-existing concepts in the image. To\nalleviate these issues, we introduce a novel recurrent framework that\nprogressively filters out irrelevant texts and enhances mask quality without\ntraining efforts. The recurrent unit is a two-stage segmenter built upon a VLM\nwith frozen weights. Thus, our model retains the VLM's broad vocabulary space\nand strengthens its segmentation capability. Experimental results show that our\nmethod outperforms not only the training-free counterparts, but also those\nfine-tuned with millions of additional data samples, and sets new\nstate-of-the-art records for both zero-shot semantic and referring image\nsegmentation tasks. Specifically, we improve the current record by 28.8, 16.0,\nand 6.9 mIoU on Pascal VOC, COCO Object, and Pascal Context.\n","authors":["Shuyang Sun","Runjia Li","Philip Torr","Xiuye Gu","Siyang Li"],"pdf_url":"https://arxiv.org/pdf/2312.07661v1.pdf","comment":"Project page: https://torrvision.com/clip_as_rnn/"},{"id":"http://arxiv.org/abs/2312.07622v1","updated":"2023-12-12T01:39:16Z","published":"2023-12-12T01:39:16Z","title":"Mathematical Language Models: A Survey","summary":"  In recent years, there has been remarkable progress in leveraging Language\nModels (LMs), encompassing Pre-trained Language Models (PLMs) and Large-scale\nLanguage Models (LLMs), within the domain of mathematics. This paper conducts a\ncomprehensive survey of mathematical LMs, systematically categorizing pivotal\nresearch endeavors from two distinct perspectives: tasks and methodologies. The\nlandscape reveals a large number of proposed mathematical LLMs, which are\nfurther delineated into instruction learning, tool-based methods, fundamental\nCoT techniques, and advanced CoT methodologies. In addition, our survey entails\nthe compilation of over 60 mathematical datasets, including training datasets,\nbenchmark datasets, and augmented datasets. Addressing the primary challenges\nand delineating future trajectories within the field of mathematical LMs, this\nsurvey is positioned as a valuable resource, poised to facilitate and inspire\nfuture innovation among researchers invested in advancing this domain.\n","authors":["Wentao Liu","Hanglei Hu","Jie Zhou","Yuyang Ding","Junsong Li","Jiayi Zeng","Mengliang He","Qin Chen","Bo Jiang","Aimin Zhou","Liang He"],"pdf_url":"https://arxiv.org/pdf/2312.07622v1.pdf","comment":"arXiv admin note: text overlap with arXiv:1705.04146,\n  arXiv:2304.10977, arXiv:2112.00114, arXiv:1905.13319, arXiv:2304.12244,\n  arXiv:2206.01347, arXiv:2006.09265 by other authors"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2312.07541v1","updated":"2023-12-12T18:59:40Z","published":"2023-12-12T18:59:40Z","title":"SMERF: Streamable Memory Efficient Radiance Fields for Real-Time\n  Large-Scene Exploration","summary":"  Recent techniques for real-time view synthesis have rapidly advanced in\nfidelity and speed, and modern methods are capable of rendering\nnear-photorealistic scenes at interactive frame rates. At the same time, a\ntension has arisen between explicit scene representations amenable to\nrasterization and neural fields built on ray marching, with state-of-the-art\ninstances of the latter surpassing the former in quality while being\nprohibitively expensive for real-time applications. In this work, we introduce\nSMERF, a view synthesis approach that achieves state-of-the-art accuracy among\nreal-time methods on large scenes with footprints up to 300 m$^2$ at a\nvolumetric resolution of 3.5 mm$^3$. Our method is built upon two primary\ncontributions: a hierarchical model partitioning scheme, which increases model\ncapacity while constraining compute and memory consumption, and a distillation\ntraining strategy that simultaneously yields high fidelity and internal\nconsistency. Our approach enables full six degrees of freedom (6DOF) navigation\nwithin a web browser and renders in real-time on commodity smartphones and\nlaptops. Extensive experiments show that our method exceeds the current\nstate-of-the-art in real-time novel view synthesis by 0.78 dB on standard\nbenchmarks and 1.78 dB on large scenes, renders frames three orders of\nmagnitude faster than state-of-the-art radiance field models, and achieves\nreal-time performance across a wide variety of commodity devices, including\nsmartphones. We encourage the reader to explore these models in person at our\nproject website: https://smerf-3d.github.io.\n","authors":["Daniel Duckworth","Peter Hedman","Christian Reiser","Peter Zhizhin","Jean-François Thibert","Mario Lučić","Richard Szeliski","Jonathan T. Barron"],"pdf_url":"https://arxiv.org/pdf/2312.07541v1.pdf","comment":"Project website: https://smerf-3d.github.io"},{"id":"http://arxiv.org/abs/2312.07539v1","updated":"2023-12-12T18:59:25Z","published":"2023-12-12T18:59:25Z","title":"HeadArtist: Text-conditioned 3D Head Generation with Self Score\n  Distillation","summary":"  This work presents HeadArtist for 3D head generation from text descriptions.\nWith a landmark-guided ControlNet serving as the generative prior, we come up\nwith an efficient pipeline that optimizes a parameterized 3D head model under\nthe supervision of the prior distillation itself. We call such a process self\nscore distillation (SSD). In detail, given a sampled camera pose, we first\nrender an image and its corresponding landmarks from the head model, and add\nsome particular level of noise onto the image. The noisy image, landmarks, and\ntext condition are then fed into the frozen ControlNet twice for noise\nprediction. Two different classifier-free guidance (CFG) weights are applied\nduring these two predictions, and the prediction difference offers a direction\non how the rendered image can better match the text of interest. Experimental\nresults suggest that our approach delivers high-quality 3D head sculptures with\nadequate geometry and photorealistic appearance, significantly outperforming\nstate-ofthe-art methods. We also show that the same pipeline well supports\nediting the generated heads, including both geometry deformation and appearance\nchange.\n","authors":["Hongyu Liu","Xuan Wang","Ziyu Wan","Yujun Shen","Yibing Song","Jing Liao","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2312.07539v1.pdf","comment":"Amazing results are shown in\n  https://kumapowerliu.github.io/HeadArtist"},{"id":"http://arxiv.org/abs/2312.07538v1","updated":"2023-12-12T18:59:21Z","published":"2023-12-12T18:59:21Z","title":"Anatomically Constrained Implicit Face Models","summary":"  Coordinate based implicit neural representations have gained rapid popularity\nin recent years as they have been successfully used in image, geometry and\nscene modeling tasks. In this work, we present a novel use case for such\nimplicit representations in the context of learning anatomically constrained\nface models. Actor specific anatomically constrained face models are the state\nof the art in both facial performance capture and performance retargeting.\nDespite their practical success, these anatomical models are slow to evaluate\nand often require extensive data capture to be built. We propose the anatomical\nimplicit face model; an ensemble of implicit neural networks that jointly learn\nto model the facial anatomy and the skin surface with high-fidelity, and can\nreadily be used as a drop in replacement to conventional blendshape models.\nGiven an arbitrary set of skin surface meshes of an actor and only a neutral\nshape with estimated skull and jaw bones, our method can recover a dense\nanatomical substructure which constrains every point on the facial surface. We\ndemonstrate the usefulness of our approach in several tasks ranging from shape\nfitting, shape editing, and performance retargeting.\n","authors":["Prashanth Chandran","Gaspard Zoss"],"pdf_url":"https://arxiv.org/pdf/2312.07538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07537v1","updated":"2023-12-12T18:59:16Z","published":"2023-12-12T18:59:16Z","title":"FreeInit: Bridging Initialization Gap in Video Diffusion Models","summary":"  Though diffusion-based video generation has witnessed rapid progress, the\ninference results of existing models still exhibit unsatisfactory temporal\nconsistency and unnatural dynamics. In this paper, we delve deep into the noise\ninitialization of video diffusion models, and discover an implicit\ntraining-inference gap that attributes to the unsatisfactory inference quality.\nOur key findings are: 1) the spatial-temporal frequency distribution of the\ninitial latent at inference is intrinsically different from that for training,\nand 2) the denoising process is significantly influenced by the low-frequency\ncomponents of the initial noise. Motivated by these observations, we propose a\nconcise yet effective inference sampling strategy, FreeInit, which\nsignificantly improves temporal consistency of videos generated by diffusion\nmodels. Through iteratively refining the spatial-temporal low-frequency\ncomponents of the initial latent during inference, FreeInit is able to\ncompensate the initialization gap between training and inference, thus\neffectively improving the subject appearance and temporal consistency of\ngeneration results. Extensive experiments demonstrate that FreeInit\nconsistently enhances the generation results of various text-to-video\ngeneration models without additional training.\n","authors":["Tianxing Wu","Chenyang Si","Yuming Jiang","Ziqi Huang","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2312.07537v1.pdf","comment":"Project page: https://tianxingwu.github.io/pages/FreeInit/ Code:\n  https://github.com/TianxingWu/FreeInit"},{"id":"http://arxiv.org/abs/2312.07536v1","updated":"2023-12-12T18:59:14Z","published":"2023-12-12T18:59:14Z","title":"FreeControl: Training-Free Spatial Control of Any Text-to-Image\n  Diffusion Model with Any Condition","summary":"  Recent approaches such as ControlNet offer users fine-grained spatial control\nover text-to-image (T2I) diffusion models. However, auxiliary modules have to\nbe trained for each type of spatial condition, model architecture, and\ncheckpoint, putting them at odds with the diverse intents and preferences a\nhuman designer would like to convey to the AI models during the content\ncreation process. In this work, we present FreeControl, a training-free\napproach for controllable T2I generation that supports multiple conditions,\narchitectures, and checkpoints simultaneously. FreeControl designs structure\nguidance to facilitate the structure alignment with a guidance image, and\nappearance guidance to enable the appearance sharing between images generated\nusing the same seed. Extensive qualitative and quantitative experiments\ndemonstrate the superior performance of FreeControl across a variety of\npre-trained T2I models. In particular, FreeControl facilitates convenient\ntraining-free control over many different architectures and checkpoints, allows\nthe challenging input conditions on which most of the existing training-free\nmethods fail, and achieves competitive synthesis quality with training-based\napproaches.\n","authors":["Sicheng Mo","Fangzhou Mu","Kuan Heng Lin","Yanli Liu","Bochen Guan","Yin Li","Bolei Zhou"],"pdf_url":"https://arxiv.org/pdf/2312.07536v1.pdf","comment":"Project Page: https://genforce.github.io/freecontrol/"},{"id":"http://arxiv.org/abs/2308.16876v2","updated":"2023-12-12T18:59:06Z","published":"2023-08-31T17:23:50Z","title":"SportsSloMo: A New Benchmark and Baselines for Human-centric Video Frame\n  Interpolation","summary":"  Human-centric video frame interpolation has great potential for improving\npeople's entertainment experiences and finding commercial applications in the\nsports analysis industry, e.g., synthesizing slow-motion videos. Although there\nare multiple benchmark datasets available in the community, none of them is\ndedicated for human-centric scenarios. To bridge this gap, we introduce\nSportsSloMo, a benchmark consisting of more than 130K video clips and 1M video\nframes of high-resolution ($\\geq$720p) slow-motion sports videos crawled from\nYouTube. We re-train several state-of-the-art methods on our benchmark, and the\nresults show a decrease in their accuracy compared to other datasets. It\nhighlights the difficulty of our benchmark and suggests that it poses\nsignificant challenges even for the best-performing methods, as human bodies\nare highly deformable and occlusions are frequent in sports videos. To improve\nthe accuracy, we introduce two loss terms considering the human-aware priors,\nwhere we add auxiliary supervision to panoptic segmentation and human keypoints\ndetection, respectively. The loss terms are model agnostic and can be easily\nplugged into any video frame interpolation approaches. Experimental results\nvalidate the effectiveness of our proposed loss terms, leading to consistent\nperformance improvement over 5 existing models, which establish strong baseline\nmodels on our benchmark. The dataset and code can be found at:\nhttps://neu-vi.github.io/SportsSlomo/.\n","authors":["Jiaben Chen","Huaizu Jiang"],"pdf_url":"https://arxiv.org/pdf/2308.16876v2.pdf","comment":"Project Page: https://neu-vi.github.io/SportsSlomo/"},{"id":"http://arxiv.org/abs/2312.07533v1","updated":"2023-12-12T18:58:18Z","published":"2023-12-12T18:58:18Z","title":"VILA: On Pre-training for Visual Language Models","summary":"  Visual language models (VLMs) rapidly progressed with the recent success of\nlarge language models. There have been growing efforts on visual instruction\ntuning to extend the LLM with visual inputs, but lacks an in-depth study of the\nvisual language pre-training process, where the model learns to perform joint\nmodeling on both modalities. In this work, we examine the design options for\nVLM pre-training by augmenting LLM towards VLM through step-by-step\ncontrollable comparisons. We introduce three main findings: (1) freezing LLMs\nduring pre-training can achieve decent zero-shot performance, but lack\nin-context learning capability, which requires unfreezing the LLM; (2)\ninterleaved pre-training data is beneficial whereas image-text pairs alone are\nnot optimal; (3) re-blending text-only instruction data to image-text data\nduring instruction fine-tuning not only remedies the degradation of text-only\ntasks, but also boosts VLM task accuracy. With an enhanced pre-training recipe\nwe build VILA, a Visual Language model family that consistently outperforms the\nstate-of-the-art models, e.g., LLaVA-1.5, across main benchmarks without bells\nand whistles. Multi-modal pre-training also helps unveil appealing properties\nof VILA, including multi-image reasoning, enhanced in-context learning, and\nbetter world knowledge.\n","authors":["Ji Lin","Hongxu Yin","Wei Ping","Yao Lu","Pavlo Molchanov","Andrew Tao","Huizi Mao","Jan Kautz","Mohammad Shoeybi","Song Han"],"pdf_url":"https://arxiv.org/pdf/2312.07533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07532v1","updated":"2023-12-12T18:58:02Z","published":"2023-12-12T18:58:02Z","title":"Interfacing Foundation Models' Embeddings","summary":"  We present FIND, a generalized interface for aligning foundation models'\nembeddings. As shown in teaser figure, a lightweight transformer interface\nwithout tuning any foundation model weights is enough for a unified image\n(segmentation) and dataset-level (retrieval) understanding. The proposed\ninterface has the following favorable attributes: (1) Generalizable. It applies\nto various tasks spanning retrieval, segmentation, \\textit{etc.}, under the\nsame architecture and weights. (2) Prototypable. Different tasks are able to be\nimplemented through prototyping attention masks and embedding types. (3)\nExtendable. The proposed interface is adaptive to new tasks, and new models.\n(4) Interleavable. With the benefit of multi-task multi-modal training, the\nproposed interface creates an interleaved shared embedding space. In light of\nthe interleaved embedding space, we introduce the FIND-Bench, which introduces\nnew training and evaluation annotations to the COCO dataset for interleave\nsegmentation and retrieval. Our approach achieves state-of-the-art performance\non FIND-Bench and competitive performance on standard retrieval and\nsegmentation settings. The training, evaluation, and demo code as well as the\ndataset have been released at https://github.com/UX-Decoder/FIND.\n","authors":["Xueyan Zou","Linjie Li","Jianfeng Wang","Jianwei Yang","Mingyu Ding","Zhengyuan Yang","Feng Li","Hao Zhang","Shilong Liu","Arul Aravinthan","Yong Jae Lee","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2312.07532v1.pdf","comment":"CODE: https://github.com/UX-Decoder/FIND"},{"id":"http://arxiv.org/abs/2312.07531v1","updated":"2023-12-12T18:57:46Z","published":"2023-12-12T18:57:46Z","title":"WHAM: Reconstructing World-grounded Humans with Accurate 3D Motion","summary":"  The estimation of 3D human motion from video has progressed rapidly but\ncurrent methods still have several key limitations. First, most methods\nestimate the human in camera coordinates. Second, prior work on estimating\nhumans in global coordinates often assumes a flat ground plane and produces\nfoot sliding. Third, the most accurate methods rely on computationally\nexpensive optimization pipelines, limiting their use to offline applications.\nFinally, existing video-based methods are surprisingly less accurate than\nsingle-frame methods. We address these limitations with WHAM (World-grounded\nHumans with Accurate Motion), which accurately and efficiently reconstructs 3D\nhuman motion in a global coordinate system from video. WHAM learns to lift 2D\nkeypoint sequences to 3D using motion capture data and fuses this with video\nfeatures, integrating motion context and visual information. WHAM exploits\ncamera angular velocity estimated from a SLAM method together with human motion\nto estimate the body's global trajectory. We combine this with a contact-aware\ntrajectory refinement method that lets WHAM capture human motion in diverse\nconditions, such as climbing stairs. WHAM outperforms all existing 3D human\nmotion recovery methods across multiple in-the-wild benchmarks. Code will be\navailable for research purposes at http://wham.is.tue.mpg.de/\n","authors":["Soyong Shin","Juyong Kim","Eni Halilaj","Michael J. Black"],"pdf_url":"https://arxiv.org/pdf/2312.07531v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07530v1","updated":"2023-12-12T18:57:25Z","published":"2023-12-12T18:57:25Z","title":"Weakly Supervised 3D Object Detection via Multi-Level Visual Guidance","summary":"  Weakly supervised 3D object detection aims to learn a 3D detector with lower\nannotation cost, e.g., 2D labels. Unlike prior work which still relies on few\naccurate 3D annotations, we propose a framework to study how to leverage\nconstraints between 2D and 3D domains without requiring any 3D labels.\nSpecifically, we employ visual data from three perspectives to establish\nconnections between 2D and 3D domains. First, we design a feature-level\nconstraint to align LiDAR and image features based on object-aware regions.\nSecond, the output-level constraint is developed to enforce the overlap between\n2D and projected 3D box estimations. Finally, the training-level constraint is\nutilized by producing accurate and consistent 3D pseudo-labels that align with\nthe visual data. We conduct extensive experiments on the KITTI dataset to\nvalidate the effectiveness of the proposed three constraints. Without using any\n3D labels, our method achieves favorable performance against state-of-the-art\napproaches and is competitive with the method that uses 500-frame 3D\nannotations. Code and models will be made publicly available at\nhttps://github.com/kuanchihhuang/VG-W3D.\n","authors":["Kuan-Chih Huang","Yi-Hsuan Tsai","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2312.07530v1.pdf","comment":"Project page: https://github.com/kuanchihhuang/VG-W3D"},{"id":"http://arxiv.org/abs/2312.07526v1","updated":"2023-12-12T18:55:29Z","published":"2023-12-12T18:55:29Z","title":"RTMO: Towards High-Performance One-Stage Real-Time Multi-Person Pose\n  Estimation","summary":"  Real-time multi-person pose estimation presents significant challenges in\nbalancing speed and precision. While two-stage top-down methods slow down as\nthe number of people in the image increases, existing one-stage methods often\nfail to simultaneously deliver high accuracy and real-time performance. This\npaper introduces RTMO, a one-stage pose estimation framework that seamlessly\nintegrates coordinate classification by representing keypoints using dual 1-D\nheatmaps within the YOLO architecture, achieving accuracy comparable to\ntop-down methods while maintaining high speed. We propose a dynamic coordinate\nclassifier and a tailored loss function for heatmap learning, specifically\ndesigned to address the incompatibilities between coordinate classification and\ndense prediction models. RTMO outperforms state-of-the-art one-stage pose\nestimators, achieving 1.1% higher AP on COCO while operating about 9 times\nfaster with the same backbone. Our largest model, RTMO-l, attains 74.8% AP on\nCOCO val2017 and 141 FPS on a single V100 GPU, demonstrating its efficiency and\naccuracy. The code and models are available at\nhttps://github.com/open-mmlab/mmpose/tree/dev-1.x/projects/rtmo.\n","authors":["Peng Lu","Tao Jiang","Yining Li","Xiangtai Li","Kai Chen","Wenming Yang"],"pdf_url":"https://arxiv.org/pdf/2312.07526v1.pdf","comment":"project page:\n  https://github.com/open-mmlab/mmpose/tree/dev-1.x/projects/rtmo"},{"id":"http://arxiv.org/abs/2312.07509v1","updated":"2023-12-12T18:43:05Z","published":"2023-12-12T18:43:05Z","title":"PEEKABOO: Interactive Video Generation via Masked-Diffusion","summary":"  Recently there has been a lot of progress in text-to-video generation, with\nstate-of-the-art models being capable of generating high quality, realistic\nvideos. However, these models lack the capability for users to interactively\ncontrol and generate videos, which can potentially unlock new areas of\napplication. As a first step towards this goal, we tackle the problem of\nendowing diffusion-based video generation models with interactive\nspatio-temporal control over their output. To this end, we take inspiration\nfrom the recent advances in segmentation literature to propose a novel\nspatio-temporal masked attention module - Peekaboo. This module is a\ntraining-free, no-inference-overhead addition to off-the-shelf video generation\nmodels which enables spatio-temporal control. We also propose an evaluation\nbenchmark for the interactive video generation task. Through extensive\nqualitative and quantitative evaluation, we establish that Peekaboo enables\ncontrol video generation and even obtains a gain of upto 3.8x in mIoU over\nbaseline models.\n","authors":["Yash Jain","Anshul Nasery","Vibhav Vineet","Harkirat Behl"],"pdf_url":"https://arxiv.org/pdf/2312.07509v1.pdf","comment":"Project webpage - https://jinga-lala.github.io/projects/Peekaboo/"},{"id":"http://arxiv.org/abs/2312.07507v1","updated":"2023-12-12T18:41:30Z","published":"2023-12-12T18:41:30Z","title":"NAC-TCN: Temporal Convolutional Networks with Causal Dilated\n  Neighborhood Attention for Emotion Understanding","summary":"  In the task of emotion recognition from videos, a key improvement has been to\nfocus on emotions over time rather than a single frame. There are many\narchitectures to address this task such as GRUs, LSTMs, Self-Attention,\nTransformers, and Temporal Convolutional Networks (TCNs). However, these\nmethods suffer from high memory usage, large amounts of operations, or poor\ngradients. We propose a method known as Neighborhood Attention with\nConvolutions TCN (NAC-TCN) which incorporates the benefits of attention and\nTemporal Convolutional Networks while ensuring that causal relationships are\nunderstood which results in a reduction in computation and memory cost. We\naccomplish this by introducing a causal version of Dilated Neighborhood\nAttention while incorporating it with convolutions. Our model achieves\ncomparable, better, or state-of-the-art performance over TCNs, TCAN, LSTMs, and\nGRUs while requiring fewer parameters on standard emotion recognition datasets.\nWe publish our code online for easy reproducibility and use in other projects.\n","authors":["Alexander Mehta","William Yang"],"pdf_url":"https://arxiv.org/pdf/2312.07507v1.pdf","comment":"8 pages, presented at ICVIP 2023"},{"id":"http://arxiv.org/abs/2312.07504v1","updated":"2023-12-12T18:39:52Z","published":"2023-12-12T18:39:52Z","title":"COLMAP-Free 3D Gaussian Splatting","summary":"  While neural rendering has led to impressive advances in scene reconstruction\nand novel view synthesis, it relies heavily on accurately pre-computed camera\nposes. To relax this constraint, multiple efforts have been made to train\nNeural Radiance Fields (NeRFs) without pre-processed camera poses. However, the\nimplicit representations of NeRFs provide extra challenges to optimize the 3D\nstructure and camera poses at the same time. On the other hand, the recently\nproposed 3D Gaussian Splatting provides new opportunities given its explicit\npoint cloud representations. This paper leverages both the explicit geometric\nrepresentation and the continuity of the input video stream to perform novel\nview synthesis without any SfM preprocessing. We process the input frames in a\nsequential manner and progressively grow the 3D Gaussians set by taking one\ninput frame at a time, without the need to pre-compute the camera poses. Our\nmethod significantly improves over previous approaches in view synthesis and\ncamera pose estimation under large motion changes. Our project page is\nhttps://oasisyang.github.io/colmap-free-3dgs\n","authors":["Yang Fu","Sifei Liu","Amey Kulkarni","Jan Kautz","Alexei A. Efros","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2312.07504v1.pdf","comment":"Project Page: https://oasisyang.github.io/colmap-free-3dgs"},{"id":"http://arxiv.org/abs/2206.04281v4","updated":"2023-12-12T18:36:39Z","published":"2022-06-09T05:17:00Z","title":"Local Spatiotemporal Representation Learning for\n  Longitudinally-consistent Neuroimage Analysis","summary":"  Recent self-supervised advances in medical computer vision exploit global and\nlocal anatomical self-similarity for pretraining prior to downstream tasks such\nas segmentation. However, current methods assume i.i.d. image acquisition,\nwhich is invalid in clinical study designs where follow-up longitudinal scans\ntrack subject-specific temporal changes. Further, existing self-supervised\nmethods for medically-relevant image-to-image architectures exploit only\nspatial or temporal self-similarity and only do so via a loss applied at a\nsingle image-scale, with naive multi-scale spatiotemporal extensions collapsing\nto degenerate solutions. To these ends, this paper makes two contributions: (1)\nIt presents a local and multi-scale spatiotemporal representation learning\nmethod for image-to-image architectures trained on longitudinal images. It\nexploits the spatiotemporal self-similarity of learned multi-scale\nintra-subject features for pretraining and develops several feature-wise\nregularizations that avoid collapsed identity representations; (2) During\nfinetuning, it proposes a surprisingly simple self-supervised segmentation\nconsistency regularization to exploit intra-subject correlation. Benchmarked in\nthe one-shot segmentation setting, the proposed framework outperforms both\nwell-tuned randomly-initialized baselines and current self-supervised\ntechniques designed for both i.i.d. and longitudinal datasets. These\nimprovements are demonstrated across both longitudinal neurodegenerative adult\nMRI and developing infant brain MRI and yield both higher performance and\nlongitudinal consistency.\n","authors":["Mengwei Ren","Neel Dey","Martin A. Styner","Kelly Botteron","Guido Gerig"],"pdf_url":"https://arxiv.org/pdf/2206.04281v4.pdf","comment":"Accepted at NeurIPS 2022"},{"id":"http://arxiv.org/abs/2212.05909v2","updated":"2023-12-12T18:35:59Z","published":"2022-12-12T14:19:34Z","title":"NFResNet: Multi-scale and U-shaped Networks for Deblurring","summary":"  Multi-Scale and U-shaped Networks are widely used in various image\nrestoration problems, including deblurring. Keeping in mind the wide range of\napplications, we present a comparison of these architectures and their effects\non image deblurring. We also introduce a new block called as NFResblock. It\nconsists of a Fast Fourier Transformation layer and a series of modified\nNon-Linear Activation Free Blocks. Based on these architectures and additions,\nwe introduce NFResnet and NFResnet+, which are modified multi-scale and U-Net\narchitectures, respectively. We also use three different loss functions to\ntrain these architectures: Charbonnier Loss, Edge Loss, and Frequency\nReconstruction Loss. Extensive experiments on the Deep Video Deblurring\ndataset, along with ablation studies for each component, have been presented in\nthis paper. The proposed architectures achieve a considerable increase in Peak\nSignal to Noise (PSNR) ratio and Structural Similarity Index (SSIM) value.\n","authors":["Tanish Mittal","Preyansh Agrawal","Esha Pahwa","Aarya Makwana"],"pdf_url":"https://arxiv.org/pdf/2212.05909v2.pdf","comment":"Due to limitations in GPU Compute, We weren't able to test the paper\n  on the popularly used GoPro Dataset which is mostly used for testing image\n  deblurring problems. Afterwards the submission on Arxiv, We observed that we\n  missed comparison of our results with some State-of-the-art papers like ARVo\n  & Gated Spatio-Temporal Attention-Guided Video Deblurring"},{"id":"http://arxiv.org/abs/2312.07500v1","updated":"2023-12-12T18:34:56Z","published":"2023-12-12T18:34:56Z","title":"Multi-Branch Network for Imagery Emotion Prediction","summary":"  For a long time, images have proved perfect at both storing and conveying\nrich semantics, especially human emotions. A lot of research has been conducted\nto provide machines with the ability to recognize emotions in photos of people.\nPrevious methods mostly focus on facial expressions but fail to consider the\nscene context, meanwhile scene context plays an important role in predicting\nemotions, leading to more accurate results. In addition,\nValence-Arousal-Dominance (VAD) values offer a more precise quantitative\nunderstanding of continuous emotions, yet there has been less emphasis on\npredicting them compared to discrete emotional categories. In this paper, we\npresent a novel Multi-Branch Network (MBN), which utilizes various source\ninformation, including faces, bodies, and scene contexts to predict both\ndiscrete and continuous emotions in an image. Experimental results on EMOTIC\ndataset, which contains large-scale images of people in unconstrained\nsituations labeled with 26 discrete categories of emotions and VAD values, show\nthat our proposed method significantly outperforms state-of-the-art methods\nwith 28.4% in mAP and 0.93 in MAE. The results highlight the importance of\nutilizing multiple contextual information in emotion prediction and illustrate\nthe potential of our proposed method in a wide range of applications, such as\neffective computing, human-computer interaction, and social robotics. Source\ncode:\nhttps://github.com/BaoNinh2808/Multi-Branch-Network-for-Imagery-Emotion-Prediction\n","authors":["Quoc-Bao Ninh","Hai-Chan Nguyen","Triet Huynh","Trung-Nghia Le"],"pdf_url":"https://arxiv.org/pdf/2312.07500v1.pdf","comment":"SOICT 2023"},{"id":"http://arxiv.org/abs/2312.07495v1","updated":"2023-12-12T18:28:59Z","published":"2023-12-12T18:28:59Z","title":"Exploring Plain ViT Reconstruction for Multi-class Unsupervised Anomaly\n  Detection","summary":"  This work studies the recently proposed challenging and practical Multi-class\nUnsupervised Anomaly Detection (MUAD) task, which only requires normal images\nfor training while simultaneously testing both normal/anomaly images for\nmultiple classes. Existing reconstruction-based methods typically adopt pyramid\nnetworks as encoders/decoders to obtain multi-resolution features, accompanied\nby elaborate sub-modules with heavier handcraft engineering designs for more\nprecise localization. In contrast, a plain Vision Transformer (ViT) with simple\narchitecture has been shown effective in multiple domains, which is simpler,\nmore effective, and elegant. Following this spirit, this paper explores plain\nViT architecture for MUAD. Specifically, we abstract a Meta-AD concept by\ninducing current reconstruction-based methods. Then, we instantiate a novel and\nelegant plain ViT-based symmetric ViTAD structure, effectively designed step by\nstep from three macro and four micro perspectives. In addition, this paper\nreveals several interesting findings for further exploration. Finally, we\npropose a comprehensive and fair evaluation benchmark on eight metrics for the\nMUAD task. Based on a naive training recipe, ViTAD achieves state-of-the-art\n(SoTA) results and efficiency on the MVTec AD and VisA datasets without bells\nand whistles, obtaining 85.4 mAD that surpasses SoTA UniAD by +3.0, and only\nrequiring 1.1 hours and 2.3G GPU memory to complete model training by a single\nV100 GPU. Source code, models, and more results are available at\nhttps://zhangzjn.github.io/projects/ViTAD.\n","authors":["Jiangning Zhang","Xuhai Chen","Yabiao Wang","Chengjie Wang","Yong Liu","Xiangtai Li","Ming-Hsuan Yang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2312.07495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07489v1","updated":"2023-12-12T18:24:44Z","published":"2023-12-12T18:24:44Z","title":"NearbyPatchCL: Leveraging Nearby Patches for Self-Supervised Patch-Level\n  Multi-Class Classification in Whole-Slide Images","summary":"  Whole-slide image (WSI) analysis plays a crucial role in cancer diagnosis and\ntreatment. In addressing the demands of this critical task, self-supervised\nlearning (SSL) methods have emerged as a valuable resource, leveraging their\nefficiency in circumventing the need for a large number of annotations, which\ncan be both costly and time-consuming to deploy supervised methods.\nNevertheless, patch-wise representation may exhibit instability in performance,\nprimarily due to class imbalances stemming from patch selection within WSIs. In\nthis paper, we introduce Nearby Patch Contrastive Learning (NearbyPatchCL), a\nnovel self-supervised learning method that leverages nearby patches as positive\nsamples and a decoupled contrastive loss for robust representation learning.\nOur method demonstrates a tangible enhancement in performance for downstream\ntasks involving patch-level multi-class classification. Additionally, we curate\na new dataset derived from WSIs sourced from the Canine Cutaneous Cancer\nHistology, thus establishing a benchmark for the rigorous evaluation of\npatch-level multi-class classification methodologies. Intensive experiments\nshow that our method significantly outperforms the supervised baseline and\nstate-of-the-art SSL methods with top-1 classification accuracy of 87.56%. Our\nmethod also achieves comparable results while utilizing a mere 1% of labeled\ndata, a stark contrast to the 100% labeled data requirement of other\napproaches. Source code: https://github.com/nvtien457/NearbyPatchCL\n","authors":["Gia-Bao Le","Van-Tien Nguyen","Trung-Nghia Le","Minh-Triet Tran"],"pdf_url":"https://arxiv.org/pdf/2312.07489v1.pdf","comment":"MMM 2024"},{"id":"http://arxiv.org/abs/2312.07488v1","updated":"2023-12-12T18:24:15Z","published":"2023-12-12T18:24:15Z","title":"LMDrive: Closed-Loop End-to-End Driving with Large Language Models","summary":"  Despite significant recent progress in the field of autonomous driving,\nmodern methods still struggle and can incur serious accidents when encountering\nlong-tail unforeseen events and challenging urban scenarios. On the one hand,\nlarge language models (LLM) have shown impressive reasoning capabilities that\napproach \"Artificial General Intelligence\". On the other hand, previous\nautonomous driving methods tend to rely on limited-format inputs (e.g. sensor\ndata and navigation waypoints), restricting the vehicle's ability to understand\nlanguage information and interact with humans. To this end, this paper\nintroduces LMDrive, a novel language-guided, end-to-end, closed-loop autonomous\ndriving framework. LMDrive uniquely processes and integrates multi-modal sensor\ndata with natural language instructions, enabling interaction with humans and\nnavigation software in realistic instructional settings. To facilitate further\nresearch in language-based closed-loop autonomous driving, we also publicly\nrelease the corresponding dataset which includes approximately 64K\ninstruction-following data clips, and the LangAuto benchmark that tests the\nsystem's ability to handle complex instructions and challenging driving\nscenarios. Extensive closed-loop experiments are conducted to demonstrate\nLMDrive's effectiveness. To the best of our knowledge, we're the very first\nwork to leverage LLMs for closed-loop end-to-end autonomous driving. Codes can\nbe found at https://github.com/opendilab/LMDrive\n","authors":["Hao Shao","Yuxuan Hu","Letian Wang","Steven L. Waslander","Yu Liu","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2312.07488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05695v2","updated":"2023-12-12T18:23:42Z","published":"2023-12-09T22:23:57Z","title":"The Counterattack of CNNs in Self-Supervised Learning: Larger Kernel\n  Size might be All You Need","summary":"  Vision Transformers have been rapidly uprising in computer vision thanks to\ntheir outstanding scaling trends, and gradually replacing convolutional neural\nnetworks (CNNs). Recent works on self-supervised learning (SSL) introduce\nsiamese pre-training tasks, on which Transformer backbones continue to\ndemonstrate ever stronger results than CNNs. People come to believe that\nTransformers or self-attention modules are inherently more suitable than CNNs\nin the context of SSL. However, it is noteworthy that most if not all prior\narts of SSL with CNNs chose the standard ResNets as their backbones, whose\narchitecture effectiveness is known to already lag behind advanced Vision\nTransformers. Therefore, it remains unclear whether the self-attention\noperation is crucial for the recent advances in SSL - or CNNs can deliver the\nsame excellence with more advanced designs, too? Can we close the SSL\nperformance gap between Transformers and CNNs? To answer these intriguing\nquestions, we apply self-supervised pre-training to the recently proposed,\nstronger lager-kernel CNN architecture and conduct an apple-to-apple comparison\nwith Transformers, in their SSL performance. Our results show that we are able\nto build pure CNN SSL architectures that perform on par with or better than the\nbest SSL-trained Transformers, by just scaling up convolutional kernel sizes\nbesides other small tweaks. Impressively, when transferring to the downstream\ntasks \\texttt{MS COCO} detection and segmentation, our SSL pre-trained CNN\nmodel (trained in 100 epochs) achieves the same good performance as the\n300-epoch pre-trained Transformer counterpart. We hope this work can help to\nbetter understand what is essential (or not) for self-supervised learning\nbackbones.\n","authors":["Tianjin Huang","Tianlong Chen","Zhangyang Wang","Shiwei Liu"],"pdf_url":"https://arxiv.org/pdf/2312.05695v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07485v1","updated":"2023-12-12T18:21:36Z","published":"2023-12-12T18:21:36Z","title":"MinD-3D: Reconstruct High-quality 3D objects in Human Brain","summary":"  In this paper, we introduce Recon3DMind, a groundbreaking task focused on\nreconstructing 3D visuals from Functional Magnetic Resonance Imaging (fMRI)\nsignals. This represents a major step forward in cognitive neuroscience and\ncomputer vision. To support this task, we present the fMRI-Shape dataset,\nutilizing 360-degree view videos of 3D objects for comprehensive fMRI signal\ncapture. Containing 55 categories of common objects from daily life, this\ndataset will bolster future research endeavors. We also propose MinD-3D, a\nnovel and effective three-stage framework that decodes and reconstructs the\nbrain's 3D visual information from fMRI signals. This method starts by\nextracting and aggregating features from fMRI frames using a neuro-fusion\nencoder, then employs a feature bridge diffusion model to generate\ncorresponding visual features, and ultimately recovers the 3D object through a\ngenerative transformer decoder. Our experiments demonstrate that this method\neffectively extracts features that are valid and highly correlated with visual\nregions of interest (ROIs) in fMRI signals. Notably, it not only reconstructs\n3D objects with high semantic relevance and spatial similarity but also\nsignificantly deepens our understanding of the human brain's 3D visual\nprocessing capabilities. Project page at: https://jianxgao.github.io/MinD-3D.\n","authors":["Jianxiong Gao","Yuqian Fu","Yun Wang","Xuelin Qian","Jianfeng Feng","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2312.07485v1.pdf","comment":"14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2305.17421v2","updated":"2023-12-12T18:14:41Z","published":"2023-05-27T09:01:21Z","title":"FoPro-KD: Fourier Prompted Effective Knowledge Distillation for\n  Long-Tailed Medical Image Recognition","summary":"  Representational transfer from publicly available models is a promising\ntechnique for improving medical image classification, especially in long-tailed\ndatasets with rare diseases. However, existing methods often overlook the\nfrequency-dependent behavior of these models, thereby limiting their\neffectiveness in transferring representations and generalizations to rare\ndiseases. In this paper, we propose FoPro-KD, a novel framework that leverages\nthe power of frequency patterns learned from frozen pre-trained models to\nenhance their transferability and compression, presenting a few unique\ninsights: 1) We demonstrate that leveraging representations from publicly\navailable pre-trained models can substantially improve performance,\nspecifically for rare classes, even when utilizing representations from a\nsmaller pre-trained model. 2) We observe that pre-trained models exhibit\nfrequency preferences, which we explore using our proposed Fourier Prompt\nGenerator (FPG), allowing us to manipulate specific frequencies in the input\nimage, enhancing the discriminative representational transfer. 3) By amplifying\nor diminishing these frequencies in the input image, we enable Effective\nKnowledge Distillation (EKD). EKD facilitates the transfer of knowledge from\npre-trained models to smaller models. Through extensive experiments in\nlong-tailed gastrointestinal image recognition and skin lesion classification,\nwhere rare diseases are prevalent, our FoPro-KD framework outperforms existing\nmethods, enabling more accessible medical models for rare disease\nclassification. Code is available at https://github.com/xmed-lab/FoPro-KD.\n","authors":["Marawan Elbatel","Robert Martí","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2305.17421v2.pdf","comment":"Accepted at IEEE TMI, code is available at\n  https://github.com/xmed-lab/FoPro-KD"},{"id":"http://arxiv.org/abs/2312.07478v1","updated":"2023-12-12T18:07:57Z","published":"2023-12-12T18:07:57Z","title":"Double-Flow GAN model for the reconstruction of perceived faces from\n  brain activities","summary":"  Face plays an important role in human's visual perception, and reconstructing\nperceived faces from brain activities is challenging because of its difficulty\nin extracting high-level features and maintaining consistency of multiple face\nattributes, such as expression, identity, gender, etc. In this study, we\nproposed a novel reconstruction framework, which we called Double-Flow GAN,\nthat can enhance the capability of discriminator and handle imbalances in\nimages from certain domains that are too easy for generators. We also designed\na pretraining process that uses features extracted from images as conditions\nfor making it possible to pretrain the conditional reconstruction model from\nfMRI in a larger pure image dataset. Moreover, we developed a simple pretrained\nmodel to perform fMRI alignment to alleviate the problem of cross-subject\nreconstruction due to the variations of brain structure among different\nsubjects. We conducted experiments by using our proposed method and\nstate-of-the-art reconstruction models. Our results demonstrated that our\nmethod showed significant reconstruction performance, outperformed the previous\nreconstruction models, and exhibited a good generation ability.\n","authors":["Zihao Wang","Jing Zhao","Hui Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.07478v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07472v1","updated":"2023-12-12T17:55:45Z","published":"2023-12-12T17:55:45Z","title":"MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active\n  Perception","summary":"  It is a long-lasting goal to design an embodied system that can solve\nlong-horizon open-world tasks in human-like ways. However, existing approaches\nusually struggle with compound difficulties caused by the logic-aware\ndecomposition and context-aware execution of these tasks. To this end, we\nintroduce MP5, an open-ended multimodal embodied system built upon the\nchallenging Minecraft simulator, which can decompose feasible sub-objectives,\ndesign sophisticated situation-aware plans, and perform embodied action\ncontrol, with frequent communication with a goal-conditioned active perception\nscheme. Specifically, MP5 is developed on top of recent advances in Multimodal\nLarge Language Models (MLLMs), and the system is modulated into functional\nmodules that can be scheduled and collaborated to ultimately solve pre-defined\ncontext- and process-dependent tasks. Extensive experiments prove that MP5 can\nachieve a 22% success rate on difficult process-dependent tasks and a 91%\nsuccess rate on tasks that heavily depend on the context. Moreover, MP5\nexhibits a remarkable ability to address many open-ended tasks that are\nentirely novel.\n","authors":["Yiran Qin","Enshen Zhou","Qichang Liu","Zhenfei Yin","Lu Sheng","Ruimao Zhang","Yu Qiao","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2312.07472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07466v1","updated":"2023-12-12T17:47:13Z","published":"2023-12-12T17:47:13Z","title":"Efficient Object Detection in Autonomous Driving using Spiking Neural\n  Networks: Performance, Energy Consumption Analysis, and Insights into\n  Open-set Object Discovery","summary":"  Besides performance, efficiency is a key design driver of technologies\nsupporting vehicular perception. Indeed, a well-balanced trade-off between\nperformance and energy consumption is crucial for the sustainability of\nautonomous vehicles. In this context, the diversity of real-world contexts in\nwhich autonomous vehicles can operate motivates the need for empowering\nperception models with the capability to detect, characterize and identify\nnewly appearing objects by themselves. In this manuscript we elaborate on this\nthreefold conundrum (performance, efficiency and open-world learning) for\nobject detection modeling tasks over image data collected from vehicular\nscenarios. Specifically, we show that well-performing and efficient models can\nbe realized by virtue of Spiking Neural Networks (SNNs), reaching competitive\nlevels of detection performance when compared to their non-spiking counterparts\nat dramatic energy consumption savings (up to 85%) and a slightly improved\nrobustness against image noise. Our experiments herein offered also expose\nqualitatively the complexity of detecting new objects based on the preliminary\nresults of a simple approach to discriminate potential object proposals in the\ncaptured image.\n","authors":["Aitor Martinez Seras","Javier Del Ser","Pablo Garcia-Bringas"],"pdf_url":"https://arxiv.org/pdf/2312.07466v1.pdf","comment":"8 pages, 5 figures, presented at ITSC2023"},{"id":"http://arxiv.org/abs/2312.07460v1","updated":"2023-12-12T17:37:16Z","published":"2023-12-12T17:37:16Z","title":"Empirical Validation of Conformal Prediction for Trustworthy Skin\n  Lesions Classification","summary":"  Uncertainty quantification is a pivotal field that contributes to the\nrealization of reliable and robust systems. By providing complementary\ninformation, it becomes instrumental in fortifying safe decisions, particularly\nwithin high-risk applications. Nevertheless, a comprehensive understanding of\nthe advantages and limitations inherent in various methods within the medical\nimaging field necessitates further research coupled with in-depth analysis. In\nthis paper, we explore Conformal Prediction, an emerging distribution-free\nuncertainty quantification technique, along with Monte Carlo Dropout and\nEvidential Deep Learning methods. Our comprehensive experiments provide a\ncomparative performance analysis for skin lesion classification tasks across\nthe three quantification methods. Furthermore, We present insights into the\neffectiveness of each method in handling Out-of-Distribution samples from\ndomain-shifted datasets. Based on our experimental findings, our conclusion\nhighlights the robustness and consistent performance of conformal prediction\nacross diverse conditions. This positions it as the preferred choice for\ndecision-making in safety-critical applications.\n","authors":["Jamil Fayyad","Shadi Alijani","Homayoun Najjaran"],"pdf_url":"https://arxiv.org/pdf/2312.07460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.08769v3","updated":"2023-12-12T17:29:05Z","published":"2023-09-15T21:22:51Z","title":"The Use of Multi-Scale Fiducial Markers To Aid Takeoff and Landing\n  Navigation by Rotorcraft","summary":"  This paper quantifies the performance of visual SLAM that leverages\nmulti-scale fiducial markers (i.e., artificial landmarks that can be detected\nat a wide range of distances) to show its potential for reliable takeoff and\nlanding navigation in rotorcraft. Prior work has shown that square markers with\na black-and-white pattern of grid cells can be used to improve the performance\nof visual SLAM with color cameras. We extend this prior work to allow nested\nmarker layouts. We evaluate performance during semi-autonomous takeoff and\nlanding operations in a variety of environmental conditions by a DJI Matrice\n300 RTK rotorcraft with two FLIR Blackfly color cameras, using RTK GNSS to\nobtain ground truth pose estimates. Performance measures include absolute\ntrajectory error and the fraction of the number of estimated poses to the total\nframe. We release all of our results -- our dataset and the code of the\nimplementation of the visual SLAM with fiducial markers -- to the public as\nopen-source.\n","authors":["Jongwon Lee","Su Yeon Choi","Timothy Bretl"],"pdf_url":"https://arxiv.org/pdf/2309.08769v3.pdf","comment":"Accepted at the 2024 AIAA SciTech"},{"id":"http://arxiv.org/abs/2311.06000v2","updated":"2023-12-12T17:19:07Z","published":"2023-11-10T11:23:28Z","title":"Keystroke Verification Challenge (KVC): Biometric and Fairness Benchmark\n  Evaluation","summary":"  Analyzing keystroke dynamics (KD) for biometric verification has several\nadvantages: it is among the most discriminative behavioral traits; keyboards\nare among the most common human-computer interfaces, being the primary means\nfor users to enter textual data; its acquisition does not require additional\nhardware, and its processing is relatively lightweight; and it allows for\ntransparently recognizing subjects. However, the heterogeneity of experimental\nprotocols and metrics, and the limited size of the databases adopted in the\nliterature impede direct comparisons between different systems, thus\nrepresenting an obstacle in the advancement of keystroke biometrics. To\nalleviate this aspect, we present a new experimental framework to benchmark\nKD-based biometric verification performance and fairness based on tweet-long\nsequences of variable transcript text from over 185,000 subjects, acquired\nthrough desktop and mobile keyboards, extracted from the Aalto Keystroke\nDatabases. The framework runs on CodaLab in the form of the Keystroke\nVerification Challenge (KVC). Moreover, we also introduce a novel fairness\nmetric, the Skewed Impostor Ratio (SIR), to capture inter- and\nintra-demographic group bias patterns in the verification scores. We\ndemonstrate the usefulness of the proposed framework by employing two\nstate-of-the-art keystroke verification systems, TypeNet and TypeFormer, to\ncompare different sets of input features, achieving a less privacy-invasive\nsystem, by discarding the analysis of text content (ASCII codes of the keys\npressed) in favor of extended features in the time domain. Our experiments show\nthat this approach allows to maintain satisfactory performance.\n","authors":["Giuseppe Stragapede","Ruben Vera-Rodriguez","Ruben Tolosana","Aythami Morales","Naser Damer","Julian Fierrez","Javier Ortega-Garcia"],"pdf_url":"https://arxiv.org/pdf/2311.06000v2.pdf","comment":"13 pages, 4 figure, 5 pages"},{"id":"http://arxiv.org/abs/2312.07437v1","updated":"2023-12-12T17:04:26Z","published":"2023-12-12T17:04:26Z","title":"Medical Image Classification Using Transfer Learning and Chaos Game\n  Optimization on the Internet of Medical Things","summary":"  The Internet of Medical Things (IoMT) has dramatically benefited medical\nprofessionals that patients and physicians can access from all regions.\nAlthough the automatic detection and prediction of diseases such as melanoma\nand leukemia is still being researched and studied in IoMT, existing approaches\nare not able to achieve a high degree of efficiency. Thus, with a new approach\nthat provides better results, patients would access the adequate treatments\nearlier and the death rate would be reduced. Therefore, this paper introduces\nan IoMT proposal for medical images classification that may be used anywhere,\ni.e. it is an ubiquitous approach. It was design in two stages: first, we\nemploy a Transfer Learning (TL)-based method for feature extraction, which is\ncarried out using MobileNetV3; second, we use the Chaos Game Optimization (CGO)\nfor feature selection, with the aim of excluding unnecessary features and\nimproving the performance, which is key in IoMT. Our methodology was evaluated\nusing ISIC-2016, PH2, and Blood-Cell datasets. The experimental results\nindicated that the proposed approach obtained an accuracy of 88.39% on\nISIC-2016, 97.52% on PH2, and 88.79% on Blood-cell. Moreover, our approach had\nsuccessful performances for the metrics employed compared to other existing\nmethods.\n","authors":["Alhassan Mabrouk","Abdelghani Dahou","Mohamed Abd Elaziz","Rebeca P. Díaz Redondo","Mohammed Kayed"],"pdf_url":"https://arxiv.org/pdf/2312.07437v1.pdf","comment":"22 pages, 12 figures, journal"},{"id":"http://arxiv.org/abs/2312.07435v1","updated":"2023-12-12T17:00:46Z","published":"2023-12-12T17:00:46Z","title":"Cross-modal Contrastive Learning with Asymmetric Co-attention Network\n  for Video Moment Retrieval","summary":"  Video moment retrieval is a challenging task requiring fine-grained\ninteractions between video and text modalities. Recent work in image-text\npretraining has demonstrated that most existing pretrained models suffer from\ninformation asymmetry due to the difference in length between visual and\ntextual sequences. We question whether the same problem also exists in the\nvideo-text domain with an auxiliary need to preserve both spatial and temporal\ninformation. Thus, we evaluate a recently proposed solution involving the\naddition of an asymmetric co-attention network for video grounding tasks.\nAdditionally, we incorporate momentum contrastive loss for robust,\ndiscriminative representation learning in both modalities. We note that the\nintegration of these supplementary modules yields better performance compared\nto state-of-the-art models on the TACoS dataset and comparable results on\nActivityNet Captions, all while utilizing significantly fewer parameters with\nrespect to baseline.\n","authors":["Love Panta","Prashant Shrestha","Brabeem Sapkota","Amrita Bhattarai","Suresh Manandhar","Anand Kumar Sah"],"pdf_url":"https://arxiv.org/pdf/2312.07435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07428v1","updated":"2023-12-12T16:53:18Z","published":"2023-12-12T16:53:18Z","title":"Ensemble Federated Learning: an approach for collaborative pneumonia\n  diagnosis","summary":"  Federated learning is a very convenient approach for scenarios where (i) the\nexchange of data implies privacy concerns and/or (ii) a quick reaction is\nneeded. In smart healthcare systems, both aspects are usually required. In this\npaper, we work on the first scenario, where preserving privacy is key and,\nconsequently, building a unique and massive medical image data set by fusing\ndifferent data sets from different medical institutions or research centers\n(computation nodes) is not an option. We propose an ensemble federated learning\n(EFL) approach that is based on the following characteristics: First, each\ncomputation node works with a different data set (but of the same type). They\nwork locally and apply an ensemble approach combining eight well-known CNN\nmodels (densenet169, mobilenetv2, xception, inceptionv3, vgg16, resnet50,\ndensenet121, and resnet152v2) on Chest X-ray images. Second, the best two local\nmodels are used to create a local ensemble model that is shared with a central\nnode. Third, the ensemble models are aggregated to obtain a global model, which\nis shared with the computation nodes to continue with a new iteration. This\nprocedure continues until there are no changes in the best local models. We\nhave performed different experiments to compare our approach with centralized\nones (with or without an ensemble approach)\\color{black}. The results conclude\nthat our proposal outperforms these ones in Chest X-ray images (achieving an\naccuracy of 96.63\\%) and offers very competitive results compared to other\nproposals in the literature.\n","authors":["Alhassan Mabrouk","Rebeca P. Díaz Redondo","Mohamed Abd Elaziz","Mohammed Kayed"],"pdf_url":"https://arxiv.org/pdf/2312.07428v1.pdf","comment":"15 pages, 9 figures, journal"},{"id":"http://arxiv.org/abs/2312.07425v1","updated":"2023-12-12T16:48:53Z","published":"2023-12-12T16:48:53Z","title":"Deep Internal Learning: Deep Learning from a Single Input","summary":"  Deep learning in general focuses on training a neural network from large\nlabeled datasets. Yet, in many cases there is value in training a network just\nfrom the input at hand. This may involve training a network from scratch using\na single input or adapting an already trained network to a provided input\nexample at inference time. This survey paper aims at covering deep\ninternal-learning techniques that have been proposed in the past few years for\nthese two important directions. While our main focus will be on image\nprocessing problems, most of the approaches that we survey are derived for\ngeneral signals (vectors with recurring patterns that can be distinguished from\nnoise) and are therefore applicable to other modalities. We believe that the\ntopic of internal-learning is very important in many signal and image\nprocessing problems where training data is scarce and diversity is large on the\none hand, and on the other, there is a lot of structure in the data that can be\nexploited.\n","authors":["Tom Tirer","Raja Giryes","Se Young Chun","Yonina C. Eldar"],"pdf_url":"https://arxiv.org/pdf/2312.07425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07424v1","updated":"2023-12-12T16:48:07Z","published":"2023-12-12T16:48:07Z","title":"How Well Does GPT-4V(ision) Adapt to Distribution Shifts? A Preliminary\n  Investigation","summary":"  In machine learning, generalization against distribution shifts -- where\ndeployment conditions diverge from the training scenarios -- is crucial,\nparticularly in fields like climate modeling, biomedicine, and autonomous\ndriving. The emergence of foundation models, distinguished by their extensive\npretraining and task versatility, has led to an increased interest in their\nadaptability to distribution shifts. GPT-4V(ision) acts as the most advanced\npublicly accessible multimodal foundation model, with extensive applications\nacross various domains, including anomaly detection, video understanding, image\ngeneration, and medical diagnosis. However, its robustness against data\ndistributions remains largely underexplored. Addressing this gap, this study\nrigorously evaluates GPT-4V's adaptability and generalization capabilities in\ndynamic environments, benchmarking against prominent models like CLIP and\nLLaVA. We delve into GPT-4V's zero-shot generalization across 13 diverse\ndatasets spanning natural, medical, and molecular domains. We further\ninvestigate its adaptability to controlled data perturbations and examine the\nefficacy of in-context learning as a tool to enhance its adaptation. Our\nfindings delineate GPT-4V's capability boundaries in distribution shifts,\nshedding light on its strengths and limitations across various scenarios.\nImportantly, this investigation contributes to our understanding of how AI\nfoundation models generalize to distribution shifts, offering pivotal insights\ninto their adaptability and robustness. Code is publicly available at\nhttps://github.com/jameszhou-gl/gpt-4v-distribution-shift.\n","authors":["Zhongyi Han","Guanglin Zhou","Rundong He","Jindong Wang","Xing Xie","Tailin Wu","Yilong Yin","Salman Khan","Lina Yao","Tongliang Liu","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.07424v1.pdf","comment":"62 pages, 39 figures"},{"id":"http://arxiv.org/abs/2312.07423v1","updated":"2023-12-12T16:45:52Z","published":"2023-12-12T16:45:52Z","title":"Holoported Characters: Real-time Free-viewpoint Rendering of Humans from\n  Sparse RGB Cameras","summary":"  We present the first approach to render highly realistic free-viewpoint\nvideos of a human actor in general apparel, from sparse multi-view recording to\ndisplay, in real-time at an unprecedented 4K resolution. At inference, our\nmethod only requires four camera views of the moving actor and the respective\n3D skeletal pose. It handles actors in wide clothing, and reproduces even\nfine-scale dynamic detail, e.g. clothing wrinkles, face expressions, and hand\ngestures. At training time, our learning-based approach expects dense\nmulti-view video and a rigged static surface scan of the actor. Our method\ncomprises three main stages. Stage 1 is a skeleton-driven neural approach for\nhigh-quality capture of the detailed dynamic mesh geometry. Stage 2 is a novel\nsolution to create a view-dependent texture using four test-time camera views\nas input. Finally, stage 3 comprises a new image-based refinement network\nrendering the final 4K image given the output from the previous stages. Our\napproach establishes a new benchmark for real-time rendering resolution and\nquality using sparse input camera views, unlocking possibilities for immersive\ntelepresence.\n","authors":["Ashwath Shetty","Marc Habermann","Guoxing Sun","Diogo Luvizon","Vladislav Golyanik","Christian Theobalt"],"pdf_url":"https://arxiv.org/pdf/2312.07423v1.pdf","comment":"Project page: https://vcai.mpi-inf.mpg.de/projects/holochar/"},{"id":"http://arxiv.org/abs/2312.07418v1","updated":"2023-12-12T16:39:12Z","published":"2023-12-12T16:39:12Z","title":"Attention Based Encoder Decoder Model for Video Captioning in Nepali\n  (2023)","summary":"  Video captioning in Nepali, a language written in the Devanagari script,\npresents a unique challenge due to the lack of existing academic work in this\ndomain. This work develops a novel encoder-decoder paradigm for Nepali video\ncaptioning to tackle this difficulty. LSTM and GRU sequence-to-sequence models\nare used in the model to produce related textual descriptions based on features\nretrieved from video frames using CNNs. Using Google Translate and manual\npost-editing, a Nepali video captioning dataset is generated from the Microsoft\nResearch Video Description Corpus (MSVD) dataset created using Google\nTranslate, and manual post-editing work. The efficacy of the model for\nDevanagari-scripted video captioning is demonstrated by BLEU, METOR, and ROUGE\nmeasures, which are used to assess its performance.\n","authors":["Kabita Parajuli","Shashidhar Ram Joshi"],"pdf_url":"https://arxiv.org/pdf/2312.07418v1.pdf","comment":"MSVD, Encoder, Decoder LSTM, GRU, Attention Mechanism"},{"id":"http://arxiv.org/abs/2312.07409v1","updated":"2023-12-12T16:28:08Z","published":"2023-12-12T16:28:08Z","title":"DiffMorpher: Unleashing the Capability of Diffusion Models for Image\n  Morphing","summary":"  Diffusion models have achieved remarkable image generation quality surpassing\nprevious generative models. However, a notable limitation of diffusion models,\nin comparison to GANs, is their difficulty in smoothly interpolating between\ntwo image samples, due to their highly unstructured latent space. Such a smooth\ninterpolation is intriguing as it naturally serves as a solution for the image\nmorphing task with many applications. In this work, we present DiffMorpher, the\nfirst approach enabling smooth and natural image interpolation using diffusion\nmodels. Our key idea is to capture the semantics of the two images by fitting\ntwo LoRAs to them respectively, and interpolate between both the LoRA\nparameters and the latent noises to ensure a smooth semantic transition, where\ncorrespondence automatically emerges without the need for annotation. In\naddition, we propose an attention interpolation and injection technique and a\nnew sampling schedule to further enhance the smoothness between consecutive\nimages. Extensive experiments demonstrate that DiffMorpher achieves starkly\nbetter image morphing effects than previous methods across a variety of object\ncategories, bridging a critical functional gap that distinguished diffusion\nmodels from GANs.\n","authors":["Kaiwen Zhang","Yifan Zhou","Xudong Xu","Xingang Pan","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2312.07409v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.01971v2","updated":"2023-12-12T16:27:57Z","published":"2023-05-03T08:36:06Z","title":"District-scale surface temperatures generated from high-resolution\n  longitudinal thermal infrared images","summary":"  The paper describes a dataset that was collected by infrared thermography,\nwhich is a non-contact, non-intrusive technique to collect data and analyze the\nbuilt environment in various aspects. While most studies focus on the city and\nbuilding scales, the rooftop observatory provides high temporal and spatial\nresolution observations with dynamic interactions on the district scale. The\nrooftop infrared thermography observatory with a multi-modal platform that is\ncapable of assessing a wide range of dynamic processes in urban systems was\ndeployed in Singapore. It was placed on the top of two buildings that overlook\nthe outdoor context of the campus of the National University of Singapore. The\nplatform collects remote sensing data from tropical areas on a temporal scale,\nallowing users to determine the temperature trend of individual features such\nas buildings, roads, and vegetation. The dataset includes 1,365,921 thermal\nimages collected on average at approximately 10 seconds intervals from two\nlocations during ten months.\n","authors":["Subin Lin","Vasantha Ramani","Miguel Martin","Pandarasamy Arjunan","Adrian Chong","Filip Biljecki","Marcel Ignatius","Kameshwar Poolla","Clayton Miller"],"pdf_url":"https://arxiv.org/pdf/2305.01971v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07408v1","updated":"2023-12-12T16:27:35Z","published":"2023-12-12T16:27:35Z","title":"Turbo: Informativity-Driven Acceleration Plug-In for Vision-Language\n  Models","summary":"  Vision-Language Large Models (VLMs) have become primary backbone of AI, due\nto the impressive performance. However, their expensive computation costs,\ni.e., throughput and delay, impede potentials in real-world scenarios. To\nachieve acceleration for VLMs, most existing methods focus on the model\nperspective: pruning, distillation, quantification, but completely overlook the\ndata-perspective redundancy. To fill the overlook, this paper pioneers the\nseverity of data redundancy, and designs one plug-and-play Turbo module guided\nby information degree to prune inefficient tokens from visual or textual data.\nIn pursuit of efficiency-performance trade-offs, information degree takes two\nkey factors into consideration: mutual redundancy and semantic value.\nConcretely, the former evaluates the data duplication between sequential\ntokens; while the latter evaluates each token by its contribution to the\noverall semantics. As a result, tokens with high information degree carry less\nredundancy and stronger semantics. For VLMs' calculation, Turbo works as a\nuser-friendly plug-in that sorts data referring to information degree,\nutilizing only top-level ones to save costs. Its advantages are multifaceted,\ne.g., being generally compatible to various VLMs across understanding and\ngeneration, simple use without retraining and trivial engineering efforts. On\nmultiple public VLMs benchmarks, we conduct extensive experiments to reveal the\ngratifying acceleration of Turbo, under negligible performance drop.\n","authors":["Chen Ju","Haicheng Wang","Zeqian Li","Xu Chen","Zhonghua Zhai","Weilin Huang","Shuai Xiao"],"pdf_url":"https://arxiv.org/pdf/2312.07408v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07395v1","updated":"2023-12-12T16:10:19Z","published":"2023-12-12T16:10:19Z","title":"A Simple Recipe for Contrastively Pre-training Video-First Encoders\n  Beyond 16 Frames","summary":"  Understanding long, real-world videos requires modeling of long-range visual\ndependencies. To this end, we explore video-first architectures, building on\nthe common paradigm of transferring large-scale, image--text models to video\nvia shallow temporal fusion. However, we expose two limitations to the\napproach: (1) decreased spatial capabilities, likely due to poor\nvideo--language alignment in standard video datasets, and (2) higher memory\nconsumption, bottlenecking the number of frames that can be processed. To\nmitigate the memory bottleneck, we systematically analyze the memory/accuracy\ntrade-off of various efficient methods: factorized attention,\nparameter-efficient image-to-video adaptation, input masking, and\nmulti-resolution patchification. Surprisingly, simply masking large portions of\nthe video (up to 75%) during contrastive pre-training proves to be one of the\nmost robust ways to scale encoders to videos up to 4.3 minutes at 1 FPS. Our\nsimple approach for training long video-to-text models, which scales to 1B\nparameters, does not add new architectural complexity and is able to outperform\nthe popular paradigm of using much larger LLMs as an information aggregator\nover segment-based information on benchmarks with long-range temporal\ndependencies (YouCook2, EgoSchema).\n","authors":["Pinelopi Papalampidi","Skanda Koppula","Shreya Pathak","Justin Chiu","Joe Heyward","Viorica Patraucean","Jiajun Shen","Antoine Miech","Andrew Zisserman","Aida Nematzdeh"],"pdf_url":"https://arxiv.org/pdf/2312.07395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.12424v3","updated":"2023-12-12T16:08:18Z","published":"2023-06-21T17:59:51Z","title":"VisoGender: A dataset for benchmarking gender bias in image-text pronoun\n  resolution","summary":"  We introduce VisoGender, a novel dataset for benchmarking gender bias in\nvision-language models. We focus on occupation-related biases within a\nhegemonic system of binary gender, inspired by Winograd and Winogender schemas,\nwhere each image is associated with a caption containing a pronoun relationship\nof subjects and objects in the scene. VisoGender is balanced by gender\nrepresentation in professional roles, supporting bias evaluation in two ways:\ni) resolution bias, where we evaluate the difference between pronoun resolution\naccuracies for image subjects with gender presentations perceived as masculine\nversus feminine by human annotators and ii) retrieval bias, where we compare\nratios of professionals perceived to have masculine and feminine gender\npresentations retrieved for a gender-neutral search query. We benchmark several\nstate-of-the-art vision-language models and find that they demonstrate bias in\nresolving binary gender in complex scenes. While the direction and magnitude of\ngender bias depends on the task and the model being evaluated, captioning\nmodels are generally less biased than Vision-Language Encoders. Dataset and\ncode are available at https://github.com/oxai/visogender\n","authors":["Siobhan Mackenzie Hall","Fernanda Gonçalves Abrantes","Hanwen Zhu","Grace Sodunke","Aleksandar Shtedritski","Hannah Rose Kirk"],"pdf_url":"https://arxiv.org/pdf/2306.12424v3.pdf","comment":"NeurIPS Datasets and Benchmarks 2023. Data and code available at\n  https://github.com/oxai/visogender"},{"id":"http://arxiv.org/abs/2312.07389v1","updated":"2023-12-12T16:05:12Z","published":"2023-12-12T16:05:12Z","title":"Eroding Trust In Aerial Imagery: Comprehensive Analysis and Evaluation\n  Of Adversarial Attacks In Geospatial Systems","summary":"  In critical operations where aerial imagery plays an essential role, the\nintegrity and trustworthiness of data are paramount. The emergence of\nadversarial attacks, particularly those that exploit control over labels or\nemploy physically feasible trojans, threatens to erode that trust, making the\nanalysis and mitigation of these attacks a matter of urgency. We demonstrate\nhow adversarial attacks can degrade confidence in geospatial systems,\nspecifically focusing on scenarios where the attacker's control over labels is\nrestricted and the use of realistic threat vectors. Proposing and evaluating\nseveral innovative attack methodologies, including those tailored to overhead\nimages, we empirically show their threat to remote sensing systems using\nhigh-quality SpaceNet datasets. Our experimentation reflects the unique\nchallenges posed by aerial imagery, and these preliminary results not only\nreveal the potential risks but also highlight the non-trivial nature of the\nproblem compared to recent works.\n","authors":["Michael Lanier","Aayush Dhakal","Zhexiao Xiong","Arthur Li","Nathan Jacobs","Yevgeniy Vorobeychik"],"pdf_url":"https://arxiv.org/pdf/2312.07389v1.pdf","comment":"Accepted at IEEE AIRP 2023"},{"id":"http://arxiv.org/abs/2312.07384v1","updated":"2023-12-12T16:00:55Z","published":"2023-12-12T16:00:55Z","title":"Unsupervised Temporal Action Localization via Self-paced Incremental\n  Learning","summary":"  Recently, temporal action localization (TAL) has garnered significant\ninterest in information retrieval community. However, existing\nsupervised/weakly supervised methods are heavily dependent on extensive labeled\ntemporal boundaries and action categories, which is labor-intensive and\ntime-consuming. Although some unsupervised methods have utilized the\n``iteratively clustering and localization'' paradigm for TAL, they still suffer\nfrom two pivotal impediments: 1) unsatisfactory video clustering confidence,\nand 2) unreliable video pseudolabels for model training. To address these\nlimitations, we present a novel self-paced incremental learning model to\nenhance clustering and localization training simultaneously, thereby\nfacilitating more effective unsupervised TAL. Concretely, we improve the\nclustering confidence through exploring the contextual feature-robust visual\ninformation. Thereafter, we design two (constant- and variable- speed)\nincremental instance learning strategies for easy-to-hard model training, thus\nensuring the reliability of these video pseudolabels and further improving\noverall localization performance. Extensive experiments on two public datasets\nhave substantiated the superiority of our model over several state-of-the-art\ncompetitors.\n","authors":["Haoyu Tang","Han Jiang","Mingzhu Xu","Yupeng Hu","Jihua Zhu","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2312.07384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07385v1","updated":"2023-12-12T16:00:55Z","published":"2023-12-12T16:00:55Z","title":"GSmoothFace: Generalized Smooth Talking Face Generation via Fine Grained\n  3D Face Guidance","summary":"  Although existing speech-driven talking face generation methods achieve\nsignificant progress, they are far from real-world application due to the\navatar-specific training demand and unstable lip movements. To address the\nabove issues, we propose the GSmoothFace, a novel two-stage generalized talking\nface generation model guided by a fine-grained 3d face model, which can\nsynthesize smooth lip dynamics while preserving the speaker's identity. Our\nproposed GSmoothFace model mainly consists of the Audio to Expression\nPrediction (A2EP) module and the Target Adaptive Face Translation (TAFT)\nmodule. Specifically, we first develop the A2EP module to predict expression\nparameters synchronized with the driven speech. It uses a transformer to\ncapture the long-term audio context and learns the parameters from the\nfine-grained 3D facial vertices, resulting in accurate and smooth\nlip-synchronization performance. Afterward, the well-designed TAFT module,\nempowered by Morphology Augmented Face Blending (MAFB), takes the predicted\nexpression parameters and target video as inputs to modify the facial region of\nthe target video without distorting the background content. The TAFT\neffectively exploits the identity appearance and background context in the\ntarget video, which makes it possible to generalize to different speakers\nwithout retraining. Both quantitative and qualitative experiments confirm the\nsuperiority of our method in terms of realism, lip synchronization, and visual\nquality. See the project page for code, data, and request pre-trained models:\nhttps://zhanghm1995.github.io/GSmoothFace.\n","authors":["Haiming Zhang","Zhihao Yuan","Chaoda Zheng","Xu Yan","Baoyuan Wang","Guanbin Li","Song Wu","Shuguang Cui","Zhen Li"],"pdf_url":"https://arxiv.org/pdf/2312.07385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07381v1","updated":"2023-12-12T15:57:03Z","published":"2023-12-12T15:57:03Z","title":"ScribblePrompt: Fast and Flexible Interactive Segmentation for Any\n  Medical Image","summary":"  Semantic medical image segmentation is a crucial part of both scientific\nresearch and clinical care. With enough labelled data, deep learning models can\nbe trained to accurately automate specific medical image segmentation tasks.\nHowever, manually segmenting images to create training data is highly labor\nintensive. In this paper, we present ScribblePrompt, an interactive\nsegmentation framework for medical imaging that enables human annotators to\nsegment unseen structures using scribbles, clicks, and bounding boxes.\nScribbles are an intuitive and effective form of user interaction for complex\ntasks, however most existing methods focus on click-based interactions. We\nintroduce algorithms for simulating realistic scribbles that enable training\nmodels that are amenable to multiple types of interaction. To achieve\ngeneralization to new tasks, we train on a diverse collection of 65 open-access\nbiomedical datasets -- using both real and synthetic labels. We test\nScribblePrompt on multiple network architectures and unseen datasets, and\ndemonstrate that it can be used in real-time on a single CPU. We evaluate\nScribblePrompt using manually-collected scribbles, simulated interactions, and\na user study. ScribblePrompt outperforms existing methods in all our\nevaluations. In the user study, ScribblePrompt reduced annotation time by 28%\nwhile improving Dice by 15% compared to existing methods. We showcase\nScribblePrompt in an online demo and provide code at\nhttps://scribbleprompt.csail.mit.edu\n","authors":["Hallee E. Wong","Marianne Rakic","John Guttag","Adrian V. Dalca"],"pdf_url":"https://arxiv.org/pdf/2312.07381v1.pdf","comment":"Project Website: https://scribbleprompt.csail.mit.edu"},{"id":"http://arxiv.org/abs/2312.07378v1","updated":"2023-12-12T15:48:12Z","published":"2023-12-12T15:48:12Z","title":"X4D-SceneFormer: Enhanced Scene Understanding on 4D Point Cloud Videos\n  through Cross-modal Knowledge Transfer","summary":"  The field of 4D point cloud understanding is rapidly developing with the goal\nof analyzing dynamic 3D point cloud sequences. However, it remains a\nchallenging task due to the sparsity and lack of texture in point clouds.\nMoreover, the irregularity of point cloud poses a difficulty in aligning\ntemporal information within video sequences. To address these issues, we\npropose a novel cross-modal knowledge transfer framework, called\nX4D-SceneFormer. This framework enhances 4D-Scene understanding by transferring\ntexture priors from RGB sequences using a Transformer architecture with\ntemporal relationship mining. Specifically, the framework is designed with a\ndual-branch architecture, consisting of an 4D point cloud transformer and a\nGradient-aware Image Transformer (GIT). During training, we employ multiple\nknowledge transfer techniques, including temporal consistency losses and masked\nself-attention, to strengthen the knowledge transfer between modalities. This\nleads to enhanced performance during inference using single-modal 4D point\ncloud inputs. Extensive experiments demonstrate the superior performance of our\nframework on various 4D point cloud video understanding tasks, including action\nrecognition, action segmentation and semantic segmentation. The results achieve\n1st places, i.e., 85.3% (+7.9%) accuracy and 47.3% (+5.0%) mIoU for 4D action\nsegmentation and semantic segmentation, on the HOI4D\nchallenge\\footnote{\\url{http://www.hoi4d.top/}.}, outperforming previous\nstate-of-the-art by a large margin. We release the code at\nhttps://github.com/jinglinglingling/X4D\n","authors":["Linglin Jing","Ying Xue","Xu Yan","Chaoda Zheng","Dong Wang","Ruimao Zhang","Zhigang Wang","Hui Fang","Bin Zhao","Zhen Li"],"pdf_url":"https://arxiv.org/pdf/2312.07378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07374v1","updated":"2023-12-12T15:43:36Z","published":"2023-12-12T15:43:36Z","title":"Relax Image-Specific Prompt Requirement in SAM: A Single Generic Prompt\n  for Segmenting Camouflaged Objects","summary":"  Camouflaged object detection (COD) approaches heavily rely on pixel-level\nannotated datasets. Weakly-supervised COD (WSCOD) approaches use sparse\nannotations like scribbles or points to reduce annotation effort, but this can\nlead to decreased accuracy. The Segment Anything Model (SAM) shows remarkable\nsegmentation ability with sparse prompts like points. However, manual prompt is\nnot always feasible, as it may not be accessible in real-world application.\nAdditionally, it only provides localization information instead of semantic\none, which can intrinsically cause ambiguity in interpreting the targets. In\nthis work, we aim to eliminate the need for manual prompt. The key idea is to\nemploy Cross-modal Chains of Thought Prompting (CCTP) to reason visual prompts\nusing the semantic information given by a generic text prompt.To that end, we\nintroduce a test-time adaptation per-instance mechanism called Generalizable\nSAM (GenSAM) to automatically enerate and optimize visual prompts the generic\ntask prompt for WSCOD. In particular, CCTP maps a single generic text prompt\nonto image-specific consensus foreground and background heatmaps using\nvision-language models, acquiring reliable visual prompts. Moreover, to\ntest-time adapt the visual prompts, we further propose Progressive Mask\nGeneration (PMG) to iteratively reweight the input image, guiding the model to\nfocus on the targets in a coarse-to-fine manner. Crucially, all network\nparameters are fixed, avoiding the need for additional training. Experiments\ndemonstrate the superiority of GenSAM. Experiments on three benchmarks\ndemonstrate that GenSAM outperforms point supervision approaches and achieves\ncomparable results to scribble supervision ones, solely relying on general task\ndescriptions as prompts. our codes is in: https://lwpyh.github.io/GenSAM/.\n","authors":["Jian Hu","Jiayi Lin","Weitong Cai","Shaogang Gong"],"pdf_url":"https://arxiv.org/pdf/2312.07374v1.pdf","comment":"This paper is accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2312.07370v1","updated":"2023-12-12T15:40:22Z","published":"2023-12-12T15:40:22Z","title":"Adversarial Semi-Supervised Domain Adaptation for Semantic Segmentation:\n  A New Role for Labeled Target Samples","summary":"  Adversarial learning baselines for domain adaptation (DA) approaches in the\ncontext of semantic segmentation are under explored in semi-supervised\nframework. These baselines involve solely the available labeled target samples\nin the supervision loss. In this work, we propose to enhance their usefulness\non both semantic segmentation and the single domain classifier neural networks.\nWe design new training objective losses for cases when labeled target data\nbehave as source samples or as real target samples. The underlying rationale is\nthat considering the set of labeled target samples as part of source domain\nhelps reducing the domain discrepancy and, hence, improves the contribution of\nthe adversarial loss. To support our approach, we consider a complementary\nmethod that mixes source and labeled target data, then applies the same\nadaptation process. We further propose an unsupervised selection procedure\nusing entropy to optimize the choice of labeled target samples for adaptation.\nWe illustrate our findings through extensive experiments on the benchmarks\nGTA5, SYNTHIA, and Cityscapes. The empirical evaluation highlights competitive\nperformance of our proposed approach.\n","authors":["Marwa Kechaou","Mokhtar Z. Alaya","Romain Hérault","Gilles Gasso"],"pdf_url":"https://arxiv.org/pdf/2312.07370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07364v1","updated":"2023-12-12T15:33:08Z","published":"2023-12-12T15:33:08Z","title":"Collapse-Oriented Adversarial Training with Triplet Decoupling for\n  Robust Image Retrieval","summary":"  Adversarial training has achieved substantial performance in defending image\nretrieval systems against adversarial examples. However, existing studies still\nsuffer from two major limitations: model collapse and weak adversary. This\npaper addresses these two limitations by proposing collapse-oriented (COLO)\nadversarial training with triplet decoupling (TRIDE). Specifically, COLO\nprevents model collapse by temporally orienting the perturbation update\ndirection with a new collapse metric, while TRIDE yields a strong adversary by\nspatially decoupling the update targets of perturbation into the anchor and the\ntwo candidates of a triplet. Experimental results demonstrate that our\nCOLO-TRIDE outperforms the current state of the art by 7% on average over 10\nrobustness metrics and across 3 popular datasets. In addition, we identify the\nfairness limitations of commonly used robustness metrics in image retrieval and\npropose a new metric for more meaningful robustness evaluation. Codes will be\nmade publicly available on GitHub.\n","authors":["Qiwei Tian","Chenhao Lin","Qian Li","Zhengyu Zhao","Chao Shen"],"pdf_url":"https://arxiv.org/pdf/2312.07364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07360v1","updated":"2023-12-12T15:30:24Z","published":"2023-12-12T15:30:24Z","title":"Boosting Latent Diffusion with Flow Matching","summary":"  Recently, there has been tremendous progress in visual synthesis and the\nunderlying generative models. Here, diffusion models (DMs) stand out\nparticularly, but lately, flow matching (FM) has also garnered considerable\ninterest. While DMs excel in providing diverse images, they suffer from long\ntraining and slow generation. With latent diffusion, these issues are only\npartially alleviated. Conversely, FM offers faster training and inference but\nexhibits less diversity in synthesis. We demonstrate that introducing FM\nbetween the Diffusion model and the convolutional decoder offers\nhigh-resolution image synthesis with reduced computational cost and model size.\nDiffusion can then efficiently provide the necessary generation diversity. FM\ncompensates for the lower resolution, mapping the small latent space to a\nhigh-dimensional one. Subsequently, the convolutional decoder of the LDM maps\nthese latents to high-resolution images. By combining the diversity of DMs, the\nefficiency of FMs, and the effectiveness of convolutional decoders, we achieve\nstate-of-the-art high-resolution image synthesis at $1024^2$ with minimal\ncomputational cost. Importantly, our approach is orthogonal to recent\napproximation and speed-up strategies for the underlying DMs, making it easily\nintegrable into various DM frameworks.\n","authors":["Johannes S. Fischer","Ming Gui","Pingchuan Ma","Nick Stracke","Stefan A. Baumann","Björn Ommer"],"pdf_url":"https://arxiv.org/pdf/2312.07360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07357v1","updated":"2023-12-12T15:26:06Z","published":"2023-12-12T15:26:06Z","title":"Automatic coral reef fish identification and 3D measurement in the wild","summary":"  In this paper we present a pipeline using stereo images in order to\nautomatically identify, track in 3D fish, and measure fish population.\n","authors":["Cyril Barrelet","Marc Chaumont","Gérard Subsol"],"pdf_url":"https://arxiv.org/pdf/2312.07357v1.pdf","comment":"This paper is in its draft version and should be improved in order to\n  be published. This paper is issued from one Year of Engineering work"},{"id":"http://arxiv.org/abs/2312.07353v1","updated":"2023-12-12T15:21:57Z","published":"2023-12-12T15:21:57Z","title":"CLIP in Medical Imaging: A Comprehensive Survey","summary":"  Contrastive Language-Image Pre-training (CLIP), a straightforward yet\neffective pre-training paradigm, successfully introduces semantic-rich text\nsupervision to vision models and has demonstrated promising results in various\ntasks due to its generalizability and interpretability. It has recently gained\nincreasing interest in the medical imaging domain, either as a powerful\npre-training paradigm for medical vision language alignment or a pre-trained\nkey component for various clinical tasks. With the aim of facilitating a deeper\nunderstanding of this promising direction, this survey offers an in-depth\nexploration of the CLIP paradigm within the domain of medical imaging,\nregarding both refined CLIP pre-training and CLIP-driven applications. Our\nsurvey (1) starts with a brief introduction to the fundamentals of CLIP\nmethodology. (2) Then, we investigate the adaptation of CLIP pre-training in\nthe medical domain, focusing on how to optimize CLIP given characteristics of\nmedical images and reports. (3) Furthermore, we explore the practical\nutilization of CLIP pre-trained models in various tasks, including\nclassification, dense prediction, and cross-modal tasks. (4) Finally, we\ndiscuss existing limitations of CLIP in the context of medical imaging and\npropose forward-looking directions to address the demands of medical imaging\ndomain. We expect that this comprehensive survey will provide researchers in\nthe field of medical image analysis with a holistic understanding of the CLIP\nparadigm and its potential implications. The project page is available at\nhttps://github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging, which will be\nregularly updated.\n","authors":["Zihao Zhao","Yuxiao Liu","Han Wu","Yonghao Li","Sheng Wang","Lin Teng","Disheng Liu","Xiang Li","Zhiming Cui","Qian Wang","Dinggang Shen"],"pdf_url":"https://arxiv.org/pdf/2312.07353v1.pdf","comment":"* These authors contributed equally. Project page available at\n  https://github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging"},{"id":"http://arxiv.org/abs/2312.07352v1","updated":"2023-12-12T15:18:15Z","published":"2023-12-12T15:18:15Z","title":"CholecTrack20: A Dataset for Multi-Class Multiple Tool Tracking in\n  Laparoscopic Surgery","summary":"  Tool tracking in surgical videos is vital in computer-assisted intervention\nfor tasks like surgeon skill assessment, safety zone estimation, and\nhuman-machine collaboration during minimally invasive procedures. The lack of\nlarge-scale datasets hampers Artificial Intelligence implementation in this\ndomain. Current datasets exhibit overly generic tracking formalization, often\nlacking surgical context: a deficiency that becomes evident when tools move out\nof the camera's scope, resulting in rigid trajectories that hinder realistic\nsurgical representation. This paper addresses the need for a more precise and\nadaptable tracking formalization tailored to the intricacies of endoscopic\nprocedures by introducing CholecTrack20, an extensive dataset meticulously\nannotated for multi-class multi-tool tracking across three perspectives\nrepresenting the various ways of considering the temporal duration of a tool\ntrajectory: (1) intraoperative, (2) intracorporeal, and (3) visibility within\nthe camera's scope. The dataset comprises 20 laparoscopic videos with over\n35,000 frames and 65,000 annotated tool instances with details on spatial\nlocation, category, identity, operator, phase, and surgical visual conditions.\nThis detailed dataset caters to the evolving assistive requirements within a\nprocedure.\n","authors":["Chinedu Innocent Nwoye","Kareem Elgohary","Anvita Srinivas","Fauzan Zaid","Joël L. Lavanchy","Nicolas Padoy"],"pdf_url":"https://arxiv.org/pdf/2312.07352v1.pdf","comment":"Surgical tool tracking dataset paper, 15 pages, 9 figures, 4 tables"},{"id":"http://arxiv.org/abs/2206.05575v4","updated":"2023-12-12T15:10:46Z","published":"2022-06-11T17:38:09Z","title":"MammoFL: Mammographic Breast Density Estimation using Federated Learning","summary":"  In this study, we automate quantitative mammographic breast density\nestimation with neural networks and show that this tool is a strong use case\nfor federated learning on multi-institutional datasets. Our dataset included\nbilateral CC-view and MLO-view mammographic images from two separate\ninstitutions. Two U-Nets were separately trained on algorithm-generated labels\nto perform segmentation of the breast and dense tissue from these images and\nsubsequently calculate breast percent density (PD). The networks were trained\nwith federated learning and compared to three non-federated baselines, one\ntrained on each single-institution dataset and one trained on the aggregated\nmulti-institution dataset. We demonstrate that training on multi-institution\ndatasets is critical to algorithm generalizability. We further show that\nfederated learning on multi-institutional datasets improves model\ngeneralization to unseen data at nearly the same level as centralized training\non multi-institutional datasets, indicating that federated learning can be\napplied to our method to improve algorithm generalizability while maintaining\npatient privacy.\n","authors":["Ramya Muthukrishnan","Angelina Heyler","Keshava Katti","Sarthak Pati","Walter Mankowski","Aprupa Alahari","Michael Sanborn","Emily F. Conant","Christopher Scott","Stacey Winham","Celine Vachon","Pratik Chaudhari","Despina Kontos","Spyridon Bakas"],"pdf_url":"https://arxiv.org/pdf/2206.05575v4.pdf","comment":"Deep learning, federated learning, mammography, breast density, risk\n  assessment"},{"id":"http://arxiv.org/abs/2308.11488v2","updated":"2023-12-12T15:10:15Z","published":"2023-08-22T15:08:02Z","title":"Opening the Vocabulary of Egocentric Actions","summary":"  Human actions in egocentric videos are often hand-object interactions\ncomposed from a verb (performed by the hand) applied to an object. Despite\ntheir extensive scaling up, egocentric datasets still face two limitations -\nsparsity of action compositions and a closed set of interacting objects. This\npaper proposes a novel open vocabulary action recognition task. Given a set of\nverbs and objects observed during training, the goal is to generalize the verbs\nto an open vocabulary of actions with seen and novel objects. To this end, we\ndecouple the verb and object predictions via an object-agnostic verb encoder\nand a prompt-based object encoder. The prompting leverages CLIP representations\nto predict an open vocabulary of interacting objects. We create open vocabulary\nbenchmarks on the EPIC-KITCHENS-100 and Assembly101 datasets; whereas\nclosed-action methods fail to generalize, our proposed method is effective. In\naddition, our object encoder significantly outperforms existing open-vocabulary\nvisual recognition methods in recognizing novel interacting objects.\n","authors":["Dibyadip Chatterjee","Fadime Sener","Shugao Ma","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2308.11488v2.pdf","comment":"NeurIPS 2023 camera ready;\n  https://dibschat.github.io/openvocab-egoAR/"},{"id":"http://arxiv.org/abs/2312.07342v1","updated":"2023-12-12T15:04:44Z","published":"2023-12-12T15:04:44Z","title":"Expand-and-Quantize: Unsupervised Semantic Segmentation Using\n  High-Dimensional Space and Product Quantization","summary":"  Unsupervised semantic segmentation (USS) aims to discover and recognize\nmeaningful categories without any labels. For a successful USS, two key\nabilities are required: 1) information compression and 2) clustering\ncapability. Previous methods have relied on feature dimension reduction for\ninformation compression, however, this approach may hinder the process of\nclustering. In this paper, we propose a novel USS framework called\nExpand-and-Quantize Unsupervised Semantic Segmentation (EQUSS), which combines\nthe benefits of high-dimensional spaces for better clustering and product\nquantization for effective information compression. Our extensive experiments\ndemonstrate that EQUSS achieves state-of-the-art results on three standard\nbenchmarks. In addition, we analyze the entropy of USS features, which is the\nfirst step towards understanding USS from the perspective of information\ntheory.\n","authors":["Jiyoung Kim","Kyuhong Shim","Insu Lee","Byonghyo Shim"],"pdf_url":"https://arxiv.org/pdf/2312.07342v1.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2312.07331v1","updated":"2023-12-12T14:47:26Z","published":"2023-12-12T14:47:26Z","title":"Coupled Confusion Correction: Learning from Crowds with Sparse\n  Annotations","summary":"  As the size of the datasets getting larger, accurately annotating such\ndatasets is becoming more impractical due to the expensiveness on both time and\neconomy. Therefore, crowd-sourcing has been widely adopted to alleviate the\ncost of collecting labels, which also inevitably introduces label noise and\neventually degrades the performance of the model. To learn from crowd-sourcing\nannotations, modeling the expertise of each annotator is a common but\nchallenging paradigm, because the annotations collected by crowd-sourcing are\nusually highly-sparse. To alleviate this problem, we propose Coupled Confusion\nCorrection (CCC), where two models are simultaneously trained to correct the\nconfusion matrices learned by each other. Via bi-level optimization, the\nconfusion matrices learned by one model can be corrected by the distilled data\nfrom the other. Moreover, we cluster the ``annotator groups'' who share similar\nexpertise so that their confusion matrices could be corrected together. In this\nway, the expertise of the annotators, especially of those who provide seldom\nlabels, could be better captured. Remarkably, we point out that the annotation\nsparsity not only means the average number of labels is low, but also there are\nalways some annotators who provide very few labels, which is neglected by\nprevious works when constructing synthetic crowd-sourcing annotations. Based on\nthat, we propose to use Beta distribution to control the generation of the\ncrowd-sourcing labels so that the synthetic annotations could be more\nconsistent with the real-world ones. Extensive experiments are conducted on two\ntypes of synthetic datasets and three real-world datasets, the results of which\ndemonstrate that CCC significantly outperforms state-of-the-art approaches.\n","authors":["Hansong Zhang","Shikun Li","Dan Zeng","Chenggang Yan","Shiming Ge"],"pdf_url":"https://arxiv.org/pdf/2312.07331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07330v1","updated":"2023-12-12T14:45:45Z","published":"2023-12-12T14:45:45Z","title":"Learned representation-guided diffusion models for large-image\n  generation","summary":"  To synthesize high-fidelity samples, diffusion models typically require\nauxiliary data to guide the generation process. However, it is impractical to\nprocure the painstaking patch-level annotation effort required in specialized\ndomains like histopathology and satellite imagery; it is often performed by\ndomain experts and involves hundreds of millions of patches. Modern-day\nself-supervised learning (SSL) representations encode rich semantic and visual\ninformation. In this paper, we posit that such representations are expressive\nenough to act as proxies to fine-grained human labels. We introduce a novel\napproach that trains diffusion models conditioned on embeddings from SSL. Our\ndiffusion models successfully project these features back to high-quality\nhistopathology and remote sensing images. In addition, we construct larger\nimages by assembling spatially consistent patches inferred from SSL embeddings,\npreserving long-range dependencies. Augmenting real data by generating\nvariations of real images improves downstream classifier accuracy for\npatch-level and larger, image-scale classification tasks. Our models are\neffective even on datasets not encountered during training, demonstrating their\nrobustness and generalizability. Generating images from learned embeddings is\nagnostic to the source of the embeddings. The SSL embeddings used to generate a\nlarge image can either be extracted from a reference image, or sampled from an\nauxiliary model conditioned on any related modality (e.g. class labels, text,\ngenomic data). As proof of concept, we introduce the text-to-large image\nsynthesis paradigm where we successfully synthesize large pathology and\nsatellite images out of text descriptions.\n","authors":["Alexandros Graikos","Srikar Yellapragada","Minh-Quan Le","Saarthak Kapse","Prateek Prasanna","Joel Saltz","Dimitris Samaras"],"pdf_url":"https://arxiv.org/pdf/2312.07330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07327v1","updated":"2023-12-12T14:43:09Z","published":"2023-12-12T14:43:09Z","title":"Adaptive Confidence Multi-View Hashing for Multimedia Retrieval","summary":"  The multi-view hash method converts heterogeneous data from multiple views\ninto binary hash codes, which is one of the critical technologies in multimedia\nretrieval. However, the current methods mainly explore the complementarity\namong multiple views while lacking confidence learning and fusion. Moreover, in\npractical application scenarios, the single-view data contain redundant noise.\nTo conduct the confidence learning and eliminate unnecessary noise, we propose\na novel Adaptive Confidence Multi-View Hashing (ACMVH) method. First, a\nconfidence network is developed to extract useful information from various\nsingle-view features and remove noise information. Furthermore, an adaptive\nconfidence multi-view network is employed to measure the confidence of each\nview and then fuse multi-view features through a weighted summation. Lastly, a\ndilation network is designed to further enhance the feature representation of\nthe fused features. To the best of our knowledge, we pioneer the application of\nconfidence learning into the field of multimedia retrieval. Extensive\nexperiments on two public datasets show that the proposed ACMVH performs better\nthan state-of-the-art methods (maximum increase of 3.24%). The source code is\navailable at https://github.com/HackerHyper/ACMVH.\n","authors":["Jian Zhu","Yu Cui","Zhangmin Huang","Xingyu Li","Lei Liu","Lingfang Zeng","Li-Rong Dai"],"pdf_url":"https://arxiv.org/pdf/2312.07327v1.pdf","comment":"accepted by International Conference on Acoustics, Speech and Signal\n  Processing 2024"},{"id":"http://arxiv.org/abs/2312.07322v1","updated":"2023-12-12T14:37:36Z","published":"2023-12-12T14:37:36Z","title":"GenHowTo: Learning to Generate Actions and State Transformations from\n  Instructional Videos","summary":"  We address the task of generating temporally consistent and physically\nplausible images of actions and object state transformations. Given an input\nimage and a text prompt describing the targeted transformation, our generated\nimages preserve the environment and transform objects in the initial image. Our\ncontributions are threefold. First, we leverage a large body of instructional\nvideos and automatically mine a dataset of triplets of consecutive frames\ncorresponding to initial object states, actions, and resulting object\ntransformations. Second, equipped with this data, we develop and train a\nconditioned diffusion model dubbed GenHowTo. Third, we evaluate GenHowTo on a\nvariety of objects and actions and show superior performance compared to\nexisting methods. In particular, we introduce a quantitative evaluation where\nGenHowTo achieves 88% and 74% on seen and unseen interaction categories,\nrespectively, outperforming prior work by a large margin.\n","authors":["Tomáš Souček","Dima Damen","Michael Wray","Ivan Laptev","Josef Sivic"],"pdf_url":"https://arxiv.org/pdf/2312.07322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07315v1","updated":"2023-12-12T14:29:57Z","published":"2023-12-12T14:29:57Z","title":"NVS-Adapter: Plug-and-Play Novel View Synthesis from a Single Image","summary":"  Transfer learning of large-scale Text-to-Image (T2I) models has recently\nshown impressive potential for Novel View Synthesis (NVS) of diverse objects\nfrom a single image. While previous methods typically train large models on\nmulti-view datasets for NVS, fine-tuning the whole parameters of T2I models not\nonly demands a high cost but also reduces the generalization capacity of T2I\nmodels in generating diverse images in a new domain. In this study, we propose\nan effective method, dubbed NVS-Adapter, which is a plug-and-play module for a\nT2I model, to synthesize novel multi-views of visual objects while fully\nexploiting the generalization capacity of T2I models. NVS-Adapter consists of\ntwo main components; view-consistency cross-attention learns the visual\ncorrespondences to align the local details of view features, and global\nsemantic conditioning aligns the semantic structure of generated views with the\nreference view. Experimental results demonstrate that the NVS-Adapter can\neffectively synthesize geometrically consistent multi-views and also achieve\nhigh performance on benchmarks without full fine-tuning of T2I models. The code\nand data are publicly available in\n~\\href{https://postech-cvlab.github.io/nvsadapter/}{https://postech-cvlab.github.io/nvsadapter/}.\n","authors":["Yoonwoo Jeong","Jinwoo Lee","Chiheon Kim","Minsu Cho","Doyup Lee"],"pdf_url":"https://arxiv.org/pdf/2312.07315v1.pdf","comment":"Project Page: https://postech-cvlab.github.io/nvsadapter/"},{"id":"http://arxiv.org/abs/2312.07311v1","updated":"2023-12-12T14:28:31Z","published":"2023-12-12T14:28:31Z","title":"Scalable Motion Style Transfer with Constrained Diffusion Generation","summary":"  Current training of motion style transfer systems relies on consistency\nlosses across style domains to preserve contents, hindering its scalable\napplication to a large number of domains and private data. Recent image\ntransfer works show the potential of independent training on each domain by\nleveraging implicit bridging between diffusion models, with the content\npreservation, however, limited to simple data patterns. We address this by\nimposing biased sampling in backward diffusion while maintaining the domain\nindependence in the training stage. We construct the bias from the source\ndomain keyframes and apply them as the gradient of content constraints,\nyielding a framework with keyframe manifold constraint gradients (KMCGs). Our\nvalidation demonstrates the success of training separate models to transfer\nbetween as many as ten dance motion styles. Comprehensive experiments find a\nsignificant improvement in preserving motion contents in comparison to baseline\nand ablative diffusion-based style transfer models. In addition, we perform a\nhuman study for a subjective assessment of the quality of generated dance\nmotions. The results validate the competitiveness of KMCGs.\n","authors":["Wenjie Yin","Yi Yu","Hang Yin","Danica Kragic","Mårten Björkman"],"pdf_url":"https://arxiv.org/pdf/2312.07311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.11093v2","updated":"2023-12-12T14:00:08Z","published":"2023-01-26T13:35:02Z","title":"Simple diffusion: End-to-end diffusion for high resolution images","summary":"  Currently, applying diffusion models in pixel space of high resolution images\nis difficult. Instead, existing approaches focus on diffusion in lower\ndimensional spaces (latent diffusion), or have multiple super-resolution levels\nof generation referred to as cascades. The downside is that these approaches\nadd additional complexity to the diffusion framework.\n  This paper aims to improve denoising diffusion for high resolution images\nwhile keeping the model as simple as possible. The paper is centered around the\nresearch question: How can one train a standard denoising diffusion models on\nhigh resolution images, and still obtain performance comparable to these\nalternate approaches?\n  The four main findings are: 1) the noise schedule should be adjusted for high\nresolution images, 2) It is sufficient to scale only a particular part of the\narchitecture, 3) dropout should be added at specific locations in the\narchitecture, and 4) downsampling is an effective strategy to avoid high\nresolution feature maps. Combining these simple yet effective techniques, we\nachieve state-of-the-art on image generation among diffusion models without\nsampling modifiers on ImageNet.\n","authors":["Emiel Hoogeboom","Jonathan Heek","Tim Salimans"],"pdf_url":"https://arxiv.org/pdf/2301.11093v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07273v1","updated":"2023-12-12T13:52:55Z","published":"2023-12-12T13:52:55Z","title":"Benchmarking Pretrained Vision Embeddings for Near- and Duplicate\n  Detection in Medical Images","summary":"  Near- and duplicate image detection is a critical concern in the field of\nmedical imaging. Medical datasets often contain similar or duplicate images\nfrom various sources, which can lead to significant performance issues and\nevaluation biases, especially in machine learning tasks due to data leakage\nbetween training and testing subsets. In this paper, we present an approach for\nidentifying near- and duplicate 3D medical images leveraging publicly available\n2D computer vision embeddings. We assessed our approach by comparing embeddings\nextracted from two state-of-the-art self-supervised pretrained models and two\ndifferent vector index structures for similarity retrieval. We generate an\nexperimental benchmark based on the publicly available Medical Segmentation\nDecathlon dataset. The proposed method yields promising results for near- and\nduplicate image detection achieving a mean sensitivity and specificity of\n0.9645 and 0.8559, respectively.\n","authors":["Tuan Truong","Farnaz Khun Jush","Matthias Lenga"],"pdf_url":"https://arxiv.org/pdf/2312.07273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07266v1","updated":"2023-12-12T13:45:56Z","published":"2023-12-12T13:45:56Z","title":"ProxyDet: Synthesizing Proxy Novel Classes via Classwise Mixup for Open\n  Vocabulary Object Detection","summary":"  Open-vocabulary object detection (OVOD) aims to recognize novel objects whose\ncategories are not included in training set. In order to classify these unseen\nclasses during training, many OVOD frameworks leverage the zero-shot capability\nof largely pretrained vision and language models, such as CLIP. To further\nimprove generalization on the unseen novel classes, several approaches proposed\nto additionally train with pseudo region labeling on the external data sources\nthat contain a substantial number of novel category labels beyond the existing\ntraining data. Albeit its simplicity, these pseudo-labeling methods still\nexhibit limited improvement with regard to the genuine novel classes that were\nnot pseudo-labeled. In this paper, we present a novel, yet simple technique\nthat helps generalization on the overall distribution of novel classes.\nInspired by our observation that numerous novel classes reside within the\nconvex hull constructed by the base (seen) classes in the CLIP embedding space,\nwe propose to synthesize proxy-novel classes approximating novel classes via\nlinear mixup between a pair of base classes. By training our detector with\nthese synthetic proxy-novel classes, we effectively explore the embedding space\nof novel classes. The experimental results on various OVOD benchmarks such as\nLVIS and COCO demonstrate superior performance on novel classes compared to the\nother state-of-the-art methods.\n","authors":["Joonhyun Jeong","Geondo Park","Jayeon Yoo","Hyungsik Jung","Heesu Kim"],"pdf_url":"https://arxiv.org/pdf/2312.07266v1.pdf","comment":"Accepted in AAAI24. Preprint"},{"id":"http://arxiv.org/abs/2312.07264v1","updated":"2023-12-12T13:44:53Z","published":"2023-12-12T13:44:53Z","title":"Dual Structure-Preserving Image Filterings for Semi-supervised Medical\n  Image Segmentation","summary":"  Semi-supervised image segmentation has attracted great attention recently.\nThe key is how to leverage unlabeled images in the training process. Most\nmethods maintain consistent predictions of the unlabeled images under\nvariations (e.g., adding noise/perturbations, or creating alternative versions)\nin the image and/or model level. In most image-level variation, medical images\noften have prior structure information, which has not been well explored. In\nthis paper, we propose novel dual structure-preserving image filterings (DSPIF)\nas the image-level variations for semi-supervised medical image segmentation.\nMotivated by connected filtering that simplifies image via filtering in\nstructure-aware tree-based image representation, we resort to the dual contrast\ninvariant Max-tree and Min-tree representation. Specifically, we propose a\nnovel connected filtering that removes topologically equivalent nodes (i.e.\nconnected components) having no siblings in the Max/Min-tree. This results in\ntwo filtered images preserving topologically critical structure. Applying such\ndual structure-preserving image filterings in mutual supervision is beneficial\nfor semi-supervised medical image segmentation. Extensive experimental results\non three benchmark datasets demonstrate that the proposed method\nsignificantly/consistently outperforms some state-of-the-art methods. The\nsource codes will be publicly available.\n","authors":["Yuliang Gu","Zhichao Sun","Xin Xiao","Yuda Zou","Zelong Liu","Yongchao Xu"],"pdf_url":"https://arxiv.org/pdf/2312.07264v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.16139v5","updated":"2023-12-12T13:39:31Z","published":"2023-08-30T16:52:20Z","title":"MedShapeNet -- A Large-Scale Dataset of 3D Medical Shapes for Computer\n  Vision","summary":"  Prior to the deep learning era, shape was commonly used to describe the\nobjects. Nowadays, state-of-the-art (SOTA) algorithms in medical imaging are\npredominantly diverging from computer vision, where voxel grids, meshes, point\nclouds, and implicit surface models are used. This is seen from numerous\nshape-related publications in premier vision conferences as well as the growing\npopularity of ShapeNet (about 51,300 models) and Princeton ModelNet (127,915\nmodels). For the medical domain, we present a large collection of anatomical\nshapes (e.g., bones, organs, vessels) and 3D models of surgical instrument,\ncalled MedShapeNet, created to facilitate the translation of data-driven vision\nalgorithms to medical applications and to adapt SOTA vision algorithms to\nmedical problems. As a unique feature, we directly model the majority of shapes\non the imaging data of real patients. As of today, MedShapeNet includes 23\ndataset with more than 100,000 shapes that are paired with annotations (ground\ntruth). Our data is freely accessible via a web interface and a Python\napplication programming interface (API) and can be used for discriminative,\nreconstructive, and variational benchmarks as well as various applications in\nvirtual, augmented, or mixed reality, and 3D printing. Exemplary, we present\nuse cases in the fields of classification of brain tumors, facial and skull\nreconstructions, multi-class anatomy completion, education, and 3D printing. In\nfuture, we will extend the data and improve the interfaces. The project pages\nare: https://medshapenet.ikim.nrw/ and\nhttps://github.com/Jianningli/medshapenet-feedback\n","authors":["Jianning Li","Zongwei Zhou","Jiancheng Yang","Antonio Pepe","Christina Gsaxner","Gijs Luijten","Chongyu Qu","Tiezheng Zhang","Xiaoxi Chen","Wenxuan Li","Marek Wodzinski","Paul Friedrich","Kangxian Xie","Yuan Jin","Narmada Ambigapathy","Enrico Nasca","Naida Solak","Gian Marco Melito","Viet Duc Vu","Afaque R. Memon","Christopher Schlachta","Sandrine De Ribaupierre","Rajnikant Patel","Roy Eagleson","Xiaojun Chen","Heinrich Mächler","Jan Stefan Kirschke","Ezequiel de la Rosa","Patrick Ferdinand Christ","Hongwei Bran Li","David G. Ellis","Michele R. Aizenberg","Sergios Gatidis","Thomas Küstner","Nadya Shusharina","Nicholas Heller","Vincent Andrearczyk","Adrien Depeursinge","Mathieu Hatt","Anjany Sekuboyina","Maximilian Löffler","Hans Liebl","Reuben Dorent","Tom Vercauteren","Jonathan Shapey","Aaron Kujawa","Stefan Cornelissen","Patrick Langenhuizen","Achraf Ben-Hamadou","Ahmed Rekik","Sergi Pujades","Edmond Boyer","Federico Bolelli","Costantino Grana","Luca Lumetti","Hamidreza Salehi","Jun Ma","Yao Zhang","Ramtin Gharleghi","Susann Beier","Arcot Sowmya","Eduardo A. Garza-Villarreal","Thania Balducci","Diego Angeles-Valdez","Roberto Souza","Leticia Rittner","Richard Frayne","Yuanfeng Ji","Vincenzo Ferrari","Soumick Chatterjee","Florian Dubost","Stefanie Schreiber","Hendrik Mattern","Oliver Speck","Daniel Haehn","Christoph John","Andreas Nürnberger","João Pedrosa","Carlos Ferreira","Guilherme Aresta","António Cunha","Aurélio Campilho","Yannick Suter","Jose Garcia","Alain Lalande","Vicky Vandenbossche","Aline Van Oevelen","Kate Duquesne","Hamza Mekhzoum","Jef Vandemeulebroucke","Emmanuel Audenaert","Claudia Krebs","Timo van Leeuwen","Evie Vereecke","Hauke Heidemeyer","Rainer Röhrig","Frank Hölzle","Vahid Badeli","Kathrin Krieger","Matthias Gunzer","Jianxu Chen","Timo van Meegdenburg","Amin Dada","Miriam Balzer","Jana Fragemann","Frederic Jonske","Moritz Rempe","Stanislav Malorodov","Fin H. Bahnsen","Constantin Seibold","Alexander Jaus","Zdravko Marinov","Paul F. Jaeger","Rainer Stiefelhagen","Ana Sofia Santos","Mariana Lindo","André Ferreira","Victor Alves","Michael Kamp","Amr Abourayya","Felix Nensa","Fabian Hörst","Alexander Brehmer","Lukas Heine","Yannik Hanusrichter","Martin Weßling","Marcel Dudda","Lars E. Podleska","Matthias A. Fink","Julius Keyl","Konstantinos Tserpes","Moon-Sung Kim","Shireen Elhabian","Hans Lamecker","Dženan Zukić","Beatriz Paniagua","Christian Wachinger","Martin Urschler","Luc Duong","Jakob Wasserthal","Peter F. Hoyer","Oliver Basu","Thomas Maal","Max J. H. Witjes","Gregor Schiele","Ti-chiun Chang","Seyed-Ahmad Ahmadi","Ping Luo","Bjoern Menze","Mauricio Reyes","Thomas M. Deserno","Christos Davatzikos","Behrus Puladi","Pascal Fua","Alan L. Yuille","Jens Kleesiek","Jan Egger"],"pdf_url":"https://arxiv.org/pdf/2308.16139v5.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2312.07258v1","updated":"2023-12-12T13:38:00Z","published":"2023-12-12T13:38:00Z","title":"SSTA: Salient Spatially Transformed Attack","summary":"  Extensive studies have demonstrated that deep neural networks (DNNs) are\nvulnerable to adversarial attacks, which brings a huge security risk to the\nfurther application of DNNs, especially for the AI models developed in the real\nworld. Despite the significant progress that has been made recently, existing\nattack methods still suffer from the unsatisfactory performance of escaping\nfrom being detected by naked human eyes due to the formulation of adversarial\nexample (AE) heavily relying on a noise-adding manner. Such mentioned\nchallenges will significantly increase the risk of exposure and result in an\nattack to be failed. Therefore, in this paper, we propose the Salient Spatially\nTransformed Attack (SSTA), a novel framework to craft imperceptible AEs, which\nenhance the stealthiness of AEs by estimating a smooth spatial transform metric\non a most critical area to generate AEs instead of adding external noise to the\nwhole image. Compared to state-of-the-art baselines, extensive experiments\nindicated that SSTA could effectively improve the imperceptibility of the AEs\nwhile maintaining a 100\\% attack success rate.\n","authors":["Renyang Liu","Wei Zhou","Sixin Wu","Jun Zhao","Kwok-Yan Lam"],"pdf_url":"https://arxiv.org/pdf/2312.07258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07255v1","updated":"2023-12-12T13:35:41Z","published":"2023-12-12T13:35:41Z","title":"GIST: Improving Parameter Efficient Fine Tuning via Knowledge\n  Interaction","summary":"  The Parameter-Efficient Fine-Tuning (PEFT) method, which adjusts or\nintroduces fewer trainable parameters to calibrate pre-trained models on\ndownstream tasks, has become a recent research interest. However, existing PEFT\nmethods within the traditional fine-tiuning framework have two main\nshortcomings: 1) They overlook the explicit association between trainable\nparameters and downstream task knowledge. 2) They neglect the interaction\nbetween the intrinsic task-agnostic knowledge of pre-trained models and the\ntask-specific knowledge in downstream tasks. To address this gap, we propose a\nnovel fine-tuning framework, named GIST, in a plug-and-play manner.\nSpecifically, our framework first introduces a trainable token, called the Gist\ntoken, when applying PEFT methods on downstream tasks. This token serves as an\naggregator of the task-specific knowledge learned by the PEFT methods and forms\nan explicit association with downstream knowledge. Furthermore, to facilitate\nexplicit interaction between task-agnostic and task-specific knowledge, we\nintroduce the concept of Knowledge Interaction via a Bidirectional\nKullback-Leibler Divergence objective. As a result, PEFT methods within our\nframework can make the pre-trained model understand downstream tasks more\ncomprehensively by leveraging the knowledge interaction. Extensive experiments\ndemonstrate the universality and scalability of our framework. Notably, on the\nVTAB-1K benchmark, we employ the Adapter (a prevalent PEFT method) within our\nGIST framework and achieve a performance boost of 2.25%, with an increase of\nonly 0.8K parameters. The Code will be released.\n","authors":["Jiacheng Ruan","Jingsheng Gao","Mingye Xie","Suncheng Xiang","Zefang Yu","Ting Liu","Yuzhuo Fu"],"pdf_url":"https://arxiv.org/pdf/2312.07255v1.pdf","comment":"17pages, 8 figures, 22 tables, Work in progress"},{"id":"http://arxiv.org/abs/2305.10124v2","updated":"2023-12-12T13:25:06Z","published":"2023-05-17T11:08:13Z","title":"Principal Uncertainty Quantification with Spatial Correlation for Image\n  Restoration Problems","summary":"  Uncertainty quantification for inverse problems in imaging has drawn much\nattention lately. Existing approaches towards this task define uncertainty\nregions based on probable values per pixel, while ignoring spatial correlations\nwithin the image, resulting in an exaggerated volume of uncertainty. In this\npaper, we propose PUQ (Principal Uncertainty Quantification) -- a novel\ndefinition and corresponding analysis of uncertainty regions that takes into\naccount spatial relationships within the image, thus providing reduced volume\nregions. Using recent advancements in generative models, we derive uncertainty\nintervals around principal components of the empirical posterior distribution,\nforming an ambiguity region that guarantees the inclusion of true unseen values\nwith a user-defined confidence probability. To improve computational efficiency\nand interpretability, we also guarantee the recovery of true unseen values\nusing only a few principal directions, resulting in more informative\nuncertainty regions. Our approach is verified through experiments on image\ncolorization, super-resolution, and inpainting; its effectiveness is shown\nthrough comparison to baseline methods, demonstrating significantly tighter\nuncertainty regions.\n","authors":["Omer Belhasin","Yaniv Romano","Daniel Freedman","Ehud Rivlin","Michael Elad"],"pdf_url":"https://arxiv.org/pdf/2305.10124v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07246v1","updated":"2023-12-12T13:22:44Z","published":"2023-12-12T13:22:44Z","title":"Unifying Correspondence, Pose and NeRF for Pose-Free Novel View\n  Synthesis from Stereo Pairs","summary":"  This work delves into the task of pose-free novel view synthesis from stereo\npairs, a challenging and pioneering task in 3D vision. Our innovative\nframework, unlike any before, seamlessly integrates 2D correspondence matching,\ncamera pose estimation, and NeRF rendering, fostering a synergistic enhancement\nof these tasks. We achieve this through designing an architecture that utilizes\na shared representation, which serves as a foundation for enhanced 3D geometry\nunderstanding. Capitalizing on the inherent interplay between the tasks, our\nunified framework is trained end-to-end with the proposed training strategy to\nimprove overall model accuracy. Through extensive evaluations across diverse\nindoor and outdoor scenes from two real-world datasets, we demonstrate that our\napproach achieves substantial improvement over previous methodologies,\nespecially in scenarios characterized by extreme viewpoint changes and the\nabsence of accurate camera poses.\n","authors":["Sunghwan Hong","Jaewoo Jung","Heeseong Shin","Jiaolong Yang","Seungryong Kim","Chong Luo"],"pdf_url":"https://arxiv.org/pdf/2312.07246v1.pdf","comment":"Project page: https://ku-cvlab.github.io/CoPoNeRF/"},{"id":"http://arxiv.org/abs/2312.07245v1","updated":"2023-12-12T13:21:03Z","published":"2023-12-12T13:21:03Z","title":"DTA: Distribution Transform-based Attack for Query-Limited Scenario","summary":"  In generating adversarial examples, the conventional black-box attack methods\nrely on sufficient feedback from the to-be-attacked models by repeatedly\nquerying until the attack is successful, which usually results in thousands of\ntrials during an attack. This may be unacceptable in real applications since\nMachine Learning as a Service Platform (MLaaS) usually only returns the final\nresult (i.e., hard-label) to the client and a system equipped with certain\ndefense mechanisms could easily detect malicious queries. By contrast, a\nfeasible way is a hard-label attack that simulates an attacked action being\npermitted to conduct a limited number of queries. To implement this idea, in\nthis paper, we bypass the dependency on the to-be-attacked model and benefit\nfrom the characteristics of the distributions of adversarial examples to\nreformulate the attack problem in a distribution transform manner and propose a\ndistribution transform-based attack (DTA). DTA builds a statistical mapping\nfrom the benign example to its adversarial counterparts by tackling the\nconditional likelihood under the hard-label black-box settings. In this way, it\nis no longer necessary to query the target model frequently. A well-trained DTA\nmodel can directly and efficiently generate a batch of adversarial examples for\na certain input, which can be used to attack un-seen models based on the\nassumed transferability. Furthermore, we surprisingly find that the\nwell-trained DTA model is not sensitive to the semantic spaces of the training\ndataset, meaning that the model yields acceptable attack performance on other\ndatasets. Extensive experiments validate the effectiveness of the proposed idea\nand the superiority of DTA over the state-of-the-art.\n","authors":["Renyang Liu","Wei Zhou","Xin Jin","Song Gao","Yuanyu Wang","Ruxin Wang"],"pdf_url":"https://arxiv.org/pdf/2312.07245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.18130v2","updated":"2023-12-12T13:09:46Z","published":"2023-11-29T22:44:32Z","title":"The Trifecta: Three simple techniques for training deeper\n  Forward-Forward networks","summary":"  Modern machine learning models are able to outperform humans on a variety of\nnon-trivial tasks. However, as the complexity of the models increases, they\nconsume significant amounts of power and still struggle to generalize\neffectively to unseen data. Local learning, which focuses on updating subsets\nof a model's parameters at a time, has emerged as a promising technique to\naddress these issues. Recently, a novel local learning algorithm, called\nForward-Forward, has received widespread attention due to its innovative\napproach to learning. Unfortunately, its application has been limited to\nsmaller datasets due to scalability issues. To this end, we propose The\nTrifecta, a collection of three simple techniques that synergize exceptionally\nwell and drastically improve the Forward-Forward algorithm on deeper networks.\nOur experiments demonstrate that our models are on par with similarly\nstructured, backpropagation-based models in both training speed and test\naccuracy on simple datasets. This is achieved by the ability to learn\nrepresentations that are informative locally, on a layer-by-layer basis, and\nretain their informativeness when propagated to deeper layers in the\narchitecture. This leads to around 84% accuracy on CIFAR-10, a notable\nimprovement (25%) over the original FF algorithm. These results highlight the\npotential of Forward-Forward as a genuine competitor to backpropagation and as\na promising research avenue.\n","authors":["Thomas Dooms","Ing Jyh Tsang","Jose Oramas"],"pdf_url":"https://arxiv.org/pdf/2311.18130v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04816v3","updated":"2023-12-12T13:06:33Z","published":"2023-10-07T14:13:14Z","title":"Hacking Generative Models with Differentiable Network Bending","summary":"  In this work, we propose a method to 'hack' generative models, pushing their\noutputs away from the original training distribution towards a new objective.\nWe inject a small-scale trainable module between the intermediate layers of the\nmodel and train it for a low number of iterations, keeping the rest of the\nnetwork frozen. The resulting output images display an uncanny quality, given\nby the tension between the original and new objectives that can be exploited\nfor artistic purposes.\n","authors":["Giacomo Aldegheri","Alina Rogalska","Ahmed Youssef","Eugenia Iofinova"],"pdf_url":"https://arxiv.org/pdf/2310.04816v3.pdf","comment":"12 pages, 10 figures, Machine Learning for Creativity and Design\n  Workshop at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2312.06643v2","updated":"2023-12-12T12:52:32Z","published":"2023-12-11T18:56:03Z","title":"Gaze Detection and Analysis for Initiating Joint Activity in Industrial\n  Human-Robot Collaboration","summary":"  Collaborative robots (cobots) are widely used in industrial applications, yet\nextensive research is still needed to enhance human-robot collaborations and\noperator experience. A potential approach to improve the collaboration\nexperience involves adapting cobot behavior based on natural cues from the\noperator. Inspired by the literature on human-human interactions, we conducted\na wizard-of-oz study to examine whether a gaze towards the cobot can serve as a\ntrigger for initiating joint activities in collaborative sessions. In this\nstudy, 37 participants engaged in an assembly task while their gaze behavior\nwas analyzed. We employ a gaze-based attention recognition model to identify\nwhen the participants look at the cobot. Our results indicate that in most\ncases (84.88\\%), the joint activity is preceded by a gaze towards the cobot.\nFurthermore, during the entire assembly cycle, the participants tend to look at\nthe cobot around the time of the joint activity. To the best of our knowledge,\nthis is the first study to analyze the natural gaze behavior of participants\nworking on a joint activity with a robot during a collaborative assembly task.\n","authors":["Pooja Prajod","Matteo Lavit Nicora","Marta Mondellini","Giovanni Tauro","Rocco Vertechy","Matteo Malosio","Elisabeth André"],"pdf_url":"https://arxiv.org/pdf/2312.06643v2.pdf","comment":"First draft for a paper submitted to Frontiers in Robotics and AI"},{"id":"http://arxiv.org/abs/2312.07231v1","updated":"2023-12-12T12:50:33Z","published":"2023-12-12T12:50:33Z","title":"Fast Training of Diffusion Transformer with Extreme Masking for 3D Point\n  Clouds Generation","summary":"  Diffusion Transformers have recently shown remarkable effectiveness in\ngenerating high-quality 3D point clouds. However, training voxel-based\ndiffusion models for high-resolution 3D voxels remains prohibitively expensive\ndue to the cubic complexity of attention operators, which arises from the\nadditional dimension of voxels. Motivated by the inherent redundancy of 3D\ncompared to 2D, we propose FastDiT-3D, a novel masked diffusion transformer\ntailored for efficient 3D point cloud generation, which greatly reduces\ntraining costs. Specifically, we draw inspiration from masked autoencoders to\ndynamically operate the denoising process on masked voxelized point clouds. We\nalso propose a novel voxel-aware masking strategy to adaptively aggregate\nbackground/foreground information from voxelized point clouds. Our method\nachieves state-of-the-art performance with an extreme masking ratio of nearly\n99%. Moreover, to improve multi-category 3D generation, we introduce\nMixture-of-Expert (MoE) in 3D diffusion model. Each category can learn a\ndistinct diffusion path with different experts, relieving gradient conflict.\nExperimental results on the ShapeNet dataset demonstrate that our method\nachieves state-of-the-art high-fidelity and diverse 3D point cloud generation\nperformance. Our FastDiT-3D improves 1-Nearest Neighbor Accuracy and Coverage\nmetrics when generating 128-resolution voxel point clouds, using only 6.5% of\nthe original training cost.\n","authors":["Shentong Mo","Enze Xie","Yue Wu","Junsong Chen","Matthias Nießner","Zhenguo Li"],"pdf_url":"https://arxiv.org/pdf/2312.07231v1.pdf","comment":"Project Page: https://dit-3d.github.io/FastDiT-3D/"},{"id":"http://arxiv.org/abs/2312.07226v1","updated":"2023-12-12T12:41:35Z","published":"2023-12-12T12:41:35Z","title":"Super-Resolution on Rotationally Scanned Photoacoustic Microscopy Images\n  Incorporating Scanning Prior","summary":"  Photoacoustic Microscopy (PAM) images integrating the advantages of optical\ncontrast and acoustic resolution have been widely used in brain studies.\nHowever, there exists a trade-off between scanning speed and image resolution.\nCompared with traditional raster scanning, rotational scanning provides good\nopportunities for fast PAM imaging by optimizing the scanning mechanism.\nRecently, there is a trend to incorporate deep learning into the scanning\nprocess to further increase the scanning speed.Yet, most such attempts are\nperformed for raster scanning while those for rotational scanning are\nrelatively rare. In this study, we propose a novel and well-performing\nsuper-resolution framework for rotational scanning-based PAM imaging. To\neliminate adjacent rows' displacements due to subject motion or high-frequency\nscanning distortion,we introduce a registration module across odd and even rows\nin the preprocessing and incorporate displacement degradation in the training.\nBesides, gradient-based patch selection is proposed to increase the probability\nof blood vessel patches being selected for training. A Transformer-based\nnetwork with a global receptive field is applied for better performance.\nExperimental results on both synthetic and real datasets demonstrate the\neffectiveness and generalizability of our proposed framework for rotationally\nscanned PAM images'super-resolution, both quantitatively and qualitatively.\nCode is available at https://github.com/11710615/PAMSR.git.\n","authors":["Kai Pan","Linyang Li","Li Lin","Pujin Cheng","Junyan Lyu","Lei Xi","Xiaoyin Tang"],"pdf_url":"https://arxiv.org/pdf/2312.07226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.13747v2","updated":"2023-12-12T12:36:26Z","published":"2023-09-24T20:32:23Z","title":"Look Ma, no code: fine tuning nnU-Net for the AutoPET II challenge by\n  only adjusting its JSON plans","summary":"  We participate in the AutoPET II challenge by modifying nnU-Net only through\nits easy to understand and modify 'nnUNetPlans.json' file. By switching to a\nUNet with residual encoder, increasing the batch size and increasing the patch\nsize we obtain a configuration that substantially outperforms the automatically\nconfigured nnU-Net baseline (5-fold cross-validation Dice score of 65.14 vs\n33.28) at the expense of increased compute requirements for model training. Our\nfinal submission ensembles the two most promising configurations.\n","authors":["Fabian Isensee","Klaus H. Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2309.13747v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07221v1","updated":"2023-12-12T12:35:59Z","published":"2023-12-12T12:35:59Z","title":"Transferring CLIP's Knowledge into Zero-Shot Point Cloud Semantic\n  Segmentation","summary":"  Traditional 3D segmentation methods can only recognize a fixed range of\nclasses that appear in the training set, which limits their application in\nreal-world scenarios due to the lack of generalization ability. Large-scale\nvisual-language pre-trained models, such as CLIP, have shown their\ngeneralization ability in the zero-shot 2D vision tasks, but are still unable\nto be applied to 3D semantic segmentation directly. In this work, we focus on\nzero-shot point cloud semantic segmentation and propose a simple yet effective\nbaseline to transfer the visual-linguistic knowledge implied in CLIP to point\ncloud encoder at both feature and output levels. Both feature-level and\noutput-level alignments are conducted between 2D and 3D encoders for effective\nknowledge transfer. Concretely, a Multi-granularity Cross-modal Feature\nAlignment (MCFA) module is proposed to align 2D and 3D features from global\nsemantic and local position perspectives for feature-level alignment. For the\noutput level, per-pixel pseudo labels of unseen classes are extracted using the\npre-trained CLIP model as supervision for the 3D segmentation model to mimic\nthe behavior of the CLIP image encoder. Extensive experiments are conducted on\ntwo popular benchmarks of point cloud segmentation. Our method outperforms\nsignificantly previous state-of-the-art methods under zero-shot setting (+29.2%\nmIoU on SemanticKITTI and 31.8% mIoU on nuScenes), and further achieves\npromising results in the annotation-free point cloud semantic segmentation\nsetting, showing its great potential for label-efficient learning.\n","authors":["Yuanbin Wang","Shaofei Huang","Yulu Gao","Zhen Wang","Rui Wang","Kehua Sheng","Bo Zhang","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2312.07221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07207v1","updated":"2023-12-12T12:20:27Z","published":"2023-12-12T12:20:27Z","title":"MCFNet: Multi-scale Covariance Feature Fusion Network for Real-time\n  Semantic Segmentation","summary":"  The low-level spatial detail information and high-level semantic abstract\ninformation are both essential to the semantic segmentation task. The features\nextracted by the deep network can obtain rich semantic information, while a lot\nof spatial information is lost. However, how to recover spatial detail\ninformation effectively and fuse it with high-level semantics has not been well\naddressed so far. In this paper, we propose a new architecture based on\nBilateral Segmentation Network (BiseNet) called Multi-scale Covariance Feature\nFusion Network (MCFNet). Specifically, this network introduces a new feature\nrefinement module and a new feature fusion module. Furthermore, a gating unit\nnamed L-Gate is proposed to filter out invalid information and fuse multi-scale\nfeatures. We evaluate our proposed model on Cityscapes, CamVid datasets and\ncompare it with the state-of-the-art methods. Extensive experiments show that\nour method achieves competitive success. On Cityscapes, we achieve 75.5% mIOU\nwith a speed of 151.3 FPS.\n","authors":["Xiaojie Fang","Xingguo Song","Xiangyin Meng","Xu Fang","Sheng Jin"],"pdf_url":"https://arxiv.org/pdf/2312.07207v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07199v1","updated":"2023-12-12T12:07:34Z","published":"2023-12-12T12:07:34Z","title":"SeasFire as a Multivariate Earth System Datacube for Wildfire Dynamics","summary":"  The global occurrence, scale, and frequency of wildfires pose significant\nthreats to ecosystem services and human livelihoods. To effectively quantify\nand attribute the antecedent conditions for wildfires, a thorough understanding\nof Earth system dynamics is imperative. In response, we introduce the SeasFire\ndatacube, a meticulously curated spatiotemporal dataset tailored for global\nsub-seasonal to seasonal wildfire modeling via Earth observation. The SeasFire\ndatacube comprises of 59 variables encompassing climate, vegetation, oceanic\nindices, and human factors, has an 8-day temporal resolution and a spatial\nresolution of 0.25 degrees, and spans from 2001 to 2021. We showcase the\nversatility of SeasFire for exploring the variability and seasonality of\nwildfire drivers, modeling causal links between ocean-climate teleconnections\nand wildfires, and predicting sub-seasonal wildfire patterns across multiple\ntimescales with a Deep Learning model. We publicly release the SeasFire\ndatacube and appeal to Earth system scientists and Machine Learning\npractitioners to use it for an improved understanding and anticipation of\nwildfires.\n","authors":["Ilektra Karasante","Lazaro Alonso","Ioannis Prapas","Akanksha Ahuja","Nuno Carvalhais","Ioannis Papoutsis"],"pdf_url":"https://arxiv.org/pdf/2312.07199v1.pdf","comment":"20 pages, 9 figures, and 5 tables"},{"id":"http://arxiv.org/abs/2312.07190v1","updated":"2023-12-12T11:53:23Z","published":"2023-12-12T11:53:23Z","title":"Noised Autoencoders for Point Annotation Restoration in Object Counting","summary":"  Object counting is a field of growing importance in domains such as security\nsurveillance, urban planning, and biology. The annotation is usually provided\nin terms of 2D points. However, the complexity of object shapes and subjective\nof annotators may lead to annotation inconsistency, potentially confusing the\nmodel during training. To alleviate this issue, we introduce the Noised\nAutoencoders (NAE) methodology, which extracts general positional knowledge\nfrom all annotations. The method involves adding random offsets to initial\npoint annotations, followed by a UNet to restore them to their original\npositions. Similar to MAE, NAE faces challenges in restoring non-generic\npoints, necessitating reliance on the most common positions inferred from\ngeneral knowledge. This reliance forms the cornerstone of our method's\neffectiveness. Different from existing noise-resistance methods, our approach\nfocus on directly improving initial point annotations. Extensive experiments\nshow that NAE yields more consistent annotations compared to the original ones,\nsteadily enhancing the performance of advanced models trained with these\nrevised annotations. \\textbf{Remarkably, the proposed approach helps to set new\nrecords in nine datasets}. We will make the NAE codes and refined point\nannotations available.\n","authors":["Yuda Zou","Xin Xiao","Peilin Zhou","Zhichao Sun","Bo Du","Yongchao Xu"],"pdf_url":"https://arxiv.org/pdf/2312.07190v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.02099v3","updated":"2023-12-12T11:41:39Z","published":"2023-06-03T12:23:17Z","title":"Enhancing Surface Neural Implicits with Curvature-Guided Sampling and\n  Uncertainty-Augmented Representations","summary":"  Neural implicits have become popular for representing surfaces because they\noffer an adaptive resolution and support arbitrary topologies. While previous\nworks rely on ground truth point clouds, they often ignore the effect of input\nquality and sampling methods during reconstructing process. In this paper, we\nintroduce a sampling method with an uncertainty-augmented surface implicit\nrepresentation that employs a sampling technique that considers the geometric\ncharacteristics of inputs. To this end, we introduce a strategy that\nefficiently computes differentiable geometric features, namely, mean\ncurvatures, to augment the sampling phase during the training period. The\nuncertainty augmentation offers insights into the occupancy and reliability of\nthe output signed distance value, thereby expanding representation capabilities\ninto open surfaces. Finally, we demonstrate that our method leads to\nstate-of-the-art reconstructions on both synthetic and real-world data.\n","authors":["Lu Sang","Abhishek Saroha","Maolin Gao","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2306.02099v3.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2306.12153v2","updated":"2023-12-12T11:33:37Z","published":"2023-06-21T10:03:56Z","title":"DIAS: A Comprehensive Dataset and Benchmark for Intracranial Artery\n  Segmentation in DSA sequences","summary":"  Digital subtraction angiography (DSA) is universally acknowledged as the gold\nstandard for examining lesion angioarchitecture, elucidating arterial blood\nsupply dynamics, and guiding endovascular interventions. The automatic\nsegmentation of intracranial arteries (IA) in DSA, which is pivotal for\nquantifying vascular morphology, plays an essential role in computer-assisted\nstroke research and clinical practices. Nevertheless, research in this specific\ndomain remains constrained, primarily owing to the unavailability of publicly\ndatasets for IA segmentation within the research community. Currently, the\npredominant focus of methodologies lies in the segmentation of single-frame DSA\nusing in-house datasets. These methods, limited by the partial inclusion of\ncontrast in single-frame DSA, encounters challenges in rendering a precise\nrepresentation of vascular structures. In this paper, we introduces DIAS, a\ndataset specifically developed for IA segmentation in DSA sequences. A\ncomprehensive benchmark has been established for evaluating DIAS, covering\nfully, weakly, and semi-supervised segmentation methods. Specifically, we\npropose a vessel sequence segmentation network that captures the spatiotemporal\nrepresentation of intravascular contrast for segmenting vessels in DSA\nsequences. For weakly-supervised learning, we propose a novel scribble\nlearning-based image segmentation framework, incorporating both scribble\nsupervision and consistency regularization. Furthermore, we introduce a random\npatch-based self-training framework that harnesses unlabeled DSA sequences to\nimprove segmentation performance. Our extensive experiments on the DIAS dataset\ndemonstrate the effectiveness of these methods as potential baselines for\nfuture research and clinical applications.\n","authors":["Wentao Liu","Tong Tian","Lemeng Wang","Weijin Xu","Haoyuan Li","Wenyi Zhao","Siyu Tian","Xipeng Pan","Huihua Yang","Feng Gao","Yiming Deng","Ruisheng Su"],"pdf_url":"https://arxiv.org/pdf/2306.12153v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2103.09151v8","updated":"2023-12-12T11:27:44Z","published":"2021-03-16T15:47:34Z","title":"Adversarial Driving: Attacking End-to-End Autonomous Driving","summary":"  As research in deep neural networks advances, deep convolutional networks\nbecome promising for autonomous driving tasks. In particular, there is an\nemerging trend of employing end-to-end neural network models for autonomous\ndriving. However, previous research has shown that deep neural network\nclassifiers are vulnerable to adversarial attacks. While for regression tasks,\nthe effect of adversarial attacks is not as well understood. In this research,\nwe devise two white-box targeted attacks against end-to-end autonomous driving\nmodels. Our attacks manipulate the behavior of the autonomous driving system by\nperturbing the input image. In an average of 800 attacks with the same attack\nstrength (epsilon=1), the image-specific and image-agnostic attack deviates the\nsteering angle from the original output by 0.478 and 0.111, respectively, which\nis much stronger than random noises that only perturbs the steering angle by\n0.002 (The steering angle ranges from [-1, 1]). Both attacks can be initiated\nin real-time on CPUs without employing GPUs. Demo video:\nhttps://youtu.be/I0i8uN2oOP0.\n","authors":["Han Wu","Syed Yunas","Sareh Rowlands","Wenjie Ruan","Johan Wahlstrom"],"pdf_url":"https://arxiv.org/pdf/2103.09151v8.pdf","comment":"Accepted by IEEE Intelligent Vehicle Symposium, 2023"},{"id":"http://arxiv.org/abs/2209.01962v6","updated":"2023-12-12T11:27:29Z","published":"2022-09-05T13:32:41Z","title":"Adversarial Detection: Attacking Object Detection in Real Time","summary":"  Intelligent robots rely on object detection models to perceive the\nenvironment. Following advances in deep learning security it has been revealed\nthat object detection models are vulnerable to adversarial attacks. However,\nprior research primarily focuses on attacking static images or offline videos.\nTherefore, it is still unclear if such attacks could jeopardize real-world\nrobotic applications in dynamic environments. This paper bridges this gap by\npresenting the first real-time online attack against object detection models.\nWe devise three attacks that fabricate bounding boxes for nonexistent objects\nat desired locations. The attacks achieve a success rate of about 90% within\nabout 20 iterations. The demo video is available at\nhttps://youtu.be/zJZ1aNlXsMU.\n","authors":["Han Wu","Syed Yunas","Sareh Rowlands","Wenjie Ruan","Johan Wahlstrom"],"pdf_url":"https://arxiv.org/pdf/2209.01962v6.pdf","comment":"Accepted by IEEE Intelligent Vehicle Symposium, 2023"},{"id":"http://arxiv.org/abs/2312.07180v1","updated":"2023-12-12T11:27:13Z","published":"2023-12-12T11:27:13Z","title":"Context-Aware Iteration Policy Network for Efficient Optical Flow\n  Estimation","summary":"  Existing recurrent optical flow estimation networks are computationally\nexpensive since they use a fixed large number of iterations to update the flow\nfield for each sample. An efficient network should skip iterations when the\nflow improvement is limited. In this paper, we develop a Context-Aware\nIteration Policy Network for efficient optical flow estimation, which\ndetermines the optimal number of iterations per sample. The policy network\nachieves this by learning contextual information to realize whether flow\nimprovement is bottlenecked or minimal. On the one hand, we use iteration\nembedding and historical hidden cell, which include previous iterations\ninformation, to convey how flow has changed from previous iterations. On the\nother hand, we use the incremental loss to make the policy network implicitly\nperceive the magnitude of optical flow improvement in the subsequent iteration.\nFurthermore, the computational complexity in our dynamic network is\ncontrollable, allowing us to satisfy various resource preferences with a single\ntrained model. Our policy network can be easily integrated into\nstate-of-the-art optical flow networks. Extensive experiments show that our\nmethod maintains performance while reducing FLOPs by about 40%/20% for the\nSintel/KITTI datasets.\n","authors":["Ri Cheng","Ruian He","Xuhao Jiang","Shili Zhou","Weimin Tan","Bo Yan"],"pdf_url":"https://arxiv.org/pdf/2312.07180v1.pdf","comment":"2024, Association for the Advancement of Artificial Intelligence"},{"id":"http://arxiv.org/abs/2312.07169v1","updated":"2023-12-12T11:13:17Z","published":"2023-12-12T11:13:17Z","title":"Semi-supervised Active Learning for Video Action Detection","summary":"  In this work, we focus on label efficient learning for video action\ndetection. We develop a novel semi-supervised active learning approach which\nutilizes both labeled as well as unlabeled data along with informative sample\nselection for action detection. Video action detection requires spatio-temporal\nlocalization along with classification, which poses several challenges for both\nactive learning informative sample selection as well as semi-supervised\nlearning pseudo label generation. First, we propose NoiseAug, a simple\naugmentation strategy which effectively selects informative samples for video\naction detection. Next, we propose fft-attention, a novel technique based on\nhigh-pass filtering which enables effective utilization of pseudo label for SSL\nin video action detection by emphasizing on relevant activity region within a\nvideo. We evaluate the proposed approach on three different benchmark datasets,\nUCF-101-24, JHMDB-21, and Youtube-VOS. First, we demonstrate its effectiveness\non video action detection where the proposed approach outperforms prior works\nin semi-supervised and weakly-supervised learning along with several baseline\napproaches in both UCF101-24 and JHMDB-21. Next, we also show its effectiveness\non Youtube-VOS for video object segmentation demonstrating its generalization\ncapability for other dense prediction tasks in videos.\n","authors":["Aayush Singh","Aayush J Rana","Akash Kumar","Shruti Vyas","Yogesh Singh Rawat"],"pdf_url":"https://arxiv.org/pdf/2312.07169v1.pdf","comment":"AAAI'24 Main Conference"},{"id":"http://arxiv.org/abs/2312.07165v1","updated":"2023-12-12T11:03:51Z","published":"2023-12-12T11:03:51Z","title":"Language-Guided Transformer for Federated Multi-Label Classification","summary":"  Federated Learning (FL) is an emerging paradigm that enables multiple users\nto collaboratively train a robust model in a privacy-preserving manner without\nsharing their private data. Most existing approaches of FL only consider\ntraditional single-label image classification, ignoring the impact when\ntransferring the task to multi-label image classification. Nevertheless, it is\nstill challenging for FL to deal with user heterogeneity in their local data\ndistribution in the real-world FL scenario, and this issue becomes even more\nsevere in multi-label image classification. Inspired by the recent success of\nTransformers in centralized settings, we propose a novel FL framework for\nmulti-label classification. Since partial label correlation may be observed by\nlocal clients during training, direct aggregation of locally updated models\nwould not produce satisfactory performances. Thus, we propose a novel FL\nframework of Language-Guided Transformer (FedLGT) to tackle this challenging\ntask, which aims to exploit and transfer knowledge across different clients for\nlearning a robust global model. Through extensive experiments on various\nmulti-label datasets (e.g., FLAIR, MS-COCO, etc.), we show that our FedLGT is\nable to achieve satisfactory performance and outperforms standard FL techniques\nunder multi-label FL scenarios. Code is available at\nhttps://github.com/Jack24658735/FedLGT.\n","authors":["I-Jieh Liu","Ci-Siang Lin","Fu-En Yang","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2312.07165v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2310.15646v2","updated":"2023-12-12T11:01:45Z","published":"2023-10-24T09:07:47Z","title":"Mean Teacher DETR with Masked Feature Alignment: A Robust Domain\n  Adaptive Detection Transformer Framework","summary":"  Unsupervised domain adaptation object detection(UDAOD) research on Detection\nTransformer(DETR) mainly focuses on feature alignment and existing methods can\nbe divided into two kinds, each of which has its unresolved issues. One-stage\nfeature alignment methods can easily lead to performance fluctuation and\ntraining stagnation. Two-stage feature alignment method based on mean teacher\ncomprises a pretraining stage followed by a self-training stage, each facing\nproblems in obtaining reliable pretrained model and achieving consistent\nperformance gains. Methods mentioned above have not yet explore how to utilize\nthe third related domain such as target-like domain to assist adaptation. To\naddress these issues, we propose a two-stage framework named MTM, i.e. Mean\nTeacher-DETR with Masked Feature Alignment. In the pretraining stage, we\nutilize labeled target-like images produced by image style transfer to avoid\nperformance fluctuation. In the self-training stage, we leverage unlabeled\ntarget images by pseudo labels based on mean teacher and propose a module\ncalled Object Queries Knowledge Transfer(OQKT) to ensure consistent performance\ngains of the student model. Most importantly, we propose masked feature\nalignment methods including Masked Domain Query-based Feature Alignment(MDQFA)\nand Masked Token-wise Feature Alignment(MTWFA) to alleviate domain shift in a\nmore robust way, which not only prevent training stagnation and lead to a\nrobust pretrained model in the pretraining stage, but also enhance the model's\ntarget performance in the self-training stage. Experiments on three challenging\nscenarios and a theoretical analysis verify the effectiveness of MTM.\n","authors":["Weixi Weng","Chun Yuan"],"pdf_url":"https://arxiv.org/pdf/2310.15646v2.pdf","comment":"AAAI2024"},{"id":"http://arxiv.org/abs/2305.16318v2","updated":"2023-12-12T10:42:46Z","published":"2023-05-25T17:59:47Z","title":"Referred by Multi-Modality: A Unified Temporal Transformer for Video\n  Object Segmentation","summary":"  Recently, video object segmentation (VOS) referred by multi-modal signals,\ne.g., language and audio, has evoked increasing attention in both industry and\nacademia. It is challenging for exploring the semantic alignment within\nmodalities and the visual correspondence across frames. However, existing\nmethods adopt separate network architectures for different modalities, and\nneglect the inter-frame temporal interaction with references. In this paper, we\npropose MUTR, a Multi-modal Unified Temporal transformer for Referring video\nobject segmentation. With a unified framework for the first time, MUTR adopts a\nDETR-style transformer and is capable of segmenting video objects designated by\neither text or audio reference. Specifically, we introduce two strategies to\nfully explore the temporal relations between videos and multi-modal signals.\nFirstly, for low-level temporal aggregation before the transformer, we enable\nthe multi-modal references to capture multi-scale visual cues from consecutive\nvideo frames. This effectively endows the text or audio signals with temporal\nknowledge and boosts the semantic alignment between modalities. Secondly, for\nhigh-level temporal interaction after the transformer, we conduct inter-frame\nfeature communication for different object embeddings, contributing to better\nobject-wise correspondence for tracking along the video. On Ref-YouTube-VOS and\nAVSBench datasets with respective text and audio references, MUTR achieves\n+4.2% and +8.7% J&F improvements to state-of-the-art methods, demonstrating our\nsignificance for unified multi-modal VOS. Code is released at\nhttps://github.com/OpenGVLab/MUTR.\n","authors":["Shilin Yan","Renrui Zhang","Ziyu Guo","Wenchao Chen","Wei Zhang","Hongyang Li","Yu Qiao","Hao Dong","Zhongjiang He","Peng Gao"],"pdf_url":"https://arxiv.org/pdf/2305.16318v2.pdf","comment":"Accepted by AAAI 2024. Code is released at\n  https://github.com/OpenGVLab/MUTR"},{"id":"http://arxiv.org/abs/2209.05166v4","updated":"2023-12-12T10:42:26Z","published":"2022-09-12T11:51:08Z","title":"Exploring Domain Incremental Video Highlights Detection with the\n  LiveFood Benchmark","summary":"  Video highlights detection (VHD) is an active research field in computer\nvision, aiming to locate the most user-appealing clips given raw video inputs.\nHowever, most VHD methods are based on the closed world assumption, i.e., a\nfixed number of highlight categories is defined in advance and all training\ndata are available beforehand. Consequently, existing methods have poor\nscalability with respect to increasing highlight domains and training data. To\naddress above issues, we propose a novel video highlights detection method\nnamed Global Prototype Encoding (GPE) to learn incrementally for adapting to\nnew domains via parameterized prototypes. To facilitate this new research\ndirection, we collect a finely annotated dataset termed LiveFood, including\nover 5,100 live gourmet videos that consist of four domains: ingredients,\ncooking, presentation, and eating. To the best of our knowledge, this is the\nfirst work to explore video highlights detection in the incremental learning\nsetting, opening up new land to apply VHD for practical scenarios where both\nthe concerned highlight domains and training data increase over time. We\ndemonstrate the effectiveness of GPE through extensive experiments. Notably,\nGPE surpasses popular domain incremental learning methods on LiveFood,\nachieving significant mAP improvements on all domains. Concerning the classic\ndatasets, GPE also yields comparable performance as previous arts. The code is\navailable at: https://github.com/ForeverPs/IncrementalVHD_GPE.\n","authors":["Sen Pei","Shixiong Xu","Xiaojie Jin"],"pdf_url":"https://arxiv.org/pdf/2209.05166v4.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2312.03018v3","updated":"2023-12-12T10:40:34Z","published":"2023-12-05T03:16:31Z","title":"DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention\n  and Text Guidance","summary":"  Image-to-video generation, which aims to generate a video starting from a\ngiven reference image, has drawn great attention. Existing methods try to\nextend pre-trained text-guided image diffusion models to image-guided video\ngeneration models. Nevertheless, these methods often result in either low\nfidelity or flickering over time due to their limitation to shallow image\nguidance and poor temporal consistency. To tackle these problems, we propose a\nhigh-fidelity image-to-video generation method by devising a frame retention\nbranch based on a pre-trained video diffusion model, named DreamVideo. Instead\nof integrating the reference image into the diffusion process at a semantic\nlevel, our DreamVideo perceives the reference image via convolution layers and\nconcatenates the features with the noisy latents as model input. By this means,\nthe details of the reference image can be preserved to the greatest extent. In\naddition, by incorporating double-condition classifier-free guidance, a single\nimage can be directed to videos of different actions by providing varying\nprompt texts. This has significant implications for controllable video\ngeneration and holds broad application prospects. We conduct comprehensive\nexperiments on the public dataset, and both quantitative and qualitative\nresults indicate that our method outperforms the state-of-the-art method.\nEspecially for fidelity, our model has a powerful image retention ability and\ndelivers the best results in UCF101 compared to other image-to-video models to\nour best knowledge. Also, precise control can be achieved by giving different\ntext prompts. Further details and comprehensive results of our model will be\npresented in https://anonymous0769.github.io/DreamVideo/.\n","authors":["Cong Wang","Jiaxi Gu","Panwen Hu","Songcen Xu","Hang Xu","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2312.03018v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.04669v3","updated":"2023-12-12T10:30:59Z","published":"2023-08-09T02:27:23Z","title":"A General Implicit Framework for Fast NeRF Composition and Rendering","summary":"  A variety of Neural Radiance Fields (NeRF) methods have recently achieved\nremarkable success in high render speed. However, current accelerating methods\nare specialized and incompatible with various implicit methods, preventing\nreal-time composition over various types of NeRF works. Because NeRF relies on\nsampling along rays, it is possible to provide general guidance for\nacceleration. To that end, we propose a general implicit pipeline for composing\nNeRF objects quickly. Our method enables the casting of dynamic shadows within\nor between objects using analytical light sources while allowing multiple NeRF\nobjects to be seamlessly placed and rendered together with any arbitrary rigid\ntransformations. Mainly, our work introduces a new surface representation known\nas Neural Depth Fields (NeDF) that quickly determines the spatial relationship\nbetween objects by allowing direct intersection computation between rays and\nimplicit surfaces. It leverages an intersection neural network to query NeRF\nfor acceleration instead of depending on an explicit spatial structure.Our\nproposed method is the first to enable both the progressive and interactive\ncomposition of NeRF objects. Additionally, it also serves as a previewing\nplugin for a range of existing NeRF works.\n","authors":["Xinyu Gao","Ziyi Yang","Yunlu Zhao","Yuxiang Sun","Xiaogang Jin","Changqing Zou"],"pdf_url":"https://arxiv.org/pdf/2308.04669v3.pdf","comment":"7 pages for main content"},{"id":"http://arxiv.org/abs/2312.05924v2","updated":"2023-12-12T10:21:12Z","published":"2023-12-10T16:14:02Z","title":"Data-Free Hard-Label Robustness Stealing Attack","summary":"  The popularity of Machine Learning as a Service (MLaaS) has led to increased\nconcerns about Model Stealing Attacks (MSA), which aim to craft a clone model\nby querying MLaaS. Currently, most research on MSA assumes that MLaaS can\nprovide soft labels and that the attacker has a proxy dataset with a similar\ndistribution. However, this fails to encapsulate the more practical scenario\nwhere only hard labels are returned by MLaaS and the data distribution remains\nelusive. Furthermore, most existing work focuses solely on stealing the model\naccuracy, neglecting the model robustness, while robustness is essential in\nsecurity-sensitive scenarios, e.g., face-scan payment. Notably, improving model\nrobustness often necessitates the use of expensive techniques such as\nadversarial training, thereby further making stealing robustness a more\nlucrative prospect. In response to these identified gaps, we introduce a novel\nData-Free Hard-Label Robustness Stealing (DFHL-RS) attack in this paper, which\nenables the stealing of both model accuracy and robustness by simply querying\nhard labels of the target model without the help of any natural data.\nComprehensive experiments demonstrate the effectiveness of our method. The\nclone model achieves a clean accuracy of 77.86% and a robust accuracy of 39.51%\nagainst AutoAttack, which are only 4.71% and 8.40% lower than the target model\non the CIFAR-10 dataset, significantly exceeding the baselines. Our code is\navailable at: https://github.com/LetheSec/DFHL-RS-Attack.\n","authors":["Xiaojian Yuan","Kejiang Chen","Wen Huang","Jie Zhang","Weiming Zhang","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2312.05924v2.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.07133v1","updated":"2023-12-12T10:07:37Z","published":"2023-12-12T10:07:37Z","title":"Text2AC-Zero: Consistent Synthesis of Animated Characters using 2D\n  Diffusion","summary":"  We propose a zero-shot approach for consistent Text-to-Animated-Characters\nsynthesis based on pre-trained Text-to-Image (T2I) diffusion models. Existing\nText-to-Video (T2V) methods are expensive to train and require large-scale\nvideo datasets to produce diverse characters and motions. At the same time,\ntheir zero-shot alternatives fail to produce temporally consistent videos. We\nstrive to bridge this gap, and we introduce a zero-shot approach that produces\ntemporally consistent videos of animated characters and requires no training or\nfine-tuning. We leverage existing text-based motion diffusion models to\ngenerate diverse motions that we utilize to guide a T2I model. To achieve\ntemporal consistency, we introduce the Spatial Latent Alignment module that\nexploits cross-frame dense correspondences that we compute to align the latents\nof the video frames. Furthermore, we propose Pixel-Wise Guidance to steer the\ndiffusion process in a direction that minimizes visual discrepancies. Our\nproposed approach generates temporally consistent videos with diverse motions\nand styles, outperforming existing zero-shot T2V approaches in terms of\npixel-wise consistency and user preference.\n","authors":["Abdelrahman Eldesokey","Peter Wonka"],"pdf_url":"https://arxiv.org/pdf/2312.07133v1.pdf","comment":"Project page: https://abdo-eldesokey.github.io/text2ac-zero/"},{"id":"http://arxiv.org/abs/2312.07132v1","updated":"2023-12-12T10:07:16Z","published":"2023-12-12T10:07:16Z","title":"Image Content Generation with Causal Reasoning","summary":"  The emergence of ChatGPT has once again sparked research in generative\nartificial intelligence (GAI). While people have been amazed by the generated\nresults, they have also noticed the reasoning potential reflected in the\ngenerated textual content. However, this current ability for causal reasoning\nis primarily limited to the domain of language generation, such as in models\nlike GPT-3. In visual modality, there is currently no equivalent research.\nConsidering causal reasoning in visual content generation is significant. This\nis because visual information contains infinite granularity. Particularly,\nimages can provide more intuitive and specific demonstrations for certain\nreasoning tasks, especially when compared to coarse-grained text. Hence, we\npropose a new image generation task called visual question answering with image\n(VQAI) and establish a dataset of the same name based on the classic\n\\textit{Tom and Jerry} animated series. Additionally, we develop a new paradigm\nfor image generation to tackle the challenges of this task. Finally, we perform\nextensive experiments and analyses, including visualizations of the generated\ncontent and discussions on the potentials and limitations. The code and data\nare publicly available under the license of CC BY-NC-SA 4.0 for academic and\nnon-commercial usage. The code and dataset are publicly available at:\nhttps://github.com/IEIT-AGI/MIX-Shannon/blob/main/projects/VQAI/lgd_vqai.md.\n","authors":["Xiaochuan Li","Baoyu Fan","Runze Zhang","Liang Jin","Di Wang","Zhenhua Guo","Yaqian Zhao","Rengang Li"],"pdf_url":"https://arxiv.org/pdf/2312.07132v1.pdf","comment":"Accepted by the 38th Annual AAAI Conference on Artificial\n  Intelligence (AAAI 2024) in December 2023"},{"id":"http://arxiv.org/abs/2312.07128v1","updated":"2023-12-12T10:04:11Z","published":"2023-12-12T10:04:11Z","title":"MS-Twins: Multi-Scale Deep Self-Attention Networks for Medical Image\n  Segmentation","summary":"  Although transformer is preferred in natural language processing, few studies\nhave applied it in the field of medical imaging. For its long-term dependency,\nthe transformer is expected to contribute to unconventional convolution neural\nnet conquer their inherent spatial induction bias. The lately suggested\ntransformer-based partition method only uses the transformer as an auxiliary\nmodule to help encode the global context into a convolutional representation.\nThere is hardly any study about how to optimum bond self-attention (the kernel\nof transformers) with convolution. To solve the problem, the article proposes\nMS-Twins (Multi-Scale Twins), which is a powerful segmentation model on account\nof the bond of self-attention and convolution. MS-Twins can better capture\nsemantic and fine-grained information by combining different scales and\ncascading features. Compared with the existing network structure, MS-Twins has\nmade significant progress on the previous method based on the transformer of\ntwo in common use data sets, Synapse and ACDC. In particular, the performance\nof MS-Twins on Synapse is 8% higher than SwinUNet. Even compared with nnUNet,\nthe best entirely convoluted medical image segmentation network, the\nperformance of MS-Twins on Synapse and ACDC still has a bit advantage.\n","authors":["Jing Xu"],"pdf_url":"https://arxiv.org/pdf/2312.07128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07125v1","updated":"2023-12-12T09:58:07Z","published":"2023-12-12T09:58:07Z","title":"Efficient Few-Shot Clinical Task Adaptation with Large Language Models","summary":"  Few-shot learning has been studied to adapt models to tasks with very few\nsamples. It holds profound significance, particularly in clinical tasks, due to\nthe high annotation cost of medical images. Several works have explored\nfew-shot learning on medical images, yet they still require a large number of\nmedical images for pre-training models to gain domain-specific priors. Vision\nfoundation models recently have achieved remarkable success in natural images.\nHence, adapting rapidly advancing vision foundation models from natural images\nto few-shot clinical tasks holds great promise. MedFMC has recently organized a\nchallenge to shed more light on this topic at NeurIPS 2023. In this work, we\npresent our challenge solution. We observe that a simple variant of fine-tuning\nwith partial freezing shows remarkable performance. Empirical evidence\ndemonstrates that this approach could outperform various common fine-tuning\nmethods under limited sample sizes. Additionally, we explore enhanced\nutilization of semantic supervision to boost performance. We propose a novel\napproach that contextualizes labels via large language models (LLMs). Our\nfindings reveal that the context generated by LLMs significantly enhances the\ndiscrimination of semantic embeddings for similar categories, resulting in a\nnotable performance improvement of 3%-5% in 1-shot settings compared to\ncommonly employed one-hot labels and other semantic supervision methods. Our\nsolution secures the 1st place in the MedFMC challenge.\n","authors":["Kaipeng Zheng","Weiran Huang","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2312.07125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.14660v5","updated":"2023-12-12T09:45:51Z","published":"2023-04-28T07:23:31Z","title":"Segment Anything Model for Medical Images?","summary":"  The Segment Anything Model (SAM) is the first foundation model for general\nimage segmentation. It has achieved impressive results on various natural image\nsegmentation tasks. However, medical image segmentation (MIS) is more\nchallenging because of the complex modalities, fine anatomical structures,\nuncertain and complex object boundaries, and wide-range object scales. To fully\nvalidate SAM's performance on medical data, we collected and sorted 53\nopen-source datasets and built a large medical segmentation dataset with 18\nmodalities, 84 objects, 125 object-modality paired targets, 1050K 2D images,\nand 6033K masks. We comprehensively analyzed different models and strategies on\nthe so-called COSMOS 1050K dataset. Our findings mainly include the following:\n1) SAM showed remarkable performance in some specific objects but was unstable,\nimperfect, or even totally failed in other situations. 2) SAM with the large\nViT-H showed better overall performance than that with the small ViT-B. 3) SAM\nperformed better with manual hints, especially box, than the Everything mode.\n4) SAM could help human annotation with high labeling quality and less time. 5)\nSAM was sensitive to the randomness in the center point and tight box prompts,\nand may suffer from a serious performance drop. 6) SAM performed better than\ninteractive methods with one or a few points, but will be outpaced as the\nnumber of points increases. 7) SAM's performance correlated to different\nfactors, including boundary complexity, intensity differences, etc. 8)\nFinetuning the SAM on specific medical tasks could improve its average DICE\nperformance by 4.39% and 6.68% for ViT-B and ViT-H, respectively. We hope that\nthis comprehensive report can help researchers explore the potential of SAM\napplications in MIS, and guide how to appropriately use and develop SAM.\n","authors":["Yuhao Huang","Xin Yang","Lian Liu","Han Zhou","Ao Chang","Xinrui Zhou","Rusi Chen","Junxuan Yu","Jiongquan Chen","Chaoyu Chen","Sijing Liu","Haozhe Chi","Xindi Hu","Kejuan Yue","Lei Li","Vicente Grau","Deng-Ping Fan","Fajin Dong","Dong Ni"],"pdf_url":"https://arxiv.org/pdf/2304.14660v5.pdf","comment":"Accepted by Medical Image Analysis. 23 pages, 18 figures, 8 tables"},{"id":"http://arxiv.org/abs/2304.14123v2","updated":"2023-12-12T09:45:00Z","published":"2023-04-27T12:17:23Z","title":"MCLFIQ: Mobile Contactless Fingerprint Image Quality","summary":"  We propose MCLFIQ: Mobile Contactless Fingerprint Image Quality, the first\nquality assessment algorithm for mobile contactless fingerprint samples. To\nthis end, we re-trained the NIST Fingerprint Image Quality (NFIQ) 2 method,\nwhich was originally designed for contact-based fingerprints, with a synthetic\ncontactless fingerprint database. We evaluate the predictive performance of the\nresulting MCLFIQ model in terms of Error-vs.-Discard Characteristic (EDC)\ncurves on three real-world contactless fingerprint databases using three\nrecognition algorithms. In experiments, the MCLFIQ method is compared against\nthe original NFIQ 2.2 method, a sharpness-based quality assessment algorithm\ndeveloped for contactless fingerprint images \\rev{and the general purpose image\nquality assessment method BRISQUE. Furthermore, benchmarks on four\ncontact-based fingerprint datasets are also conducted.}\n  Obtained results show that the fine-tuning of NFIQ 2 on synthetic contactless\nfingerprints is a viable alternative to training on real databases. Moreover,\nthe evaluation shows that our MCLFIQ method works more accurate and robust\ncompared to all baseline methods on contactless fingerprints. We suggest\nconsidering the proposed MCLFIQ method as a \\rev{starting point for the\ndevelopment of} a new standard algorithm for contactless fingerprint quality\nassessment.\n","authors":["Jannis Priesnitz","Axel Weißenfeld","Laurenz Ruzicka","Christian Rathgeb","Bernhard Strobl","Ralph Lessmann","Christoph Busch"],"pdf_url":"https://arxiv.org/pdf/2304.14123v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00343v2","updated":"2023-12-12T09:43:33Z","published":"2023-12-01T04:35:47Z","title":"OpenStereo: A Comprehensive Benchmark for Stereo Matching and Strong\n  Baseline","summary":"  Stereo matching, a pivotal technique in computer vision, plays a crucial role\nin robotics, autonomous navigation, and augmented reality. Despite the\ndevelopment of numerous impressive methods in recent years, replicating their\nresults and determining the most suitable architecture for practical\napplication remains challenging. Addressing this gap, our paper introduces a\ncomprehensive benchmark focusing on practical applicability rather than solely\non performance enhancement. Specifically, we develop a flexible and efficient\nstereo matching codebase, called OpenStereo. OpenStereo includes training and\ninference codes of more than 12 network models, making it, to our knowledge,\nthe most complete stereo matching toolbox available. Based on OpenStereo, we\nconducted experiments on the SceneFlow dataset and have achieved or surpassed\nthe performance metrics reported in the original paper. Additionally, we\nconduct an in-depth revisitation of recent developments in stereo matching\nthrough ablative experiments. These investigations inspired the creation of\nStereoBase, a simple yet strong baseline model. Our extensive comparative\nanalyses of StereoBase against numerous contemporary stereo matching methods on\nthe SceneFlow dataset demonstrate its remarkably strong performance. The source\ncode is available at https://github.com/XiandaGuo/OpenStereo.\n","authors":["Xianda Guo","Juntao Lu","Chenming Zhang","Yiqi Wang","Yiqun Duan","Tian Yang","Zheng Zhu","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2312.00343v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.01092v2","updated":"2023-12-12T09:37:12Z","published":"2023-03-02T09:26:20Z","title":"ArCL: Enhancing Contrastive Learning with Augmentation-Robust\n  Representations","summary":"  Self-Supervised Learning (SSL) is a paradigm that leverages unlabeled data\nfor model training. Empirical studies show that SSL can achieve promising\nperformance in distribution shift scenarios, where the downstream and training\ndistributions differ. However, the theoretical understanding of its\ntransferability remains limited. In this paper, we develop a theoretical\nframework to analyze the transferability of self-supervised contrastive\nlearning, by investigating the impact of data augmentation on it. Our results\nreveal that the downstream performance of contrastive learning depends largely\non the choice of data augmentation. Moreover, we show that contrastive learning\nfails to learn domain-invariant features, which limits its transferability.\nBased on these theoretical insights, we propose a novel method called\nAugmentation-robust Contrastive Learning (ArCL), which guarantees to learn\ndomain-invariant features and can be easily integrated with existing\ncontrastive learning algorithms. We conduct experiments on several datasets and\nshow that ArCL significantly improves the transferability of contrastive\nlearning.\n","authors":["Xuyang Zhao","Tianqi Du","Yisen Wang","Jun Yao","Weiran Huang"],"pdf_url":"https://arxiv.org/pdf/2303.01092v2.pdf","comment":"Accepted by ICLR 2023"},{"id":"http://arxiv.org/abs/2312.07100v1","updated":"2023-12-12T09:27:57Z","published":"2023-12-12T09:27:57Z","title":"Lightweight high-resolution Subject Matting in the Real World","summary":"  Existing saliency object detection (SOD) methods struggle to satisfy fast\ninference and accurate results simultaneously in high resolution scenes. They\nare limited by the quality of public datasets and efficient network modules for\nhigh-resolution images. To alleviate these issues, we propose to construct a\nsaliency object matting dataset HRSOM and a lightweight network PSUNet.\nConsidering efficient inference of mobile depolyment framework, we design a\nsymmetric pixel shuffle module and a lightweight module TRSU. Compared to 13\nSOD methods, the proposed PSUNet has the best objective performance on the\nhigh-resolution benchmark dataset. Evaluation results of objective assessment\nare superior compared to U$^2$Net that has 10 times of parameter amount of our\nnetwork. On Snapdragon 8 Gen 2 Mobile Platform, inference a single\n640$\\times$640 image only takes 113ms. And on the subjective assessment,\nevaluation results are better than the industry benchmark IOS16 (Lift subject\nfrom background).\n","authors":["Peng Liu","Fanyi Wang","Jingwen Su","Yanhao Zhang","Guojun Qi"],"pdf_url":"https://arxiv.org/pdf/2312.07100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.07072v4","updated":"2023-12-12T09:06:33Z","published":"2023-04-14T11:51:26Z","title":"CornerFormer: Boosting Corner Representation for Fine-Grained Structured\n  Reconstruction","summary":"  Structured reconstruction is a non-trivial dense prediction problem, which\nextracts structural information (\\eg, building corners and edges) from a raster\nimage, then reconstructs it to a 2D planar graph accordingly. Compared with\ncommon segmentation or detection problems, it significantly relays on the\ncapability that leveraging holistic geometric information for structural\nreasoning. Current transformer-based approaches tackle this challenging problem\nin a two-stage manner, which detect corners in the first model and classify the\nproposed edges (corner-pairs) in the second model. However, they separate\ntwo-stage into different models and only share the backbone encoder. Unlike the\nexisting modeling strategies, we present an enhanced corner representation\nmethod: 1) It fuses knowledge between the corner detection and edge prediction\nby sharing feature in different granularity; 2) Corner candidates are proposed\nin four heatmap channels w.r.t its direction. Both qualitative and quantitative\nevaluations demonstrate that our proposed method can better reconstruct\nfine-grained structures, such as adjacent corners and tiny edges. Consequently,\nit outperforms the state-of-the-art model by +1.9\\%@F-1 on Corner and\n+3.0\\%@F-1 on Edge.\n","authors":["Hongbo Tian","Yulong Li","Linzhi Huang","Xu Ling","Yue Yang","Jiani Hu"],"pdf_url":"https://arxiv.org/pdf/2304.07072v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07082v1","updated":"2023-12-12T09:02:56Z","published":"2023-12-12T09:02:56Z","title":"Continual Learning through Networks Splitting and Merging with\n  Dreaming-Meta-Weighted Model Fusion","summary":"  It's challenging to balance the networks stability and plasticity in\ncontinual learning scenarios, considering stability suffers from the update of\nmodel and plasticity benefits from it. Existing works usually focus more on the\nstability and restrict the learning plasticity of later tasks to avoid\ncatastrophic forgetting of learned knowledge. Differently, we propose a\ncontinual learning method named Split2MetaFusion which can achieve better\ntrade-off by employing a two-stage strategy: splitting and meta-weighted\nfusion. In this strategy, a slow model with better stability, and a fast model\nwith better plasticity are learned sequentially at the splitting stage. Then\nstability and plasticity are both kept by fusing the two models in an adaptive\nmanner. Towards this end, we design an optimizer named Task-Preferred Null\nSpace Projector(TPNSP) to the slow learning process for narrowing the fusion\ngap. To achieve better model fusion, we further design a Dreaming-Meta-Weighted\nfusion policy for better maintaining the old and new knowledge simultaneously,\nwhich doesn't require to use the previous datasets. Experimental results and\nanalysis reported in this work demonstrate the superiority of the proposed\nmethod for maintaining networks stability and keeping its plasticity. Our code\nwill be released.\n","authors":["Yi Sun","Xin Xu","Jian Li","Guanglei Xie","Yifei Shi","Qiang Fang"],"pdf_url":"https://arxiv.org/pdf/2312.07082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07079v1","updated":"2023-12-12T08:58:56Z","published":"2023-12-12T08:58:56Z","title":"Spatial-Contextual Discrepancy Information Compensation for GAN\n  Inversion","summary":"  Most existing GAN inversion methods either achieve accurate reconstruction\nbut lack editability or offer strong editability at the cost of fidelity.\nHence, how to balance the distortioneditability trade-off is a significant\nchallenge for GAN inversion. To address this challenge, we introduce a novel\nspatial-contextual discrepancy information compensationbased GAN-inversion\nmethod (SDIC), which consists of a discrepancy information prediction network\n(DIPN) and a discrepancy information compensation network (DICN). SDIC follows\na \"compensate-and-edit\" paradigm and successfully bridges the gap in image\ndetails between the original image and the reconstructed/edited image. On the\none hand, DIPN encodes the multi-level spatial-contextual information of the\noriginal and initial reconstructed images and then predicts a\nspatial-contextual guided discrepancy map with two hourglass modules. In this\nway, a reliable discrepancy map that models the contextual relationship and\ncaptures finegrained image details is learned. On the other hand, DICN\nincorporates the predicted discrepancy information into both the latent code\nand the GAN generator with different transformations, generating high-quality\nreconstructed/edited images. This effectively compensates for the loss of image\ndetails during GAN inversion. Both quantitative and qualitative experiments\ndemonstrate that our proposed method achieves the excellent\ndistortion-editability trade-off at a fast inference speed for both image\ninversion and editing tasks.\n","authors":["Ziqiang Zhang","Yan Yan","Jing-Hao Xue","Hanzi Wang"],"pdf_url":"https://arxiv.org/pdf/2312.07079v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07730v2","updated":"2023-12-12T08:56:17Z","published":"2023-09-30T02:59:49Z","title":"Domain-Controlled Prompt Learning","summary":"  Large pre-trained vision-language models, such as CLIP, have shown remarkable\ngeneralization capabilities across various tasks when appropriate text prompts\nare provided. However, adapting these models to specific domains, like remote\nsensing images (RSIs), medical images, etc, remains unexplored and challenging.\nExisting prompt learning methods often lack domain-awareness or domain-transfer\nmechanisms, leading to suboptimal performance due to the misinterpretation of\nspecific images in natural image patterns. To tackle this dilemma, we proposed\na \\textbf{Domain-Controlled Prompt Learning} for the specific domains.\nSpecifically, the large-scale specific domain foundation model (LSDM) is first\nintroduced to provide essential specific domain knowledge. Using lightweight\nneural networks, we transfer this knowledge into domain biases, which control\nboth the visual and language branches to obtain domain-adaptive prompts in a\ndirectly incorporating manner. Simultaneously, to overcome the existing\noverfitting challenge, we propose a novel noisy-adding strategy, without extra\ntrainable parameters, to help the model escape the suboptimal solution in a\nglobal domain oscillation manner. Experimental results show our method achieves\nstate-of-the-art performance in specific domain image recognition datasets. Our\ncode is available at https://github.com/caoql98/DCPL.\n","authors":["Qinglong Cao","Zhengqin Xu","Yuntian Chen","Chao Ma","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2310.07730v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07067v1","updated":"2023-12-12T08:41:18Z","published":"2023-12-12T08:41:18Z","title":"Focus on Hiders: Exploring Hidden Threats for Enhancing Adversarial\n  Training","summary":"  Adversarial training is often formulated as a min-max problem, however,\nconcentrating only on the worst adversarial examples causes alternating\nrepetitive confusion of the model, i.e., previously defended or correctly\nclassified samples are not defensible or accurately classifiable in subsequent\nadversarial training. We characterize such non-ignorable samples as \"hiders\",\nwhich reveal the hidden high-risk regions within the secure area obtained\nthrough adversarial training and prevent the model from finding the real worst\ncases. We demand the model to prevent hiders when defending against adversarial\nexamples for improving accuracy and robustness simultaneously. By rethinking\nand redefining the min-max optimization problem for adversarial training, we\npropose a generalized adversarial training algorithm called Hider-Focused\nAdversarial Training (HFAT). HFAT introduces the iterative evolution\noptimization strategy to simplify the optimization problem and employs an\nauxiliary model to reveal hiders, effectively combining the optimization\ndirections of standard adversarial training and prevention hiders. Furthermore,\nwe introduce an adaptive weighting mechanism that facilitates the model in\nadaptively adjusting its focus between adversarial examples and hiders during\ndifferent training periods. We demonstrate the effectiveness of our method\nbased on extensive experiments, and ensure that HFAT can provide higher\nrobustness and accuracy.\n","authors":["Qian Li","Yuxiao Hu","Yinpeng Dong","Dongxiao Zhang","Yuntian Chen"],"pdf_url":"https://arxiv.org/pdf/2312.07067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07066v1","updated":"2023-12-12T08:40:38Z","published":"2023-12-12T08:40:38Z","title":"DiffuVST: Narrating Fictional Scenes with Global-History-Guided\n  Denoising Models","summary":"  Recent advances in image and video creation, especially AI-based image\nsynthesis, have led to the production of numerous visual scenes that exhibit a\nhigh level of abstractness and diversity. Consequently, Visual Storytelling\n(VST), a task that involves generating meaningful and coherent narratives from\na collection of images, has become even more challenging and is increasingly\ndesired beyond real-world imagery. While existing VST techniques, which\ntypically use autoregressive decoders, have made significant progress, they\nsuffer from low inference speed and are not well-suited for synthetic scenes.\nTo this end, we propose a novel diffusion-based system DiffuVST, which models\nthe generation of a series of visual descriptions as a single conditional\ndenoising process. The stochastic and non-autoregressive nature of DiffuVST at\ninference time allows it to generate highly diverse narratives more\nefficiently. In addition, DiffuVST features a unique design with bi-directional\ntext history guidance and multimodal adapter modules, which effectively improve\ninter-sentence coherence and image-to-text fidelity. Extensive experiments on\nthe story generation task covering four fictional visual-story datasets\ndemonstrate the superiority of DiffuVST over traditional autoregressive models\nin terms of both text quality and inference speed.\n","authors":["Shengguang Wu","Mei Yuan","Qi Su"],"pdf_url":"https://arxiv.org/pdf/2312.07066v1.pdf","comment":"EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2312.07064v1","updated":"2023-12-12T08:33:34Z","published":"2023-12-12T08:33:34Z","title":"Efficient Cross-Domain Federated Learning by MixStyle Approximation","summary":"  With the advent of interconnected and sensor-equipped edge devices, Federated\nLearning (FL) has gained significant attention, enabling decentralized learning\nwhile maintaining data privacy. However, FL faces two challenges in real-world\ntasks: expensive data labeling and domain shift between source and target\nsamples. In this paper, we introduce a privacy-preserving, resource-efficient\nFL concept for client adaptation in hardware-constrained environments. Our\napproach includes server model pre-training on source data and subsequent\nfine-tuning on target data via low-end clients. The local client adaptation\nprocess is streamlined by probabilistic mixing of instance-level feature\nstatistics approximated from source and target domain data. The adapted\nparameters are transferred back to the central server and globally aggregated.\nPreliminary results indicate that our method reduces computational and\ntransmission costs while maintaining competitive performance on downstream\ntasks.\n","authors":["Manuel Röder","Leon Heller","Maximilian Münch","Frank-Michael Schleif"],"pdf_url":"https://arxiv.org/pdf/2312.07064v1.pdf","comment":"Accepted at the Adapting to Change: Reliable Multimodal Learning\n  Across Domains Workshop @ ECML PKKD 2023"},{"id":"http://arxiv.org/abs/2312.07063v1","updated":"2023-12-12T08:32:55Z","published":"2023-12-12T08:32:55Z","title":"Template Free Reconstruction of Human-object Interaction with Procedural\n  Interaction Generation","summary":"  Reconstructing human-object interaction in 3D from a single RGB image is a\nchallenging task and existing data driven methods do not generalize beyond the\nobjects present in the carefully curated 3D interaction datasets. Capturing\nlarge-scale real data to learn strong interaction and 3D shape priors is very\nexpensive due to the combinatorial nature of human-object interactions. In this\npaper, we propose ProciGen (Procedural interaction Generation), a method to\nprocedurally generate datasets with both, plausible interaction and diverse\nobject variation. We generate 1M+ human-object interaction pairs in 3D and\nleverage this large-scale data to train our HDM (Hierarchical Diffusion Model),\na novel method to reconstruct interacting human and unseen objects, without any\ntemplates. Our HDM is an image-conditioned diffusion model that learns both\nrealistic interaction and highly accurate human and object shapes. Experiments\nshow that our HDM trained with ProciGen significantly outperforms prior methods\nthat requires template meshes and that our dataset allows training methods with\nstrong generalization ability to unseen object instances. Our code and data\nwill be publicly released at:\nhttps://virtualhumans.mpi-inf.mpg.de/procigen-hdm.\n","authors":["Xianghui Xie","Bharat Lal Bhatnagar","Jan Eric Lenssen","Gerard Pons-Moll"],"pdf_url":"https://arxiv.org/pdf/2312.07063v1.pdf","comment":"23 pages, 18 figures. Project page:\n  https://virtualhumans.mpi-inf.mpg.de/procigen-hdm (will be available soon)"},{"id":"http://arxiv.org/abs/2312.07062v1","updated":"2023-12-12T08:30:09Z","published":"2023-12-12T08:30:09Z","title":"ThinkBot: Embodied Instruction Following with Thought Chain Reasoning","summary":"  Embodied Instruction Following (EIF) requires agents to complete human\ninstruction by interacting objects in complicated surrounding environments.\nConventional methods directly consider the sparse human instruction to generate\naction plans for agents, which usually fail to achieve human goals because of\nthe instruction incoherence in action descriptions. On the contrary, we propose\nThinkBot that reasons the thought chain in human instruction to recover the\nmissing action descriptions, so that the agent can successfully complete human\ngoals by following the coherent instruction. Specifically, we first design an\ninstruction completer based on large language models to recover the missing\nactions with interacted objects between consecutive human instruction, where\nthe perceived surrounding environments and the completed sub-goals are\nconsidered for instruction completion. Based on the partially observed scene\nsemantic maps, we present an object localizer to infer the position of\ninteracted objects for agents to achieve complex human goals. Extensive\nexperiments in the simulated environment show that our ThinkBot outperforms the\nstate-of-the-art EIF methods by a sizable margin in both success rate and\nexecution efficiency.\n","authors":["Guanxing Lu","Ziwei Wang","Changliu Liu","Jiwen Lu","Yansong Tang"],"pdf_url":"https://arxiv.org/pdf/2312.07062v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07061v1","updated":"2023-12-12T08:28:29Z","published":"2023-12-12T08:28:29Z","title":"MaxQ: Multi-Axis Query for N:M Sparsity Network","summary":"  N:M sparsity has received increasing attention due to its remarkable\nperformance and latency trade-off compared with structured and unstructured\nsparsity. However, existing N:M sparsity methods do not differentiate the\nrelative importance of weights among blocks and leave important weights\nunderappreciated. Besides, they directly apply N:M sparsity to the whole\nnetwork, which will cause severe information loss. Thus, they are still\nsub-optimal. In this paper, we propose an efficient and effective Multi-Axis\nQuery methodology, dubbed as MaxQ, to rectify these problems. During the\ntraining, MaxQ employs a dynamic approach to generate soft N:M masks,\nconsidering the weight importance across multiple axes. This method enhances\nthe weights with more importance and ensures more effective updates. Meanwhile,\na sparsity strategy that gradually increases the percentage of N:M weight\nblocks is applied, which allows the network to heal from the pruning-induced\ndamage progressively. During the runtime, the N:M soft masks can be precomputed\nas constants and folded into weights without causing any distortion to the\nsparse pattern and incurring additional computational overhead. Comprehensive\nexperiments demonstrate that MaxQ achieves consistent improvements across\ndiverse CNN architectures in various computer vision tasks, including image\nclassification, object detection and instance segmentation. For ResNet50 with\n1:16 sparse pattern, MaxQ can achieve 74.6\\% top-1 accuracy on ImageNet and\nimprove by over 2.8\\% over the state-of-the-art.\n","authors":["Jingyang Xiang","Siqi Li","Junhao Chen","Zhuangzhi Chen","Tianxin Huang","Linpeng Peng","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2312.07061v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2304.01168v4","updated":"2023-12-12T08:13:09Z","published":"2023-04-03T17:37:00Z","title":"DeepAccident: A Motion and Accident Prediction Benchmark for V2X\n  Autonomous Driving","summary":"  Safety is the primary priority of autonomous driving. Nevertheless, no\npublished dataset currently supports the direct and explainable safety\nevaluation for autonomous driving. In this work, we propose DeepAccident, a\nlarge-scale dataset generated via a realistic simulator containing diverse\naccident scenarios that frequently occur in real-world driving. The proposed\nDeepAccident dataset includes 57K annotated frames and 285K annotated samples,\napproximately 7 times more than the large-scale nuScenes dataset with 40k\nannotated samples. In addition, we propose a new task, end-to-end motion and\naccident prediction, which can be used to directly evaluate the accident\nprediction ability for different autonomous driving algorithms. Furthermore,\nfor each scenario, we set four vehicles along with one infrastructure to record\ndata, thus providing diverse viewpoints for accident scenarios and enabling V2X\n(vehicle-to-everything) research on perception and prediction tasks. Finally,\nwe present a baseline V2X model named V2XFormer that demonstrates superior\nperformance for motion and accident prediction and 3D object detection compared\nto the single-vehicle model.\n","authors":["Tianqi Wang","Sukmin Kim","Wenxuan Ji","Enze Xie","Chongjian Ge","Junsong Chen","Zhenguo Li","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2304.01168v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07052v1","updated":"2023-12-12T08:08:39Z","published":"2023-12-12T08:08:39Z","title":"Adjustable Robust Transformer for High Myopia Screening in Optical\n  Coherence Tomography","summary":"  Myopia is a manifestation of visual impairment caused by an excessively\nelongated eyeball. Image data is critical material for studying high myopia and\npathological myopia. Measurements of spherical equivalent and axial length are\nthe gold standards for identifying high myopia, but the available image data\nfor matching them is scarce. In addition, the criteria for defining high myopia\nvary from study to study, and therefore the inclusion of samples in automated\nscreening efforts requires an appropriate assessment of interpretability. In\nthis work, we propose a model called adjustable robust transformer (ARTran) for\nhigh myopia screening of optical coherence tomography (OCT) data. Based on\nvision transformer, we propose anisotropic patch embedding (APE) to capture\nmore discriminative features of high myopia. To make the model effective under\nvariable screening conditions, we propose an adjustable class embedding (ACE)\nto replace the fixed class token, which changes the output to adapt to\ndifferent conditions. Considering the confusion of the data at high myopia and\nlow myopia threshold, we introduce the label noise learning strategy and\npropose a shifted subspace transition matrix (SST) to enhance the robustness of\nthe model. Besides, combining the two structures proposed above, the model can\nprovide evidence for uncertainty evaluation. The experimental results\ndemonstrate the effectiveness and reliability of the proposed method. Code is\navailable at: https://github.com/maxiao0234/ARTran.\n","authors":["Xiao Ma","Zetian Zhang","Zexuan Ji","Kun Huang","Na Su","Songtao Yuan","Qiang Chen"],"pdf_url":"https://arxiv.org/pdf/2312.07052v1.pdf","comment":"11 pages, 3 figures, MICCAI 2023 - Accepted Papers; International\n  Conference on Medical Image Computing and Computer-Assisted Intervention,\n  2023: 504-514"},{"id":"http://arxiv.org/abs/2312.07051v1","updated":"2023-12-12T08:08:34Z","published":"2023-12-12T08:08:34Z","title":"Mask as Supervision: Leveraging Unified Mask Information for\n  Unsupervised 3D Pose Estimation","summary":"  Automatic estimation of 3D human pose from monocular RGB images is a\nchallenging and unsolved problem in computer vision. In a supervised manner,\napproaches heavily rely on laborious annotations and present hampered\ngeneralization ability due to the limited diversity of 3D pose datasets. To\naddress these challenges, we propose a unified framework that leverages mask as\nsupervision for unsupervised 3D pose estimation. With general unsupervised\nsegmentation algorithms, the proposed model employs skeleton and physique\nrepresentations that exploit accurate pose information from coarse to fine.\nCompared with previous unsupervised approaches, we organize the human skeleton\nin a fully unsupervised way which enables the processing of annotation-free\ndata and provides ready-to-use estimation results. Comprehensive experiments\ndemonstrate our state-of-the-art pose estimation performance on Human3.6M and\nMPI-INF-3DHP datasets. Further experiments on in-the-wild datasets also\nillustrate the capability to access more data to boost our model. Code will be\navailable at https://github.com/Charrrrrlie/Mask-as-Supervision.\n","authors":["Yuchen Yang","Yu Qiao","Xiao Sun"],"pdf_url":"https://arxiv.org/pdf/2312.07051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07048v1","updated":"2023-12-12T08:00:40Z","published":"2023-12-12T08:00:40Z","title":"Edge Wasserstein Distance Loss for Oriented Object Detection","summary":"  Regression loss design is an essential topic for oriented object detection.\nDue to the periodicity of the angle and the ambiguity of width and height\ndefinition, traditional L1-distance loss and its variants have been suffered\nfrom the metric discontinuity and the square-like problem. As a solution, the\ndistribution based methods show significant advantages by representing oriented\nboxes as distributions. Differing from exploited the Gaussian distribution to\nget analytical form of distance measure, we propose a novel oriented regression\nloss, Wasserstein Distance(EWD) loss, to alleviate the square-like problem.\nSpecifically, for the oriented box(OBox) representation, we choose a\nspecially-designed distribution whose probability density function is only\nnonzero over the edges. On this basis, we develop Wasserstein distance as the\nmeasure. Besides, based on the edge representation of OBox, the EWD loss can be\ngeneralized to quadrilateral and polynomial regression scenarios. Experiments\non multiple popular datasets and different detectors show the effectiveness of\nthe proposed method.\n","authors":["Yuke Zhu","Yumeng Ruan","Zihua Xiong","Sheng Guo"],"pdf_url":"https://arxiv.org/pdf/2312.07048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07039v1","updated":"2023-12-12T07:52:33Z","published":"2023-12-12T07:52:33Z","title":"Diff-OP3D: Bridging 2D Diffusion for Open Pose 3D Zero-Shot\n  Classification","summary":"  With the explosive 3D data growth, the urgency of utilizing zero-shot\nlearning to facilitate data labeling becomes evident. Recently, the methods via\ntransferring Contrastive Language-Image Pre-training (CLIP) to 3D vision have\nmade great progress in the 3D zero-shot classification task. However, these\nmethods primarily focus on aligned pose 3D objects (ap-3os), overlooking the\nrecognition of 3D objects with open poses (op-3os) typically encountered in\nreal-world scenarios, such as an overturned chair or a lying teddy bear. To\nthis end, we propose a more challenging benchmark for 3D open-pose zero-shot\nclassification. Echoing our benchmark, we design a concise angle-refinement\nmechanism that automatically optimizes one ideal pose as well as classifies\nthese op-3os. Furthermore, we make a first attempt to bridge 2D pre-trained\ndiffusion model as a classifer to 3D zero-shot classification without any\nadditional training. Such 2D diffusion to 3D objects proves vital in improving\nzero-shot classification for both ap-3os and op-3os. Our model notably improves\nby 3.5% and 15.8% on ModelNet10$^{\\ddag}$ and McGill$^{\\ddag}$ open pose\nbenchmarks, respectively, and surpasses the current state-of-the-art by 6.8% on\nthe aligned pose ModelNet10, affirming diffusion's efficacy in 3D zero-shot\ntasks.\n","authors":["Weiguang Zhao","Guanyu Yang","Chaolong Yang","Chenru Jiang","Yuyao Yan","Rui Zhang","Kaizhu Huang"],"pdf_url":"https://arxiv.org/pdf/2312.07039v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.16576v3","updated":"2023-12-12T07:50:06Z","published":"2023-08-31T09:19:06Z","title":"GHuNeRF: Generalizable Human NeRF from a Monocular Video","summary":"  In this paper, we tackle the challenging task of learning a generalizable\nhuman NeRF model from a monocular video. Although existing generalizable human\nNeRFs have achieved impressive results, they require muti-view images or videos\nwhich might not be always available. On the other hand, some works on\nfree-viewpoint rendering of human from monocular videos cannot be generalized\nto unseen identities. In view of these limitations, we propose GHuNeRF to learn\na generalizable human NeRF model from a monocular video of the human performer.\nWe first introduce a visibility-aware aggregation scheme to compute vertex-wise\nfeatures, which is used to construct a 3D feature volume. The feature volume\ncan only represent the overall geometry of the human performer with\ninsufficient accuracy due to the limited resolution. To solve this, we further\nenhance the volume feature with temporally aligned point-wise features using an\nattention mechanism. Finally, the enhanced feature is used for predicting\ndensity and color for each sampled point. A surface-guided sampling strategy is\nalso adopted to improve the efficiency for both training and inference. We\nvalidate our approach on the widely-used ZJU-MoCap dataset, where we achieve\ncomparable performance with existing multi-view video based approaches. We also\ntest on the monocular People-Snapshot dataset and achieve better performance\nthan existing works when only monocular video is used. Our code is available at\nthe project website.\n","authors":["Chen Li","Jiahao Lin","Gim Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2308.16576v3.pdf","comment":"Add in more baseline for comparison"},{"id":"http://arxiv.org/abs/2309.14585v2","updated":"2023-12-12T07:48:52Z","published":"2023-09-26T00:15:13Z","title":"DifAttack: Query-Efficient Black-Box Attack via Disentangled Feature\n  Space","summary":"  This work investigates efficient score-based black-box adversarial attacks\nwith a high Attack Success Rate (ASR) and good generalizability. We design a\nnovel attack method based on a Disentangled Feature space, called DifAttack,\nwhich differs significantly from the existing ones operating over the entire\nfeature space. Specifically, DifAttack firstly disentangles an image's latent\nfeature into an adversarial feature and a visual feature, where the former\ndominates the adversarial capability of an image, while the latter largely\ndetermines its visual appearance. We train an autoencoder for the\ndisentanglement by using pairs of clean images and their Adversarial Examples\n(AEs) generated from available surrogate models via white-box attack methods.\nEventually, DifAttack iteratively optimizes the adversarial feature according\nto the query feedback from the victim model until a successful AE is generated,\nwhile keeping the visual feature unaltered. In addition, due to the avoidance\nof using surrogate models' gradient information when optimizing AEs for\nblack-box models, our proposed DifAttack inherently possesses better attack\ncapability in the open-set scenario, where the training dataset of the victim\nmodel is unknown. Extensive experimental results demonstrate that our method\nachieves significant improvements in ASR and query efficiency simultaneously,\nespecially in the targeted attack and open-set scenarios. The code will be\navailable at https://github.com/csjunjun/DifAttack.git soon.\n","authors":["Liu Jun","Zhou Jiantao","Zeng Jiandian","Jinyu Tian"],"pdf_url":"https://arxiv.org/pdf/2309.14585v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.01819v2","updated":"2023-12-12T07:25:02Z","published":"2022-12-04T13:17:21Z","title":"Hierarchical Terrain Attention and Multi-Scale Rainfall Guidance For\n  Flood Image Prediction","summary":"  With the deterioration of climate, the phenomenon of rain-induced flooding\nhas become frequent. To mitigate its impact, recent works adopt convolutional\nneural network or its variants to predict the floods. However, these methods\ndirectly force the model to reconstruct the raw pixels of flood images through\na global constraint, overlooking the underlying information contained in\nterrain features and rainfall patterns. To address this, we present a novel\nframework for precise flood map prediction, which incorporates hierarchical\nterrain spatial attention to help the model focus on spatially-salient areas of\nterrain features and constructs multi-scale rainfall embedding to extensively\nintegrate rainfall pattern information into generation. To better adapt the\nmodel in various rainfall conditions, we leverage a rainfall regression loss\nfor both the generator and the discriminator as additional supervision.\nExtensive evaluations on real catchment datasets demonstrate the superior\nperformance of our method, which greatly surpasses the previous arts under\ndifferent rainfall conditions.\n","authors":["Feifei Wang","Yong Wang","Bing Li","Qidong Huang","Shaoqing Chen"],"pdf_url":"https://arxiv.org/pdf/2212.01819v2.pdf","comment":"Accepted by ICIP2023"},{"id":"http://arxiv.org/abs/2312.07021v1","updated":"2023-12-12T07:15:17Z","published":"2023-12-12T07:15:17Z","title":"Transferring Modality-Aware Pedestrian Attentive Learning\n  Visible-Infrared Person Re-identification","summary":"  Visible-infrared person re-identification (VI-ReID) aims to search the same\npedestrian of interest across visible and infrared modalities. Existing models\nmainly focus on compensating for modality-specific information to reduce\nmodality variation. However, these methods often lead to a higher computational\noverhead and may introduce interfering information when generating the\ncorresponding images or features. To address this issue, it is critical to\nleverage pedestrian-attentive features and learn modality-complete and\n-consistent representation. In this paper, a novel Transferring Modality-Aware\nPedestrian Attentive Learning (TMPA) model is proposed, focusing on the\npedestrian regions to efficiently compensate for missing modality-specific\nfeatures. Specifically, we propose a region-based data augmentation module\nPedMix to enhance pedestrian region coherence by mixing the corresponding\nregions from different modalities. A lightweight hybrid compensation module,\ni.e., the Modality Feature Transfer (MFT), is devised to integrate cross\nattention and convolution networks to fully explore the discriminative\nmodality-complete features with minimal computational overhead. Extensive\nexperiments conducted on the benchmark SYSU-MM01 and RegDB datasets\ndemonstrated the effectiveness of our proposed TMPA model.\n","authors":["Yuwei Guo","Wenhao Zhang","Licheng Jiao","Shuang Wang","Shuo Wang","Fang Liu"],"pdf_url":"https://arxiv.org/pdf/2312.07021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07009v1","updated":"2023-12-12T06:45:19Z","published":"2023-12-12T06:45:19Z","title":"Vision-language Assisted Attribute Learning","summary":"  Attribute labeling at large scale is typically incomplete and partial, posing\nsignificant challenges to model optimization. Existing attribute learning\nmethods often treat the missing labels as negative or simply ignore them all\nduring training, either of which could hamper the model performance to a great\nextent. To overcome these limitations, in this paper we leverage the available\nvision-language knowledge to explicitly disclose the missing labels for\nenhancing model learning. Given an image, we predict the likelihood of each\nmissing attribute label assisted by an off-the-shelf vision-language model, and\nrandomly select to ignore those with high scores in training. Our strategy\nstrikes a good balance between fully ignoring and negatifying the missing\nlabels, as these high scores are found to be informative on revealing label\nambiguity. Extensive experiments show that our proposed vision-language\nassisted loss can achieve state-of-the-art performance on the newly cleaned VAW\ndataset. Qualitative evaluation demonstrates the ability of the proposed method\nin predicting more complete attributes.\n","authors":["Kongming Liang","Xinran Wang","Rui Wang","Donghui Gao","Ling Jin","Weidong Liu","Xiatian Zhu","Zhanyu Ma","Jun Guo"],"pdf_url":"https://arxiv.org/pdf/2312.07009v1.pdf","comment":"Accepted by IEEE IC-NIDC 2023"},{"id":"http://arxiv.org/abs/2312.04344v2","updated":"2023-12-12T06:37:53Z","published":"2023-12-07T15:05:59Z","title":"Enhancing Medical Task Performance in GPT-4V: A Comprehensive Study on\n  Prompt Engineering Strategies","summary":"  OpenAI's latest large vision-language model (LVLM), GPT-4V(ision), has piqued\nconsiderable interest for its potential in medical applications. Despite its\npromise, recent studies and internal reviews highlight its underperformance in\nspecialized medical tasks. This paper explores the boundary of GPT-4V's\ncapabilities in medicine, particularly in processing complex imaging data from\nendoscopies, CT scans, and MRIs etc. Leveraging open-source datasets, we\nassessed its foundational competencies, identifying substantial areas for\nenhancement. Our research emphasizes prompt engineering, an often-underutilized\nstrategy for improving AI responsiveness. Through iterative testing, we refined\nthe model's prompts, significantly improving its interpretative accuracy and\nrelevance in medical imaging. From our comprehensive evaluations, we distilled\n10 effective prompt engineering techniques, each fortifying GPT-4V's medical\nacumen. These methodical enhancements facilitate more reliable, precise, and\nclinically valuable insights from GPT-4V, advancing its operability in critical\nhealthcare environments. Our findings are pivotal for those employing AI in\nmedicine, providing clear, actionable guidance on harnessing GPT-4V's full\ndiagnostic potential.\n","authors":["Pengcheng Chen","Ziyan Huang","Zhongying Deng","Tianbin Li","Yanzhou Su","Haoyu Wang","Jin Ye","Yu Qiao","Junjun He"],"pdf_url":"https://arxiv.org/pdf/2312.04344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.04356v2","updated":"2023-12-12T06:36:53Z","published":"2023-06-07T11:39:56Z","title":"Fine-Grained Visual Prompting","summary":"  Vision-Language Models (VLMs), such as CLIP, have demonstrated impressive\nzero-shot transfer capabilities in image-level visual perception. However,\nthese models have shown limited performance in instance-level tasks that demand\nprecise localization and recognition. Previous works have suggested that\nincorporating visual prompts, such as colorful boxes or circles, can improve\nthe ability of models to recognize objects of interest. Nonetheless, compared\nto language prompting, visual prompting designs are rarely explored. Existing\napproaches, which employ coarse visual cues such as colorful boxes or circles,\noften result in sub-optimal performance due to the inclusion of irrelevant and\nnoisy pixels. In this paper, we carefully study the visual prompting designs by\nexploring more fine-grained markings, such as segmentation masks and their\nvariations. In addition, we introduce a new zero-shot framework that leverages\npixel-level annotations acquired from a generalist segmentation model for\nfine-grained visual prompting. Consequently, our investigation reveals that a\nstraightforward application of blur outside the target mask, referred to as the\nBlur Reverse Mask, exhibits exceptional effectiveness. This proposed prompting\nstrategy leverages the precise mask annotations to reduce focus on weakly\nrelated regions while retaining spatial coherence between the target and the\nsurrounding background. Our Fine-Grained Visual Prompting (FGVP) demonstrates\nsuperior performance in zero-shot comprehension of referring expressions on the\nRefCOCO, RefCOCO+, and RefCOCOg benchmarks. It outperforms prior methods by an\naverage margin of 3.0% to 4.6%, with a maximum improvement of 12.5% on the\nRefCOCO+ testA subset. Code is available at https://github.com/ylingfeng/FGVP.\n","authors":["Lingfeng Yang","Yueze Wang","Xiang Li","Xinlong Wang","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2306.04356v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07006v1","updated":"2023-12-12T06:35:27Z","published":"2023-12-12T06:35:27Z","title":"Mixed Pseudo Labels for Semi-Supervised Object Detection","summary":"  While the pseudo-label method has demonstrated considerable success in\nsemi-supervised object detection tasks, this paper uncovers notable limitations\nwithin this approach. Specifically, the pseudo-label method tends to amplify\nthe inherent strengths of the detector while accentuating its weaknesses, which\nis manifested in the missed detection of pseudo-labels, particularly for small\nand tail category objects. To overcome these challenges, this paper proposes\nMixed Pseudo Labels (MixPL), consisting of Mixup and Mosaic for pseudo-labeled\ndata, to mitigate the negative impact of missed detections and balance the\nmodel's learning across different object scales. Additionally, the model's\ndetection performance on tail categories is improved by resampling labeled data\nwith relevant instances. Notably, MixPL consistently improves the performance\nof various detectors and obtains new state-of-the-art results with Faster\nR-CNN, FCOS, and DINO on COCO-Standard and COCO-Full benchmarks. Furthermore,\nMixPL also exhibits good scalability on large models, improving DINO Swin-L by\n2.5% mAP and achieving nontrivial new records (60.2% mAP) on the COCO val2017\nbenchmark without extra annotations.\n","authors":["Zeming Chen","Wenwei Zhang","Xinjiang Wang","Kai Chen","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2312.07006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.19797v2","updated":"2023-12-12T06:17:27Z","published":"2023-10-30T17:59:35Z","title":"DEFT: Dexterous Fine-Tuning for Real-World Hand Policies","summary":"  Dexterity is often seen as a cornerstone of complex manipulation. Humans are\nable to perform a host of skills with their hands, from making food to\noperating tools. In this paper, we investigate these challenges, especially in\nthe case of soft, deformable objects as well as complex, relatively\nlong-horizon tasks. However, learning such behaviors from scratch can be data\ninefficient. To circumvent this, we propose a novel approach, DEFT (DExterous\nFine-Tuning for Hand Policies), that leverages human-driven priors, which are\nexecuted directly in the real world. In order to improve upon these priors,\nDEFT involves an efficient online optimization procedure. With the integration\nof human-based learning and online fine-tuning, coupled with a soft robotic\nhand, DEFT demonstrates success across various tasks, establishing a robust,\ndata-efficient pathway toward general dexterous manipulation. Please see our\nwebsite at https://dexterous-finetuning.github.io for video results.\n","authors":["Aditya Kannan","Kenneth Shaw","Shikhar Bahl","Pragna Mannam","Deepak Pathak"],"pdf_url":"https://arxiv.org/pdf/2310.19797v2.pdf","comment":"In CoRL 2023. Website at https://dexterous-finetuning.github.io/"},{"id":"http://arxiv.org/abs/2312.06999v1","updated":"2023-12-12T06:07:21Z","published":"2023-12-12T06:07:21Z","title":"DGNet: Dynamic Gradient-guided Network with Noise Suppression for\n  Underwater Image Enhancement","summary":"  Underwater image enhancement (UIE) is a challenging task due to the complex\ndegradation caused by underwater environments. To solve this issue, previous\nmethods often idealize the degradation process, and neglect the impact of\nmedium noise and object motion on the distribution of image features, limiting\nthe generalization and adaptability of the model. Previous methods use the\nreference gradient that is constructed from original images and synthetic\nground-truth images. This may cause the network performance to be influenced by\nsome low-quality training data. Our approach utilizes predicted images to\ndynamically update pseudo-labels, adding a dynamic gradient to optimize the\nnetwork's gradient space. This process improves image quality and avoids local\noptima. Moreover, we propose a Feature Restoration and Reconstruction module\n(FRR) based on a Channel Combination Inference (CCI) strategy and a Frequency\nDomain Smoothing module (FRS). These modules decouple other degradation\nfeatures while reducing the impact of various types of noise on network\nperformance. Experiments on multiple public datasets demonstrate the\nsuperiority of our method over existing state-of-the-art approaches, especially\nin achieving performance milestones: PSNR of 25.6dB and SSIM of 0.93 on the\nUIEB dataset. Its efficiency in terms of parameter size and inference time\nfurther attests to its broad practicality. The code will be made publicly\navailable.\n","authors":["Jingchun Zhou","Zongxin He","Dehuan Zhang","Kin-man Lam","Weishi Zhang","Xianping Fu","Yi Wang","Chongyi Li"],"pdf_url":"https://arxiv.org/pdf/2312.06999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06995v1","updated":"2023-12-12T06:01:41Z","published":"2023-12-12T06:01:41Z","title":"Transformer-based No-Reference Image Quality Assessment via Supervised\n  Contrastive Learning","summary":"  Image Quality Assessment (IQA) has long been a research hotspot in the field\nof image processing, especially No-Reference Image Quality Assessment (NR-IQA).\nDue to the powerful feature extraction ability, existing Convolution Neural\nNetwork (CNN) and Transformers based NR-IQA methods have achieved considerable\nprogress. However, they still exhibit limited capability when facing unknown\nauthentic distortion datasets. To further improve NR-IQA performance, in this\npaper, a novel supervised contrastive learning (SCL) and Transformer-based\nNR-IQA model SaTQA is proposed. We first train a model on a large-scale\nsynthetic dataset by SCL (no image subjective score is required) to extract\ndegradation features of images with various distortion types and levels. To\nfurther extract distortion information from images, we propose a backbone\nnetwork incorporating the Multi-Stream Block (MSB) by combining the CNN\ninductive bias and Transformer long-term dependence modeling capability.\nFinally, we propose the Patch Attention Block (PAB) to obtain the final\ndistorted image quality score by fusing the degradation features learned from\ncontrastive learning with the perceptual distortion information extracted by\nthe backbone network. Experimental results on seven standard IQA datasets show\nthat SaTQA outperforms the state-of-the-art methods for both synthetic and\nauthentic datasets. Code is available at\nhttps://github.com/I2-Multimedia-Lab/SaTQA\n","authors":["Jinsong Shi","Pan Gao","Jie Qin"],"pdf_url":"https://arxiv.org/pdf/2312.06995v1.pdf","comment":"Accepted by AAAI24"},{"id":"http://arxiv.org/abs/2312.05799v2","updated":"2023-12-12T05:57:48Z","published":"2023-12-10T07:17:06Z","title":"SGNet: Structure Guided Network via Gradient-Frequency Awareness for\n  Depth Map Super-Resolution","summary":"  Depth super-resolution (DSR) aims to restore high-resolution (HR) depth from\nlow-resolution (LR) one, where RGB image is often used to promote this task.\nRecent image guided DSR approaches mainly focus on spatial domain to rebuild\ndepth structure. However, since the structure of LR depth is usually blurry,\nonly considering spatial domain is not very sufficient to acquire satisfactory\nresults. In this paper, we propose structure guided network (SGNet), a method\nthat pays more attention to gradient and frequency domains, both of which have\nthe inherent ability to capture high-frequency structure. Specifically, we\nfirst introduce the gradient calibration module (GCM), which employs the\naccurate gradient prior of RGB to sharpen the LR depth structure. Then we\npresent the Frequency Awareness Module (FAM) that recursively conducts multiple\nspectrum differencing blocks (SDB), each of which propagates the precise\nhigh-frequency components of RGB into the LR depth. Extensive experimental\nresults on both real and synthetic datasets demonstrate the superiority of our\nSGNet, reaching the state-of-the-art. Codes and pre-trained models are\navailable at https://github.com/yanzq95/SGNet.\n","authors":["Zhengxue Wang","Zhiqiang Yan","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2312.05799v2.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2312.04837v2","updated":"2023-12-12T05:48:14Z","published":"2023-12-08T05:23:50Z","title":"Localized Symbolic Knowledge Distillation for Visual Commonsense Models","summary":"  Instruction following vision-language (VL) models offer a flexible interface\nthat supports a broad range of multimodal tasks in a zero-shot fashion.\nHowever, interfaces that operate on full images do not directly enable the user\nto \"point to\" and access specific regions within images. This capability is\nimportant not only to support reference-grounded VL benchmarks, but also, for\npractical applications that require precise within-image reasoning. We build\nLocalized Visual Commonsense models, which allow users to specify (multiple)\nregions as input. We train our model by sampling localized commonsense\nknowledge from a large language model (LLM): specifically, we prompt an LLM to\ncollect commonsense knowledge given a global literal image description and a\nlocal literal region description automatically generated by a set of VL models.\nWith a separately trained critic model that selects high-quality examples, we\nfind that training on the localized commonsense corpus can successfully distill\nexisting VL models to support a reference-as-input interface. Empirical results\nand human evaluations in a zero-shot setup demonstrate that our distillation\nmethod results in more precise VL models of reasoning compared to a baseline of\npassing a generated referring expression to an LLM.\n","authors":["Jae Sung Park","Jack Hessel","Khyathi Raghavi Chandu","Paul Pu Liang","Ximing Lu","Peter West","Youngjae Yu","Qiuyuan Huang","Jianfeng Gao","Ali Farhadi","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2312.04837v2.pdf","comment":"Neurips 2023"},{"id":"http://arxiv.org/abs/2312.06069v2","updated":"2023-12-12T05:45:49Z","published":"2023-12-11T02:27:45Z","title":"Mining Gaze for Contrastive Learning toward Computer-Assisted Diagnosis","summary":"  Obtaining large-scale radiology reports can be difficult for medical images\ndue to various reasons, limiting the effectiveness of contrastive pre-training\nin the medical image domain and underscoring the need for alternative methods.\nIn this paper, we propose eye-tracking as an alternative to text reports, as it\nallows for the passive collection of gaze signals without disturbing\nradiologist's routine diagnosis process. By tracking the gaze of radiologists\nas they read and diagnose medical images, we can understand their visual\nattention and clinical reasoning. When a radiologist has similar gazes for two\nmedical images, it may indicate semantic similarity for diagnosis, and these\nimages should be treated as positive pairs when pre-training a\ncomputer-assisted diagnosis (CAD) network through contrastive learning.\nAccordingly, we introduce the Medical contrastive Gaze Image Pre-training\n(McGIP) as a plug-and-play module for contrastive learning frameworks. McGIP\nuses radiologist's gaze to guide contrastive pre-training. We evaluate our\nmethod using two representative types of medical images and two common types of\ngaze data. The experimental results demonstrate the practicality of McGIP,\nindicating its high potential for various clinical scenarios and applications.\n","authors":["Zihao Zhao","Sheng Wang","Qian Wang","Dinggang Shen"],"pdf_url":"https://arxiv.org/pdf/2312.06069v2.pdf","comment":"*These authors contributed equally. Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.06630v2","updated":"2023-12-12T05:38:52Z","published":"2023-12-11T18:50:09Z","title":"TMT-VIS: Taxonomy-aware Multi-dataset Joint Training for Video Instance\n  Segmentation","summary":"  Training on large-scale datasets can boost the performance of video instance\nsegmentation while the annotated datasets for VIS are hard to scale up due to\nthe high labor cost. What we possess are numerous isolated filed-specific\ndatasets, thus, it is appealing to jointly train models across the aggregation\nof datasets to enhance data volume and diversity. However, due to the\nheterogeneity in category space, as mask precision increases with the data\nvolume, simply utilizing multiple datasets will dilute the attention of models\non different taxonomies. Thus, increasing the data scale and enriching taxonomy\nspace while improving classification precision is important. In this work, we\nanalyze that providing extra taxonomy information can help models concentrate\non specific taxonomy, and propose our model named Taxonomy-aware Multi-dataset\nJoint Training for Video Instance Segmentation (TMT-VIS) to address this vital\nchallenge. Specifically, we design a two-stage taxonomy aggregation module that\nfirst compiles taxonomy information from input videos and then aggregates these\ntaxonomy priors into instance queries before the transformer decoder. We\nconduct extensive experimental evaluations on four popular and challenging\nbenchmarks, including YouTube-VIS 2019, YouTube-VIS 2021, OVIS, and UVO. Our\nmodel shows significant improvement over the baseline solutions, and sets new\nstate-of-the-art records on all benchmarks. These appealing and encouraging\nresults demonstrate the effectiveness and generality of our approach. The code\nis available at\nhttps://github.com/rkzheng99/TMT-VIS(https://github.com/rkzheng99/TMT-VIS)\n","authors":["Rongkun Zheng","Lu Qi","Xi Chen","Yi Wang","Kun Wang","Yu Qiao","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2312.06630v2.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.12474v2","updated":"2023-12-12T05:37:59Z","published":"2023-10-19T05:15:17Z","title":"Enhancing High-Resolution 3D Generation through Pixel-wise Gradient\n  Clipping","summary":"  High-resolution 3D object generation remains a challenging task primarily due\nto the limited availability of comprehensive annotated training data. Recent\nadvancements have aimed to overcome this constraint by harnessing image\ngenerative models, pretrained on extensive curated web datasets, using\nknowledge transfer techniques like Score Distillation Sampling (SDS).\nEfficiently addressing the requirements of high-resolution rendering often\nnecessitates the adoption of latent representation-based models, such as the\nLatent Diffusion Model (LDM). In this framework, a significant challenge\narises: To compute gradients for individual image pixels, it is necessary to\nbackpropagate gradients from the designated latent space through the frozen\ncomponents of the image model, such as the VAE encoder used within LDM.\nHowever, this gradient propagation pathway has never been optimized, remaining\nuncontrolled during training. We find that the unregulated gradients adversely\naffect the 3D model's capacity in acquiring texture-related information from\nthe image generative model, leading to poor quality appearance synthesis. To\naddress this overarching challenge, we propose an innovative operation termed\nPixel-wise Gradient Clipping (PGC) designed for seamless integration into\nexisting 3D generative models, thereby enhancing their synthesis quality.\nSpecifically, we control the magnitude of stochastic gradients by clipping the\npixel-wise gradients efficiently, while preserving crucial texture-related\ngradient directions. Despite this simplicity and minimal extra cost, extensive\nexperiments demonstrate the efficacy of our PGC in enhancing the performance of\nexisting 3D generative models for high-resolution object rendering.\n","authors":["Zijie Pan","Jiachen Lu","Xiatian Zhu","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.12474v2.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2307.00910v2","updated":"2023-12-12T05:34:16Z","published":"2023-07-03T10:14:33Z","title":"CoPL: Contextual Prompt Learning for Vision-Language Understanding","summary":"  Recent advances in multimodal learning has resulted in powerful\nvision-language models, whose representations are generalizable across a\nvariety of downstream tasks. Recently, their generalization ability has been\nfurther extended by incorporating trainable prompts, borrowed from the natural\nlanguage processing literature. While such prompt learning techniques have\nshown impressive results, we identify that these prompts are trained based on\nglobal image features which limits itself in two aspects: First, by using\nglobal features, these prompts could be focusing less on the discriminative\nforeground image, resulting in poor generalization to various\nout-of-distribution test cases. Second, existing work weights all prompts\nequally whereas intuitively, prompts should be reweighed according to the\nsemantics of the image. We address these as part of our proposed Contextual\nPrompt Learning (CoPL) framework, capable of aligning the prompts to the\nlocalized features of the image. Our key innovations over earlier works include\nusing local image features as part of the prompt learning process, and more\ncrucially, learning to weight these prompts based on local features that are\nappropriate for the task at hand. This gives us dynamic prompts that are both\naligned to local image features as well as aware of local contextual\nrelationships. Our extensive set of experiments on a variety of standard and\nfew-shot datasets show that our method produces substantially improved\nperformance when compared to the current state of the art methods. We also\ndemonstrate both few-shot and out-of-distribution performance to establish the\nutility of learning dynamic prompts that are aligned to local image features.\n","authors":["Koustava Goswami","Srikrishna Karanam","Prateksha Udhayanan","K J Joseph","Balaji Vasan Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2307.00910v2.pdf","comment":"Accepted at AAAI 2024"},{"id":"http://arxiv.org/abs/2312.06991v1","updated":"2023-12-12T05:23:15Z","published":"2023-12-12T05:23:15Z","title":"Attacking the Loop: Adversarial Attacks on Graph-based Loop Closure\n  Detection","summary":"  With the advancement in robotics, it is becoming increasingly common for\nlarge factories and warehouses to incorporate visual SLAM (vSLAM) enabled\nautomated robots that operate closely next to humans. This makes any\nadversarial attacks on vSLAM components potentially detrimental to humans\nworking alongside them. Loop Closure Detection (LCD) is a crucial component in\nvSLAM that minimizes the accumulation of drift in mapping, since even a small\ndrift can accumulate into a significant drift over time. A prior work by Kim et\nal., SymbioLCD2, unified visual features and semantic objects into a single\ngraph structure for finding loop closure candidates. While this provided a\nperformance improvement over visual feature-based LCD, it also created a single\npoint of vulnerability for potential graph-based adversarial attacks. Unlike\npreviously reported visual-patch based attacks, small graph perturbations are\nfar more challenging to detect, making them a more significant threat. In this\npaper, we present Adversarial-LCD, a novel black-box evasion attack framework\nthat employs an eigencentrality-based perturbation method and an SVM-RBF\nsurrogate model with a Weisfeiler-Lehman feature extractor for attacking\ngraph-based LCD. Our evaluation shows that the attack performance of\nAdversarial-LCD with the SVM-RBF surrogate model was superior to that of other\nmachine learning surrogate algorithms, including SVM-linear, SVM-polynomial,\nand Bayesian classifier, demonstrating the effectiveness of our attack\nframework. Furthermore, we show that our eigencentrality-based perturbation\nmethod outperforms other algorithms, such as Random-walk and Shortest-path,\nhighlighting the efficiency of Adversarial-LCD's perturbation selection method.\n","authors":["Jonathan J. Y. Kim","Martin Urschler","Patricia J. Riddle","Jorg S. Wicker"],"pdf_url":"https://arxiv.org/pdf/2312.06991v1.pdf","comment":"Accepted at VISIGRAPP 2024, 8 pages"},{"id":"http://arxiv.org/abs/2312.04316v2","updated":"2023-12-12T05:20:59Z","published":"2023-12-07T14:17:17Z","title":"Towards Knowledge-driven Autonomous Driving","summary":"  This paper explores the emerging knowledge-driven autonomous driving\ntechnologies. Our investigation highlights the limitations of current\nautonomous driving systems, in particular their sensitivity to data bias,\ndifficulty in handling long-tail scenarios, and lack of interpretability.\nConversely, knowledge-driven methods with the abilities of cognition,\ngeneralization and life-long learning emerge as a promising way to overcome\nthese challenges. This paper delves into the essence of knowledge-driven\nautonomous driving and examines its core components: dataset \\& benchmark,\nenvironment, and driver agent. By leveraging large language models, world\nmodels, neural rendering, and other advanced artificial intelligence\ntechniques, these components collectively contribute to a more holistic,\nadaptive, and intelligent autonomous driving system. The paper systematically\norganizes and reviews previous research efforts in this area, and provides\ninsights and guidance for future research and practical applications of\nautonomous driving. We will continually share the latest updates on\ncutting-edge developments in knowledge-driven autonomous driving along with the\nrelevant valuable open-source resources at:\n\\url{https://github.com/PJLab-ADG/awesome-knowledge-driven-AD}.\n","authors":["Xin Li","Yeqi Bai","Pinlong Cai","Licheng Wen","Daocheng Fu","Bo Zhang","Xuemeng Yang","Xinyu Cai","Tao Ma","Jianfei Guo","Xing Gao","Min Dou","Botian Shi","Yong Liu","Liang He","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2312.04316v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06988v1","updated":"2023-12-12T05:12:22Z","published":"2023-12-12T05:12:22Z","title":"MWSIS: Multimodal Weakly Supervised Instance Segmentation with 2D Box\n  Annotations for Autonomous Driving","summary":"  Instance segmentation is a fundamental research in computer vision,\nespecially in autonomous driving. However, manual mask annotation for instance\nsegmentation is quite time-consuming and costly. To address this problem, some\nprior works attempt to apply weakly supervised manner by exploring 2D or 3D\nboxes. However, no one has ever successfully segmented 2D and 3D instances\nsimultaneously by only using 2D box annotations, which could further reduce the\nannotation cost by an order of magnitude. Thus, we propose a novel framework\ncalled Multimodal Weakly Supervised Instance Segmentation (MWSIS), which\nincorporates various fine-grained label generation and correction modules for\nboth 2D and 3D modalities to improve the quality of pseudo labels, along with a\nnew multimodal cross-supervision approach, named Consistency Sparse Cross-modal\nSupervision (CSCS), to reduce the inconsistency of multimodal predictions by\nresponse distillation. Particularly, transferring the 3D backbone to downstream\ntasks not only improves the performance of the 3D detectors, but also\noutperforms fully supervised instance segmentation with only 5% fully\nsupervised annotations. On the Waymo dataset, the proposed framework\ndemonstrates significant improvements over the baseline, especially achieving\n2.59% mAP and 12.75% mAP increases for 2D and 3D instance segmentation tasks,\nrespectively. The code is available at\nhttps://github.com/jiangxb98/mwsis-plugin.\n","authors":["Guangfeng Jiang","Jun Liu","Yuzhi Wu","Wenlong Liao","Tao He","Pai Peng"],"pdf_url":"https://arxiv.org/pdf/2312.06988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16808v2","updated":"2023-12-12T05:02:59Z","published":"2023-09-28T19:30:26Z","title":"Granularity at Scale: Estimating Neighborhood Socioeconomic Indicators\n  from High-Resolution Orthographic Imagery and Hybrid Learning","summary":"  Many areas of the world are without basic information on the socioeconomic\nwell-being of the residing population due to limitations in existing data\ncollection methods. Overhead images obtained remotely, such as from satellite\nor aircraft, can help serve as windows into the state of life on the ground and\nhelp \"fill in the gaps\" where community information is sparse, with estimates\nat smaller geographic scales requiring higher resolution sensors. Concurrent\nwith improved sensor resolutions, recent advancements in machine learning and\ncomputer vision have made it possible to quickly extract features from and\ndetect patterns in image data, in the process correlating these features with\nother information. In this work, we explore how well two approaches, a\nsupervised convolutional neural network and semi-supervised clustering based on\nbag-of-visual-words, estimate population density, median household income, and\neducational attainment of individual neighborhoods from publicly available\nhigh-resolution imagery of cities throughout the United States. Results and\nanalyses indicate that features extracted from the imagery can accurately\nestimate the density (R$^2$ up to 0.81) of neighborhoods, with the supervised\napproach able to explain about half the variation in a population's income and\neducation. In addition to the presented approaches serving as a basis for\nfurther geographic generalization, the novel semi-supervised approach provides\na foundation for future work seeking to estimate fine-scale information from\naerial imagery without the need for label data.\n","authors":["Ethan Brewer","Giovani Valdrighi","Parikshit Solunke","Joao Rulff","Yurii Piadyk","Zhonghui Lv","Jorge Poco","Claudio Silva"],"pdf_url":"https://arxiv.org/pdf/2309.16808v2.pdf","comment":"Updating after a round of revisions with IEEE J-STARS"},{"id":"http://arxiv.org/abs/2309.09431v3","updated":"2023-12-12T04:47:38Z","published":"2023-09-18T02:05:52Z","title":"FactoFormer: Factorized Hyperspectral Transformers with Self-Supervised\n  Pre-Training","summary":"  Hyperspectral images (HSIs) contain rich spectral and spatial information.\nMotivated by the success of transformers in the field of natural language\nprocessing and computer vision where they have shown the ability to learn long\nrange dependencies within input data, recent research has focused on using\ntransformers for HSIs. However, current state-of-the-art hyperspectral\ntransformers only tokenize the input HSI sample along the spectral dimension,\nresulting in the under-utilization of spatial information. Moreover,\ntransformers are known to be data-hungry and their performance relies heavily\non large-scale pre-training, which is challenging due to limited annotated\nhyperspectral data. Therefore, the full potential of HSI transformers has not\nbeen fully realized. To overcome these limitations, we propose a novel\nfactorized spectral-spatial transformer that incorporates factorized\nself-supervised pre-training procedures, leading to significant improvements in\nperformance. The factorization of the inputs allows the spectral and spatial\ntransformers to better capture the interactions within the hyperspectral data\ncubes. Inspired by masked image modeling pre-training, we also devise efficient\nmasking strategies for pre-training each of the spectral and spatial\ntransformers. We conduct experiments on six publicly available datasets for HSI\nclassification task and demonstrate that our model achieves state-of-the-art\nperformance in all the datasets. The code for our model will be made available\nat https://github.com/csiro-robotics/factoformer.\n","authors":["Shaheer Mohamed","Maryam Haghighat","Tharindu Fernando","Sridha Sridharan","Clinton Fookes","Peyman Moghadam"],"pdf_url":"https://arxiv.org/pdf/2309.09431v3.pdf","comment":"Accepted to IEEE Transactions on Geoscience and Remote Sensing in\n  December 2023"},{"id":"http://arxiv.org/abs/2312.03594v3","updated":"2023-12-12T04:44:42Z","published":"2023-12-06T16:34:46Z","title":"A Task is Worth One Word: Learning with Task Prompts for High-Quality\n  Versatile Image Inpainting","summary":"  Achieving high-quality versatile image inpainting, where user-specified\nregions are filled with plausible content according to user intent, presents a\nsignificant challenge. Existing methods face difficulties in simultaneously\naddressing context-aware image inpainting and text-guided object inpainting due\nto the distinct optimal training strategies required. To overcome this\nchallenge, we introduce PowerPaint, the first high-quality and versatile\ninpainting model that excels in both tasks. First, we introduce learnable task\nprompts along with tailored fine-tuning strategies to guide the model's focus\non different inpainting targets explicitly. This enables PowerPaint to\naccomplish various inpainting tasks by utilizing different task prompts,\nresulting in state-of-the-art performance. Second, we demonstrate the\nversatility of the task prompt in PowerPaint by showcasing its effectiveness as\na negative prompt for object removal. Additionally, we leverage prompt\ninterpolation techniques to enable controllable shape-guided object inpainting.\nFinally, we extensively evaluate PowerPaint on various inpainting benchmarks to\ndemonstrate its superior performance for versatile image inpainting. We release\nour codes and models on our project page: https://powerpaint.github.io/.\n","authors":["Junhao Zhuang","Yanhong Zeng","Wenran Liu","Chun Yuan","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2312.03594v3.pdf","comment":"Project page with code: https://powerpaint.github.io/"},{"id":"http://arxiv.org/abs/2312.06979v1","updated":"2023-12-12T04:41:20Z","published":"2023-12-12T04:41:20Z","title":"On the notion of Hallucinations from the lens of Bias and Validity in\n  Synthetic CXR Images","summary":"  Medical imaging has revolutionized disease diagnosis, yet the potential is\nhampered by limited access to diverse and privacy-conscious datasets.\nOpen-source medical datasets, while valuable, suffer from data quality and\nclinical information disparities. Generative models, such as diffusion models,\naim to mitigate these challenges. At Stanford, researchers explored the utility\nof a fine-tuned Stable Diffusion model (RoentGen) for medical imaging data\naugmentation. Our work examines specific considerations to expand the Stanford\nresearch question, Could Stable Diffusion Solve a Gap in Medical Imaging Data?\nfrom the lens of bias and validity of the generated outcomes. We leveraged\nRoentGen to produce synthetic Chest-XRay (CXR) images and conducted assessments\non bias, validity, and hallucinations. Diagnostic accuracy was evaluated by a\ndisease classifier, while a COVID classifier uncovered latent hallucinations.\nThe bias analysis unveiled disparities in classification performance among\nvarious subgroups, with a pronounced impact on the Female Hispanic subgroup.\nFurthermore, incorporating race and gender into input prompts exacerbated\nfairness issues in the generated images. The quality of synthetic images\nexhibited variability, particularly in certain disease classes, where there was\nmore significant uncertainty compared to the original images. Additionally, we\nobserved latent hallucinations, with approximately 42% of the images\nincorrectly indicating COVID, hinting at the presence of hallucinatory\nelements. These identifications provide new research directions towards\ninterpretability of synthetic CXR images, for further understanding of\nassociated risks and patient safety in medical applications.\n","authors":["Gauri Bhardwaj","Yuvaraj Govindarajulu","Sundaraparipurnan Narayanan","Pavan Kulkarni","Manojkumar Parmar"],"pdf_url":"https://arxiv.org/pdf/2312.06979v1.pdf","comment":"Accepted at 37th Conference on Neural Information Processing Systems\n  (NeurIPS 2023) - \"Medical Imaging Meets NeurIPS\" Workshop"},{"id":"http://arxiv.org/abs/2312.06978v1","updated":"2023-12-12T04:38:30Z","published":"2023-12-12T04:38:30Z","title":"CLASSMix: Adaptive stain separation-based contrastive learning with\n  pseudo labeling for histopathological image classification","summary":"  Histopathological image classification is one of the critical aspects in\nmedical image analysis. Due to the high expense associated with the labeled\ndata in model training, semi-supervised learning methods have been proposed to\nalleviate the need of extensively labeled datasets. In this work, we propose a\nmodel for semi-supervised classification tasks on digital histopathological\nHematoxylin and Eosin (H&E) images. We call the new model Contrastive Learning\nwith Adaptive Stain Separation and MixUp (CLASSMix). Our model is formed by two\nmain parts: contrastive learning between adaptively stain separated Hematoxylin\nimages and Eosin images, and pseudo labeling using MixUp. We compare our model\nwith other state-of-the-art models on clear cell renal cell carcinoma (ccRCC)\ndatasets from our institution and The Cancer Genome Atlas Program (TCGA). We\ndemonstrate that our CLASSMix model has the best performance on both datasets.\nThe contributions of different parts in our model are also analyzed.\n","authors":["Bodong Zhang","Hamid Manoochehri","Man Minh Ho","Fahimeh Fooladgar","Yosep Chong","Beatrice S. Knudsen","Deepika Sirohi","Tolga Tasdizen"],"pdf_url":"https://arxiv.org/pdf/2312.06978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06409v2","updated":"2023-12-12T04:37:20Z","published":"2023-12-11T14:30:11Z","title":"PointVoxel: A Simple and Effective Pipeline for Multi-View Multi-Modal\n  3D Human Pose Estimation","summary":"  Recently, several methods have been proposed to estimate 3D human pose from\nmulti-view images and achieved impressive performance on public datasets\ncollected in relatively easy scenarios. However, there are limited approaches\nfor extracting 3D human skeletons from multimodal inputs (e.g., RGB and\npointcloud) that can enhance the accuracy of predicting 3D poses in challenging\nsituations. We fill this gap by introducing a pipeline called PointVoxel that\nfuses multi-view RGB and pointcloud inputs to obtain 3D human poses. We\ndemonstrate that volumetric representation is an effective architecture for\nintegrating these different modalities. Moreover, in order to overcome the\nchallenges of annotating 3D human pose labels in difficult scenarios, we\ndevelop a synthetic dataset generator for pretraining and design an\nunsupervised domain adaptation strategy so that we can obtain a well-trained 3D\nhuman pose estimator without using any manual annotations. We evaluate our\napproach on four datasets (two public datasets, one synthetic dataset, and one\nchallenging dataset named BasketBall collected by ourselves), showing promising\nresults. The code and dataset will be released soon.\n","authors":["Zhiyu Pan","Zhicheng Zhong","Wenxuan Guo","Yifan Chen","Jianjiang Feng","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2312.06409v2.pdf","comment":"14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2312.06971v1","updated":"2023-12-12T04:16:03Z","published":"2023-12-12T04:16:03Z","title":"CCM: Adding Conditional Controls to Text-to-Image Consistency Models","summary":"  Consistency Models (CMs) have showed a promise in creating visual content\nefficiently and with high quality. However, the way to add new conditional\ncontrols to the pretrained CMs has not been explored. In this technical report,\nwe consider alternative strategies for adding ControlNet-like conditional\ncontrol to CMs and present three significant findings. 1) ControlNet trained\nfor diffusion models (DMs) can be directly applied to CMs for high-level\nsemantic controls but struggles with low-level detail and realism control. 2)\nCMs serve as an independent class of generative models, based on which\nControlNet can be trained from scratch using Consistency Training proposed by\nSong et al. 3) A lightweight adapter can be jointly optimized under multiple\nconditions through Consistency Training, allowing for the swift transfer of\nDMs-based ControlNet to CMs. We study these three solutions across various\nconditional controls, including edge, depth, human pose, low-resolution image\nand masked image with text-to-image latent consistency models.\n","authors":["Jie Xiao","Kai Zhu","Han Zhang","Zhiheng Liu","Yujun Shen","Yu Liu","Xueyang Fu","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2312.06971v1.pdf","comment":"Project Page: https://swiftforce.github.io/CCM"},{"id":"http://arxiv.org/abs/2312.06968v1","updated":"2023-12-12T04:05:15Z","published":"2023-12-12T04:05:15Z","title":"Hallucination Augmented Contrastive Learning for Multimodal Large\n  Language Model","summary":"  Multi-modal large language models (MLLMs) have been shown to efficiently\nintegrate natural language with visual information to handle multi-modal tasks.\nHowever, MLLMs still face a fundamental limitation of hallucinations, where\nthey tend to generate erroneous or fabricated information. In this paper, we\naddress hallucinations in MLLMs from a novel perspective of representation\nlearning. We first analyzed the representation distribution of textual and\nvisual tokens in MLLM, revealing two important findings: 1) there is a\nsignificant gap between textual and visual representations, indicating\nunsatisfactory cross-modal representation alignment; 2) representations of\ntexts that contain and do not contain hallucinations are entangled, making it\nchallenging to distinguish them. These two observations inspire us with a\nsimple yet effective method to mitigate hallucinations. Specifically, we\nintroduce contrastive learning into MLLMs and use text with hallucination as\nhard negative examples, naturally bringing representations of non-hallucinative\ntext and visual samples closer while pushing way representations of\nnon-hallucinating and hallucinative text. We evaluate our method quantitatively\nand qualitatively, showing its effectiveness in reducing hallucination\noccurrences and improving performance across multiple benchmarks. On the\nMMhal-Bench benchmark, our method obtains a 34.66% /29.5% improvement over the\nbaseline MiniGPT-4/LLaVA.\n","authors":["Chaoya Jiang","Haiyang Xu","Mengfan Dong","Jiaxing Chen","Wei Ye","Ming Yan","Qinghao Ye","Ji Zhang","Fei Huang","Shikun Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.06968v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05743v2","updated":"2023-12-12T03:56:33Z","published":"2023-12-10T03:46:01Z","title":"Building Variable-sized Models via Learngene Pool","summary":"  Recently, Stitchable Neural Networks (SN-Net) is proposed to stitch some\npre-trained networks for quickly building numerous networks with different\ncomplexity and performance trade-offs. In this way, the burdens of designing or\ntraining the variable-sized networks, which can be used in application\nscenarios with diverse resource constraints, are alleviated. However, SN-Net\nstill faces a few challenges. 1) Stitching from multiple independently\npre-trained anchors introduces high storage resource consumption. 2) SN-Net\nfaces challenges to build smaller models for low resource constraints. 3).\nSN-Net uses an unlearned initialization method for stitch layers, limiting the\nfinal performance. To overcome these challenges, motivated by the recently\nproposed Learngene framework, we propose a novel method called Learngene Pool.\nBriefly, Learngene distills the critical knowledge from a large pre-trained\nmodel into a small part (termed as learngene) and then expands this small part\ninto a few variable-sized models. In our proposed method, we distill one\npretrained large model into multiple small models whose network blocks are used\nas learngene instances to construct the learngene pool. Since only one large\nmodel is used, we do not need to store more large models as SN-Net and after\ndistilling, smaller learngene instances can be created to build small models to\nsatisfy low resource constraints. We also insert learnable transformation\nmatrices between the instances to stitch them into variable-sized models to\nimprove the performance of these models. Exhaustive experiments have been\nimplemented and the results validate the effectiveness of the proposed\nLearngene Pool compared with SN-Net.\n","authors":["Boyu Shi","Shiyu Xia","Xu Yang","Haokun Chen","Zhiqiang Kou","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2312.05743v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06960v1","updated":"2023-12-12T03:39:07Z","published":"2023-12-12T03:39:07Z","title":"Remote Sensing Vision-Language Foundation Models without Annotations via\n  Ground Remote Alignment","summary":"  We introduce a method to train vision-language models for remote-sensing\nimages without using any textual annotations. Our key insight is to use\nco-located internet imagery taken on the ground as an intermediary for\nconnecting remote-sensing images and language. Specifically, we train an image\nencoder for remote sensing images to align with the image encoder of CLIP using\na large amount of paired internet and satellite images. Our unsupervised\napproach enables the training of a first-of-its-kind large-scale vision\nlanguage model (VLM) for remote sensing images at two different resolutions. We\nshow that these VLMs enable zero-shot, open-vocabulary image classification,\nretrieval, segmentation and visual question answering for satellite images. On\neach of these tasks, our VLM trained without textual annotations outperforms\nexisting VLMs trained with supervision, with gains of up to 20% for\nclassification and 80% for segmentation.\n","authors":["Utkarsh Mall","Cheng Perng Phoo","Meilin Kelsey Liu","Carl Vondrick","Bharath Hariharan","Kavita Bala"],"pdf_url":"https://arxiv.org/pdf/2312.06960v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06958v1","updated":"2023-12-12T03:37:57Z","published":"2023-12-12T03:37:57Z","title":"PatchMorph: A Stochastic Deep Learning Approach for Unsupervised 3D\n  Brain Image Registration with Small Patches","summary":"  We introduce \"PatchMorph,\" an new stochastic deep learning algorithm tailored\nfor unsupervised 3D brain image registration. Unlike other methods, our method\nuses compact patches of a constant small size to derive solutions that can\ncombine global transformations with local deformations. This approach minimizes\nthe memory footprint of the GPU during training, but also enables us to operate\non numerous amounts of randomly overlapping small patches during inference to\nmitigate image and patch boundary problems. PatchMorph adeptly handles world\ncoordinate transformations between two input images, accommodating variances in\nattributes such as spacing, array sizes, and orientations. The spatial\nresolution of patches transitions from coarse to fine, addressing both global\nand local attributes essential for aligning the images. Each patch offers a\nunique perspective, together converging towards a comprehensive solution.\nExperiments on human T1 MRI brain images and marmoset brain images from serial\n2-photon tomography affirm PatchMorph's superior performance.\n","authors":["Henrik Skibbe","Michal Byra","Akiya Watakabe","Tetsuo Yamamori","Marco Reisert"],"pdf_url":"https://arxiv.org/pdf/2312.06958v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2312.07806v1","updated":"2023-12-12T23:56:51Z","published":"2023-12-12T23:56:51Z","title":"Contextually Affinitive Neighborhood Refinery for Deep Clustering","summary":"  Previous endeavors in self-supervised learning have enlightened the research\nof deep clustering from an instance discrimination perspective. Built upon this\nfoundation, recent studies further highlight the importance of grouping\nsemantically similar instances. One effective method to achieve this is by\npromoting the semantic structure preserved by neighborhood consistency.\nHowever, the samples in the local neighborhood may be limited due to their\nclose proximity to each other, which may not provide substantial and diverse\nsupervision signals. Inspired by the versatile re-ranking methods in the\ncontext of image retrieval, we propose to employ an efficient online re-ranking\nprocess to mine more informative neighbors in a Contextually Affinitive\n(ConAff) Neighborhood, and then encourage the cross-view neighborhood\nconsistency. To further mitigate the intrinsic neighborhood noises near cluster\nboundaries, we propose a progressively relaxed boundary filtering strategy to\ncircumvent the issues brought by noisy neighbors. Our method can be easily\nintegrated into the generic self-supervised frameworks and outperforms the\nstate-of-the-art methods on several popular benchmarks.\n","authors":["Chunlin Yu","Ye Shi","Jingya Wang"],"pdf_url":"https://arxiv.org/pdf/2312.07806v1.pdf","comment":"Accepted to NeurIPS 2023"},{"id":"http://arxiv.org/abs/2312.07804v1","updated":"2023-12-12T23:51:07Z","published":"2023-12-12T23:51:07Z","title":"Uncertainty Visualization via Low-Dimensional Posterior Projections","summary":"  In ill-posed inverse problems, it is commonly desirable to obtain insight\ninto the full spectrum of plausible solutions, rather than extracting only a\nsingle reconstruction. Information about the plausible solutions and their\nlikelihoods is encoded in the posterior distribution. However, for\nhigh-dimensional data, this distribution is challenging to visualize. In this\nwork, we introduce a new approach for estimating and visualizing posteriors by\nemploying energy-based models (EBMs) over low-dimensional subspaces.\nSpecifically, we train a conditional EBM that receives an input measurement and\na set of directions that span some low-dimensional subspace of solutions, and\noutputs the probability density function of the posterior within that space. We\ndemonstrate the effectiveness of our method across a diverse range of datasets\nand image restoration problems, showcasing its strength in uncertainty\nquantification and visualization. As we show, our method outperforms a baseline\nthat projects samples from a diffusion-based posterior sampler, while being\norders of magnitude faster. Furthermore, it is more accurate than a baseline\nthat assumes a Gaussian posterior.\n","authors":["Omer Yair","Elias Nehme","Tomer Michaeli"],"pdf_url":"https://arxiv.org/pdf/2312.07804v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.07050v3","updated":"2023-12-12T23:00:25Z","published":"2023-06-12T11:55:33Z","title":"Revisiting Token Pruning for Object Detection and Instance Segmentation","summary":"  Vision Transformers (ViTs) have shown impressive performance in computer\nvision, but their high computational cost, quadratic in the number of tokens,\nlimits their adoption in computation-constrained applications. However, this\nlarge number of tokens may not be necessary, as not all tokens are equally\nimportant. In this paper, we investigate token pruning to accelerate inference\nfor object detection and instance segmentation, extending prior works from\nimage classification. Through extensive experiments, we offer four insights for\ndense tasks: (i) tokens should not be completely pruned and discarded, but\nrather preserved in the feature maps for later use. (ii) reactivating\npreviously pruned tokens can further enhance model performance. (iii) a dynamic\npruning rate based on images is better than a fixed pruning rate. (iv) a\nlightweight, 2-layer MLP can effectively prune tokens, achieving accuracy\ncomparable with complex gating networks with a simpler design. We assess the\neffects of these design decisions on the COCO dataset and introduce an approach\nthat incorporates these findings, showing a reduction in performance decline\nfrom ~1.5 mAP to ~0.3 mAP in both boxes and masks, compared to existing token\npruning methods. In relation to the dense counterpart that utilizes all tokens,\nour method realizes an increase in inference speed, achieving up to 34% faster\nperformance for the entire network and 46% for the backbone.\n","authors":["Yifei Liu","Mathias Gehrig","Nico Messikommer","Marco Cannici","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2306.07050v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07784v1","updated":"2023-12-12T22:57:14Z","published":"2023-12-12T22:57:14Z","title":"Robust MRI Reconstruction by Smoothed Unrolling (SMUG)","summary":"  As the popularity of deep learning (DL) in the field of magnetic resonance\nimaging (MRI) continues to rise, recent research has indicated that DL-based\nMRI reconstruction models might be excessively sensitive to minor input\ndisturbances, including worst-case additive perturbations. This sensitivity\noften leads to unstable, aliased images. This raises the question of how to\ndevise DL techniques for MRI reconstruction that can be robust to train-test\nvariations. To address this problem, we propose a novel image reconstruction\nframework, termed Smoothed Unrolling (SMUG), which advances a deep\nunrolling-based MRI reconstruction model using a randomized smoothing\n(RS)-based robust learning approach. RS, which improves the tolerance of a\nmodel against input noises, has been widely used in the design of adversarial\ndefense approaches for image classification tasks. Yet, we find that the\nconventional design that applies RS to the entire DL-based MRI model is\nineffective. In this paper, we show that SMUG and its variants address the\nabove issue by customizing the RS process based on the unrolling architecture\nof a DL-based MRI reconstruction model. Compared to the vanilla RS approach, we\nshow that SMUG improves the robustness of MRI reconstruction with respect to a\ndiverse set of instability sources, including worst-case and random noise\nperturbations to input measurements, varying measurement sampling rates, and\ndifferent numbers of unrolling steps. Furthermore, we theoretically analyze the\nrobustness of our method in the presence of perturbations.\n","authors":["Shijun Liang","Van Hoang Minh Nguyen","Jinghan Jia","Ismail Alkhouri","Sijia Liu","Saiprasad Ravishankar"],"pdf_url":"https://arxiv.org/pdf/2312.07784v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.01881v6","updated":"2023-12-12T22:56:44Z","published":"2022-03-03T17:48:23Z","title":"Measuring Self-Supervised Representation Quality for Downstream\n  Classification using Discriminative Features","summary":"  Self-supervised learning (SSL) has shown impressive results in downstream\nclassification tasks. However, there is limited work in understanding their\nfailure modes and interpreting their learned representations. In this paper, we\nstudy the representation space of state-of-the-art self-supervised models\nincluding SimCLR, SwaV, MoCo, BYOL, DINO, SimSiam, VICReg and Barlow Twins.\nWithout the use of class label information, we discover discriminative features\nthat correspond to unique physical attributes in images, present mostly in\ncorrectly-classified representations. Using these features, we can compress the\nrepresentation space by up to 40% without significantly affecting linear\nclassification performance. We then propose Self-Supervised Representation\nQuality Score (or Q-Score), an unsupervised score that can reliably predict if\na given sample is likely to be mis-classified during linear evaluation,\nachieving AUPRC of 91.45 on ImageNet-100 and 78.78 on ImageNet-1K. Q-Score can\nalso be used as a regularization term on pre-trained encoders to remedy\nlow-quality representations. Fine-tuning with Q-Score regularization can boost\nthe linear probing accuracy of SSL models by up to 5.8% on ImageNet-100 and\n3.7% on ImageNet-1K compared to their baselines. Finally, using gradient\nheatmaps and Salient ImageNet masks, we define a metric to quantify the\ninterpretability of each representation. We show that discriminative features\nare strongly correlated to core attributes and, enhancing these features\nthrough Q-score regularization makes SSL representations more interpretable.\n","authors":["Neha Kalibhat","Kanika Narang","Hamed Firooz","Maziar Sanjabi","Soheil Feizi"],"pdf_url":"https://arxiv.org/pdf/2203.01881v6.pdf","comment":"Published at AAAI 2024"},{"id":"http://arxiv.org/abs/2310.03149v3","updated":"2023-12-12T22:42:29Z","published":"2023-10-04T20:26:59Z","title":"Attributing Learned Concepts in Neural Networks to Training Data","summary":"  By now there is substantial evidence that deep learning models learn certain\nhuman-interpretable features as part of their internal representations of data.\nAs having the right (or wrong) concepts is critical to trustworthy machine\nlearning systems, it is natural to ask which inputs from the model's original\ntraining set were most important for learning a concept at a given layer. To\nanswer this, we combine data attribution methods with methods for probing the\nconcepts learned by a model. Training network and probe ensembles for two\nconcept datasets on a range of network layers, we use the recently developed\nTRAK method for large-scale data attribution. We find some evidence for\nconvergence, where removing the 10,000 top attributing images for a concept and\nretraining the model does not change the location of the concept in the network\nnor the probing sparsity of the concept. This suggests that rather than being\nhighly dependent on a few specific examples, the features that inform the\ndevelopment of a concept are spread in a more diffuse manner across its\nexemplars, implying robustness in concept formation.\n","authors":["Nicholas Konz","Charles Godfrey","Madelyn Shapiro","Jonathan Tu","Henry Kvinge","Davis Brown"],"pdf_url":"https://arxiv.org/pdf/2310.03149v3.pdf","comment":"ATTRIB Workshop at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.05718v2","updated":"2023-12-12T22:13:57Z","published":"2023-10-09T13:39:26Z","title":"EdVAE: Mitigating Codebook Collapse with Evidential Discrete Variational\n  Autoencoders","summary":"  Codebook collapse is a common problem in training deep generative models with\ndiscrete representation spaces like Vector Quantized Variational Autoencoders\n(VQ-VAEs). We observe that the same problem arises for the alternatively\ndesigned discrete variational autoencoders (dVAEs) whose encoder directly\nlearns a distribution over the codebook embeddings to represent the data. We\nhypothesize that using the softmax function to obtain a probability\ndistribution causes the codebook collapse by assigning overconfident\nprobabilities to the best matching codebook elements. In this paper, we propose\na novel way to incorporate evidential deep learning (EDL) instead of softmax to\ncombat the codebook collapse problem of dVAE. We evidentially monitor the\nsignificance of attaining the probability distribution over the codebook\nembeddings, in contrast to softmax usage. Our experiments using various\ndatasets show that our model, called EdVAE, mitigates codebook collapse while\nimproving the reconstruction performance, and enhances the codebook usage\ncompared to dVAE and VQ-VAE based models. Our code can be found at\nhttps://github.com/ituvisionlab/EdVAE .\n","authors":["Gulcin Baykal","Melih Kandemir","Gozde Unal"],"pdf_url":"https://arxiv.org/pdf/2310.05718v2.pdf","comment":"submitted to Pattern Recognition"},{"id":"http://arxiv.org/abs/2312.07729v1","updated":"2023-12-12T20:46:14Z","published":"2023-12-12T20:46:14Z","title":"MedYOLO: A Medical Image Object Detection Framework","summary":"  Artificial intelligence-enhanced identification of organs, lesions, and other\nstructures in medical imaging is typically done using convolutional neural\nnetworks (CNNs) designed to make voxel-accurate segmentations of the region of\ninterest. However, the labels required to train these CNNs are time-consuming\nto generate and require attention from subject matter experts to ensure\nquality. For tasks where voxel-level precision is not required, object\ndetection models offer a viable alternative that can reduce annotation effort.\nDespite this potential application, there are few options for general purpose\nobject detection frameworks available for 3-D medical imaging. We report on\nMedYOLO, a 3-D object detection framework using the one-shot detection method\nof the YOLO family of models and designed for use with medical imaging. We\ntested this model on four different datasets: BRaTS, LIDC, an abdominal organ\nComputed Tomography (CT) dataset, and an ECG-gated heart CT dataset. We found\nour models achieve high performance on commonly present medium and large-sized\nstructures such as the heart, liver, and pancreas even without hyperparameter\ntuning. However, the models struggle with very small or rarely present\nstructures.\n","authors":["Joseph Sobek","Jose R. Medina Inojosa","Betsy J. Medina Inojosa","S. M. Rassoulinejad-Mousavi","Gian Marco Conte","Francisco Lopez-Jimenez","Bradley J. Erickson"],"pdf_url":"https://arxiv.org/pdf/2312.07729v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07723v1","updated":"2023-12-12T20:36:36Z","published":"2023-12-12T20:36:36Z","title":"Automated Behavioral Analysis Using Instance Segmentation","summary":"  Animal behavior analysis plays a crucial role in various fields, such as life\nscience and biomedical research. However, the scarcity of available data and\nthe high cost associated with obtaining a large number of labeled datasets pose\nsignificant challenges. In this research, we propose a novel approach that\nleverages instance segmentation-based transfer learning to address these\nissues. By capitalizing on fine-tuning the classification head of the instance\nsegmentation network, we enable the tracking of multiple animals and facilitate\nbehavior analysis in laboratory-recorded videos. To demonstrate the\neffectiveness of our method, we conducted a series of experiments, revealing\nthat our approach achieves exceptional performance levels, comparable to human\ncapabilities, across a diverse range of animal behavior analysis tasks.\nMoreover, we emphasize the practicality of our solution, as it requires only a\nsmall number of labeled images for training. To facilitate the adoption and\nfurther development of our method, we have developed an open-source\nimplementation named Annolid (An annotation and instance segmentation-based\nmultiple animal tracking and behavior analysis package). The codebase is\npublicly available on GitHub at https://github.com/cplab/annolid. This resource\nserves as a valuable asset for researchers and practitioners interested in\nadvancing animal behavior analysis through state-of-the-art techniques.\n","authors":["Chen Yang","Jeremy Forest","Matthew Einhorn","Thomas A. Cleland"],"pdf_url":"https://arxiv.org/pdf/2312.07723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.09337v2","updated":"2023-12-12T20:35:03Z","published":"2023-06-15T17:59:20Z","title":"Generative Proxemics: A Prior for 3D Social Interaction from Images","summary":"  Social interaction is a fundamental aspect of human behavior and\ncommunication. The way individuals position themselves in relation to others,\nalso known as proxemics, conveys social cues and affects the dynamics of social\ninteraction. Reconstructing such interaction from images presents challenges\nbecause of mutual occlusion and the limited availability of large training\ndatasets. To address this, we present a novel approach that learns a prior over\nthe 3D proxemics two people in close social interaction and demonstrate its use\nfor single-view 3D reconstruction. We start by creating 3D training data of\ninteracting people using image datasets with contact annotations. We then model\nthe proxemics using a novel denoising diffusion model called BUDDI that learns\nthe joint distribution over the poses of two people in close social\ninteraction. Sampling from our generative proxemics model produces realistic 3D\nhuman interactions, which we validate through a perceptual study. We use BUDDI\nin reconstructing two people in close proximity from a single image without any\ncontact annotation via an optimization approach that uses the diffusion model\nas a prior. Our approach recovers accurate and plausible 3D social interactions\nfrom noisy initial estimates, outperforming state-of-the-art methods. Our code,\ndata, and model are availableat our project website at: muelea.github.io/buddi.\n","authors":["Lea Müller","Vickie Ye","Georgios Pavlakos","Michael Black","Angjoo Kanazawa"],"pdf_url":"https://arxiv.org/pdf/2306.09337v2.pdf","comment":"Project website: muelea.github.io/buddi"},{"id":"http://arxiv.org/abs/2309.00752v2","updated":"2023-12-12T20:10:04Z","published":"2023-09-01T22:31:32Z","title":"Affine-Transformation-Invariant Image Classification by Differentiable\n  Arithmetic Distribution Module","summary":"  Although Convolutional Neural Networks (CNNs) have achieved promising results\nin image classification, they still are vulnerable to affine transformations\nincluding rotation, translation, flip and shuffle. The drawback motivates us to\ndesign a module which can alleviate the impact from different affine\ntransformations. Thus, in this work, we introduce a more robust substitute by\nincorporating distribution learning techniques, focusing particularly on\nlearning the spatial distribution information of pixels in images. To rectify\nthe issue of non-differentiability of prior distribution learning methods that\nrely on traditional histograms, we adopt the Kernel Density Estimation (KDE) to\nformulate differentiable histograms. On this foundation, we present a novel\nDifferentiable Arithmetic Distribution Module (DADM), which is designed to\nextract the intrinsic probability distributions from images. The proposed\napproach is able to enhance the model's robustness to affine transformations\nwithout sacrificing its feature extraction capabilities, thus bridging the gap\nbetween traditional CNNs and distribution-based learning. We validate the\neffectiveness of the proposed approach through ablation study and comparative\nexperiments with LeNet.\n","authors":["Zijie Tan","Guanfang Dong","Chenqiu Zhao","Anup Basu"],"pdf_url":"https://arxiv.org/pdf/2309.00752v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07705v1","updated":"2023-12-12T20:08:59Z","published":"2023-12-12T20:08:59Z","title":"Brain-optimized inference improves reconstructions of fMRI brain\n  activity","summary":"  The release of large datasets and developments in AI have led to dramatic\nimprovements in decoding methods that reconstruct seen images from human brain\nactivity. We evaluate the prospect of further improving recent decoding methods\nby optimizing for consistency between reconstructions and brain activity during\ninference. We sample seed reconstructions from a base decoding method, then\niteratively refine these reconstructions using a brain-optimized encoding model\nthat maps images to brain activity. At each iteration, we sample a small\nlibrary of images from an image distribution (a diffusion model) conditioned on\na seed reconstruction from the previous iteration. We select those that best\napproximate the measured brain activity when passed through our encoding model,\nand use these images for structural guidance during the generation of the small\nlibrary in the next iteration. We reduce the stochasticity of the image\ndistribution at each iteration, and stop when a criterion on the \"width\" of the\nimage distribution is met. We show that when this process is applied to recent\ndecoding methods, it outperforms the base decoding method as measured by human\nraters, a variety of image feature metrics, and alignment to brain activity.\nThese results demonstrate that reconstruction quality can be significantly\nimproved by explicitly aligning decoding distributions to brain activity\ndistributions, even when the seed reconstruction is output from a\nstate-of-the-art decoding algorithm. Interestingly, the rate of refinement\nvaries systematically across visual cortex, with earlier visual areas generally\nconverging more slowly and preferring narrower image distributions, relative to\nhigher-level brain areas. Brain-optimized inference thus offers a succinct and\nnovel method for improving reconstructions and exploring the diversity of\nrepresentations across visual brain areas.\n","authors":["Reese Kneeland","Jordyn Ojeda","Ghislain St-Yves","Thomas Naselaris"],"pdf_url":"https://arxiv.org/pdf/2312.07705v1.pdf","comment":"7 pages, 8 figures, submitted to the 2023 AAAI Workshop on Brain\n  Encoding and Decoding. arXiv admin note: text overlap with arXiv:2306.00927"},{"id":"http://arxiv.org/abs/2310.08320v2","updated":"2023-12-12T19:54:02Z","published":"2023-10-12T13:33:04Z","title":"Defending Our Privacy With Backdoors","summary":"  The proliferation of large AI models trained on uncurated, often sensitive\nweb-scraped data has raised significant privacy concerns. One of the concerns\nis that adversaries can extract information about the training data using\nprivacy attacks. Unfortunately, the task of removing specific information from\nthe models without sacrificing performance is not straightforward and has\nproven to be challenging. We propose a rather easy yet effective defense based\non backdoor attacks to remove private information such as names of individuals\nfrom models, and focus in this work on text encoders. Specifically, through\nstrategic insertion of backdoors, we align the embeddings of sensitive phrases\nwith those of neutral terms-\"a person\" instead of the person's name. Our\nempirical results demonstrate the effectiveness of our backdoor-based defense\non CLIP by assessing its performance using a specialized privacy attack for\nzero-shot classifiers. Our approach provides not only a new \"dual-use\"\nperspective on backdoor attacks, but also presents a promising avenue to\nenhance the privacy of individuals within models trained on uncurated\nweb-scraped data.\n","authors":["Dominik Hintersdorf","Lukas Struppek","Daniel Neider","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2310.08320v2.pdf","comment":"14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2212.01789v3","updated":"2023-12-12T19:15:59Z","published":"2022-12-04T10:40:35Z","title":"Multiscale Structure Guided Diffusion for Image Deblurring","summary":"  Diffusion Probabilistic Models (DPMs) have recently been employed for image\ndeblurring, formulated as an image-conditioned generation process that maps\nGaussian noise to the high-quality image, conditioned on the blurry input.\nImage-conditioned DPMs (icDPMs) have shown more realistic results than\nregression-based methods when trained on pairwise in-domain data. However,\ntheir robustness in restoring images is unclear when presented with\nout-of-domain images as they do not impose specific degradation models or\nintermediate constraints. To this end, we introduce a simple yet effective\nmultiscale structure guidance as an implicit bias that informs the icDPM about\nthe coarse structure of the sharp image at the intermediate layers. This guided\nformulation leads to a significant improvement of the deblurring results,\nparticularly on unseen domain. The guidance is extracted from the latent space\nof a regression network trained to predict the clean-sharp target at multiple\nlower resolutions, thus maintaining the most salient sharp structures. With\nboth the blurry input and multiscale guidance, the icDPM model can better\nunderstand the blur and recover the clean image. We evaluate a single-dataset\ntrained model on diverse datasets and demonstrate more robust deblurring\nresults with fewer artifacts on unseen data. Our method outperforms existing\nbaselines, achieving state-of-the-art perceptual quality while keeping\ncompetitive distortion metrics.\n","authors":["Mengwei Ren","Mauricio Delbracio","Hossein Talebi","Guido Gerig","Peyman Milanfar"],"pdf_url":"https://arxiv.org/pdf/2212.01789v3.pdf","comment":"Camera ready for ICCV2023"},{"id":"http://arxiv.org/abs/2312.07669v1","updated":"2023-12-12T19:03:04Z","published":"2023-12-12T19:03:04Z","title":"GMTalker: Gaussian Mixture based Emotional talking video Portraits","summary":"  Synthesizing high-fidelity and emotion-controllable talking video portraits,\nwith audio-lip sync, vivid expression, realistic head pose, and eye blink, is\nan important and challenging task in recent years. Most of the existing methods\nsuffer in achieving personalized precise emotion control or continuously\ninterpolating between different emotions and generating diverse motion. To\naddress these problems, we present GMTalker, a Gaussian mixture based emotional\ntalking portraits generation framework. Specifically, we propose a Gaussian\nMixture based Expression Generator (GMEG) which can construct a continuous and\nmulti-modal latent space, achieving more flexible emotion manipulation.\nFurthermore, we introduce a normalizing flow based motion generator pretrained\non the dataset with a wide-range motion to generate diverse motions. Finally,\nwe propose a personalized emotion-guided head generator with an Emotion Mapping\nNetwork (EMN) which can synthesize high-fidelity and faithful emotional video\nportraits. Both quantitative and qualitative experiments demonstrate our method\noutperforms previous methods in image quality, photo-realism, emotion accuracy\nand motion diversity.\n","authors":["Yibo Xia","Lizhen Wang","Xiang Deng","Xiaoyan Luo","Yebin Liu"],"pdf_url":"https://arxiv.org/pdf/2312.07669v1.pdf","comment":"Project page: https://bob35buaa.github.io/GMTalker"},{"id":"http://arxiv.org/abs/2312.07661v1","updated":"2023-12-12T19:00:04Z","published":"2023-12-12T19:00:04Z","title":"CLIP as RNN: Segment Countless Visual Concepts without Training Endeavor","summary":"  Existing open-vocabulary image segmentation methods require a fine-tuning\nstep on mask annotations and/or image-text datasets. Mask labels are\nlabor-intensive, which limits the number of categories in segmentation\ndatasets. As a result, the open-vocabulary capacity of pre-trained VLMs is\nseverely reduced after fine-tuning. However, without fine-tuning, VLMs trained\nunder weak image-text supervision tend to make suboptimal mask predictions when\nthere are text queries referring to non-existing concepts in the image. To\nalleviate these issues, we introduce a novel recurrent framework that\nprogressively filters out irrelevant texts and enhances mask quality without\ntraining efforts. The recurrent unit is a two-stage segmenter built upon a VLM\nwith frozen weights. Thus, our model retains the VLM's broad vocabulary space\nand strengthens its segmentation capability. Experimental results show that our\nmethod outperforms not only the training-free counterparts, but also those\nfine-tuned with millions of additional data samples, and sets new\nstate-of-the-art records for both zero-shot semantic and referring image\nsegmentation tasks. Specifically, we improve the current record by 28.8, 16.0,\nand 6.9 mIoU on Pascal VOC, COCO Object, and Pascal Context.\n","authors":["Shuyang Sun","Runjia Li","Philip Torr","Xiuye Gu","Siyang Li"],"pdf_url":"https://arxiv.org/pdf/2312.07661v1.pdf","comment":"Project page: https://torrvision.com/clip_as_rnn/"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2312.07517v1","updated":"2023-12-12T18:48:25Z","published":"2023-12-12T18:48:25Z","title":"Search Optimization with Query Likelihood Boosting and Two-Level\n  Approximate Search for Edge Devices","summary":"  We present a novel search optimization solution for approximate nearest\nneighbor (ANN) search on resource-constrained edge devices. Traditional ANN\napproaches fall short in meeting the specific demands of real-world scenarios,\ne.g., skewed query likelihood distribution and search on large-scale indices\nwith a low latency and small footprint. To address these limitations, we\nintroduce two key components: a Query Likelihood Boosted Tree (QLBT) to\noptimize average search latency for frequently used small datasets, and a\ntwo-level approximate search algorithm to enable efficient retrieval with large\ndatasets on edge devices. We perform thorough evaluation on simulated and real\ndata and demonstrate QLBT can significantly reduce latency by 15% on real data\nand our two-level search algorithm successfully achieve deployable accuracy and\nlatency on a 10 million dataset for edge devices. In addition, we provide a\ncomprehensive protocol for configuring and optimizing on-device search\nalgorithm through extensive empirical studies.\n","authors":["Jianwei Zhang","Helian Feng","Xin He","Grant P. Strimel","Farhad Ghassemi","Ali Kebarighotbi"],"pdf_url":"https://arxiv.org/pdf/2312.07517v1.pdf","comment":"4 pages, 3 figures, accepted at ECI workshop @ CIKM 2023"},{"id":"http://arxiv.org/abs/2203.16218v3","updated":"2023-12-12T15:47:10Z","published":"2022-03-30T11:40:36Z","title":"APG: Adaptive Parameter Generation Network for Click-Through Rate\n  Prediction","summary":"  In many web applications, deep learning-based CTR prediction models (deep CTR\nmodels for short) are widely adopted. Traditional deep CTR models learn\npatterns in a static manner, i.e., the network parameters are the same across\nall the instances. However, such a manner can hardly characterize each of the\ninstances which may have different underlying distributions. It actually limits\nthe representation power of deep CTR models, leading to sub-optimal results. In\nthis paper, we propose an efficient, effective, and universal module, named as\nAdaptive Parameter Generation network (APG), which can dynamically generate\nparameters for deep CTR models on-the-fly based on different instances.\nExtensive experimental evaluation results show that APG can be applied to a\nvariety of deep CTR models and significantly improve their performance.\nMeanwhile, APG can reduce the time cost by 38.7\\% and memory usage by 96.6\\%\ncompared to a regular deep CTR model. We have deployed APG in the industrial\nsponsored search system and achieved 3\\% CTR gain and 1\\% RPM gain\nrespectively.\n","authors":["Bencheng Yan","Pengjie Wang","Kai Zhang","Feng Li","Hongbo Deng","Jian Xu","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2203.16218v3.pdf","comment":"NeurIPS 2022, 16 pages; The first two authors contributed equally to\n  this work"},{"id":"http://arxiv.org/abs/2305.07402v3","updated":"2023-12-12T14:04:34Z","published":"2023-05-12T11:58:15Z","title":"Synergistic Interplay between Search and Large Language Models for\n  Information Retrieval","summary":"  Information retrieval (IR) plays a crucial role in locating relevant\nresources from vast amounts of data, and its applications have evolved from\ntraditional knowledge bases to modern retrieval models (RMs). The emergence of\nlarge language models (LLMs) has further revolutionized the IR field by\nenabling users to interact with search systems in natural languages. In this\npaper, we explore the advantages and disadvantages of LLMs and RMs,\nhighlighting their respective strengths in understanding user-issued queries\nand retrieving up-to-date information. To leverage the benefits of both\nparadigms while circumventing their limitations, we propose InteR, a novel\nframework that facilitates information refinement through synergy between RMs\nand LLMs. InteR allows RMs to expand knowledge in queries using LLM-generated\nknowledge collections and enables LLMs to enhance prompt formulation using\nretrieved documents. This iterative refinement process augments the inputs of\nRMs and LLMs, leading to more accurate retrieval. Experiments on large-scale\nretrieval benchmarks involving web search and low-resource retrieval tasks\ndemonstrate that InteR achieves overall superior zero-shot retrieval\nperformance compared to state-of-the-art methods, even those using relevance\njudgment. Source code is available at https://github.com/Cyril-JZ/InteR\n","authors":["Jiazhan Feng","Chongyang Tao","Xiubo Geng","Tao Shen","Can Xu","Guodong Long","Dongyan Zhao","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2305.07402v3.pdf","comment":"Pre-print. Work in progress"},{"id":"http://arxiv.org/abs/2312.07160v1","updated":"2023-12-12T10:55:34Z","published":"2023-12-12T10:55:34Z","title":"Audience Prospecting for Dynamic-Product-Ads in Native Advertising","summary":"  With yearly revenue exceeding one billion USD, Yahoo Gemini native\nadvertising marketplace serves more than two billion impressions daily to\nhundreds of millions of unique users. One of the fastest growing segments of\nGemini native is dynamic-product-ads (DPA), where major advertisers, such as\nAmazon and Walmart, provide catalogs with millions of products for the system\nto choose from and present to users. The subject of this work is finding and\nexpanding the right audience for each DPA ad, which is one of the many\nchallenges DPA presents. Approaches such as targeting various user groups,\ne.g., users who already visited the advertisers' websites (Retargeting), users\nthat searched for certain products (Search-Prospecting), or users that reside\nin preferred locations (Location-Prospecting), have limited audience expansion\ncapabilities. In this work we present two new approaches for audience expansion\nthat also maintain predefined performance goals. The Conversion-Prospecting\napproach predicts DPA conversion rates based on Gemini native logged data, and\ncalculates the expected cost-per-action (CPA) for determining users'\neligibility to products and optimizing DPA bids in Gemini native auctions. To\nsupport new advertisers and products, the Trending-Prospecting approach matches\ntrending products to users by learning their tendency towards products from\nadvertisers' sites logged events. The tendency scores indicate the popularity\nof the product and the similarity of the user to those who have previously\nengaged with this product. The two new prospecting approaches were tested\nonline, serving real Gemini native traffic, demonstrating impressive DPA\ndelivery and DPA revenue lifts while maintaining most traffic within the\nacceptable CPA range (i.e., performance goal). After a successful testing\nphase, the proposed approaches are currently in production and serve all Gemini\nnative traffic.\n","authors":["Eliran Abutbul","Yohay Kaplan","Naama Krasne","Oren Somekh Or David","Omer Duvdevany","Evgeny Segal"],"pdf_url":"https://arxiv.org/pdf/2312.07160v1.pdf","comment":"In Proc. IeeeBigData'2023 (Industry and Government Program)"},{"id":"http://arxiv.org/abs/2312.07036v1","updated":"2023-12-12T07:41:05Z","published":"2023-12-12T07:41:05Z","title":"Debiasing Sequential Recommenders through Distributionally Robust\n  Optimization over System Exposure","summary":"  Sequential recommendation (SR) models are typically trained on user-item\ninteractions which are affected by the system exposure bias, leading to the\nuser preference learned from the biased SR model not being fully consistent\nwith the true user preference. Exposure bias refers to the fact that user\ninteractions are dependent upon the partial items exposed to the user. Existing\ndebiasing methods do not make full use of the system exposure data and suffer\nfrom sub-optimal recommendation performance and high variance. In this paper,\nwe propose to debias sequential recommenders through Distributionally Robust\nOptimization (DRO) over system exposure data. The key idea is to utilize DRO to\noptimize the worst-case error over an uncertainty set to safeguard the model\nagainst distributional discrepancy caused by the exposure bias. The main\nchallenge to apply DRO for exposure debiasing in SR lies in how to construct\nthe uncertainty set and avoid the overestimation of user preference on biased\nsamples. Moreover, how to evaluate the debiasing effect on biased test set is\nalso an open question. To this end, we first introduce an exposure simulator\ntrained upon the system exposure data to calculate the exposure distribution,\nwhich is then regarded as the nominal distribution to construct the uncertainty\nset of DRO. Then, we introduce a penalty to items with high exposure\nprobability to avoid the overestimation of user preference for biased samples.\nFinally, we design a debiased self-normalized inverse propensity score (SNIPS)\nevaluator for evaluating the debiasing effect on the biased offline test set.\nWe conduct extensive experiments on two real-world datasets to verify the\neffectiveness of the proposed methods. Experimental results demonstrate the\nsuperior exposure debiasing performance of proposed methods. Codes and data are\navailable at \\url{https://github.com/nancheng58/DebiasedSR_DRO}.\n","authors":["Jiyuan Yang","Yue Ding","Yidan Wang","Pengjie Ren","Zhumin Chen","Fei Cai","Jun Ma","Rui Zhang","Zhaochun Ren","Xin Xin"],"pdf_url":"https://arxiv.org/pdf/2312.07036v1.pdf","comment":"Accept by WSDM 2024"},{"id":"http://arxiv.org/abs/2212.07679v5","updated":"2023-12-12T05:44:56Z","published":"2022-12-15T09:23:31Z","title":"Fast and exact fixed-radius neighbor search based on sorting","summary":"  Fixed-radius near neighbor search is a fundamental data operation that\nretrieves all data points within a user-specified distance to a query point.\nThere are efficient algorithms that can provide fast approximate query\nresponses, but they often have a very compute-intensive indexing phase and\nrequire careful parameter tuning. Therefore, exact brute force and tree-based\nsearch methods are still widely used. Here we propose a new fixed-radius near\nneighbor search method, called SNN, that significantly improves over brute\nforce and tree-based methods in terms of index and query time, provably returns\nexact results, and requires no parameter tuning. SNN exploits a sorting of the\ndata points by their first principal component to prune the query search space.\nFurther speedup is gained from an efficient implementation using high-level\nBasic Linear Algebra Subprograms (BLAS). We provide theoretical analysis of our\nmethod and demonstrate its practical performance when used stand-alone and when\napplied within the DBSCAN clustering algorithm.\n","authors":["Xinye Chen","Stefan Güttel"],"pdf_url":"https://arxiv.org/pdf/2212.07679v5.pdf","comment":"arXiv admin note: text overlap with arXiv:2202.01456"},{"id":"http://arxiv.org/abs/2312.05994v2","updated":"2023-12-12T05:11:08Z","published":"2023-12-10T20:57:01Z","title":"mir_ref: A Representation Evaluation Framework for Music Information\n  Retrieval Tasks","summary":"  Music Information Retrieval (MIR) research is increasingly leveraging\nrepresentation learning to obtain more compact, powerful music audio\nrepresentations for various downstream MIR tasks. However, current\nrepresentation evaluation methods are fragmented due to discrepancies in audio\nand label preprocessing, downstream model and metric implementations, data\navailability, and computational resources, often leading to inconsistent and\nlimited results. In this work, we introduce mir_ref, an MIR Representation\nEvaluation Framework focused on seamless, transparent, local-first experiment\norchestration to support representation development. It features\nimplementations of a variety of components such as MIR datasets, tasks,\nembedding models, and tools for result analysis and visualization, while\nfacilitating the implementation of custom components. To demonstrate its\nutility, we use it to conduct an extensive evaluation of several embedding\nmodels across various tasks and datasets, including evaluating their robustness\nto various audio perturbations and the ease of extracting relevant information\nfrom them.\n","authors":["Christos Plachouras","Pablo Alonso-Jiménez","Dmitry Bogdanov"],"pdf_url":"https://arxiv.org/pdf/2312.05994v2.pdf","comment":"Machine Learning for Audio Workshop, Neural Information Processing\n  Systems (NeurIPS) 2023, New Orleans, LA"},{"id":"http://arxiv.org/abs/2312.06648v2","updated":"2023-12-12T03:37:59Z","published":"2023-12-11T18:57:35Z","title":"Dense X Retrieval: What Retrieval Granularity Should We Use?","summary":"  Dense retrieval has become a prominent method to obtain relevant context or\nworld knowledge in open-domain NLP tasks. When we use a learned dense retriever\non a retrieval corpus at inference time, an often-overlooked design choice is\nthe retrieval unit in which the corpus is indexed, e.g. document, passage, or\nsentence. We discover that the retrieval unit choice significantly impacts the\nperformance of both retrieval and downstream tasks. Distinct from the typical\napproach of using passages or sentences, we introduce a novel retrieval unit,\nproposition, for dense retrieval. Propositions are defined as atomic\nexpressions within text, each encapsulating a distinct factoid and presented in\na concise, self-contained natural language format. We conduct an empirical\ncomparison of different retrieval granularity. Our results reveal that\nproposition-based retrieval significantly outperforms traditional passage or\nsentence-based methods in dense retrieval. Moreover, retrieval by proposition\nalso enhances the performance of downstream QA tasks, since the retrieved texts\nare more condensed with question-relevant information, reducing the need for\nlengthy input tokens and minimizing the inclusion of extraneous, irrelevant\ninformation.\n","authors":["Tong Chen","Hongwei Wang","Sihao Chen","Wenhao Yu","Kaixin Ma","Xinran Zhao","Hongming Zhang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2312.06648v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07796v1","updated":"2023-12-12T23:22:57Z","published":"2023-12-12T23:22:57Z","title":"Harnessing Retrieval-Augmented Generation (RAG) for Uncovering Knowledge\n  Gaps","summary":"  The paper presents a methodology for uncovering knowledge gaps on the\ninternet using the Retrieval Augmented Generation (RAG) model. By simulating\nuser search behaviour, the RAG system identifies and addresses gaps in\ninformation retrieval systems. The study demonstrates the effectiveness of the\nRAG system in generating relevant suggestions with a consistent accuracy of\n93%. The methodology can be applied in various fields such as scientific\ndiscovery, educational enhancement, research development, market analysis,\nsearch engine optimisation, and content development. The results highlight the\nvalue of identifying and understanding knowledge gaps to guide future\nendeavours.\n","authors":["Joan Figuerola Hurtado"],"pdf_url":"https://arxiv.org/pdf/2312.07796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08393v1","updated":"2023-12-12T17:40:16Z","published":"2023-12-12T17:40:16Z","title":"Multi-criteria recommendation systems to foster online grocery","summary":"  With the exponential increase in information, it has become imperative to\ndesign mechanisms that allow users to access what matters to them as quickly as\npossible. The recommendation system ($RS$) with information technology\ndevelopment is the solution, it is an intelligent system. Various types of data\ncan be collected on items of interest to users and presented as\nrecommendations. $RS$ also play a very important role in e-commerce. The\npurpose of recommending a product is to designate the most appropriate\ndesignation for a specific product. The major challenges when recommending\nproducts are insufficient information about the products and the categories to\nwhich they belong. In this paper, we transform the product data using two\nmethods of document representation: bag-of-words (BOW) and the neural\nnetwork-based document combination known as vector-based (Doc2Vec). We propose\nthree-criteria recommendation systems (product, package, and health) for each\ndocument representation method to foster online grocery, which depends on\nproduct characteristics such as (composition, packaging, nutrition table,\nallergen, etc.). For our evaluation, we conducted a user and expert survey.\nFinally, we have compared the performance of these three criteria for each\ndocument representation method, discovering that the neural network-based\n(Doc2Vec) performs better and completely alters the results.\n","authors":["Manar Mohamed Hafez","Rebeca P. Díaz Redondo","Ana Fernández-Vilas","Héctor Olivera Pazó"],"pdf_url":"https://arxiv.org/pdf/2312.08393v1.pdf","comment":"30 pages, 8 images, journal"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2312.07540v1","updated":"2023-12-12T18:59:30Z","published":"2023-12-12T18:59:30Z","title":"diff History for Long-Context Language Agents","summary":"  Language Models (LMs) offer an exciting solution for general-purpose embodied\ncontrol. However, a key technical issue arises when using an LM-based\ncontroller: environment observations must be converted to text, which coupled\nwith history, leads to prohibitively large textual prompts. As a result, prior\nwork in LM agents is limited to restricted domains with either small\nobservation size or minimal needs for interaction history. In this paper, we\nintroduce a simple and highly effective solution to these issues. We exploit\nthe fact that consecutive text observations have high similarity and propose to\ncompress them via the Unix diff command. We demonstrate our approach in\nNetHack, a complex rogue-like video game, that requires long-horizon reasoning\nfor decision-making and is far from solved, particularly for neural agents.\nDiff history offers an average of 4x increase in the length of the text-based\ninteraction history available to the LM. This observational compression along\nwith the benefits of abstraction yields a 7x improvement in game score on\nheld-out environment instances over state-of-the-art baselines. It also\noutperforms prior agents that use visual observations by over 40%.\n","authors":["Ulyana Piterbarg","Lerrel Pinto","Rob Fergus"],"pdf_url":"https://arxiv.org/pdf/2312.07540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07535v1","updated":"2023-12-12T18:59:06Z","published":"2023-12-12T18:59:06Z","title":"Improved Frequency Estimation Algorithms with and without Predictions","summary":"  Estimating frequencies of elements appearing in a data stream is a key task\nin large-scale data analysis. Popular sketching approaches to this problem\n(e.g., CountMin and CountSketch) come with worst-case guarantees that\nprobabilistically bound the error of the estimated frequencies for any possible\ninput. The work of Hsu et al. (2019) introduced the idea of using machine\nlearning to tailor sketching algorithms to the specific data distribution they\nare being run on. In particular, their learning-augmented frequency estimation\nalgorithm uses a learned heavy-hitter oracle which predicts which elements will\nappear many times in the stream. We give a novel algorithm, which in some\nparameter regimes, already theoretically outperforms the learning based\nalgorithm of Hsu et al. without the use of any predictions. Augmenting our\nalgorithm with heavy-hitter predictions further reduces the error and improves\nupon the state of the art. Empirically, our algorithms achieve superior\nperformance in all experiments compared to prior approaches.\n","authors":["Anders Aamand","Justin Y. Chen","Huy Lê Nguyen","Sandeep Silwal","Ali Vakilian"],"pdf_url":"https://arxiv.org/pdf/2312.07535v1.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2312.07534v1","updated":"2023-12-12T18:58:42Z","published":"2023-12-12T18:58:42Z","title":"Cosmological Field Emulation and Parameter Inference with Diffusion\n  Models","summary":"  Cosmological simulations play a crucial role in elucidating the effect of\nphysical parameters on the statistics of fields and on constraining parameters\ngiven information on density fields. We leverage diffusion generative models to\naddress two tasks of importance to cosmology -- as an emulator for cold dark\nmatter density fields conditional on input cosmological parameters $\\Omega_m$\nand $\\sigma_8$, and as a parameter inference model that can return constraints\non the cosmological parameters of an input field. We show that the model is\nable to generate fields with power spectra that are consistent with those of\nthe simulated target distribution, and capture the subtle effect of each\nparameter on modulations in the power spectrum. We additionally explore their\nutility as parameter inference models and find that we can obtain tight\nconstraints on cosmological parameters.\n","authors":["Nayantara Mudur","Carolina Cuesta-Lazaro","Douglas P. Finkbeiner"],"pdf_url":"https://arxiv.org/pdf/2312.07534v1.pdf","comment":"7 pages, 5 figures, Accepted at the Machine Learning and the Physical\n  Sciences workshop, NeurIPS 2023"},{"id":"http://arxiv.org/abs/2312.07529v1","updated":"2023-12-12T18:56:14Z","published":"2023-12-12T18:56:14Z","title":"Topological Obstructions and How to Avoid Them","summary":"  Incorporating geometric inductive biases into models can aid interpretability\nand generalization, but encoding to a specific geometric structure can be\nchallenging due to the imposed topological constraints. In this paper, we\ntheoretically and empirically characterize obstructions to training encoders\nwith geometric latent spaces. We show that local optima can arise due to\nsingularities (e.g. self-intersection) or due to an incorrect degree or winding\nnumber. We then discuss how normalizing flows can potentially circumvent these\nobstructions by defining multimodal variational distributions. Inspired by this\nobservation, we propose a new flow-based model that maps data points to\nmultimodal distributions over geometric spaces and empirically evaluate our\nmodel on 2 domains. We observe improved stability during training and a higher\nchance of converging to a homeomorphic encoder.\n","authors":["Babak Esmaeili","Robin Walters","Heiko Zimmermann","Jan-Willem van de Meent"],"pdf_url":"https://arxiv.org/pdf/2312.07529v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07516v1","updated":"2023-12-12T18:47:12Z","published":"2023-12-12T18:47:12Z","title":"Learning finitely correlated states: stability of the spectral\n  reconstruction","summary":"  We show that marginals of subchains of length $t$ of any finitely correlated\ntranslation invariant state on a chain can be learned, in trace distance, with\n$O(t^2)$ copies -- with an explicit dependence on local dimension, memory\ndimension and spectral properties of a certain map constructed from the state\n-- and computational complexity polynomial in $t$. The algorithm requires only\nthe estimation of a marginal of a controlled size, in the worst case bounded by\na multiple of the minimum bond dimension, from which it reconstructs a\ntranslation invariant matrix product operator. In the analysis, a central role\nis played by the theory of operator systems. A refined error bound can be\nproven for $C^*$-finitely correlated states, which have an operational\ninterpretation in terms of sequential quantum channels applied to the memory\nsystem. We can also obtain an analogous error bound for a class of matrix\nproduct density operators reconstructible by local marginals. In this case, a\nlinear number of marginals must be estimated, obtaining a sample complexity of\n$\\tilde{O}(t^3)$. The learning algorithm also works for states that are only\nclose to a finitely correlated state, with the potential of providing\ncompetitive algorithms for other interesting families of states.\n","authors":["Marco Fanizza","Niklas Galke","Josep Lumbreras","Cambyse Rouzé","Andreas Winter"],"pdf_url":"https://arxiv.org/pdf/2312.07516v1.pdf","comment":"27+7 pages, 6 figures"},{"id":"http://arxiv.org/abs/2312.07511v1","updated":"2023-12-12T18:44:19Z","published":"2023-12-12T18:44:19Z","title":"A Hitchhiker's Guide to Geometric GNNs for 3D Atomic Systems","summary":"  Recent advances in computational modelling of atomic systems, spanning\nmolecules, proteins, and materials, represent them as geometric graphs with\natoms embedded as nodes in 3D Euclidean space. In these graphs, the geometric\nattributes transform according to the inherent physical symmetries of 3D atomic\nsystems, including rotations and translations in Euclidean space, as well as\nnode permutations. In recent years, Geometric Graph Neural Networks have\nemerged as the preferred machine learning architecture powering applications\nranging from protein structure prediction to molecular simulations and material\ngeneration. Their specificity lies in the inductive biases they leverage --\nsuch as physical symmetries and chemical properties -- to learn informative\nrepresentations of these geometric graphs. In this opinionated paper, we\nprovide a comprehensive and self-contained overview of the field of Geometric\nGNNs for 3D atomic systems. We cover fundamental background material and\nintroduce a pedagogical taxonomy of Geometric GNN architectures:(1) invariant\nnetworks, (2) equivariant networks in Cartesian basis, (3) equivariant networks\nin spherical basis, and (4) unconstrained networks. Additionally, we outline\nkey datasets and application areas and suggest future research directions. The\nobjective of this work is to present a structured perspective on the field,\nmaking it accessible to newcomers and aiding practitioners in gaining an\nintuition for its mathematical abstractions.\n","authors":["Alexandre Duval","Simon V. Mathis","Chaitanya K. Joshi","Victor Schmidt","Santiago Miret","Fragkiskos D. Malliaros","Taco Cohen","Pietro Lio","Yoshua Bengio","Michael Bronstein"],"pdf_url":"https://arxiv.org/pdf/2312.07511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07509v1","updated":"2023-12-12T18:43:05Z","published":"2023-12-12T18:43:05Z","title":"PEEKABOO: Interactive Video Generation via Masked-Diffusion","summary":"  Recently there has been a lot of progress in text-to-video generation, with\nstate-of-the-art models being capable of generating high quality, realistic\nvideos. However, these models lack the capability for users to interactively\ncontrol and generate videos, which can potentially unlock new areas of\napplication. As a first step towards this goal, we tackle the problem of\nendowing diffusion-based video generation models with interactive\nspatio-temporal control over their output. To this end, we take inspiration\nfrom the recent advances in segmentation literature to propose a novel\nspatio-temporal masked attention module - Peekaboo. This module is a\ntraining-free, no-inference-overhead addition to off-the-shelf video generation\nmodels which enables spatio-temporal control. We also propose an evaluation\nbenchmark for the interactive video generation task. Through extensive\nqualitative and quantitative evaluation, we establish that Peekaboo enables\ncontrol video generation and even obtains a gain of upto 3.8x in mIoU over\nbaseline models.\n","authors":["Yash Jain","Anshul Nasery","Vibhav Vineet","Harkirat Behl"],"pdf_url":"https://arxiv.org/pdf/2312.07509v1.pdf","comment":"Project webpage - https://jinga-lala.github.io/projects/Peekaboo/"},{"id":"http://arxiv.org/abs/2210.16386v3","updated":"2023-12-12T18:42:37Z","published":"2022-10-28T20:02:21Z","title":"Non-Stationary Bandits with Auto-Regressive Temporal Dependency","summary":"  Traditional multi-armed bandit (MAB) frameworks, predominantly examined under\nstochastic or adversarial settings, often overlook the temporal dynamics\ninherent in many real-world applications such as recommendation systems and\nonline advertising. This paper introduces a novel non-stationary MAB framework\nthat captures the temporal structure of these real-world dynamics through an\nauto-regressive (AR) reward structure. We propose an algorithm that integrates\ntwo key mechanisms: (i) an alternation mechanism adept at leveraging temporal\ndependencies to dynamically balance exploration and exploitation, and (ii) a\nrestarting mechanism designed to discard out-of-date information. Our algorithm\nachieves a regret upper bound that nearly matches the lower bound, with regret\nmeasured against a robust dynamic benchmark. Finally, via a real-world case\nstudy on tourism demand prediction, we demonstrate both the efficacy of our\nalgorithm and the broader applicability of our techniques to more complex,\nrapidly evolving time series.\n","authors":["Qinyi Chen","Negin Golrezaei","Djallel Bouneffouf"],"pdf_url":"https://arxiv.org/pdf/2210.16386v3.pdf","comment":"45 pages, 8 figures"},{"id":"http://arxiv.org/abs/2206.04281v4","updated":"2023-12-12T18:36:39Z","published":"2022-06-09T05:17:00Z","title":"Local Spatiotemporal Representation Learning for\n  Longitudinally-consistent Neuroimage Analysis","summary":"  Recent self-supervised advances in medical computer vision exploit global and\nlocal anatomical self-similarity for pretraining prior to downstream tasks such\nas segmentation. However, current methods assume i.i.d. image acquisition,\nwhich is invalid in clinical study designs where follow-up longitudinal scans\ntrack subject-specific temporal changes. Further, existing self-supervised\nmethods for medically-relevant image-to-image architectures exploit only\nspatial or temporal self-similarity and only do so via a loss applied at a\nsingle image-scale, with naive multi-scale spatiotemporal extensions collapsing\nto degenerate solutions. To these ends, this paper makes two contributions: (1)\nIt presents a local and multi-scale spatiotemporal representation learning\nmethod for image-to-image architectures trained on longitudinal images. It\nexploits the spatiotemporal self-similarity of learned multi-scale\nintra-subject features for pretraining and develops several feature-wise\nregularizations that avoid collapsed identity representations; (2) During\nfinetuning, it proposes a surprisingly simple self-supervised segmentation\nconsistency regularization to exploit intra-subject correlation. Benchmarked in\nthe one-shot segmentation setting, the proposed framework outperforms both\nwell-tuned randomly-initialized baselines and current self-supervised\ntechniques designed for both i.i.d. and longitudinal datasets. These\nimprovements are demonstrated across both longitudinal neurodegenerative adult\nMRI and developing infant brain MRI and yield both higher performance and\nlongitudinal consistency.\n","authors":["Mengwei Ren","Neel Dey","Martin A. Styner","Kelly Botteron","Guido Gerig"],"pdf_url":"https://arxiv.org/pdf/2206.04281v4.pdf","comment":"Accepted at NeurIPS 2022"},{"id":"http://arxiv.org/abs/2312.07492v1","updated":"2023-12-12T18:27:44Z","published":"2023-12-12T18:27:44Z","title":"SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in\n  Generative Language Models","summary":"  Current datasets for unwanted social bias auditing are limited to studying\nprotected demographic features such as race and gender. In this work, we\nintroduce a comprehensive benchmark that is meant to capture the amplification\nof social bias, via stigmas, in generative language models. We start with a\ncomprehensive list of 93 stigmas documented in social science literature and\ncurate a question-answering (QA) dataset which involves simple social\nsituations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts, with a\nvariety of prompt styles, carefully constructed to systematically test for both\nsocial bias and model robustness. We present results for SocialStigmaQA with\ntwo widely used open source generative language models and we demonstrate that\nthe output generated by these models considerably amplifies existing social\nbias against stigmatized groups. Specifically, we find that the proportion of\nsocially biased output ranges from 45% to 59% across a variety of decoding\nstrategies and prompting styles. We discover that the deliberate design of the\ntemplates in our benchmark (e.g., by adding biasing text to the prompt or\nvarying the answer that indicates bias) impact the model tendencies to generate\nsocially biased output. Additionally, we report on patterns in the generated\nchain-of-thought output, finding a variety of problems from subtle bias to\nevidence of a lack of reasoning.\n  Warning: This paper contains examples of text which is toxic, biased, and\nharmful.\n","authors":["Manish Nagireddy","Lamogha Chiazor","Moninder Singh","Ioana Baldini"],"pdf_url":"https://arxiv.org/pdf/2312.07492v1.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2305.17421v2","updated":"2023-12-12T18:14:41Z","published":"2023-05-27T09:01:21Z","title":"FoPro-KD: Fourier Prompted Effective Knowledge Distillation for\n  Long-Tailed Medical Image Recognition","summary":"  Representational transfer from publicly available models is a promising\ntechnique for improving medical image classification, especially in long-tailed\ndatasets with rare diseases. However, existing methods often overlook the\nfrequency-dependent behavior of these models, thereby limiting their\neffectiveness in transferring representations and generalizations to rare\ndiseases. In this paper, we propose FoPro-KD, a novel framework that leverages\nthe power of frequency patterns learned from frozen pre-trained models to\nenhance their transferability and compression, presenting a few unique\ninsights: 1) We demonstrate that leveraging representations from publicly\navailable pre-trained models can substantially improve performance,\nspecifically for rare classes, even when utilizing representations from a\nsmaller pre-trained model. 2) We observe that pre-trained models exhibit\nfrequency preferences, which we explore using our proposed Fourier Prompt\nGenerator (FPG), allowing us to manipulate specific frequencies in the input\nimage, enhancing the discriminative representational transfer. 3) By amplifying\nor diminishing these frequencies in the input image, we enable Effective\nKnowledge Distillation (EKD). EKD facilitates the transfer of knowledge from\npre-trained models to smaller models. Through extensive experiments in\nlong-tailed gastrointestinal image recognition and skin lesion classification,\nwhere rare diseases are prevalent, our FoPro-KD framework outperforms existing\nmethods, enabling more accessible medical models for rare disease\nclassification. Code is available at https://github.com/xmed-lab/FoPro-KD.\n","authors":["Marawan Elbatel","Robert Martí","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2305.17421v2.pdf","comment":"Accepted at IEEE TMI, code is available at\n  https://github.com/xmed-lab/FoPro-KD"},{"id":"http://arxiv.org/abs/2312.07466v1","updated":"2023-12-12T17:47:13Z","published":"2023-12-12T17:47:13Z","title":"Efficient Object Detection in Autonomous Driving using Spiking Neural\n  Networks: Performance, Energy Consumption Analysis, and Insights into\n  Open-set Object Discovery","summary":"  Besides performance, efficiency is a key design driver of technologies\nsupporting vehicular perception. Indeed, a well-balanced trade-off between\nperformance and energy consumption is crucial for the sustainability of\nautonomous vehicles. In this context, the diversity of real-world contexts in\nwhich autonomous vehicles can operate motivates the need for empowering\nperception models with the capability to detect, characterize and identify\nnewly appearing objects by themselves. In this manuscript we elaborate on this\nthreefold conundrum (performance, efficiency and open-world learning) for\nobject detection modeling tasks over image data collected from vehicular\nscenarios. Specifically, we show that well-performing and efficient models can\nbe realized by virtue of Spiking Neural Networks (SNNs), reaching competitive\nlevels of detection performance when compared to their non-spiking counterparts\nat dramatic energy consumption savings (up to 85%) and a slightly improved\nrobustness against image noise. Our experiments herein offered also expose\nqualitatively the complexity of detecting new objects based on the preliminary\nresults of a simple approach to discriminate potential object proposals in the\ncaptured image.\n","authors":["Aitor Martinez Seras","Javier Del Ser","Pablo Garcia-Bringas"],"pdf_url":"https://arxiv.org/pdf/2312.07466v1.pdf","comment":"8 pages, 5 figures, presented at ITSC2023"},{"id":"http://arxiv.org/abs/2309.13788v2","updated":"2023-12-12T17:35:23Z","published":"2023-09-25T00:45:07Z","title":"Can LLM-Generated Misinformation Be Detected?","summary":"  The advent of Large Language Models (LLMs) has made a transformative impact.\nHowever, the potential that LLMs such as ChatGPT can be exploited to generate\nmisinformation has posed a serious concern to online safety and public trust. A\nfundamental research question is: will LLM-generated misinformation cause more\nharm than human-written misinformation? We propose to tackle this question from\nthe perspective of detection difficulty. We first build a taxonomy of\nLLM-generated misinformation. Then we categorize and validate the potential\nreal-world methods for generating misinformation with LLMs. Then, through\nextensive empirical investigation, we discover that LLM-generated\nmisinformation can be harder to detect for humans and detectors compared to\nhuman-written misinformation with the same semantics, which suggests it can\nhave more deceptive styles and potentially cause more harm. We also discuss the\nimplications of our discovery on combating misinformation in the age of LLMs\nand the countermeasures.\n","authors":["Canyu Chen","Kai Shu"],"pdf_url":"https://arxiv.org/pdf/2309.13788v2.pdf","comment":"The code, dataset and more resources on LLMs and misinformation will\n  be released on the project website: https://llm-misinformation.github.io/"},{"id":"http://arxiv.org/abs/2312.07457v1","updated":"2023-12-12T17:34:42Z","published":"2023-12-12T17:34:42Z","title":"Dynamics Harmonic Analysis of Robotic Systems: Application in\n  Data-Driven Koopman Modelling","summary":"  We introduce the use of harmonic analysis to decompose the state space of\nsymmetric robotic systems into orthogonal isotypic subspaces. These are\nlower-dimensional spaces that capture distinct, symmetric, and synergistic\nmotions. For linear dynamics, we characterize how this decomposition leads to a\nsubdivision of the dynamics into independent linear systems on each subspace, a\nproperty we term dynamics harmonic analysis (DHA). To exploit this property, we\nuse Koopman operator theory to propose an equivariant deep-learning\narchitecture that leverages the properties of DHA to learn a global linear\nmodel of system dynamics. Our architecture, validated on synthetic systems and\nthe dynamics of locomotion of a quadrupedal robot, demonstrates enhanced\ngeneralization, sample efficiency, and interpretability, with less trainable\nparameters and computational costs.\n","authors":["Daniel Ordoñez-Apraez","Vladimir Kostic","Giulio Turrisi","Pietro Novelli","Carlos Mastalli","Claudio Semini","Massimiliano Pontil"],"pdf_url":"https://arxiv.org/pdf/2312.07457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07439v1","updated":"2023-12-12T17:06:39Z","published":"2023-12-12T17:06:39Z","title":"BIRB: A Generalization Benchmark for Information Retrieval in\n  Bioacoustics","summary":"  The ability for a machine learning model to cope with differences in training\nand deployment conditions--e.g. in the presence of distribution shift or the\ngeneralization to new classes altogether--is crucial for real-world use cases.\nHowever, most empirical work in this area has focused on the image domain with\nartificial benchmarks constructed to measure individual aspects of\ngeneralization. We present BIRB, a complex benchmark centered on the retrieval\nof bird vocalizations from passively-recorded datasets given focal recordings\nfrom a large citizen science corpus available for training. We propose a\nbaseline system for this collection of tasks using representation learning and\na nearest-centroid search. Our thorough empirical evaluation and analysis\nsurfaces open research directions, suggesting that BIRB fills the need for a\nmore realistic and complex benchmark to drive progress on robustness to\ndistribution shifts and generalization of ML models.\n","authors":["Jenny Hamer","Eleni Triantafillou","Bart van Merrienboer","Stefan Kahl","Holger Klinck","Tom Denton","Vincent Dumoulin"],"pdf_url":"https://arxiv.org/pdf/2312.07439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07435v1","updated":"2023-12-12T17:00:46Z","published":"2023-12-12T17:00:46Z","title":"Cross-modal Contrastive Learning with Asymmetric Co-attention Network\n  for Video Moment Retrieval","summary":"  Video moment retrieval is a challenging task requiring fine-grained\ninteractions between video and text modalities. Recent work in image-text\npretraining has demonstrated that most existing pretrained models suffer from\ninformation asymmetry due to the difference in length between visual and\ntextual sequences. We question whether the same problem also exists in the\nvideo-text domain with an auxiliary need to preserve both spatial and temporal\ninformation. Thus, we evaluate a recently proposed solution involving the\naddition of an asymmetric co-attention network for video grounding tasks.\nAdditionally, we incorporate momentum contrastive loss for robust,\ndiscriminative representation learning in both modalities. We note that the\nintegration of these supplementary modules yields better performance compared\nto state-of-the-art models on the TACoS dataset and comparable results on\nActivityNet Captions, all while utilizing significantly fewer parameters with\nrespect to baseline.\n","authors":["Love Panta","Prashant Shrestha","Brabeem Sapkota","Amrita Bhattarai","Suresh Manandhar","Anand Kumar Sah"],"pdf_url":"https://arxiv.org/pdf/2312.07435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07434v1","updated":"2023-12-12T17:00:13Z","published":"2023-12-12T17:00:13Z","title":"Multi-Modal Conformal Prediction Regions by Optimizing Convex Shape\n  Templates","summary":"  Conformal prediction is a statistical tool for producing prediction regions\nfor machine learning models that are valid with high probability. A key\ncomponent of conformal prediction algorithms is a non-conformity score function\nthat quantifies how different a model's prediction is from the unknown ground\ntruth value. Essentially, these functions determine the shape and the size of\nthe conformal prediction regions. However, little work has gone into finding\nnon-conformity score functions that produce prediction regions that are\nmulti-modal and practical, i.e., that can efficiently be used in engineering\napplications. We propose a method that optimizes parameterized shape template\nfunctions over calibration data, which results in non-conformity score\nfunctions that produce prediction regions with minimum volume. Our approach\nresults in prediction regions that are multi-modal, so they can properly\ncapture residuals of distributions that have multiple modes, and practical, so\neach region is convex and can be easily incorporated into downstream tasks,\nsuch as a motion planner using conformal prediction regions. Our method applies\nto general supervised learning tasks, while we illustrate its use in\ntime-series prediction. We provide a toolbox and present illustrative case\nstudies of F16 fighter jets and autonomous vehicles, showing an up to $68\\%$\nreduction in prediction region area.\n","authors":["Renukanandan Tumu","Matthew Cleaveland","Rahul Mangharam","George J. Pappas","Lars Lindemann"],"pdf_url":"https://arxiv.org/pdf/2312.07434v1.pdf","comment":"14 pages, 5 figures. The source code and toolbox are available at\n  https://github.com/nandantumu/conformal_region_designer"},{"id":"http://arxiv.org/abs/2308.08641v2","updated":"2023-12-12T16:54:49Z","published":"2023-08-16T19:32:29Z","title":"Non-monotone Sequential Submodular Maximization","summary":"  In this paper, we study a fundamental problem in submodular optimization,\nwhich is called sequential submodular maximization. Specifically, we aim to\nselect and rank a group of $k$ items from a ground set $V$ such that the\nweighted summation of $k$ (possibly non-monotone) submodular functions $f_1,\n\\cdots ,f_k: 2^V \\rightarrow \\mathbb{R}^+$ is maximized, here each function\n$f_j$ takes the first $j$ items from this sequence as input. The existing\nresearch on sequential submodular maximization has predominantly concentrated\non the monotone setting, assuming that the submodular functions are\nnon-decreasing. However, in various real-world scenarios, like diversity-aware\nrecommendation systems, adding items to an existing set might negatively impact\nthe overall utility. In response, this paper pioneers the examination of the\naforementioned problem with non-monotone submodular functions and offers\neffective solutions for both flexible and fixed length constraints, as well as\na special case with identical utility functions. The empirical evaluations\nfurther validate the effectiveness of our proposed algorithms in the domain of\nvideo recommendations. The results of this research have implications in\nvarious fields, including recommendation systems and assortment optimization,\nwhere the ordering of items significantly impacts the overall value obtained.\n","authors":["Shaojie Tang","Jing Yuan"],"pdf_url":"https://arxiv.org/pdf/2308.08641v2.pdf","comment":"This paper has been accepted for publication at AAAI 2024"},{"id":"http://arxiv.org/abs/2312.07425v1","updated":"2023-12-12T16:48:53Z","published":"2023-12-12T16:48:53Z","title":"Deep Internal Learning: Deep Learning from a Single Input","summary":"  Deep learning in general focuses on training a neural network from large\nlabeled datasets. Yet, in many cases there is value in training a network just\nfrom the input at hand. This may involve training a network from scratch using\na single input or adapting an already trained network to a provided input\nexample at inference time. This survey paper aims at covering deep\ninternal-learning techniques that have been proposed in the past few years for\nthese two important directions. While our main focus will be on image\nprocessing problems, most of the approaches that we survey are derived for\ngeneral signals (vectors with recurring patterns that can be distinguished from\nnoise) and are therefore applicable to other modalities. We believe that the\ntopic of internal-learning is very important in many signal and image\nprocessing problems where training data is scarce and diversity is large on the\none hand, and on the other, there is a lot of structure in the data that can be\nexploited.\n","authors":["Tom Tirer","Raja Giryes","Se Young Chun","Yonina C. Eldar"],"pdf_url":"https://arxiv.org/pdf/2312.07425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07424v1","updated":"2023-12-12T16:48:07Z","published":"2023-12-12T16:48:07Z","title":"How Well Does GPT-4V(ision) Adapt to Distribution Shifts? A Preliminary\n  Investigation","summary":"  In machine learning, generalization against distribution shifts -- where\ndeployment conditions diverge from the training scenarios -- is crucial,\nparticularly in fields like climate modeling, biomedicine, and autonomous\ndriving. The emergence of foundation models, distinguished by their extensive\npretraining and task versatility, has led to an increased interest in their\nadaptability to distribution shifts. GPT-4V(ision) acts as the most advanced\npublicly accessible multimodal foundation model, with extensive applications\nacross various domains, including anomaly detection, video understanding, image\ngeneration, and medical diagnosis. However, its robustness against data\ndistributions remains largely underexplored. Addressing this gap, this study\nrigorously evaluates GPT-4V's adaptability and generalization capabilities in\ndynamic environments, benchmarking against prominent models like CLIP and\nLLaVA. We delve into GPT-4V's zero-shot generalization across 13 diverse\ndatasets spanning natural, medical, and molecular domains. We further\ninvestigate its adaptability to controlled data perturbations and examine the\nefficacy of in-context learning as a tool to enhance its adaptation. Our\nfindings delineate GPT-4V's capability boundaries in distribution shifts,\nshedding light on its strengths and limitations across various scenarios.\nImportantly, this investigation contributes to our understanding of how AI\nfoundation models generalize to distribution shifts, offering pivotal insights\ninto their adaptability and robustness. Code is publicly available at\nhttps://github.com/jameszhou-gl/gpt-4v-distribution-shift.\n","authors":["Zhongyi Han","Guanglin Zhou","Rundong He","Jindong Wang","Xing Xie","Tailin Wu","Yilong Yin","Salman Khan","Lina Yao","Tongliang Liu","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.07424v1.pdf","comment":"62 pages, 39 figures"},{"id":"http://arxiv.org/abs/2312.07420v1","updated":"2023-12-12T16:44:47Z","published":"2023-12-12T16:44:47Z","title":"FairSISA: Ensemble Post-Processing to Improve Fairness of Unlearning in\n  LLMs","summary":"  Training large language models (LLMs) is a costly endeavour in terms of time\nand computational resources. The large amount of training data used during the\nunsupervised pre-training phase makes it difficult to verify all data and,\nunfortunately, undesirable data may be ingested during training. Re-training\nfrom scratch is impractical and has led to the creation of the 'unlearning'\ndiscipline where models are modified to \"unlearn\" undesirable information\nwithout retraining. However, any modification can alter the behaviour of LLMs,\nespecially on key dimensions such as fairness. This is the first work that\nexamines this interplay between unlearning and fairness for LLMs. In\nparticular, we focus on a popular unlearning framework known as SISA [Bourtoule\net al., 2021], which creates an ensemble of models trained on disjoint shards.\nWe evaluate the performance-fairness trade-off for SISA, and empirically\ndemsontrate that SISA can indeed reduce fairness in LLMs. To remedy this, we\npropose post-processing bias mitigation techniques for ensemble models produced\nby SISA. We adapt the post-processing fairness improvement technique from\n[Hardt et al., 2016] to design three methods that can handle model ensembles,\nand prove that one of the methods is an optimal fair predictor for ensemble of\nmodels. Through experimental results, we demonstrate the efficacy of our\npost-processing framework called 'FairSISA'.\n","authors":["Swanand Ravindra Kadhe","Anisa Halimi","Ambrish Rawat","Nathalie Baracaldo"],"pdf_url":"https://arxiv.org/pdf/2312.07420v1.pdf","comment":"Accepted in NeurIPS 2023 Workshop on Socially Responsible Language\n  Modelling Research (SoLaR)"},{"id":"http://arxiv.org/abs/2205.04619v3","updated":"2023-12-12T16:43:44Z","published":"2022-05-10T01:30:24Z","title":"Risk Preferences of Learning Algorithms","summary":"  Agents' learning from feedback shapes economic outcomes, and many economic\ndecision-makers today employ learning algorithms to make consequential choices.\nThis note shows that a widely used learning algorithm, $\\varepsilon$-Greedy,\nexhibits emergent risk aversion: it prefers actions with lower variance. When\npresented with actions of the same expectation, under a wide range of\nconditions, $\\varepsilon$-Greedy chooses the lower-variance action with\nprobability approaching one. This emergent preference can have wide-ranging\nconsequences, ranging from concerns about fairness to homogenization, and holds\ntransiently even when the riskier action has a strictly higher expected payoff.\nWe discuss two methods to correct this bias. The first method requires the\nalgorithm to reweight data as a function of how likely the actions were to be\nchosen. The second requires the algorithm to have optimistic estimates of\nactions for which it has not collected much data. We show that risk-neutrality\nis restored with these corrections.\n","authors":["Andreas Haupt","Aroon Narayanan"],"pdf_url":"https://arxiv.org/pdf/2205.04619v3.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2312.07414v1","updated":"2023-12-12T16:34:32Z","published":"2023-12-12T16:34:32Z","title":"QSMVM: QoS-aware and social-aware multimetric routing protocol for\n  video-streaming services over MANETs","summary":"  A mobile ad hoc network (MANET) is a set of autonomous mobile devices\nconnected by wireless links in a distributed manner and without a fixed\ninfrastructure. Real-time multimedia services, such as video-streaming over\nMANETs, offers very promising applications, e.g. two members of a group of\ntourists who want to share a video transmitted through the MANET they form; a\nvideo-streaming service deployed over a MANET where users watch a film; among\nother examples. On the other hand, social web technologies, where people\nactively interact online with others through social networks, are leading to a\nsocialization of networks. Information of interaction among users is being used\nto provide socially-enhanced software. To achieve this, we need to know the\nstrength of the relationship between a given user and each user they interact\nwith. This strength of the relationship can be measured through a concept\ncalled tie strength (TS), first introduced by Mark Granovetter in 1973. In this\narticle, we modify our previous proposal named multipath multimedia dynamic\nsource routing (MMDSR) protocol to include a social metric TS in the decisions\ntaken by the forwarding algorithm. We find a trade-off between the quality of\nservice (QoS) and the trust level between users who form the forwarding path in\nthe MANET. Our goal is to increase the trust metric while the QoS is not\naffected significantly.\n","authors":["Efraín Palacios Jara","Ahmad Mohamad Mezhe","Mónica Aguilar Igartua","Rebeca P. Díaz Redondo","Ana Fernández Vilas"],"pdf_url":"https://arxiv.org/pdf/2312.07414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07413v1","updated":"2023-12-12T16:34:19Z","published":"2023-12-12T16:34:19Z","title":"AI capabilities can be significantly improved without expensive\n  retraining","summary":"  State-of-the-art AI systems can be significantly improved without expensive\nretraining via \"post-training enhancements\"-techniques applied after initial\ntraining like fine-tuning the system to use a web browser. We review recent\npost-training enhancements, categorizing them into five types: tool-use,\nprompting methods, scaffolding, solution selection, and data generation.\nDifferent enhancements improve performance on different tasks, making it hard\nto compare their significance. So we translate improvements from different\nenhancements into a common currency, the compute-equivalent gain: how much\nadditional training compute would be needed to improve performance by the same\namount as the enhancement. Our non-experimental work shows that post-training\nenhancements have significant benefits: most surveyed enhancements improve\nbenchmark performance by more than a 5x increase in training compute, some by\nmore than 20x. Post-training enhancements are relatively cheap to develop:\nfine-tuning costs are typically <1% of the original training cost. Governing\nthe development of capable post-training enhancements may be challenging\nbecause frontier models could be enhanced by a wide range of actors.\n","authors":["Tom Davidson","Jean-Stanislas Denain","Pablo Villalobos","Guillem Bas"],"pdf_url":"https://arxiv.org/pdf/2312.07413v1.pdf","comment":"30 pages, 24 figures"},{"id":"http://arxiv.org/abs/2303.08459v2","updated":"2023-12-12T16:31:34Z","published":"2023-03-15T09:03:58Z","title":"Forecasting Intraday Power Output by a Set of PV Systems using Recurrent\n  Neural Networks and Physical Covariates","summary":"  Accurate intraday forecasts of the power output by PhotoVoltaic (PV) systems\nare critical to improve the operation of energy distribution grids. We describe\na neural autoregressive model which aims at performing such intraday forecasts.\nWe build upon a physical, deterministic PV performance model, the output of\nwhich being used as covariates in the context of the neural model. In addition,\nour application data relates to a geographically distributed set of PV systems.\nWe address all PV sites with a single neural model, which embeds the\ninformation about the PV site in specific covariates. We use a scale-free\napproach which does rely on explicit modelling of seasonal effects. Our\nproposal repurposes a model initially used in the retail sector, and discloses\na novel truncated Gaussian output distribution. An ablation study and a\ncomparison to alternative architectures from the literature shows that the\ncomponents in the best performing proposed model variant work synergistically\nto reach a skill score of 15.72% with respect to the physical model, used as a\nbaseline.\n","authors":["Pierrick Bruneau","David Fiorelli","Christian Braun","Daniel Koster"],"pdf_url":"https://arxiv.org/pdf/2303.08459v2.pdf","comment":"23 pages, 7 figures"},{"id":"http://arxiv.org/abs/2312.07405v1","updated":"2023-12-12T16:25:05Z","published":"2023-12-12T16:25:05Z","title":"ICL Markup: Structuring In-Context Learning using Soft-Token Tags","summary":"  Large pretrained language models (LLMs) can be rapidly adapted to a wide\nvariety of tasks via a text-to-text approach, where the instruction and input\nare fed to the model in natural language. Combined with in-context learning\n(ICL), this paradigm is impressively flexible and powerful. However, it also\nburdens users with an overwhelming number of choices, many of them arbitrary.\nInspired by markup languages like HTML, we contribute a method of using\nsoft-token tags to compose prompt templates. This approach reduces arbitrary\ndecisions and streamlines the application of ICL. Our method is a form of\nmeta-learning for ICL; it learns these tags in advance during a\nparameter-efficient fine-tuning ``warm-up'' process. The tags can subsequently\nbe used in templates for ICL on new, unseen tasks without any additional\nfine-tuning. Our experiments with this approach yield promising initial\nresults, improving LLM performance on important enterprise applications such as\nfew-shot and open-world intent detection, as well as text classification in\nnews and legal domains.\n","authors":["Marc-Etienne Brunet","Ashton Anderson","Richard Zemel"],"pdf_url":"https://arxiv.org/pdf/2312.07405v1.pdf","comment":"R0-FoMo: Workshop on Robustness of Few-shot and Zero-shot Learning in\n  Foundation Models at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2304.12130v3","updated":"2023-12-12T16:18:51Z","published":"2023-04-24T14:33:34Z","title":"Reconstructing Turbulent Flows Using Physics-Aware Spatio-Temporal\n  Dynamics and Test-Time Refinement","summary":"  Simulating turbulence is critical for many societally important applications\nin aerospace engineering, environmental science, the energy industry, and\nbiomedicine. Large eddy simulation (LES) has been widely used as an alternative\nto direct numerical simulation (DNS) for simulating turbulent flows due to its\nreduced computational cost. However, LES is unable to capture all of the scales\nof turbulent transport accurately. Reconstructing DNS from low-resolution LES\nis critical for many scientific and engineering disciplines, but it poses many\nchallenges to existing super-resolution methods due to the spatio-temporal\ncomplexity of turbulent flows. In this work, we propose a new physics-guided\nneural network for reconstructing the sequential DNS from low-resolution LES\ndata. The proposed method leverages the partial differential equation that\nunderlies the flow dynamics in the design of spatio-temporal model\narchitecture. A degradation-based refinement method is also developed to\nenforce physical constraints and further reduce the accumulated reconstruction\nerrors over long periods. The results on two different types of turbulent flow\ndata confirm the superiority of the proposed method in reconstructing the\nhigh-resolution DNS data and preserving the physical characteristics of flow\ntransport.\n","authors":["Shengyu Chen","Tianshu Bao","Peyman Givi","Can Zheng","Xiaowei Jia"],"pdf_url":"https://arxiv.org/pdf/2304.12130v3.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2312.07392v1","updated":"2023-12-12T16:05:55Z","published":"2023-12-12T16:05:55Z","title":"ReRoGCRL: Representation-based Robustness in Goal-Conditioned\n  Reinforcement Learning","summary":"  While Goal-Conditioned Reinforcement Learning (GCRL) has gained attention,\nits algorithmic robustness, particularly against adversarial perturbations,\nremains unexplored. Unfortunately, the attacks and robust representation\ntraining methods specifically designed for traditional RL are not so effective\nwhen applied to GCRL. To address this challenge, we propose the\n\\textit{Semi-Contrastive Representation} attack, a novel approach inspired by\nthe adversarial contrastive attack. Unlike existing attacks in RL, it only\nnecessitates information from the policy function and can be seamlessly\nimplemented during deployment. Furthermore, to mitigate the vulnerability of\nexisting GCRL algorithms, we introduce \\textit{Adversarial Representation\nTactics}. This strategy combines \\textit{Semi-Contrastive Adversarial\nAugmentation} with \\textit{Sensitivity-Aware Regularizer}. It improves the\nadversarial robustness of the underlying agent against various types of\nperturbations. Extensive experiments validate the superior performance of our\nattack and defence mechanism across multiple state-of-the-art GCRL algorithms.\nOur tool {\\bf ReRoGCRL} is available at\n\\url{https://github.com/TrustAI/ReRoGCRL}.\n","authors":["Xiangyu Yin","Sihao Wu","Jiaxu Liu","Meng Fang","Xingyu Zhao","Xiaowei Huang","Wenjie Ruan"],"pdf_url":"https://arxiv.org/pdf/2312.07392v1.pdf","comment":"This paper has been accepted in AAAI24\n  (https://aaai.org/aaai-conference/)"},{"id":"http://arxiv.org/abs/2308.12696v2","updated":"2023-12-12T16:03:24Z","published":"2023-08-24T10:29:25Z","title":"Disentanglement Learning via Topology","summary":"  We propose TopDis (Topological Disentanglement), a method for learning\ndisentangled representations via adding multi-scale topological loss term.\nDisentanglement is a crucial property of data representations substantial for\nthe explainability and robustness of deep learning models and a step towards\nhigh-level cognition. The state-of-the-art method based on VAE minimizes the\ntotal correlation of the joint distribution of latent variables. We take a\ndifferent perspective on disentanglement by analyzing topological properties of\ndata manifolds. In particular, we optimize the topological similarity for data\nmanifolds traversals. To the best of our knowledge, our paper is the first one\nto propose a differentiable topological loss for disentanglement. Our\nexperiments have shown that the proposed topological loss improves\ndisentanglement scores such as MIG, FactorVAE score, SAP score and DCI\ndisentanglement score with respect to state-of-the-art results. Our method\nworks in an unsupervised manner, permitting to apply it for problems without\nlabeled factors of variation. Additionally, we show how to use the proposed\ntopological loss to find disentangled directions in a trained GAN.\n","authors":["Nikita Balabin","Daria Voronkova","Ilya Trofimov","Evgeny Burnaev","Serguei Barannikov"],"pdf_url":"https://arxiv.org/pdf/2308.12696v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.16218v3","updated":"2023-12-12T15:47:10Z","published":"2022-03-30T11:40:36Z","title":"APG: Adaptive Parameter Generation Network for Click-Through Rate\n  Prediction","summary":"  In many web applications, deep learning-based CTR prediction models (deep CTR\nmodels for short) are widely adopted. Traditional deep CTR models learn\npatterns in a static manner, i.e., the network parameters are the same across\nall the instances. However, such a manner can hardly characterize each of the\ninstances which may have different underlying distributions. It actually limits\nthe representation power of deep CTR models, leading to sub-optimal results. In\nthis paper, we propose an efficient, effective, and universal module, named as\nAdaptive Parameter Generation network (APG), which can dynamically generate\nparameters for deep CTR models on-the-fly based on different instances.\nExtensive experimental evaluation results show that APG can be applied to a\nvariety of deep CTR models and significantly improve their performance.\nMeanwhile, APG can reduce the time cost by 38.7\\% and memory usage by 96.6\\%\ncompared to a regular deep CTR model. We have deployed APG in the industrial\nsponsored search system and achieved 3\\% CTR gain and 1\\% RPM gain\nrespectively.\n","authors":["Bencheng Yan","Pengjie Wang","Kai Zhang","Feng Li","Hongbo Deng","Jian Xu","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2203.16218v3.pdf","comment":"NeurIPS 2022, 16 pages; The first two authors contributed equally to\n  this work"},{"id":"http://arxiv.org/abs/2312.07371v1","updated":"2023-12-12T15:40:38Z","published":"2023-12-12T15:40:38Z","title":"Privacy-Aware Energy Consumption Modeling of Connected Battery Electric\n  Vehicles using Federated Learning","summary":"  Battery Electric Vehicles (BEVs) are increasingly significant in modern\ncities due to their potential to reduce air pollution. Precise and real-time\nestimation of energy consumption for them is imperative for effective itinerary\nplanning and optimizing vehicle systems, which can reduce driving range anxiety\nand decrease energy costs. As public awareness of data privacy increases,\nadopting approaches that safeguard data privacy in the context of BEV energy\nconsumption modeling is crucial. Federated Learning (FL) is a promising\nsolution mitigating the risk of exposing sensitive information to third parties\nby allowing local data to remain on devices and only sharing model updates with\na central server. Our work investigates the potential of using FL methods, such\nas FedAvg, and FedPer, to improve BEV energy consumption prediction while\nmaintaining user privacy. We conducted experiments using data from 10 BEVs\nunder simulated real-world driving conditions. Our results demonstrate that the\nFedAvg-LSTM model achieved a reduction of up to 67.84\\% in the MAE value of the\nprediction results. Furthermore, we explored various real-world scenarios and\ndiscussed how FL methods can be employed in those cases. Our findings show that\nFL methods can effectively improve the performance of BEV energy consumption\nprediction while maintaining user privacy.\n","authors":["Sen Yan","Hongyuan Fang","Ji Li","Tomas Ward","Noel O'Connor","Mingming Liu"],"pdf_url":"https://arxiv.org/pdf/2312.07371v1.pdf","comment":"This paper is accepted by IEEE Transactions on Transportation\n  Electrification (TTE) on December 4, 2023. (13 pages, 6 figures, and 6\n  tables)"},{"id":"http://arxiv.org/abs/2302.07321v2","updated":"2023-12-12T15:32:45Z","published":"2023-02-14T20:05:42Z","title":"On Classification-Calibration of Gamma-Phi Losses","summary":"  Gamma-Phi losses constitute a family of multiclass classification loss\nfunctions that generalize the logistic and other common losses, and have found\napplication in the boosting literature. We establish the first general\nsufficient condition for the classification-calibration (CC) of such losses. To\nour knowledge, this sufficient condition gives the first family of nonconvex\nmulticlass surrogate losses for which CC has been fully justified. In addition,\nwe show that a previously proposed sufficient condition is in fact not\nsufficient. This contribution highlights a technical issue that is important in\nthe study of multiclass CC but has been neglected in prior work.\n","authors":["Yutong Wang","Clayton D. Scott"],"pdf_url":"https://arxiv.org/pdf/2302.07321v2.pdf","comment":"Appeared in COLT 2023"},{"id":"http://arxiv.org/abs/2305.05218v2","updated":"2023-12-12T15:23:42Z","published":"2023-05-09T07:28:12Z","title":"Graph Neural Network-based surrogate model for granular flows","summary":"  Accurate simulation of granular flow dynamics is crucial for assessing\nvarious geotechnical risks, including landslides and debris flows. Granular\nflows involve a dynamic rearrangement of particles exhibiting complex\ntransitions from solid-like to fluid-like responses. Traditional continuum and\ndiscrete numerical methods are limited by their computational cost in\nsimulating large-scale systems. Statistical or machine learning-based models\noffer an alternative. Still, they are largely empirical, based on a limited set\nof parameters. Due to their permutation-dependent learning, traditional machine\nlearning-based models require huge training data to generalize. To resolve\nthese problems, we use a graph neural network, a state-of-the-art machine\nlearning architecture that learns local interactions. Graphs represent the\nstate of dynamically changing granular flows and the interaction laws, such as\nenergy and momentum exchange between grains. We develop a graph neural\nnetwork-based simulator (GNS) that takes the current state of granular flow and\npredicts the next state using Euler explicit integration by learning the local\ninteraction laws. We train GNS on different granular trajectories. We then\nassess the performance of GNS by predicting granular column collapse. GNS\naccurately predicts flow dynamics for column collapses with different aspect\nratios unseen during training. GNS is hundreds of times faster than\nhigh-fidelity numerical simulators. The model also generalizes to domains much\nlarger than the training data, handling more than twice the number of particles\nthan it was trained on.\n","authors":["Yongjin Choi","Krishna Kumar"],"pdf_url":"https://arxiv.org/pdf/2305.05218v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.03907v2","updated":"2023-12-12T15:16:22Z","published":"2023-04-08T04:23:46Z","title":"Stochastic Nonlinear Control via Finite-dimensional Spectral Dynamic\n  Embedding","summary":"  This paper presents an approach, Spectral Dynamics Embedding Control (SDEC),\nto optimal control for nonlinear stochastic systems. This method leverages an\ninfinite-dimensional feature to linearly represent the state-action value\nfunction and exploits finite-dimensional truncation approximation for practical\nimplementation. To characterize the effectiveness of these finite dimensional\napproximations, we provide an in-depth theoretical analysis to characterize the\napproximation error induced by the finite-dimension truncation and statistical\nerror induced by finite-sample approximation in both policy evaluation and\npolicy optimization. Our analysis includes two prominent kernel approximation\nmethods: truncations onto random features and Nystrom features. We also\nempirically test the algorithm and compare the performance with Koopman-based,\niLQR, and energy-based methods on a few benchmark problems.\n","authors":["Tongzheng Ren","Zhaolin Ren","Na Li","Haitong Ma","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2304.03907v2.pdf","comment":"Added analysis of Nystrom features, more streamlined proofs, and more\n  extensive numerical studies"},{"id":"http://arxiv.org/abs/2206.05575v4","updated":"2023-12-12T15:10:46Z","published":"2022-06-11T17:38:09Z","title":"MammoFL: Mammographic Breast Density Estimation using Federated Learning","summary":"  In this study, we automate quantitative mammographic breast density\nestimation with neural networks and show that this tool is a strong use case\nfor federated learning on multi-institutional datasets. Our dataset included\nbilateral CC-view and MLO-view mammographic images from two separate\ninstitutions. Two U-Nets were separately trained on algorithm-generated labels\nto perform segmentation of the breast and dense tissue from these images and\nsubsequently calculate breast percent density (PD). The networks were trained\nwith federated learning and compared to three non-federated baselines, one\ntrained on each single-institution dataset and one trained on the aggregated\nmulti-institution dataset. We demonstrate that training on multi-institution\ndatasets is critical to algorithm generalizability. We further show that\nfederated learning on multi-institutional datasets improves model\ngeneralization to unseen data at nearly the same level as centralized training\non multi-institutional datasets, indicating that federated learning can be\napplied to our method to improve algorithm generalizability while maintaining\npatient privacy.\n","authors":["Ramya Muthukrishnan","Angelina Heyler","Keshava Katti","Sarthak Pati","Walter Mankowski","Aprupa Alahari","Michael Sanborn","Emily F. Conant","Christopher Scott","Stacey Winham","Celine Vachon","Pratik Chaudhari","Despina Kontos","Spyridon Bakas"],"pdf_url":"https://arxiv.org/pdf/2206.05575v4.pdf","comment":"Deep learning, federated learning, mammography, breast density, risk\n  assessment"},{"id":"http://arxiv.org/abs/2312.07335v1","updated":"2023-12-12T14:53:18Z","published":"2023-12-12T14:53:18Z","title":"Momentum Particle Maximum Likelihood","summary":"  Maximum likelihood estimation (MLE) of latent variable models is often recast\nas an optimization problem over the extended space of parameters and\nprobability distributions. For example, the Expectation Maximization (EM)\nalgorithm can be interpreted as coordinate descent applied to a suitable free\nenergy functional over this space. Recently, this perspective has been combined\nwith insights from optimal transport and Wasserstein gradient flows to develop\nparticle-based algorithms applicable to wider classes of models than standard\nEM.\n  Drawing inspiration from prior works which interpret `momentum-enriched'\noptimisation algorithms as discretizations of ordinary differential equations,\nwe propose an analogous dynamical systems-inspired approach to minimizing the\nfree energy functional over the extended space of parameters and probability\ndistributions. The result is a dynamic system that blends elements of\nNesterov's Accelerated Gradient method, the underdamped Langevin diffusion, and\nparticle methods.\n  Under suitable assumptions, we establish quantitative convergence of the\nproposed system to the unique minimiser of the functional in continuous time.\nWe then propose a numerical discretization of this system which enables its\napplication to parameter estimation in latent variable models. Through\nnumerical experiments, we demonstrate that the resulting algorithm converges\nfaster than existing methods and compares favourably with other (approximate)\nMLE algorithms.\n","authors":["Jen Ning Lim","Juan Kuntz","Samuel Power","Adam M. Johansen"],"pdf_url":"https://arxiv.org/pdf/2312.07335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.18694v2","updated":"2023-12-12T14:48:16Z","published":"2023-11-30T16:39:46Z","title":"Balancing Summarization and Change Detection in Graph Streams","summary":"  This study addresses the issue of balancing graph summarization and graph\nchange detection. Graph summarization compresses large-scale graphs into a\nsmaller scale. However, the question remains: To what extent should the\noriginal graph be compressed? This problem is solved from the perspective of\ngraph change detection, aiming to detect statistically significant changes\nusing a stream of summary graphs. If the compression rate is extremely high,\nimportant changes can be ignored, whereas if the compression rate is extremely\nlow, false alarms may increase with more memory. This implies that there is a\ntrade-off between compression rate in graph summarization and accuracy in\nchange detection. We propose a novel quantitative methodology to balance this\ntrade-off to simultaneously realize reliable graph summarization and change\ndetection. We introduce a probabilistic structure of hierarchical latent\nvariable model into a graph, thereby designing a parameterized summary graph on\nthe basis of the minimum description length principle. The parameter specifying\nthe summary graph is then optimized so that the accuracy of change detection is\nguaranteed to suppress Type I error probability (probability of raising false\nalarms) to be less than a given confidence level. First, we provide a\ntheoretical framework for connecting graph summarization with change detection.\nThen, we empirically demonstrate its effectiveness on synthetic and real\ndatasets.\n","authors":["Shintaro Fukushima","Kenji Yamanishi"],"pdf_url":"https://arxiv.org/pdf/2311.18694v2.pdf","comment":"6 pages, Accepted to 23rd IEEE International Conference on Data\n  Mining (ICDM2023)"},{"id":"http://arxiv.org/abs/2312.07331v1","updated":"2023-12-12T14:47:26Z","published":"2023-12-12T14:47:26Z","title":"Coupled Confusion Correction: Learning from Crowds with Sparse\n  Annotations","summary":"  As the size of the datasets getting larger, accurately annotating such\ndatasets is becoming more impractical due to the expensiveness on both time and\neconomy. Therefore, crowd-sourcing has been widely adopted to alleviate the\ncost of collecting labels, which also inevitably introduces label noise and\neventually degrades the performance of the model. To learn from crowd-sourcing\nannotations, modeling the expertise of each annotator is a common but\nchallenging paradigm, because the annotations collected by crowd-sourcing are\nusually highly-sparse. To alleviate this problem, we propose Coupled Confusion\nCorrection (CCC), where two models are simultaneously trained to correct the\nconfusion matrices learned by each other. Via bi-level optimization, the\nconfusion matrices learned by one model can be corrected by the distilled data\nfrom the other. Moreover, we cluster the ``annotator groups'' who share similar\nexpertise so that their confusion matrices could be corrected together. In this\nway, the expertise of the annotators, especially of those who provide seldom\nlabels, could be better captured. Remarkably, we point out that the annotation\nsparsity not only means the average number of labels is low, but also there are\nalways some annotators who provide very few labels, which is neglected by\nprevious works when constructing synthetic crowd-sourcing annotations. Based on\nthat, we propose to use Beta distribution to control the generation of the\ncrowd-sourcing labels so that the synthetic annotations could be more\nconsistent with the real-world ones. Extensive experiments are conducted on two\ntypes of synthetic datasets and three real-world datasets, the results of which\ndemonstrate that CCC significantly outperforms state-of-the-art approaches.\n","authors":["Hansong Zhang","Shikun Li","Dan Zeng","Chenggang Yan","Shiming Ge"],"pdf_url":"https://arxiv.org/pdf/2312.07331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07316v1","updated":"2023-12-12T14:30:30Z","published":"2023-12-12T14:30:30Z","title":"GateNet: A novel Neural Network Architecture for Automated Flow\n  Cytometry Gating","summary":"  Flow cytometry is widely used to identify cell populations in patient-derived\nfluids such as peripheral blood (PB) or cerebrospinal fluid (CSF). While\nubiquitous in research and clinical practice, flow cytometry requires gating,\ni.e. cell type identification which requires labor-intensive and error-prone\nmanual adjustments. To facilitate this process, we designed GateNet, the first\nneural network architecture enabling full end-to-end automated gating without\nthe need to correct for batch effects. We train GateNet with over 8,000,000\nevents based on N=127 PB and CSF samples which were manually labeled\nindependently by four experts. We show that for novel, unseen samples, GateNet\nachieves human-level performance (F1 score ranging from 0.910 to 0.997). In\naddition we apply GateNet to a publicly available dataset confirming\ngeneralization with an F1 score of 0.936. As our implementation utilizes\ngraphics processing units (GPU), gating only needs 15 microseconds per event.\nImportantly, we also show that GateNet only requires ~10 samples to reach\nhuman-level performance, rendering it widely applicable in all domains of flow\ncytometry.\n","authors":["Lukas Fisch","Michael O. Heming","Andreas Schulte-Mecklenbeck","Catharina C. Gross","Stefan Zumdick","Carlotta Barkhau","Daniel Emden","Jan Ernsting","Ramona Leenings","Kelvin Sarink","Nils R. Winter","Udo Dannlowski","Heinz Wiendl","Gerd Meyer zu Hörste","Tim Hahn"],"pdf_url":"https://arxiv.org/pdf/2312.07316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07311v1","updated":"2023-12-12T14:28:31Z","published":"2023-12-12T14:28:31Z","title":"Scalable Motion Style Transfer with Constrained Diffusion Generation","summary":"  Current training of motion style transfer systems relies on consistency\nlosses across style domains to preserve contents, hindering its scalable\napplication to a large number of domains and private data. Recent image\ntransfer works show the potential of independent training on each domain by\nleveraging implicit bridging between diffusion models, with the content\npreservation, however, limited to simple data patterns. We address this by\nimposing biased sampling in backward diffusion while maintaining the domain\nindependence in the training stage. We construct the bias from the source\ndomain keyframes and apply them as the gradient of content constraints,\nyielding a framework with keyframe manifold constraint gradients (KMCGs). Our\nvalidation demonstrates the success of training separate models to transfer\nbetween as many as ten dance motion styles. Comprehensive experiments find a\nsignificant improvement in preserving motion contents in comparison to baseline\nand ablative diffusion-based style transfer models. In addition, we perform a\nhuman study for a subjective assessment of the quality of generated dance\nmotions. The results validate the competitiveness of KMCGs.\n","authors":["Wenjie Yin","Yi Yu","Hang Yin","Danica Kragic","Mårten Björkman"],"pdf_url":"https://arxiv.org/pdf/2312.07311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07296v1","updated":"2023-12-12T14:14:40Z","published":"2023-12-12T14:14:40Z","title":"Complex Recurrent Spectral Network","summary":"  This paper presents a novel approach to advancing artificial intelligence\n(AI) through the development of the Complex Recurrent Spectral Network\n($\\mathbb{C}$-RSN), an innovative variant of the Recurrent Spectral Network\n(RSN) model. The $\\mathbb{C}$-RSN is designed to address a critical limitation\nin existing neural network models: their inability to emulate the complex\nprocesses of biological neural networks dynamically and accurately. By\nintegrating key concepts from dynamical systems theory and leveraging\nprinciples from statistical mechanics, the $\\mathbb{C}$-RSN model introduces\nlocalized non-linearity, complex fixed eigenvalues, and a distinct separation\nof memory and input processing functionalities. These features collectively\nenable the $\\mathbb{C}$-RSN evolving towards a dynamic, oscillating final state\nthat more closely mirrors biological cognition. Central to this work is the\nexploration of how the $\\mathbb{C}$-RSN manages to capture the rhythmic,\noscillatory dynamics intrinsic to biological systems, thanks to its complex\neigenvalue structure and the innovative segregation of its linear and\nnon-linear components. The model's ability to classify data through a\ntime-dependent function, and the localization of information processing, is\ndemonstrated with an empirical evaluation using the MNIST dataset. Remarkably,\ndistinct items supplied as a sequential input yield patterns in time which bear\nthe indirect imprint of the insertion order (and of the time of separation\nbetween contiguous insertions).\n","authors":["Lorenzo Chicchi","Lorenzo Giambagli","Lorenzo Buffoni","Raffaele Marino","Duccio Fanelli"],"pdf_url":"https://arxiv.org/pdf/2312.07296v1.pdf","comment":"27 pages, 4 figures"},{"id":"http://arxiv.org/abs/2308.03743v2","updated":"2023-12-12T14:13:35Z","published":"2023-08-07T17:51:09Z","title":"The Copycat Perceptron: Smashing Barriers Through Collective Learning","summary":"  We characterize the equilibrium properties of a model of $y$ coupled binary\nperceptrons in the teacher-student scenario, subject to a learning rule, with\nan explicit ferromagnetic coupling proportional to the Hamming distance between\nthe students' weights. In contrast to recent works, we analyze a more general\nsetting in which thermal noise is present that affects each student's\ngeneralization performance. In the nonzero temperature regime, we find that the\ncoupling of replicas produces a bend of the phase diagram towards smaller\nvalues of $\\alpha$: This suggests that the free energy landscape gets smoother\naround the solution with perfect generalization (i.e., the teacher's) at a\nfixed fraction of examples, allowing standard thermal updates such as Simulated\nAnnealing to easily reach the teacher solution and avoid entrapment in\nmetastable states as it happens in the unreplicated case, even in the so-called\ncomputationally easy regime. These results provide additional analytic and\nnumerical evidence for the recently conjectured Bayes-optimal property of\nReplicated Simulated Annealing (RSA) for a sufficient number of replicas. From\na learning perspective, these results also suggest that multiple students\nworking together (in this case reviewing the same data) are able to learn the\nsame rule both significantly faster and with fewer examples, a property that\ncould be exploited in the context of cooperative and federated learning.\n","authors":["Giovanni Catania","Aurélien Decelle","Beatriz Seoane"],"pdf_url":"https://arxiv.org/pdf/2308.03743v2.pdf","comment":"2 figures in the main, 4 figures in the appendix"},{"id":"http://arxiv.org/abs/2312.07285v1","updated":"2023-12-12T14:00:29Z","published":"2023-12-12T14:00:29Z","title":"Forced Exploration in Bandit Problems","summary":"  The multi-armed bandit(MAB) is a classical sequential decision problem. Most\nwork requires assumptions about the reward distribution (e.g., bounded), while\npractitioners may have difficulty obtaining information about these\ndistributions to design models for their problems, especially in non-stationary\nMAB problems. This paper aims to design a multi-armed bandit algorithm that can\nbe implemented without using information about the reward distribution while\nstill achieving substantial regret upper bounds. To this end, we propose a\nnovel algorithm alternating between greedy rule and forced exploration. Our\nmethod can be applied to Gaussian, Bernoulli and other subgaussian\ndistributions, and its implementation does not require additional information.\nWe employ a unified analysis method for different forced exploration strategies\nand provide problem-dependent regret upper bounds for stationary and\npiecewise-stationary settings. Furthermore, we compare our algorithm with\npopular bandit algorithms on different reward distributions.\n","authors":["Han Qi","Fei Guo","Li Zhu"],"pdf_url":"https://arxiv.org/pdf/2312.07285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.11093v2","updated":"2023-12-12T14:00:08Z","published":"2023-01-26T13:35:02Z","title":"Simple diffusion: End-to-end diffusion for high resolution images","summary":"  Currently, applying diffusion models in pixel space of high resolution images\nis difficult. Instead, existing approaches focus on diffusion in lower\ndimensional spaces (latent diffusion), or have multiple super-resolution levels\nof generation referred to as cascades. The downside is that these approaches\nadd additional complexity to the diffusion framework.\n  This paper aims to improve denoising diffusion for high resolution images\nwhile keeping the model as simple as possible. The paper is centered around the\nresearch question: How can one train a standard denoising diffusion models on\nhigh resolution images, and still obtain performance comparable to these\nalternate approaches?\n  The four main findings are: 1) the noise schedule should be adjusted for high\nresolution images, 2) It is sufficient to scale only a particular part of the\narchitecture, 3) dropout should be added at specific locations in the\narchitecture, and 4) downsampling is an effective strategy to avoid high\nresolution feature maps. Combining these simple yet effective techniques, we\nachieve state-of-the-art on image generation among diffusion models without\nsampling modifiers on ImageNet.\n","authors":["Emiel Hoogeboom","Jonathan Heek","Tim Salimans"],"pdf_url":"https://arxiv.org/pdf/2301.11093v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07282v1","updated":"2023-12-12T13:59:37Z","published":"2023-12-12T13:59:37Z","title":"Class Probability Matching Using Kernel Methods for Label Shift\n  Adaptation","summary":"  In domain adaptation, covariate shift and label shift problems are two\ndistinct and complementary tasks. In covariate shift adaptation where the\ndifferences in data distribution arise from variations in feature\nprobabilities, existing approaches naturally address this problem based on\n\\textit{feature probability matching} (\\textit{FPM}). However, for label shift\nadaptation where the differences in data distribution stem solely from\nvariations in class probability, current methods still use FPM on the\n$d$-dimensional feature space to estimate the class probability ratio on the\none-dimensional label space. To address label shift adaptation more naturally\nand effectively, inspired by a new representation of the source domain's class\nprobability, we propose a new framework called \\textit{class probability\nmatching} (\\textit{CPM}) which matches two class probability functions on the\none-dimensional label space to estimate the class probability ratio,\nfundamentally different from FPM operating on the $d$-dimensional feature\nspace. Furthermore, by incorporating the kernel logistic regression into the\nCPM framework to estimate the conditional probability, we propose an algorithm\ncalled \\textit{class probability matching using kernel methods}\n(\\textit{CPMKM}) for label shift adaptation. From the theoretical perspective,\nwe establish the optimal convergence rates of CPMKM with respect to the\ncross-entropy loss for multi-class label shift adaptation. From the\nexperimental perspective, comparisons on real datasets demonstrate that CPMKM\noutperforms existing FPM-based and maximum-likelihood-based algorithms.\n","authors":["Hongwei Wen","Annika Betken","Hanyuan Hang"],"pdf_url":"https://arxiv.org/pdf/2312.07282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07281v1","updated":"2023-12-12T13:59:26Z","published":"2023-12-12T13:59:26Z","title":"Safe Multi-Task Bayesian Optimization","summary":"  Bayesian optimization has become a powerful tool for safe online optimization\nof systems, due to its high sample efficiency and noise robustness. For further\nspeed-up reduced physical models of the system can be incorporated into the\noptimization to accelerate the process, since the models are able to offer an\napproximation of the actual system, and sampling from them is significantly\ncheaper. The similarity between model and reality is represented by additional\nhyperparameters and learned within the optimization process. Safety is an\nimportant criteria for online optimization methods like Bayesian optimization,\nwhich has been addressed by recent literature, which provide safety guarantees\nunder the assumption of known hyperparameters. However, in practice this is not\napplicable. Therefore, we extend the robust Gaussian process uniform error\nbounds to meet the multi-task setting, which involves the calculation of a\nconfidence region from the hyperparameter posterior distribution utilizing\nMarkov chain Monte Carlo methods. Then, using the robust safety bounds,\nBayesian optimization is applied to safely optimize the system while\nincorporating measurements of the models. Simulations show that the\noptimization can be significantly accelerated compared to other\nstate-of-the-art safe Bayesian optimization methods depending on the fidelity\nof the models.\n","authors":["Jannis O. Lübsen","Christian Hespe","Annika Eichler"],"pdf_url":"https://arxiv.org/pdf/2312.07281v1.pdf","comment":"Submitted to L4DC 2024"},{"id":"http://arxiv.org/abs/2312.07271v1","updated":"2023-12-12T13:51:25Z","published":"2023-12-12T13:51:25Z","title":"Analyze the Robustness of Classifiers under Label Noise","summary":"  This study explores the robustness of label noise classifiers, aiming to\nenhance model resilience against noisy data in complex real-world scenarios.\nLabel noise in supervised learning, characterized by erroneous or imprecise\nlabels, significantly impairs model performance. This research focuses on the\nincreasingly pertinent issue of label noise's impact on practical applications.\nAddressing the prevalent challenge of inaccurate training data labels, we\nintegrate adversarial machine learning (AML) and importance reweighting\ntechniques. Our approach involves employing convolutional neural networks (CNN)\nas the foundational model, with an emphasis on parameter adjustment for\nindividual training samples. This strategy is designed to heighten the model's\nfocus on samples critically influencing performance.\n","authors":["Cheng Zeng","Yixuan Xu","Jiaqi Tian"],"pdf_url":"https://arxiv.org/pdf/2312.07271v1.pdf","comment":"21 pages, 11 figures"},{"id":"http://arxiv.org/abs/2308.16139v5","updated":"2023-12-12T13:39:31Z","published":"2023-08-30T16:52:20Z","title":"MedShapeNet -- A Large-Scale Dataset of 3D Medical Shapes for Computer\n  Vision","summary":"  Prior to the deep learning era, shape was commonly used to describe the\nobjects. Nowadays, state-of-the-art (SOTA) algorithms in medical imaging are\npredominantly diverging from computer vision, where voxel grids, meshes, point\nclouds, and implicit surface models are used. This is seen from numerous\nshape-related publications in premier vision conferences as well as the growing\npopularity of ShapeNet (about 51,300 models) and Princeton ModelNet (127,915\nmodels). For the medical domain, we present a large collection of anatomical\nshapes (e.g., bones, organs, vessels) and 3D models of surgical instrument,\ncalled MedShapeNet, created to facilitate the translation of data-driven vision\nalgorithms to medical applications and to adapt SOTA vision algorithms to\nmedical problems. As a unique feature, we directly model the majority of shapes\non the imaging data of real patients. As of today, MedShapeNet includes 23\ndataset with more than 100,000 shapes that are paired with annotations (ground\ntruth). Our data is freely accessible via a web interface and a Python\napplication programming interface (API) and can be used for discriminative,\nreconstructive, and variational benchmarks as well as various applications in\nvirtual, augmented, or mixed reality, and 3D printing. Exemplary, we present\nuse cases in the fields of classification of brain tumors, facial and skull\nreconstructions, multi-class anatomy completion, education, and 3D printing. In\nfuture, we will extend the data and improve the interfaces. The project pages\nare: https://medshapenet.ikim.nrw/ and\nhttps://github.com/Jianningli/medshapenet-feedback\n","authors":["Jianning Li","Zongwei Zhou","Jiancheng Yang","Antonio Pepe","Christina Gsaxner","Gijs Luijten","Chongyu Qu","Tiezheng Zhang","Xiaoxi Chen","Wenxuan Li","Marek Wodzinski","Paul Friedrich","Kangxian Xie","Yuan Jin","Narmada Ambigapathy","Enrico Nasca","Naida Solak","Gian Marco Melito","Viet Duc Vu","Afaque R. Memon","Christopher Schlachta","Sandrine De Ribaupierre","Rajnikant Patel","Roy Eagleson","Xiaojun Chen","Heinrich Mächler","Jan Stefan Kirschke","Ezequiel de la Rosa","Patrick Ferdinand Christ","Hongwei Bran Li","David G. Ellis","Michele R. Aizenberg","Sergios Gatidis","Thomas Küstner","Nadya Shusharina","Nicholas Heller","Vincent Andrearczyk","Adrien Depeursinge","Mathieu Hatt","Anjany Sekuboyina","Maximilian Löffler","Hans Liebl","Reuben Dorent","Tom Vercauteren","Jonathan Shapey","Aaron Kujawa","Stefan Cornelissen","Patrick Langenhuizen","Achraf Ben-Hamadou","Ahmed Rekik","Sergi Pujades","Edmond Boyer","Federico Bolelli","Costantino Grana","Luca Lumetti","Hamidreza Salehi","Jun Ma","Yao Zhang","Ramtin Gharleghi","Susann Beier","Arcot Sowmya","Eduardo A. Garza-Villarreal","Thania Balducci","Diego Angeles-Valdez","Roberto Souza","Leticia Rittner","Richard Frayne","Yuanfeng Ji","Vincenzo Ferrari","Soumick Chatterjee","Florian Dubost","Stefanie Schreiber","Hendrik Mattern","Oliver Speck","Daniel Haehn","Christoph John","Andreas Nürnberger","João Pedrosa","Carlos Ferreira","Guilherme Aresta","António Cunha","Aurélio Campilho","Yannick Suter","Jose Garcia","Alain Lalande","Vicky Vandenbossche","Aline Van Oevelen","Kate Duquesne","Hamza Mekhzoum","Jef Vandemeulebroucke","Emmanuel Audenaert","Claudia Krebs","Timo van Leeuwen","Evie Vereecke","Hauke Heidemeyer","Rainer Röhrig","Frank Hölzle","Vahid Badeli","Kathrin Krieger","Matthias Gunzer","Jianxu Chen","Timo van Meegdenburg","Amin Dada","Miriam Balzer","Jana Fragemann","Frederic Jonske","Moritz Rempe","Stanislav Malorodov","Fin H. Bahnsen","Constantin Seibold","Alexander Jaus","Zdravko Marinov","Paul F. Jaeger","Rainer Stiefelhagen","Ana Sofia Santos","Mariana Lindo","André Ferreira","Victor Alves","Michael Kamp","Amr Abourayya","Felix Nensa","Fabian Hörst","Alexander Brehmer","Lukas Heine","Yannik Hanusrichter","Martin Weßling","Marcel Dudda","Lars E. Podleska","Matthias A. Fink","Julius Keyl","Konstantinos Tserpes","Moon-Sung Kim","Shireen Elhabian","Hans Lamecker","Dženan Zukić","Beatriz Paniagua","Christian Wachinger","Martin Urschler","Luc Duong","Jakob Wasserthal","Peter F. Hoyer","Oliver Basu","Thomas Maal","Max J. H. Witjes","Gregor Schiele","Ti-chiun Chang","Seyed-Ahmad Ahmadi","Ping Luo","Bjoern Menze","Mauricio Reyes","Thomas M. Deserno","Christos Davatzikos","Behrus Puladi","Pascal Fua","Alan L. Yuille","Jens Kleesiek","Jan Egger"],"pdf_url":"https://arxiv.org/pdf/2308.16139v5.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2307.08131v3","updated":"2023-12-12T13:36:28Z","published":"2023-07-16T19:04:48Z","title":"INFLECT-DGNN: Influencer Prediction with Dynamic Graph Neural Networks","summary":"  Leveraging network information for predictive modeling has become widespread\nin many domains. Within the realm of referral and targeted marketing,\ninfluencer detection stands out as an area that could greatly benefit from the\nincorporation of dynamic network representation due to the ongoing development\nof customer-brand relationships. To elaborate this idea, we introduce\nINFLECT-DGNN, a new framework for INFLuencer prEdiCTion with Dynamic Graph\nNeural Networks that combines Graph Neural Networks (GNN) and Recurrent Neural\nNetworks (RNN) with weighted loss functions, the Synthetic Minority\nOversampling TEchnique (SMOTE) adapted for graph data, and a carefully crafted\nrolling-window strategy. To evaluate predictive performance, we utilize a\nunique corporate data set with networks of three cities and derive a\nprofit-driven evaluation methodology for influencer prediction. Our results\nshow how using RNN to encode temporal attributes alongside GNNs significantly\nimproves predictive performance. We compare the results of various models to\ndemonstrate the importance of capturing graph representation, temporal\ndependencies, and using a profit-driven methodology for evaluation.\n","authors":["Elena Tiukhova","Emiliano Penaloza","María Óskarsdóttir","Bart Baesens","Monique Snoeck","Cristián Bravo"],"pdf_url":"https://arxiv.org/pdf/2307.08131v3.pdf","comment":"26 pages, 10 figures"},{"id":"http://arxiv.org/abs/2312.07252v1","updated":"2023-12-12T13:28:53Z","published":"2023-12-12T13:28:53Z","title":"Identifying Drivers of Predictive Uncertainty using Variance Feature\n  Attribution","summary":"  Explainability and uncertainty quantification are two pillars of trustable\nartificial intelligence. However, the reasoning behind uncertainty estimates is\ngenerally left unexplained. Identifying the drivers of uncertainty complements\nexplanations of point predictions in recognizing potential model limitations.\nIt facilitates the detection of oversimplification in the uncertainty\nestimation process. Explanations of uncertainty enhance communication and trust\nin decisions. They allow for verifying whether the main drivers of model\nuncertainty are relevant and may impact model usage. So far, the subject of\nexplaining uncertainties has been rarely studied. The few exceptions in\nexisting literature are tailored to Bayesian neural networks or rely heavily on\ntechnically intricate approaches, hindering their broad adoption. We propose\nvariance feature attribution, a simple and scalable solution to explain\npredictive aleatoric uncertainties. First, we estimate uncertainty as\npredictive variance by equipping a neural network with a Gaussian output\ndistribution by adding a variance output neuron. Thereby, we can rely on\npre-trained point prediction models and fine-tune them for meaningful variance\nestimation. Second, we apply out-of-the-box explainers on the variance output\nof these models to explain the uncertainty estimation. We evaluate our approach\nin a synthetic setting where the data-generating process is known. We show that\nour method can explain uncertainty influences more reliably and faster than the\nestablished baseline CLUE. We fine-tune a state-of-the-art age regression model\nto estimate uncertainty and obtain attributions. Our explanations highlight\npotential sources of uncertainty, such as laugh lines. Variance feature\nattribution provides accurate explanations for uncertainty estimates with\nlittle modifications to the model architecture and low computational overhead.\n","authors":["Pascal Iversen","Simon Witzke","Katharina Baum","Bernhard Y. Renard"],"pdf_url":"https://arxiv.org/pdf/2312.07252v1.pdf","comment":"Simon Witzke and Pascal Iversen contributed equally"},{"id":"http://arxiv.org/abs/2312.07248v1","updated":"2023-12-12T13:25:32Z","published":"2023-12-12T13:25:32Z","title":"Multi-Granularity Framework for Unsupervised Representation Learning of\n  Time Series","summary":"  Representation learning plays a critical role in the analysis of time series\ndata and has high practical value across a wide range of applications.\nincluding trend analysis, time series data retrieval and forecasting. In\npractice, data confusion is a significant issue as it can considerably impact\nthe effectiveness and accuracy of data analysis, machine learning models and\ndecision-making processes. In general, previous studies did not consider the\nvariability at various levels of granularity, thus resulting in inadequate\ninformation utilization, which further exacerbated the issue of data confusion.\nThis paper proposes an unsupervised framework to realize multi-granularity\nrepresentation learning for time series. Specifically, we employed a\ncross-granularity transformer to develop an association between fine- and\ncoarse-grained representations. In addition, we introduced a retrieval task as\nan unsupervised training task to learn the multi-granularity representation of\ntime series. Moreover, a novel loss function was designed to obtain the\ncomprehensive multi-granularity representation of the time series via\nunsupervised learning. The experimental results revealed that the proposed\nframework demonstrates significant advantages over alternative representation\nlearning models.\n","authors":["Chengyang Ye","Qiang Ma"],"pdf_url":"https://arxiv.org/pdf/2312.07248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.18130v2","updated":"2023-12-12T13:09:46Z","published":"2023-11-29T22:44:32Z","title":"The Trifecta: Three simple techniques for training deeper\n  Forward-Forward networks","summary":"  Modern machine learning models are able to outperform humans on a variety of\nnon-trivial tasks. However, as the complexity of the models increases, they\nconsume significant amounts of power and still struggle to generalize\neffectively to unseen data. Local learning, which focuses on updating subsets\nof a model's parameters at a time, has emerged as a promising technique to\naddress these issues. Recently, a novel local learning algorithm, called\nForward-Forward, has received widespread attention due to its innovative\napproach to learning. Unfortunately, its application has been limited to\nsmaller datasets due to scalability issues. To this end, we propose The\nTrifecta, a collection of three simple techniques that synergize exceptionally\nwell and drastically improve the Forward-Forward algorithm on deeper networks.\nOur experiments demonstrate that our models are on par with similarly\nstructured, backpropagation-based models in both training speed and test\naccuracy on simple datasets. This is achieved by the ability to learn\nrepresentations that are informative locally, on a layer-by-layer basis, and\nretain their informativeness when propagated to deeper layers in the\narchitecture. This leads to around 84% accuracy on CIFAR-10, a notable\nimprovement (25%) over the original FF algorithm. These results highlight the\npotential of Forward-Forward as a genuine competitor to backpropagation and as\na promising research avenue.\n","authors":["Thomas Dooms","Ing Jyh Tsang","Jose Oramas"],"pdf_url":"https://arxiv.org/pdf/2311.18130v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04816v3","updated":"2023-12-12T13:06:33Z","published":"2023-10-07T14:13:14Z","title":"Hacking Generative Models with Differentiable Network Bending","summary":"  In this work, we propose a method to 'hack' generative models, pushing their\noutputs away from the original training distribution towards a new objective.\nWe inject a small-scale trainable module between the intermediate layers of the\nmodel and train it for a low number of iterations, keeping the rest of the\nnetwork frozen. The resulting output images display an uncanny quality, given\nby the tension between the original and new objectives that can be exploited\nfor artistic purposes.\n","authors":["Giacomo Aldegheri","Alina Rogalska","Ahmed Youssef","Eugenia Iofinova"],"pdf_url":"https://arxiv.org/pdf/2310.04816v3.pdf","comment":"12 pages, 10 figures, Machine Learning for Creativity and Design\n  Workshop at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2312.05549v2","updated":"2023-12-12T13:06:33Z","published":"2023-12-09T11:35:25Z","title":"Multi-granularity Causal Structure Learning","summary":"  Unveil, model, and comprehend the causal mechanisms underpinning natural\nphenomena stand as fundamental endeavors across myriad scientific disciplines.\nMeanwhile, new knowledge emerges when discovering causal relationships from\ndata. Existing causal learning algorithms predominantly focus on the isolated\neffects of variables, overlook the intricate interplay of multiple variables\nand their collective behavioral patterns. Furthermore, the ubiquity of\nhigh-dimensional data exacts a substantial temporal cost for causal algorithms.\nIn this paper, we develop a novel method called MgCSL (Multi-granularity Causal\nStructure Learning), which first leverages sparse auto-encoder to explore\ncoarse-graining strategies and causal abstractions from micro-variables to\nmacro-ones. MgCSL then takes multi-granularity variables as inputs to train\nmultilayer perceptrons and to delve the causality between variables. To enhance\nthe efficacy on high-dimensional data, MgCSL introduces a simplified acyclicity\nconstraint to adeptly search the directed acyclic graph among variables.\nExperimental results show that MgCSL outperforms competitive baselines, and\nfinds out explainable causal connections on fMRI datasets.\n","authors":["Jiaxuan Liang","Jun Wang","Guoxian Yu","Shuyin Xia","Guoyin Wang"],"pdf_url":"https://arxiv.org/pdf/2312.05549v2.pdf","comment":"Accepted by the Thirty-Eighth AAAI Conference on Artificial\n  Intelligence (AAAI2024)"},{"id":"http://arxiv.org/abs/2312.07231v1","updated":"2023-12-12T12:50:33Z","published":"2023-12-12T12:50:33Z","title":"Fast Training of Diffusion Transformer with Extreme Masking for 3D Point\n  Clouds Generation","summary":"  Diffusion Transformers have recently shown remarkable effectiveness in\ngenerating high-quality 3D point clouds. However, training voxel-based\ndiffusion models for high-resolution 3D voxels remains prohibitively expensive\ndue to the cubic complexity of attention operators, which arises from the\nadditional dimension of voxels. Motivated by the inherent redundancy of 3D\ncompared to 2D, we propose FastDiT-3D, a novel masked diffusion transformer\ntailored for efficient 3D point cloud generation, which greatly reduces\ntraining costs. Specifically, we draw inspiration from masked autoencoders to\ndynamically operate the denoising process on masked voxelized point clouds. We\nalso propose a novel voxel-aware masking strategy to adaptively aggregate\nbackground/foreground information from voxelized point clouds. Our method\nachieves state-of-the-art performance with an extreme masking ratio of nearly\n99%. Moreover, to improve multi-category 3D generation, we introduce\nMixture-of-Expert (MoE) in 3D diffusion model. Each category can learn a\ndistinct diffusion path with different experts, relieving gradient conflict.\nExperimental results on the ShapeNet dataset demonstrate that our method\nachieves state-of-the-art high-fidelity and diverse 3D point cloud generation\nperformance. Our FastDiT-3D improves 1-Nearest Neighbor Accuracy and Coverage\nmetrics when generating 128-resolution voxel point clouds, using only 6.5% of\nthe original training cost.\n","authors":["Shentong Mo","Enze Xie","Yue Wu","Junsong Chen","Matthias Nießner","Zhenguo Li"],"pdf_url":"https://arxiv.org/pdf/2312.07231v1.pdf","comment":"Project Page: https://dit-3d.github.io/FastDiT-3D/"},{"id":"http://arxiv.org/abs/2302.11091v2","updated":"2023-12-12T12:46:39Z","published":"2023-02-22T01:57:42Z","title":"GTRL: An Entity Group-Aware Temporal Knowledge Graph Representation\n  Learning Method","summary":"  Temporal Knowledge Graph (TKG) representation learning embeds entities and\nevent types into a continuous low-dimensional vector space by integrating the\ntemporal information, which is essential for downstream tasks, e.g., event\nprediction and question answering. Existing methods stack multiple graph\nconvolution layers to model the influence of distant entities, leading to the\nover-smoothing problem. To alleviate the problem, recent studies infuse\nreinforcement learning to obtain paths that contribute to modeling the\ninfluence of distant entities. However, due to the limited number of hops,\nthese studies fail to capture the correlation between entities that are far\napart and even unreachable. To this end, we propose GTRL, an entity Group-aware\nTemporal knowledge graph Representation Learning method. GTRL is the first work\nthat incorporates the entity group modeling to capture the correlation between\nentities by stacking only a finite number of layers. Specifically, the entity\ngroup mapper is proposed to generate entity groups from entities in a learning\nway. Based on entity groups, the implicit correlation encoder is introduced to\ncapture implicit correlations between any pairwise entity groups. In addition,\nthe hierarchical GCNs are exploited to accomplish the message aggregation and\nrepresentation updating on the entity group graph and the entity graph.\nFinally, GRUs are employed to capture the temporal dependency in TKGs.\nExtensive experiments on three real-world datasets demonstrate that GTRL\nachieves the state-of-the-art performances on the event prediction task,\noutperforming the best baseline by an average of 13.44%, 9.65%, 12.15%, and\n15.12% in MRR, Hits@1, Hits@3, and Hits@10, respectively.\n","authors":["Xing Tang","Ling Chen"],"pdf_url":"https://arxiv.org/pdf/2302.11091v2.pdf","comment":"Accepted by TKDE, 16 pages, and 9 figures"},{"id":"http://arxiv.org/abs/2312.07208v1","updated":"2023-12-12T12:21:08Z","published":"2023-12-12T12:21:08Z","title":"Experimental Investigation of Machine Learning based Soft-Failure\n  Management using the Optical Spectrum","summary":"  The demand for high-speed data is exponentially growing. To conquer this,\noptical networks underwent significant changes getting more complex and\nversatile. The increasing complexity necessitates the fault management to be\nmore adaptive to enhance network assurance. In this paper, we experimentally\ncompare the performance of soft-failure management of different machine\nlearning algorithms. We further introduce a machine-learning based soft-failure\nmanagement framework. It utilizes a variational autoencoder based generative\nadversarial network (VAE-GAN) running on optical spectral data obtained by\noptical spectrum analyzers. The framework is able to reliably run on a fraction\nof available training data as well as identifying unknown failure types. The\ninvestigations show, that the VAE-GAN outperforms the other machine learning\nalgorithms when up to 10\\% of the total training data is available in\nidentification tasks. Furthermore, the advanced training mechanism for the GAN\nshows a high F1-score for unknown spectrum identification. The failure\nlocalization comparison shows the advantage of a low complexity neural network\nin combination with a VAE over established machine learning algorithms.\n","authors":["Lars E. Kruse","Sebastian Kühl","Annika Dochhan","Stephan Pachnicke"],"pdf_url":"https://arxiv.org/pdf/2312.07208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06436v2","updated":"2023-12-12T12:19:31Z","published":"2023-12-11T15:07:58Z","title":"Reward Certification for Policy Smoothed Reinforcement Learning","summary":"  Reinforcement Learning (RL) has achieved remarkable success in\nsafety-critical areas, but it can be weakened by adversarial attacks. Recent\nstudies have introduced \"smoothed policies\" in order to enhance its robustness.\nYet, it is still challenging to establish a provable guarantee to certify the\nbound of its total reward. Prior methods relied primarily on computing bounds\nusing Lipschitz continuity or calculating the probability of cumulative reward\nabove specific thresholds. However, these techniques are only suited for\ncontinuous perturbations on the RL agent's observations and are restricted to\nperturbations bounded by the $l_2$-norm. To address these limitations, this\npaper proposes a general black-box certification method capable of directly\ncertifying the cumulative reward of the smoothed policy under various\n$l_p$-norm bounded perturbations. Furthermore, we extend our methodology to\ncertify perturbations on action spaces. Our approach leverages f-divergence to\nmeasure the distinction between the original distribution and the perturbed\ndistribution, subsequently determining the certification bound by solving a\nconvex optimisation problem. We provide a comprehensive theoretical analysis\nand run sufficient experiments in multiple environments. Our results show that\nour method not only improves the certified lower bound of mean cumulative\nreward but also demonstrates better efficiency than state-of-the-art\ntechniques.\n","authors":["Ronghui Mu","Leandro Soriano Marcolino","Tianle Zhang","Yanghao Zhang","Xiaowei Huang","Wenjie Ruan"],"pdf_url":"https://arxiv.org/pdf/2312.06436v2.pdf","comment":"This paper will be presented in AAAI2024"},{"id":"http://arxiv.org/abs/2312.07206v1","updated":"2023-12-12T12:19:13Z","published":"2023-12-12T12:19:13Z","title":"A churn prediction dataset from the telecom sector: a new benchmark for\n  uplift modeling","summary":"  Uplift modeling, also known as individual treatment effect (ITE) estimation,\nis an important approach for data-driven decision making that aims to identify\nthe causal impact of an intervention on individuals. This paper introduces a\nnew benchmark dataset for uplift modeling focused on churn prediction, coming\nfrom a telecom company in Belgium, Orange Belgium. Churn, in this context,\nrefers to customers terminating their subscription to the telecom service. This\nis the first publicly available dataset offering the possibility to evaluate\nthe efficiency of uplift modeling on the churn prediction problem. Moreover,\nits unique characteristics make it more challenging than the few other public\nuplift datasets.\n","authors":["Théo Verhelst","Denis Mercier","Jeevan Shrestha","Gianluca Bontempi"],"pdf_url":"https://arxiv.org/pdf/2312.07206v1.pdf","comment":"8 pages, 2 figures, 5 tables, post-proceedings of the ECML PKDD 2023\n  Workshop on Uplift Modeling and Causal Machine Learning for Operational\n  Decision Making"},{"id":"http://arxiv.org/abs/2312.07186v1","updated":"2023-12-12T11:48:56Z","published":"2023-12-12T11:48:56Z","title":"Towards Optimal Sobolev Norm Rates for the Vector-Valued Regularized\n  Least-Squares Algorithm","summary":"  We present the first optimal rates for infinite-dimensional vector-valued\nridge regression on a continuous scale of norms that interpolate between $L_2$\nand the hypothesis space, which we consider as a vector-valued reproducing\nkernel Hilbert space. These rates allow to treat the misspecified case in which\nthe true regression function is not contained in the hypothesis space. We\ncombine standard assumptions on the capacity of the hypothesis space with a\nnovel tensor product construction of vector-valued interpolation spaces in\norder to characterize the smoothness of the regression function. Our upper\nbound not only attains the same rate as real-valued kernel ridge regression,\nbut also removes the assumption that the target regression function is bounded.\nFor the lower bound, we reduce the problem to the scalar setting using a\nprojection argument. We show that these rates are optimal in most cases and\nindependent of the dimension of the output space. We illustrate our results for\nthe special case of vector-valued Sobolev spaces.\n","authors":["Zhu Li","Dimitri Meunier","Mattes Mollenhauer","Arthur Gretton"],"pdf_url":"https://arxiv.org/pdf/2312.07186v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2208.01711"},{"id":"http://arxiv.org/abs/2312.07182v1","updated":"2023-12-12T11:38:09Z","published":"2023-12-12T11:38:09Z","title":"Classifying complex documents: comparing bespoke solutions to large\n  language models","summary":"  Here we search for the best automated classification approach for a set of\ncomplex legal documents. Our classification task is not trivial: our aim is to\nclassify ca 30,000 public courthouse records from 12 states and 267 counties at\ntwo different levels using nine sub-categories. Specifically, we investigated\nwhether a fine-tuned large language model (LLM) can achieve the accuracy of a\nbespoke custom-trained model, and what is the amount of fine-tuning necessary.\n","authors":["Glen Hopkins","Kristjan Kalm"],"pdf_url":"https://arxiv.org/pdf/2312.07182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07178v1","updated":"2023-12-12T11:22:31Z","published":"2023-12-12T11:22:31Z","title":"Beyond Expected Return: Accounting for Policy Reproducibility when\n  Evaluating Reinforcement Learning Algorithms","summary":"  Many applications in Reinforcement Learning (RL) usually have noise or\nstochasticity present in the environment. Beyond their impact on learning,\nthese uncertainties lead the exact same policy to perform differently, i.e.\nyield different return, from one roll-out to another. Common evaluation\nprocedures in RL summarise the consequent return distributions using solely the\nexpected return, which does not account for the spread of the distribution. Our\nwork defines this spread as the policy reproducibility: the ability of a policy\nto obtain similar performance when rolled out many times, a crucial property in\nsome real-world applications. We highlight that existing procedures that only\nuse the expected return are limited on two fronts: first an infinite number of\nreturn distributions with a wide range of performance-reproducibility\ntrade-offs can have the same expected return, limiting its effectiveness when\nused for comparing policies; second, the expected return metric does not leave\nany room for practitioners to choose the best trade-off value for considered\napplications. In this work, we address these limitations by recommending the\nuse of Lower Confidence Bound, a metric taken from Bayesian optimisation that\nprovides the user with a preference parameter to choose a desired\nperformance-reproducibility trade-off. We also formalise and quantify policy\nreproducibility, and demonstrate the benefit of our metrics using extensive\nexperiments of popular RL algorithms on common uncertain RL tasks.\n","authors":["Manon Flageat","Bryan Lim","Antoine Cully"],"pdf_url":"https://arxiv.org/pdf/2312.07178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07175v1","updated":"2023-12-12T11:18:56Z","published":"2023-12-12T11:18:56Z","title":"Instrumental Variable Estimation for Causal Inference in Longitudinal\n  Data with Time-Dependent Latent Confounders","summary":"  Causal inference from longitudinal observational data is a challenging\nproblem due to the difficulty in correctly identifying the time-dependent\nconfounders, especially in the presence of latent time-dependent confounders.\nInstrumental variable (IV) is a powerful tool for addressing the latent\nconfounders issue, but the traditional IV technique cannot deal with latent\ntime-dependent confounders in longitudinal studies. In this work, we propose a\nnovel Time-dependent Instrumental Factor Model (TIFM) for time-varying causal\neffect estimation from data with latent time-dependent confounders. At each\ntime-step, the proposed TIFM method employs the Recurrent Neural Network (RNN)\narchitecture to infer latent IV, and then uses the inferred latent IV factor\nfor addressing the confounding bias caused by the latent time-dependent\nconfounders. We provide a theoretical analysis for the proposed TIFM method\nregarding causal effect estimation in longitudinal data. Extensive evaluation\nwith synthetic datasets demonstrates the effectiveness of TIFM in addressing\ncausal effect estimation over time. We further apply TIFM to a climate dataset\nto showcase the potential of the proposed method in tackling real-world\nproblems.\n","authors":["Debo Cheng","Ziqi Xu","Jiuyong Li","Lin Liu","Jixue Liu","Wentao Gao","Thuc Duy Le"],"pdf_url":"https://arxiv.org/pdf/2312.07175v1.pdf","comment":"13 pages, 7 figures and 3 tables"},{"id":"http://arxiv.org/abs/2312.07174v1","updated":"2023-12-12T11:18:43Z","published":"2023-12-12T11:18:43Z","title":"Investigation into the Training Dynamics of Learned Optimizers","summary":"  Optimization is an integral part of modern deep learning. Recently, the\nconcept of learned optimizers has emerged as a way to accelerate this\noptimization process by replacing traditional, hand-crafted algorithms with\nmeta-learned functions. Despite the initial promising results of these methods,\nissues with stability and generalization still remain, limiting their practical\nuse. Moreover, their inner workings and behavior under different conditions are\nnot yet fully understood, making it difficult to come up with improvements. For\nthis reason, our work examines their optimization trajectories from the\nperspective of network architecture symmetries and parameter update\ndistributions. Furthermore, by contrasting the learned optimizers with their\nmanually designed counterparts, we identify several key insights that\ndemonstrate how each approach can benefit from the strengths of the other.\n","authors":["Jan Sobotka","Petr Šimánek","Daniel Vašata"],"pdf_url":"https://arxiv.org/pdf/2312.07174v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07168v1","updated":"2023-12-12T11:13:13Z","published":"2023-12-12T11:13:13Z","title":"Equivariant Flow Matching with Hybrid Probability Transport","summary":"  The generation of 3D molecules requires simultaneously deciding the\ncategorical features~(atom types) and continuous features~(atom coordinates).\nDeep generative models, especially Diffusion Models (DMs), have demonstrated\neffectiveness in generating feature-rich geometries. However, existing DMs\ntypically suffer from unstable probability dynamics with inefficient sampling\nspeed. In this paper, we introduce geometric flow matching, which enjoys the\nadvantages of both equivariant modeling and stabilized probability dynamics.\nMore specifically, we propose a hybrid probability path where the coordinates\nprobability path is regularized by an equivariant optimal transport, and the\ninformation between different modalities is aligned. Experimentally, the\nproposed method could consistently achieve better performance on multiple\nmolecule generation benchmarks with 4.75$\\times$ speed up of sampling on\naverage.\n","authors":["Yuxuan Song","Jingjing Gong","Minkai Xu","Ziyao Cao","Yanyan Lan","Stefano Ermon","Hao Zhou","Wei-Ying Ma"],"pdf_url":"https://arxiv.org/pdf/2312.07168v1.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.07289v2","updated":"2023-12-12T11:06:48Z","published":"2023-11-13T12:33:33Z","title":"A probabilistic forecast methodology for volatile electricity prices in\n  the Australian National Electricity Market","summary":"  The South Australia region of the Australian National Electricity Market\n(NEM) displays some of the highest levels of price volatility observed in\nmodern electricity markets. This paper outlines an approach to probabilistic\nforecasting under these extreme conditions, including spike filtration and\nseveral post-processing steps. We propose using quantile regression as an\nensemble tool for probabilistic forecasting, with our combined forecasts\nachieving superior results compared to all constituent models. Within our\nensemble framework, we demonstrate that averaging models with varying training\nlength periods leads to a more adaptive model and increased prediction\naccuracy. The applicability of the final model is evaluated by comparing our\nmedian forecasts with the point forecasts available from the Australian NEM\noperator, with our model outperforming these NEM forecasts by a significant\nmargin.\n","authors":["Cameron Cornell","Nam Trong Dinh","S. Ali Pourmousavi"],"pdf_url":"https://arxiv.org/pdf/2311.07289v2.pdf","comment":"This manuscript has been accepted for publication in International\n  Journal of Forecasting"},{"id":"http://arxiv.org/abs/2312.07165v1","updated":"2023-12-12T11:03:51Z","published":"2023-12-12T11:03:51Z","title":"Language-Guided Transformer for Federated Multi-Label Classification","summary":"  Federated Learning (FL) is an emerging paradigm that enables multiple users\nto collaboratively train a robust model in a privacy-preserving manner without\nsharing their private data. Most existing approaches of FL only consider\ntraditional single-label image classification, ignoring the impact when\ntransferring the task to multi-label image classification. Nevertheless, it is\nstill challenging for FL to deal with user heterogeneity in their local data\ndistribution in the real-world FL scenario, and this issue becomes even more\nsevere in multi-label image classification. Inspired by the recent success of\nTransformers in centralized settings, we propose a novel FL framework for\nmulti-label classification. Since partial label correlation may be observed by\nlocal clients during training, direct aggregation of locally updated models\nwould not produce satisfactory performances. Thus, we propose a novel FL\nframework of Language-Guided Transformer (FedLGT) to tackle this challenging\ntask, which aims to exploit and transfer knowledge across different clients for\nlearning a robust global model. Through extensive experiments on various\nmulti-label datasets (e.g., FLAIR, MS-COCO, etc.), we show that our FedLGT is\nable to achieve satisfactory performance and outperforms standard FL techniques\nunder multi-label FL scenarios. Code is available at\nhttps://github.com/Jack24658735/FedLGT.\n","authors":["I-Jieh Liu","Ci-Siang Lin","Fu-En Yang","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2312.07165v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.07161v1","updated":"2023-12-12T10:56:27Z","published":"2023-12-12T10:56:27Z","title":"One-dimensional Convolutional Neural Networks for Detecting Transiting\n  Exoplanets","summary":"  The transit method is one of the most relevant exoplanet detection\ntechniques, which consists of detecting periodic eclipses in the light curves\nof stars. This is not always easy due to the presence of noise in the light\ncurves, which is induced, for example, by the response of a telescope to\nstellar flux. For this reason, we aimed to develop an artificial neural network\nmodel that is able to detect these transits in light curves obtained from\ndifferent telescopes and surveys. We created artificial light curves with and\nwithout transits to try to mimic those expected for the extended mission of the\nKepler telescope (K2) in order to train and validate a 1D convolutional neural\nnetwork model, which was later tested, obtaining an accuracy of 99.02 % and an\nestimated error (loss function) of 0.03. These results, among others, helped to\nconfirm that the 1D CNN is a good choice for working with non-phased-folded\nMandel and Agol light curves with transits. It also reduces the number of light\ncurves that have to be visually inspected to decide if they present\ntransit-like signals and decreases the time needed for analyzing each (with\nrespect to traditional analysis).\n","authors":["Santiago Iglesias Álvarez","Enrique Díez Alonso","María Luisa Sánchez","Javier Rodríguez Rodríguez","Fernando Sánchez Lasheras","Francisco Javier de Cos Juez"],"pdf_url":"https://arxiv.org/pdf/2312.07161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.04669v3","updated":"2023-12-12T10:30:59Z","published":"2023-08-09T02:27:23Z","title":"A General Implicit Framework for Fast NeRF Composition and Rendering","summary":"  A variety of Neural Radiance Fields (NeRF) methods have recently achieved\nremarkable success in high render speed. However, current accelerating methods\nare specialized and incompatible with various implicit methods, preventing\nreal-time composition over various types of NeRF works. Because NeRF relies on\nsampling along rays, it is possible to provide general guidance for\nacceleration. To that end, we propose a general implicit pipeline for composing\nNeRF objects quickly. Our method enables the casting of dynamic shadows within\nor between objects using analytical light sources while allowing multiple NeRF\nobjects to be seamlessly placed and rendered together with any arbitrary rigid\ntransformations. Mainly, our work introduces a new surface representation known\nas Neural Depth Fields (NeDF) that quickly determines the spatial relationship\nbetween objects by allowing direct intersection computation between rays and\nimplicit surfaces. It leverages an intersection neural network to query NeRF\nfor acceleration instead of depending on an explicit spatial structure.Our\nproposed method is the first to enable both the progressive and interactive\ncomposition of NeRF objects. Additionally, it also serves as a previewing\nplugin for a range of existing NeRF works.\n","authors":["Xinyu Gao","Ziyi Yang","Yunlu Zhao","Yuxiang Sun","Xiaogang Jin","Changqing Zou"],"pdf_url":"https://arxiv.org/pdf/2308.04669v3.pdf","comment":"7 pages for main content"},{"id":"http://arxiv.org/abs/2312.07145v1","updated":"2023-12-12T10:28:51Z","published":"2023-12-12T10:28:51Z","title":"Contextual Bandits with Online Neural Regression","summary":"  Recent works have shown a reduction from contextual bandits to online\nregression under a realizability assumption [Foster and Rakhlin, 2020, Foster\nand Krishnamurthy, 2021]. In this work, we investigate the use of neural\nnetworks for such online regression and associated Neural Contextual Bandits\n(NeuCBs). Using existing results for wide networks, one can readily show a\n${\\mathcal{O}}(\\sqrt{T})$ regret for online regression with square loss, which\nvia the reduction implies a ${\\mathcal{O}}(\\sqrt{K} T^{3/4})$ regret for\nNeuCBs. Departing from this standard approach, we first show a\n$\\mathcal{O}(\\log T)$ regret for online regression with almost convex losses\nthat satisfy QG (Quadratic Growth) condition, a generalization of the PL\n(Polyak-\\L ojasiewicz) condition, and that have a unique minima. Although not\ndirectly applicable to wide networks since they do not have unique minima, we\nshow that adding a suitable small random perturbation to the network\npredictions surprisingly makes the loss satisfy QG with unique minima. Based on\nsuch a perturbed prediction, we show a ${\\mathcal{O}}(\\log T)$ regret for\nonline regression with both squared loss and KL loss, and subsequently convert\nthese respectively to $\\tilde{\\mathcal{O}}(\\sqrt{KT})$ and\n$\\tilde{\\mathcal{O}}(\\sqrt{KL^*} + K)$ regret for NeuCB, where $L^*$ is the\nloss of the best policy. Separately, we also show that existing regret bounds\nfor NeuCBs are $\\Omega(T)$ or assume i.i.d. contexts, unlike this work.\nFinally, our experimental results on various datasets demonstrate that our\nalgorithms, especially the one based on KL loss, persistently outperform\nexisting algorithms.\n","authors":["Rohan Deb","Yikun Ban","Shiliang Zuo","Jingrui He","Arindam Banerjee"],"pdf_url":"https://arxiv.org/pdf/2312.07145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07142v1","updated":"2023-12-12T10:24:32Z","published":"2023-12-12T10:24:32Z","title":"General Tail Bounds for Non-Smooth Stochastic Mirror Descent","summary":"  In this paper, we provide novel tail bounds on the optimization error of\nStochastic Mirror Descent for convex and Lipschitz objectives. Our analysis\nextends the existing tail bounds from the classical light-tailed Sub-Gaussian\nnoise case to heavier-tailed noise regimes. We study the optimization error of\nthe last iterate as well as the average of the iterates. We instantiate our\nresults in two important cases: a class of noise with exponential tails and one\nwith polynomial tails. A remarkable feature of our results is that they do not\nrequire an upper bound on the diameter of the domain. Finally, we support our\ntheory with illustrative experiments that compare the behavior of the average\nof the iterates with that of the last iterate in heavy-tailed noise regimes.\n","authors":["Khaled Eldowa","Andrea Paudice"],"pdf_url":"https://arxiv.org/pdf/2312.07142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05924v2","updated":"2023-12-12T10:21:12Z","published":"2023-12-10T16:14:02Z","title":"Data-Free Hard-Label Robustness Stealing Attack","summary":"  The popularity of Machine Learning as a Service (MLaaS) has led to increased\nconcerns about Model Stealing Attacks (MSA), which aim to craft a clone model\nby querying MLaaS. Currently, most research on MSA assumes that MLaaS can\nprovide soft labels and that the attacker has a proxy dataset with a similar\ndistribution. However, this fails to encapsulate the more practical scenario\nwhere only hard labels are returned by MLaaS and the data distribution remains\nelusive. Furthermore, most existing work focuses solely on stealing the model\naccuracy, neglecting the model robustness, while robustness is essential in\nsecurity-sensitive scenarios, e.g., face-scan payment. Notably, improving model\nrobustness often necessitates the use of expensive techniques such as\nadversarial training, thereby further making stealing robustness a more\nlucrative prospect. In response to these identified gaps, we introduce a novel\nData-Free Hard-Label Robustness Stealing (DFHL-RS) attack in this paper, which\nenables the stealing of both model accuracy and robustness by simply querying\nhard labels of the target model without the help of any natural data.\nComprehensive experiments demonstrate the effectiveness of our method. The\nclone model achieves a clean accuracy of 77.86% and a robust accuracy of 39.51%\nagainst AutoAttack, which are only 4.71% and 8.40% lower than the target model\non the CIFAR-10 dataset, significantly exceeding the baselines. Our code is\navailable at: https://github.com/LetheSec/DFHL-RS-Attack.\n","authors":["Xiaojian Yuan","Kejiang Chen","Wen Huang","Jie Zhang","Weiming Zhang","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2312.05924v2.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2306.06871v3","updated":"2023-12-12T10:19:11Z","published":"2023-06-12T05:10:10Z","title":"Improving Offline-to-Online Reinforcement Learning with Q-Ensembles","summary":"  Offline reinforcement learning (RL) is a learning paradigm where an agent\nlearns from a fixed dataset of experience. However, learning solely from a\nstatic dataset can limit the performance due to the lack of exploration. To\novercome it, offline-to-online RL combines offline pre-training with online\nfine-tuning, which enables the agent to further refine its policy by\ninteracting with the environment in real-time. Despite its benefits, existing\noffline-to-online RL methods suffer from performance degradation and slow\nimprovement during the online phase. To tackle these challenges, we propose a\nnovel framework called Ensemble-based Offline-to-Online (E2O) RL. By increasing\nthe number of Q-networks, we seamlessly bridge offline pre-training and online\nfine-tuning without degrading performance. Moreover, to expedite online\nperformance enhancement, we appropriately loosen the pessimism of Q-value\nestimation and incorporate ensemble-based exploration mechanisms into our\nframework. Experimental results demonstrate that E2O can substantially improve\nthe training stability, learning efficiency, and final performance of existing\noffline RL methods during online fine-tuning on a range of locomotion and\nnavigation tasks, significantly outperforming existing offline-to-online RL\nmethods.\n","authors":["Kai Zhao","Yi Ma","Jianye Hao","Jinyi Liu","Yan Zheng","Zhaopeng Meng"],"pdf_url":"https://arxiv.org/pdf/2306.06871v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07133v1","updated":"2023-12-12T10:07:37Z","published":"2023-12-12T10:07:37Z","title":"Text2AC-Zero: Consistent Synthesis of Animated Characters using 2D\n  Diffusion","summary":"  We propose a zero-shot approach for consistent Text-to-Animated-Characters\nsynthesis based on pre-trained Text-to-Image (T2I) diffusion models. Existing\nText-to-Video (T2V) methods are expensive to train and require large-scale\nvideo datasets to produce diverse characters and motions. At the same time,\ntheir zero-shot alternatives fail to produce temporally consistent videos. We\nstrive to bridge this gap, and we introduce a zero-shot approach that produces\ntemporally consistent videos of animated characters and requires no training or\nfine-tuning. We leverage existing text-based motion diffusion models to\ngenerate diverse motions that we utilize to guide a T2I model. To achieve\ntemporal consistency, we introduce the Spatial Latent Alignment module that\nexploits cross-frame dense correspondences that we compute to align the latents\nof the video frames. Furthermore, we propose Pixel-Wise Guidance to steer the\ndiffusion process in a direction that minimizes visual discrepancies. Our\nproposed approach generates temporally consistent videos with diverse motions\nand styles, outperforming existing zero-shot T2V approaches in terms of\npixel-wise consistency and user preference.\n","authors":["Abdelrahman Eldesokey","Peter Wonka"],"pdf_url":"https://arxiv.org/pdf/2312.07133v1.pdf","comment":"Project page: https://abdo-eldesokey.github.io/text2ac-zero/"},{"id":"http://arxiv.org/abs/2208.01711v3","updated":"2023-12-12T10:06:41Z","published":"2022-08-02T19:47:43Z","title":"Optimal Rates for Regularized Conditional Mean Embedding Learning","summary":"  We address the consistency of a kernel ridge regression estimate of the\nconditional mean embedding (CME), which is an embedding of the conditional\ndistribution of $Y$ given $X$ into a target reproducing kernel Hilbert space\n$\\mathcal{H}_Y$. The CME allows us to take conditional expectations of target\nRKHS functions, and has been employed in nonparametric causal and Bayesian\ninference. We address the misspecified setting, where the target CME is in the\nspace of Hilbert-Schmidt operators acting from an input interpolation space\nbetween $\\mathcal{H}_X$ and $L_2$, to $\\mathcal{H}_Y$. This space of operators\nis shown to be isomorphic to a newly defined vector-valued interpolation space.\nUsing this isomorphism, we derive a novel and adaptive statistical learning\nrate for the empirical CME estimator under the misspecified setting. Our\nanalysis reveals that our rates match the optimal $O(\\log n / n)$ rates without\nassuming $\\mathcal{H}_Y$ to be finite dimensional. We further establish a lower\nbound on the learning rate, which shows that the obtained upper bound is\noptimal.\n","authors":["Zhu Li","Dimitri Meunier","Mattes Mollenhauer","Arthur Gretton"],"pdf_url":"https://arxiv.org/pdf/2208.01711v3.pdf","comment":"Typos & revised argument for the Gaussian kernel. Results unchanged"},{"id":"http://arxiv.org/abs/2311.18718v2","updated":"2023-12-12T09:56:23Z","published":"2023-11-30T17:19:18Z","title":"Steering Deep Feature Learning with Backward Aligned Feature Updates","summary":"  Deep learning succeeds by doing hierarchical feature learning, yet tuning\nHyper-Parameters (HP) such as initialization scales, learning rates etc., only\ngive indirect control over this behavior. In this paper, we propose the\nalignment between the feature updates and the backward pass as a key notion to\npredict, measure and control feature learning. On the one hand, we show that\nwhen alignment holds, the magnitude of feature updates after one SGD step is\nrelated to the magnitude of the forward and backward passes by a simple and\ngeneral formula. This leads to techniques to automatically adjust HPs\n(initialization scales and learning rates) at initialization and throughout\ntraining to attain a desired feature learning behavior. On the other hand, we\nshow that, at random initialization, this alignment is determined by the\nspectrum of a certain kernel, and that well-conditioned layer-to-layer\nJacobians (aka dynamical isometry) implies alignment. Finally, we investigate\nReLU MLPs and ResNets in the large width-then-depth limit. Combining hints from\nrandom matrix theory and numerical experiments, we show that (i) in MLP with\niid initializations, alignment degenerates with depth, making it impossible to\nstart training, and that (ii) in ResNets, the branch scale\n$1/\\sqrt{\\text{depth}}$ is the only one maintaining non-trivial alignment at\ninfinite depth.\n","authors":["Lénaïc Chizat","Praneeth Netrapalli"],"pdf_url":"https://arxiv.org/pdf/2311.18718v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04673v3","updated":"2023-12-12T09:52:54Z","published":"2023-11-08T13:29:08Z","title":"Compressive Recovery of Sparse Precision Matrices","summary":"  We consider the problem of learning a graph modeling the statistical\nrelations of the $d$ variables from a dataset with $n$ samples $X \\in\n\\mathbb{R}^{n \\times d}$. Standard approaches amount to searching for a\nprecision matrix $\\Theta$ representative of a Gaussian graphical model that\nadequately explains the data. However, most maximum likelihood-based estimators\nusually require storing the $d^{2}$ values of the empirical covariance matrix,\nwhich can become prohibitive in a high-dimensional setting. In this work, we\nadopt a compressive viewpoint and aim to estimate a sparse $\\Theta$ from a\n\\emph{sketch} of the data, i.e. a low-dimensional vector of size $m \\ll d^{2}$\ncarefully designed from $X$ using non-linear random features. Under certain\nassumptions on the spectrum of $\\Theta$ (or its condition number), we show that\nit is possible to estimate it from a sketch of size\n$m=\\Omega\\left((d+2k)\\log(d)\\right)$ where $k$ is the maximal number of edges\nof the underlying graph. These information-theoretic guarantees are inspired by\ncompressed sensing theory and involve restricted isometry properties and\ninstance optimal decoders. We investigate the possibility of achieving\npractical recovery with an iterative algorithm based on the graphical lasso,\nviewed as a specific denoiser. We compare our approach and graphical lasso on\nsynthetic datasets, demonstrating its favorable performance even when the\ndataset is compressed.\n","authors":["Titouan Vayer","Etienne Lasalle","Rémi Gribonval","Paulo Gonçalves"],"pdf_url":"https://arxiv.org/pdf/2311.04673v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.14660v5","updated":"2023-12-12T09:45:51Z","published":"2023-04-28T07:23:31Z","title":"Segment Anything Model for Medical Images?","summary":"  The Segment Anything Model (SAM) is the first foundation model for general\nimage segmentation. It has achieved impressive results on various natural image\nsegmentation tasks. However, medical image segmentation (MIS) is more\nchallenging because of the complex modalities, fine anatomical structures,\nuncertain and complex object boundaries, and wide-range object scales. To fully\nvalidate SAM's performance on medical data, we collected and sorted 53\nopen-source datasets and built a large medical segmentation dataset with 18\nmodalities, 84 objects, 125 object-modality paired targets, 1050K 2D images,\nand 6033K masks. We comprehensively analyzed different models and strategies on\nthe so-called COSMOS 1050K dataset. Our findings mainly include the following:\n1) SAM showed remarkable performance in some specific objects but was unstable,\nimperfect, or even totally failed in other situations. 2) SAM with the large\nViT-H showed better overall performance than that with the small ViT-B. 3) SAM\nperformed better with manual hints, especially box, than the Everything mode.\n4) SAM could help human annotation with high labeling quality and less time. 5)\nSAM was sensitive to the randomness in the center point and tight box prompts,\nand may suffer from a serious performance drop. 6) SAM performed better than\ninteractive methods with one or a few points, but will be outpaced as the\nnumber of points increases. 7) SAM's performance correlated to different\nfactors, including boundary complexity, intensity differences, etc. 8)\nFinetuning the SAM on specific medical tasks could improve its average DICE\nperformance by 4.39% and 6.68% for ViT-B and ViT-H, respectively. We hope that\nthis comprehensive report can help researchers explore the potential of SAM\napplications in MIS, and guide how to appropriately use and develop SAM.\n","authors":["Yuhao Huang","Xin Yang","Lian Liu","Han Zhou","Ao Chang","Xinrui Zhou","Rusi Chen","Junxuan Yu","Jiongquan Chen","Chaoyu Chen","Sijing Liu","Haozhe Chi","Xindi Hu","Kejuan Yue","Lei Li","Vicente Grau","Deng-Ping Fan","Fajin Dong","Dong Ni"],"pdf_url":"https://arxiv.org/pdf/2304.14660v5.pdf","comment":"Accepted by Medical Image Analysis. 23 pages, 18 figures, 8 tables"},{"id":"http://arxiv.org/abs/2312.07112v1","updated":"2023-12-12T09:39:52Z","published":"2023-12-12T09:39:52Z","title":"Generating High-Resolution Regional Precipitation Using Conditional\n  Diffusion Model","summary":"  Climate downscaling is a crucial technique within climate research, serving\nto project low-resolution (LR) climate data to higher resolutions (HR).\nPrevious research has demonstrated the effectiveness of deep learning for\ndownscaling tasks. However, most deep learning models for climate downscaling\nmay not perform optimally for high scaling factors (i.e., 4x, 8x) due to their\nlimited ability to capture the intricate details required for generating HR\nclimate data. Furthermore, climate data behaves differently from image data,\nnecessitating a nuanced approach when employing deep generative models. In\nresponse to these challenges, this paper presents a deep generative model for\ndownscaling climate data, specifically precipitation on a regional scale. We\nemploy a denoising diffusion probabilistic model (DDPM) conditioned on multiple\nLR climate variables. The proposed model is evaluated using precipitation data\nfrom the Community Earth System Model (CESM) v1.2.2 simulation. Our results\ndemonstrate significant improvements over existing baselines, underscoring the\neffectiveness of the conditional diffusion model in downscaling climate data.\n","authors":["Naufal Shidqi","Chaeyoon Jeong","Sungwon Park","Elke Zeller","Arjun Babu Nellikkattil","Karandeep Singh"],"pdf_url":"https://arxiv.org/pdf/2312.07112v1.pdf","comment":"5 pages, the 9th Joint Conference of Korean Artificial Intelligence\n  Association, KAIA 2023"},{"id":"http://arxiv.org/abs/2312.07110v1","updated":"2023-12-12T09:39:03Z","published":"2023-12-12T09:39:03Z","title":"LLMs Perform Poorly at Concept Extraction in Cyber-security Research\n  Literature","summary":"  The cybersecurity landscape evolves rapidly and poses threats to\norganizations. To enhance resilience, one needs to track the latest\ndevelopments and trends in the domain. It has been demonstrated that standard\nbibliometrics approaches show their limits in such a fast-evolving domain. For\nthis purpose, we use large language models (LLMs) to extract relevant knowledge\nentities from cybersecurity-related texts. We use a subset of arXiv preprints\non cybersecurity as our data and compare different LLMs in terms of entity\nrecognition (ER) and relevance. The results suggest that LLMs do not produce\ngood knowledge entities that reflect the cybersecurity context, but our results\nshow some potential for noun extractors. For this reason, we developed a noun\nextractor boosted with some statistical analysis to extract specific and\nrelevant compound nouns from the domain. Later, we tested our model to identify\ntrends in the LLM domain. We observe some limitations, but it offers promising\nresults to monitor the evolution of emergent trends.\n","authors":["Maxime Würsch","Andrei Kucharavy","Dimitri Percia David","Alain Mermoud"],"pdf_url":"https://arxiv.org/pdf/2312.07110v1.pdf","comment":"24 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.01092v2","updated":"2023-12-12T09:37:12Z","published":"2023-03-02T09:26:20Z","title":"ArCL: Enhancing Contrastive Learning with Augmentation-Robust\n  Representations","summary":"  Self-Supervised Learning (SSL) is a paradigm that leverages unlabeled data\nfor model training. Empirical studies show that SSL can achieve promising\nperformance in distribution shift scenarios, where the downstream and training\ndistributions differ. However, the theoretical understanding of its\ntransferability remains limited. In this paper, we develop a theoretical\nframework to analyze the transferability of self-supervised contrastive\nlearning, by investigating the impact of data augmentation on it. Our results\nreveal that the downstream performance of contrastive learning depends largely\non the choice of data augmentation. Moreover, we show that contrastive learning\nfails to learn domain-invariant features, which limits its transferability.\nBased on these theoretical insights, we propose a novel method called\nAugmentation-robust Contrastive Learning (ArCL), which guarantees to learn\ndomain-invariant features and can be easily integrated with existing\ncontrastive learning algorithms. We conduct experiments on several datasets and\nshow that ArCL significantly improves the transferability of contrastive\nlearning.\n","authors":["Xuyang Zhao","Tianqi Du","Yisen Wang","Jun Yao","Weiran Huang"],"pdf_url":"https://arxiv.org/pdf/2303.01092v2.pdf","comment":"Accepted by ICLR 2023"},{"id":"http://arxiv.org/abs/2312.07103v1","updated":"2023-12-12T09:33:03Z","published":"2023-12-12T09:33:03Z","title":"The Computational Complexity of Concise Hypersphere Classification","summary":"  Hypersphere classification is a classical and foundational method that can\nprovide easy-to-process explanations for the classification of real-valued and\nbinary data. However, obtaining an (ideally concise) explanation via\nhypersphere classification is much more difficult when dealing with binary data\nthan real-valued data. In this paper, we perform the first complexity-theoretic\nstudy of the hypersphere classification problem for binary data. We use the\nfine-grained parameterized complexity paradigm to analyze the impact of\nstructural properties that may be present in the input data as well as\npotential conciseness constraints. Our results include stronger lower bounds\nand new fixed-parameter algorithms for hypersphere classification of binary\ndata, which can find an exact and concise explanation when one exists.\n","authors":["Eduard Eiben","Robert Ganian","Iyad Kanj","Sebastian Ordyniak","Stefan Szeider"],"pdf_url":"https://arxiv.org/pdf/2312.07103v1.pdf","comment":"Short version appeared at ICML 2023"},{"id":"http://arxiv.org/abs/1902.10664v6","updated":"2023-12-12T09:24:32Z","published":"2019-02-27T17:55:06Z","title":"Local Function Complexity for Active Learning via Mixture of Gaussian\n  Processes","summary":"  Inhomogeneities in real-world data, e.g., due to changes in the observation\nnoise level or variations in the structural complexity of the source function,\npose a unique set of challenges for statistical inference. Accounting for them\ncan greatly improve predictive power when physical resources or computation\ntime is limited. In this paper, we draw on recent theoretical results on the\nestimation of local function complexity (LFC), derived from the domain of local\npolynomial smoothing (LPS), to establish a notion of local structural\ncomplexity, which is used to develop a model-agnostic active learning (AL)\nframework. Due to its reliance on pointwise estimates, the LPS model class is\nnot robust and scalable concerning large input space dimensions that typically\ncome along with real-world problems. Here, we derive and estimate the Gaussian\nprocess regression (GPR)-based analog of the LPS-based LFC and use it as a\nsubstitute in the above framework to make it robust and scalable. We assess the\neffectiveness of our LFC estimate in an AL application on a prototypical\nlow-dimensional synthetic dataset, before taking on the challenging real-world\ntask of reconstructing a quantum chemical force field for a small organic\nmolecule and demonstrating state-of-the-art performance with a significantly\nreduced training demand.\n","authors":["Danny Panknin","Stefan Chmiela","Klaus-Robert Müller","Shinichi Nakajima"],"pdf_url":"https://arxiv.org/pdf/1902.10664v6.pdf","comment":"30 pages (+18 pages of references and appendices), 20 figures"},{"id":"http://arxiv.org/abs/2312.07087v1","updated":"2023-12-12T09:09:45Z","published":"2023-12-12T09:09:45Z","title":"Toward Robustness in Multi-label Classification: A Data Augmentation\n  Strategy against Imbalance and Noise","summary":"  Multi-label classification poses challenges due to imbalanced and noisy\nlabels in training data. We propose a unified data augmentation method, named\nBalanceMix, to address these challenges. Our approach includes two samplers for\nimbalanced labels, generating minority-augmented instances with high diversity.\nIt also refines multi-labels at the label-wise granularity, categorizing noisy\nlabels as clean, re-labeled, or ambiguous for robust optimization. Extensive\nexperiments on three benchmark datasets demonstrate that BalanceMix outperforms\nexisting state-of-the-art methods. We release the code at\nhttps://github.com/DISL-Lab/BalanceMix.\n","authors":["Hwanjun Song","Minseok Kim","Jae-Gil Lee"],"pdf_url":"https://arxiv.org/pdf/2312.07087v1.pdf","comment":"This paper was accepted at AAAI 2024. We upload the full version of\n  our paper on arXiv due to the page limit of AAAI"},{"id":"http://arxiv.org/abs/2210.06434v4","updated":"2023-12-12T09:05:34Z","published":"2022-10-12T17:33:47Z","title":"Cross-client Label Propagation for Transductive and Semi-Supervised\n  Federated Learning","summary":"  We present Cross-Client Label Propagation(XCLP), a new method for\ntransductive federated learning. XCLP estimates a data graph jointly from the\ndata of multiple clients and computes labels for the unlabeled data by\npropagating label information across the graph. To avoid clients having to\nshare their data with anyone, XCLP employs two cryptographically secure\nprotocols: secure Hamming distance computation and secure summation. We\ndemonstrate two distinct applications of XCLP within federated learning. In the\nfirst, we use it in a one-shot way to predict labels for unseen test points. In\nthe second, we use it to repeatedly pseudo-label unlabeled training data in a\nfederated semi-supervised setting. Experiments on both real federated and\nstandard benchmark datasets show that in both applications XCLP achieves higher\nclassification accuracy than alternative approaches.\n","authors":["Jonathan Scott","Michelle Yeo","Christoph H. Lampert"],"pdf_url":"https://arxiv.org/pdf/2210.06434v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.16164v2","updated":"2023-12-12T08:48:57Z","published":"2023-07-30T08:18:39Z","title":"Adaptive learning of density ratios in RKHS","summary":"  Estimating the ratio of two probability densities from finitely many\nobservations of the densities is a central problem in machine learning and\nstatistics with applications in two-sample testing, divergence estimation,\ngenerative modeling, covariate shift adaptation, conditional density\nestimation, and novelty detection. In this work, we analyze a large class of\ndensity ratio estimation methods that minimize a regularized Bregman divergence\nbetween the true density ratio and a model in a reproducing kernel Hilbert\nspace (RKHS). We derive new finite-sample error bounds, and we propose a\nLepskii type parameter choice principle that minimizes the bounds without\nknowledge of the regularity of the density ratio. In the special case of\nquadratic loss, our method adaptively achieves a minimax optimal error rate. A\nnumerical illustration is provided.\n","authors":["Werner Zellinger","Stefan Kindermann","Sergei V. Pereverzyev"],"pdf_url":"https://arxiv.org/pdf/2307.16164v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07069v1","updated":"2023-12-12T08:43:20Z","published":"2023-12-12T08:43:20Z","title":"Context Matter: Data-Efficient Augmentation of Large Language Models for\n  Scientific Applications","summary":"  In this paper, we explore the challenges inherent to Large Language Models\n(LLMs) like GPT-4, particularly their propensity for hallucinations, logic\nmistakes, and incorrect conclusions when tasked with answering complex\nquestions. The capacity of LLMs to present erroneous answers in a coherent and\nsemantically rigorous manner further complicates the detection of factual\ninaccuracies. This issue is especially pronounced in fields that require\nspecialized expertise. Our work delves into these challenges, aiming to enhance\nthe understanding and mitigation of such errors, thereby contributing to the\nimprovement of LLM accuracy and reliability in scientific and other specialized\ndomains. Our findings reveal a non-linear relationship between the context's\nrelevancy and the answers' measured quality. In addition, we demonstrate that\nwith the correct calibration, it is possible to automate the grading procedure\n-- a finding suggesting that, at least to some degree, the LLMs can be used to\nself-examine the quality of their own performance. Finally, we describe an\nexperimental platform that can be seen as a proof-of-concept of the techniques\ndescribed in this work.\n","authors":["Xiang Li","Haoran Tang","Siyu Chen","Ziwei Wang","Anurag Maravi","Marcin Abram"],"pdf_url":"https://arxiv.org/pdf/2312.07069v1.pdf","comment":"11 pages, 6 figures, 4 tables, 3 pages of supplementary material"},{"id":"http://arxiv.org/abs/2312.07067v1","updated":"2023-12-12T08:41:18Z","published":"2023-12-12T08:41:18Z","title":"Focus on Hiders: Exploring Hidden Threats for Enhancing Adversarial\n  Training","summary":"  Adversarial training is often formulated as a min-max problem, however,\nconcentrating only on the worst adversarial examples causes alternating\nrepetitive confusion of the model, i.e., previously defended or correctly\nclassified samples are not defensible or accurately classifiable in subsequent\nadversarial training. We characterize such non-ignorable samples as \"hiders\",\nwhich reveal the hidden high-risk regions within the secure area obtained\nthrough adversarial training and prevent the model from finding the real worst\ncases. We demand the model to prevent hiders when defending against adversarial\nexamples for improving accuracy and robustness simultaneously. By rethinking\nand redefining the min-max optimization problem for adversarial training, we\npropose a generalized adversarial training algorithm called Hider-Focused\nAdversarial Training (HFAT). HFAT introduces the iterative evolution\noptimization strategy to simplify the optimization problem and employs an\nauxiliary model to reveal hiders, effectively combining the optimization\ndirections of standard adversarial training and prevention hiders. Furthermore,\nwe introduce an adaptive weighting mechanism that facilitates the model in\nadaptively adjusting its focus between adversarial examples and hiders during\ndifferent training periods. We demonstrate the effectiveness of our method\nbased on extensive experiments, and ensure that HFAT can provide higher\nrobustness and accuracy.\n","authors":["Qian Li","Yuxiao Hu","Yinpeng Dong","Dongxiao Zhang","Yuntian Chen"],"pdf_url":"https://arxiv.org/pdf/2312.07067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.02904v3","updated":"2023-12-12T08:40:56Z","published":"2023-02-06T16:18:48Z","title":"Rethinking Gauss-Newton for learning over-parameterized models","summary":"  This work studies the global convergence and implicit bias of Gauss Newton's\n(GN) when optimizing over-parameterized one-hidden layer networks in the\nmean-field regime. We first establish a global convergence result for GN in the\ncontinuous-time limit exhibiting a faster convergence rate compared to GD due\nto improved conditioning. We then perform an empirical study on a synthetic\nregression task to investigate the implicit bias of GN's method. While GN is\nconsistently faster than GD in finding a global optimum, the learned model\ngeneralizes well on test data when starting from random initial weights with a\nsmall variance and using a small step size to slow down convergence.\nSpecifically, our study shows that such a setting results in a hidden learning\nphenomenon, where the dynamics are able to recover features with good\ngeneralization properties despite the model having sub-optimal training and\ntest performances due to an under-optimized linear layer. This study exhibits a\ntrade-off between the convergence speed of GN and the generalization ability of\nthe learned solution.\n","authors":["Michael Arbel","Romain Menegaux","Pierre Wolinski"],"pdf_url":"https://arxiv.org/pdf/2302.02904v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07064v1","updated":"2023-12-12T08:33:34Z","published":"2023-12-12T08:33:34Z","title":"Efficient Cross-Domain Federated Learning by MixStyle Approximation","summary":"  With the advent of interconnected and sensor-equipped edge devices, Federated\nLearning (FL) has gained significant attention, enabling decentralized learning\nwhile maintaining data privacy. However, FL faces two challenges in real-world\ntasks: expensive data labeling and domain shift between source and target\nsamples. In this paper, we introduce a privacy-preserving, resource-efficient\nFL concept for client adaptation in hardware-constrained environments. Our\napproach includes server model pre-training on source data and subsequent\nfine-tuning on target data via low-end clients. The local client adaptation\nprocess is streamlined by probabilistic mixing of instance-level feature\nstatistics approximated from source and target domain data. The adapted\nparameters are transferred back to the central server and globally aggregated.\nPreliminary results indicate that our method reduces computational and\ntransmission costs while maintaining competitive performance on downstream\ntasks.\n","authors":["Manuel Röder","Leon Heller","Maximilian Münch","Frank-Michael Schleif"],"pdf_url":"https://arxiv.org/pdf/2312.07064v1.pdf","comment":"Accepted at the Adapting to Change: Reliable Multimodal Learning\n  Across Domains Workshop @ ECML PKKD 2023"},{"id":"http://arxiv.org/abs/2304.01168v4","updated":"2023-12-12T08:13:09Z","published":"2023-04-03T17:37:00Z","title":"DeepAccident: A Motion and Accident Prediction Benchmark for V2X\n  Autonomous Driving","summary":"  Safety is the primary priority of autonomous driving. Nevertheless, no\npublished dataset currently supports the direct and explainable safety\nevaluation for autonomous driving. In this work, we propose DeepAccident, a\nlarge-scale dataset generated via a realistic simulator containing diverse\naccident scenarios that frequently occur in real-world driving. The proposed\nDeepAccident dataset includes 57K annotated frames and 285K annotated samples,\napproximately 7 times more than the large-scale nuScenes dataset with 40k\nannotated samples. In addition, we propose a new task, end-to-end motion and\naccident prediction, which can be used to directly evaluate the accident\nprediction ability for different autonomous driving algorithms. Furthermore,\nfor each scenario, we set four vehicles along with one infrastructure to record\ndata, thus providing diverse viewpoints for accident scenarios and enabling V2X\n(vehicle-to-everything) research on perception and prediction tasks. Finally,\nwe present a baseline V2X model named V2XFormer that demonstrates superior\nperformance for motion and accident prediction and 3D object detection compared\nto the single-vehicle model.\n","authors":["Tianqi Wang","Sukmin Kim","Wenxuan Ji","Enze Xie","Chongjian Ge","Junsong Chen","Zhenguo Li","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2304.01168v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06564v2","updated":"2023-12-12T08:09:34Z","published":"2023-12-11T17:49:25Z","title":"Promoting Counterfactual Robustness through Diversity","summary":"  Counterfactual explanations shed light on the decisions of black-box models\nby explaining how an input can be altered to obtain a favourable decision from\nthe model (e.g., when a loan application has been rejected). However, as noted\nrecently, counterfactual explainers may lack robustness in the sense that a\nminor change in the input can cause a major change in the explanation. This can\ncause confusion on the user side and open the door for adversarial attacks. In\nthis paper, we study some sources of non-robustness. While there are\nfundamental reasons for why an explainer that returns a single counterfactual\ncannot be robust in all instances, we show that some interesting robustness\nguarantees can be given by reporting multiple rather than a single\ncounterfactual. Unfortunately, the number of counterfactuals that need to be\nreported for the theoretical guarantees to hold can be prohibitively large. We\ntherefore propose an approximation algorithm that uses a diversity criterion to\nselect a feasible number of most relevant explanations and study its robustness\nempirically. Our experiments indicate that our method improves the\nstate-of-the-art in generating robust explanations, while maintaining other\ndesirable properties and providing competitive computational performance.\n","authors":["Francesco Leofante","Nico Potyka"],"pdf_url":"https://arxiv.org/pdf/2312.06564v2.pdf","comment":"Accepted at AAAI 2024"},{"id":"http://arxiv.org/abs/2309.03648v3","updated":"2023-12-12T08:07:57Z","published":"2023-09-07T11:29:16Z","title":"Promoting Fairness in GNNs: A Characterization of Stability","summary":"  The Lipschitz bound, a technique from robust statistics, can limit the\nmaximum changes in the output concerning the input, taking into account\nassociated irrelevant biased factors. It is an efficient and provable method\nfor examining the output stability of machine learning models without incurring\nadditional computation costs. Recently, Graph Neural Networks (GNNs), which\noperate on non-Euclidean data, have gained significant attention. However, no\nprevious research has investigated the GNN Lipschitz bounds to shed light on\nstabilizing model outputs, especially when working on non-Euclidean data with\ninherent biases. Given the inherent biases in common graph data used for GNN\ntraining, it poses a serious challenge to constraining the GNN output\nperturbations induced by input biases, thereby safeguarding fairness during\ntraining. Recently, despite the Lipschitz constant's use in controlling the\nstability of Euclideanneural networks, the calculation of the precise Lipschitz\nconstant remains elusive for non-Euclidean neural networks like GNNs,\nespecially within fairness contexts. To narrow this gap, we begin with the\ngeneral GNNs operating on an attributed graph, and formulate a Lipschitz bound\nto limit the changes in the output regarding biases associated with the input.\nAdditionally, we theoretically analyze how the Lipschitz constant of a GNN\nmodel could constrain the output perturbations induced by biases learned from\ndata for fairness training. We experimentally validate the Lipschitz bound's\neffectiveness in limiting biases of the model output. Finally, from a training\ndynamics perspective, we demonstrate why the theoretical Lipschitz bound can\neffectively guide the GNN training to better trade-off between accuracy and\nfairness.\n","authors":["Yaning Jia","Chunhui Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.03648v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07046v1","updated":"2023-12-12T07:56:57Z","published":"2023-12-12T07:56:57Z","title":"Rethinking Compression: Reduced Order Modelling of Latent Features in\n  Large Language Models","summary":"  Due to the substantial scale of Large Language Models (LLMs), the direct\napplication of conventional compression methodologies proves impractical. The\ncomputational demands associated with even minimal gradient updates present\nchallenges, particularly on consumer-grade hardware. This paper introduces an\ninnovative approach for the parametric and practical compression of LLMs based\non reduced order modelling, which entails low-rank decomposition within the\nfeature space and re-parameterization in the weight space. Notably, this\ncompression technique operates in a layer-wise manner, obviating the need for a\nGPU device and enabling the compression of billion-scale models within\nstringent constraints of both memory and time. Our method represents a\nsignificant advancement in model compression by leveraging matrix\ndecomposition, demonstrating superior efficacy compared to the prevailing\nstate-of-the-art structured pruning method.\n","authors":["Arnav Chavan","Nahush Lele","Deepak Gupta"],"pdf_url":"https://arxiv.org/pdf/2312.07046v1.pdf","comment":"Brief technical report; Code will be made available at\n  https://github.com/transmuteAI/trailmet/tree/main/trailmet/algorithms/llm-rom"},{"id":"http://arxiv.org/abs/2312.07044v1","updated":"2023-12-12T07:55:48Z","published":"2023-12-12T07:55:48Z","title":"Large Foundation Models for Power Systems","summary":"  Foundation models, such as Large Language Models (LLMs), can respond to a\nwide range of format-free queries without any task-specific data collection or\nmodel training, creating various research and application opportunities for the\nmodeling and operation of large-scale power systems. In this paper, we outline\nhow such large foundation model such as GPT-4 are developed, and discuss how\nthey can be leveraged in challenging power and energy system tasks. We first\ninvestigate the potential of existing foundation models by validating their\nperformance on four representative tasks across power system domains, including\nthe optimal power flow (OPF), electric vehicle (EV) scheduling, knowledge\nretrieval for power engineering technical reports, and situation awareness. Our\nresults indicate strong capabilities of such foundation models on boosting the\nefficiency and reliability of power system operational pipelines. We also\nprovide suggestions and projections on future deployment of foundation models\nin power system applications.\n","authors":["Chenghao Huang","Siyang Li","Ruohong Liu","Hao Wang","Yize Chen"],"pdf_url":"https://arxiv.org/pdf/2312.07044v1.pdf","comment":"Code available at https://github.com/chennnnnyize/LLM_PowerSystems"},{"id":"http://arxiv.org/abs/2309.14585v2","updated":"2023-12-12T07:48:52Z","published":"2023-09-26T00:15:13Z","title":"DifAttack: Query-Efficient Black-Box Attack via Disentangled Feature\n  Space","summary":"  This work investigates efficient score-based black-box adversarial attacks\nwith a high Attack Success Rate (ASR) and good generalizability. We design a\nnovel attack method based on a Disentangled Feature space, called DifAttack,\nwhich differs significantly from the existing ones operating over the entire\nfeature space. Specifically, DifAttack firstly disentangles an image's latent\nfeature into an adversarial feature and a visual feature, where the former\ndominates the adversarial capability of an image, while the latter largely\ndetermines its visual appearance. We train an autoencoder for the\ndisentanglement by using pairs of clean images and their Adversarial Examples\n(AEs) generated from available surrogate models via white-box attack methods.\nEventually, DifAttack iteratively optimizes the adversarial feature according\nto the query feedback from the victim model until a successful AE is generated,\nwhile keeping the visual feature unaltered. In addition, due to the avoidance\nof using surrogate models' gradient information when optimizing AEs for\nblack-box models, our proposed DifAttack inherently possesses better attack\ncapability in the open-set scenario, where the training dataset of the victim\nmodel is unknown. Extensive experimental results demonstrate that our method\nachieves significant improvements in ASR and query efficiency simultaneously,\nespecially in the targeted attack and open-set scenarios. The code will be\navailable at https://github.com/csjunjun/DifAttack.git soon.\n","authors":["Liu Jun","Zhou Jiantao","Zeng Jiandian","Jinyu Tian"],"pdf_url":"https://arxiv.org/pdf/2309.14585v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03998v2","updated":"2023-12-12T07:48:11Z","published":"2023-12-07T02:30:40Z","title":"Series2Vec: Similarity-based Self-supervised Representation Learning for\n  Time Series Classification","summary":"  We argue that time series analysis is fundamentally different in nature to\neither vision or natural language processing with respect to the forms of\nmeaningful self-supervised learning tasks that can be defined. Motivated by\nthis insight, we introduce a novel approach called \\textit{Series2Vec} for\nself-supervised representation learning. Unlike other self-supervised methods\nin time series, which carry the risk of positive sample variants being less\nsimilar to the anchor sample than series in the negative set, Series2Vec is\ntrained to predict the similarity between two series in both temporal and\nspectral domains through a self-supervised task. Series2Vec relies primarily on\nthe consistency of the unsupervised similarity step, rather than the intrinsic\nquality of the similarity measurement, without the need for hand-crafted data\naugmentation. To further enforce the network to learn similar representations\nfor similar time series, we propose a novel approach that applies\norder-invariant attention to each representation within the batch during\ntraining. Our evaluation of Series2Vec on nine large real-world datasets, along\nwith the UCR/UEA archive, shows enhanced performance compared to current\nstate-of-the-art self-supervised techniques for time series. Additionally, our\nextensive experiments show that Series2Vec performs comparably with fully\nsupervised training and offers high efficiency in datasets with limited-labeled\ndata. Finally, we show that the fusion of Series2Vec with other representation\nlearning models leads to enhanced performance for time series classification.\nCode and models are open-source at\n\\url{https://github.com/Navidfoumani/Series2Vec.}\n","authors":["Navid Mohammadi Foumani","Chang Wei Tan","Geoffrey I. Webb","Hamid Rezatofighi","Mahsa Salehi"],"pdf_url":"https://arxiv.org/pdf/2312.03998v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07035v1","updated":"2023-12-12T07:40:23Z","published":"2023-12-12T07:40:23Z","title":"HyperRouter: Towards Efficient Training and Inference of Sparse Mixture\n  of Experts","summary":"  By routing input tokens to only a few split experts, Sparse\nMixture-of-Experts has enabled efficient training of large language models.\nRecent findings suggest that fixing the routers can achieve competitive\nperformance by alleviating the collapsing problem, where all experts eventually\nlearn similar representations. However, this strategy has two key limitations:\n(i) the policy derived from random routers might be sub-optimal, and (ii) it\nrequires extensive resources during training and evaluation, leading to limited\nefficiency gains. This work introduces \\HyperRout, which dynamically generates\nthe router's parameters through a fixed hypernetwork and trainable embeddings\nto achieve a balance between training the routers and freezing them to learn an\nimproved routing policy. Extensive experiments across a wide range of tasks\ndemonstrate the superior performance and efficiency gains of \\HyperRouter\ncompared to existing routing methods. Our implementation is publicly available\nat {\\url{{https://github.com/giangdip2410/HyperRouter}}}.\n","authors":["Giang Do","Khiem Le","Quang Pham","TrungTin Nguyen","Thanh-Nam Doan","Bint T. Nguyen","Chenghao Liu","Savitha Ramasamy","Xiaoli Li","Steven Hoi"],"pdf_url":"https://arxiv.org/pdf/2312.07035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07032v1","updated":"2023-12-12T07:35:27Z","published":"2023-12-12T07:35:27Z","title":"Ahpatron: A New Budgeted Online Kernel Learning Machine with Tighter\n  Mistake Bound","summary":"  In this paper, we study the mistake bound of online kernel learning on a\nbudget. We propose a new budgeted online kernel learning model, called\nAhpatron, which significantly improves the mistake bound of previous work and\nresolves the open problem posed by Dekel, Shalev-Shwartz, and Singer (2005). We\nfirst present an aggressive variant of Perceptron, named AVP, a model without\nbudget, which uses an active updating rule. Then we design a new budget\nmaintenance mechanism, which removes a half of examples,and projects the\nremoved examples onto a hypothesis space spanned by the remaining examples.\nAhpatron adopts the above mechanism to approximate AVP. Theoretical analyses\nprove that Ahpatron has tighter mistake bounds, and experimental results show\nthat Ahpatron outperforms the state-of-the-art algorithms on the same or a\nsmaller budget.\n","authors":["Yun Liao","Junfan Li","Shizhong Liao","Qinghua Hu","Jianwu Dang"],"pdf_url":"https://arxiv.org/pdf/2312.07032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07022v1","updated":"2023-12-12T07:15:28Z","published":"2023-12-12T07:15:28Z","title":"EdgePruner: Poisoned Edge Pruning in Graph Contrastive Learning","summary":"  Graph Contrastive Learning (GCL) is unsupervised graph representation\nlearning that can obtain useful representation of unknown nodes. The node\nrepresentation can be utilized as features of downstream tasks. However, GCL is\nvulnerable to poisoning attacks as with existing learning models. A\nstate-of-the-art defense cannot sufficiently negate adverse effects by poisoned\ngraphs although such a defense introduces adversarial training in the GCL. To\nachieve further improvement, pruning adversarial edges is important. To the\nbest of our knowledge, the feasibility remains unexplored in the GCL domain. In\nthis paper, we propose a simple defense for GCL, EdgePruner. We focus on the\nfact that the state-of-the-art poisoning attack on GCL tends to mainly add\nadversarial edges to create poisoned graphs, which means that pruning edges is\nimportant to sanitize the graphs. Thus, EdgePruner prunes edges that contribute\nto minimizing the contrastive loss based on the node representation obtained\nafter training on poisoned graphs by GCL. Furthermore, we focus on the fact\nthat nodes with distinct features are connected by adversarial edges in\npoisoned graphs. Thus, we introduce feature similarity between neighboring\nnodes to help more appropriately determine adversarial edges. This similarity\nis helpful in further eliminating adverse effects from poisoned graphs on\nvarious datasets. Finally, EdgePruner outputs a graph that yields the minimum\ncontrastive loss as the sanitized graph. Our results demonstrate that pruning\nadversarial edges is feasible on six datasets. EdgePruner can improve the\naccuracy of node classification under the attack by up to 5.55% compared with\nthat of the state-of-the-art defense. Moreover, we show that EdgePruner is\nimmune to an adaptive attack.\n","authors":["Hiroya Kato","Kento Hasegawa","Seira Hidano","Kazuhide Fukushima"],"pdf_url":"https://arxiv.org/pdf/2312.07022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06363v2","updated":"2023-12-12T06:53:27Z","published":"2023-12-11T13:11:04Z","title":"MMICT: Boosting Multi-Modal Fine-Tuning with In-Context Examples","summary":"  Although In-Context Learning (ICL) brings remarkable performance gains to\nLarge Language Models (LLMs), the improvements remain lower than fine-tuning on\ndownstream tasks. This paper introduces Multi-Modal In-Context Tuning (MMICT),\na novel multi-modal fine-tuning paradigm that boosts multi-modal fine-tuning by\nfully leveraging the promising ICL capability of multi-modal LLMs (MM-LLMs). We\npropose the Multi-Modal Hub (M-Hub), a unified module that captures various\nmulti-modal features according to different inputs and objectives. Based on\nM-Hub, MMICT enables MM-LLMs to learn from in-context visual-guided textual\nfeatures and subsequently generate outputs conditioned on the textual-guided\nvisual features. Moreover, leveraging the flexibility of M-Hub, we design a\nvariety of in-context demonstrations. Extensive experiments on a diverse range\nof downstream multi-modal tasks demonstrate that MMICT significantly\noutperforms traditional fine-tuning strategy and the vanilla ICT method that\ndirectly takes the concatenation of all information from different modalities\nas input.\n","authors":["Tao Chen","Enwei Zhang","Yuting Gao","Ke Li","Xing Sun","Yan Zhang","Hui Li"],"pdf_url":"https://arxiv.org/pdf/2312.06363v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04344v2","updated":"2023-12-12T06:37:53Z","published":"2023-12-07T15:05:59Z","title":"Enhancing Medical Task Performance in GPT-4V: A Comprehensive Study on\n  Prompt Engineering Strategies","summary":"  OpenAI's latest large vision-language model (LVLM), GPT-4V(ision), has piqued\nconsiderable interest for its potential in medical applications. Despite its\npromise, recent studies and internal reviews highlight its underperformance in\nspecialized medical tasks. This paper explores the boundary of GPT-4V's\ncapabilities in medicine, particularly in processing complex imaging data from\nendoscopies, CT scans, and MRIs etc. Leveraging open-source datasets, we\nassessed its foundational competencies, identifying substantial areas for\nenhancement. Our research emphasizes prompt engineering, an often-underutilized\nstrategy for improving AI responsiveness. Through iterative testing, we refined\nthe model's prompts, significantly improving its interpretative accuracy and\nrelevance in medical imaging. From our comprehensive evaluations, we distilled\n10 effective prompt engineering techniques, each fortifying GPT-4V's medical\nacumen. These methodical enhancements facilitate more reliable, precise, and\nclinically valuable insights from GPT-4V, advancing its operability in critical\nhealthcare environments. Our findings are pivotal for those employing AI in\nmedicine, providing clear, actionable guidance on harnessing GPT-4V's full\ndiagnostic potential.\n","authors":["Pengcheng Chen","Ziyan Huang","Zhongying Deng","Tianbin Li","Yanzhou Su","Haoyu Wang","Jin Ye","Yu Qiao","Junjun He"],"pdf_url":"https://arxiv.org/pdf/2312.04344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05496v2","updated":"2023-12-12T06:23:53Z","published":"2023-12-09T07:51:01Z","title":"Flexible Cross-Modal Steganography via Implicit Representations","summary":"  We present INRSteg, an innovative lossless steganography framework based on a\nnovel data form Implicit Neural Representations (INR) that is modal-agnostic.\nOur framework is considered for effectively hiding multiple data without\naltering the original INR ensuring high-quality stego data. The neural\nrepresentations of secret data are first concatenated to have independent paths\nthat do not overlap, then weight freezing techniques are applied to the\ndiagonal blocks of the weight matrices for the concatenated network to preserve\nthe weights of secret data while additional free weights in the off-diagonal\nblocks of weight matrices are fitted to the cover data. Our framework can\nperform unexplored cross-modal steganography for various modalities including\nimage, audio, video, and 3D shapes, and it achieves state-of-the-art\nperformance compared to previous intra-modal steganographic methods.\n","authors":["Seoyun Yang","Sojeong Song","Chang D. Yoo","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2312.05496v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07003v1","updated":"2023-12-12T06:21:30Z","published":"2023-12-12T06:21:30Z","title":"RACER: Rational Artificial Intelligence Car-following-model Enhanced by\n  Reality","summary":"  This paper introduces RACER, the Rational Artificial Intelligence\nCar-following model Enhanced by Reality, a cutting-edge deep learning\ncar-following model, that satisfies partial derivative constraints, designed to\npredict Adaptive Cruise Control (ACC) driving behavior while staying\ntheoretically feasible. Unlike conventional models, RACER effectively\nintegrates Rational Driving Constraints (RDCs), crucial tenets of actual\ndriving, resulting in strikingly accurate and realistic predictions. Against\nestablished models like the Optimal Velocity Relative Velocity (OVRV), a\ncar-following Neural Network (NN), and a car-following Physics-Informed Neural\nNetwork (PINN), RACER excels across key metrics, such as acceleration,\nvelocity, and spacing. Notably, it displays a perfect adherence to the RDCs,\nregistering zero violations, in stark contrast to other models. This study\nhighlights the immense value of incorporating physical constraints within AI\nmodels, especially for augmenting safety measures in transportation. It also\npaves the way for future research to test these models against human driving\ndata, with the potential to guide safer and more rational driving behavior. The\nversatility of the proposed model, including its potential to incorporate\nadditional derivative constraints and broader architectural applications,\nenhances its appeal and broadens its impact within the scientific community.\n","authors":["Tianyi Li","Alexander Halatsis","Raphael Stern"],"pdf_url":"https://arxiv.org/pdf/2312.07003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.19797v2","updated":"2023-12-12T06:17:27Z","published":"2023-10-30T17:59:35Z","title":"DEFT: Dexterous Fine-Tuning for Real-World Hand Policies","summary":"  Dexterity is often seen as a cornerstone of complex manipulation. Humans are\nable to perform a host of skills with their hands, from making food to\noperating tools. In this paper, we investigate these challenges, especially in\nthe case of soft, deformable objects as well as complex, relatively\nlong-horizon tasks. However, learning such behaviors from scratch can be data\ninefficient. To circumvent this, we propose a novel approach, DEFT (DExterous\nFine-Tuning for Hand Policies), that leverages human-driven priors, which are\nexecuted directly in the real world. In order to improve upon these priors,\nDEFT involves an efficient online optimization procedure. With the integration\nof human-based learning and online fine-tuning, coupled with a soft robotic\nhand, DEFT demonstrates success across various tasks, establishing a robust,\ndata-efficient pathway toward general dexterous manipulation. Please see our\nwebsite at https://dexterous-finetuning.github.io for video results.\n","authors":["Aditya Kannan","Kenneth Shaw","Shikhar Bahl","Pragna Mannam","Deepak Pathak"],"pdf_url":"https://arxiv.org/pdf/2310.19797v2.pdf","comment":"In CoRL 2023. Website at https://dexterous-finetuning.github.io/"},{"id":"http://arxiv.org/abs/2312.06635v2","updated":"2023-12-12T06:04:14Z","published":"2023-12-11T18:51:59Z","title":"Gated Linear Attention Transformers with Hardware-Efficient Training","summary":"  Transformers with linear attention allow for efficient parallel training but\ncan simultaneously be formulated as an RNN with 2D (matrix-valued) hidden\nstates, thus enjoying linear (with respect to output length) inference\ncomplexity. Recent works such as RetNet (Sun et al., 2023) and TransNormerLLM\n(Qin et al., 2023a) observe that adding a global decay term to the additive RNN\nupdate rule greatly improves performance, sometimes outperforming standard\nTransformers with softmax attention when trained at scale. In this work we show\nthat adding a data-dependent gating mechanism further improves performance. We\nderive a parallel form of this gated linear attention layer that enables\nefficient training. However, a straightforward, numerically stable\nimplementation of this parallel form requires generalized matrix\nmultiplications in log-space for numerical stability, and thus cannot take\nadvantage of tensor cores on modern GPUs which are optimized for standard\nmatrix multiplications. We develop a hardware-efficient version of the parallel\nform that can still make use of tensor cores through block-parallel\ncomputations over sequence chunks. Experiments on moderate-scale language\nmodeling (340M-parameter models trained on 15B tokens, 1.3B-parameter models\ntrained on 100B tokens) show that gated linear attention (GLA) Transformers\nperform competitively against a strong LLaMA-architecture Transformer baseline\n(Touvron et al., 2023) as well as Mamba (Gu & Dao, 2023), a recently introduced\nstate-space model with a data-dependent state transition mechanism. For\ntraining speed, our Triton-based implementation performs comparably to\nCUDA-optimized FlashAttention-2 (Dao, 2023) under the regular 2048 training\nlength setting, while outperforming FlashAttention-2 when training on longer\nsequences beyond 4096.\n","authors":["Songlin Yang","Bailin Wang","Yikang Shen","Rameswar Panda","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2312.06635v2.pdf","comment":"fix code link"},{"id":"http://arxiv.org/abs/2305.15546v2","updated":"2023-12-12T05:40:30Z","published":"2023-05-24T20:22:43Z","title":"Regret-Optimal Model-Free Reinforcement Learning for Discounted MDPs\n  with Short Burn-In Time","summary":"  A crucial problem in reinforcement learning is learning the optimal policy.\nWe study this in tabular infinite-horizon discounted Markov decision processes\nunder the online setting. The existing algorithms either fail to achieve regret\noptimality or have to incur a high memory and computational cost. In addition,\nexisting optimal algorithms all require a long burn-in time in order to achieve\noptimal sample efficiency, i.e., their optimality is not guaranteed unless\nsample size surpasses a high threshold. We address both open problems by\nintroducing a model-free algorithm that employs variance reduction and a novel\ntechnique that switches the execution policy in a slow-yet-adaptive manner.\nThis is the first regret-optimal model-free algorithm in the discounted\nsetting, with the additional benefit of a low burn-in time.\n","authors":["Xiang Ji","Gen Li"],"pdf_url":"https://arxiv.org/pdf/2305.15546v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06993v1","updated":"2023-12-12T05:35:30Z","published":"2023-12-12T05:35:30Z","title":"Dynamically configured physics-informed neural network in topology\n  optimization applications","summary":"  Integration of machine learning (ML) into the topology optimization (TO)\nframework is attracting increasing attention, but data acquisition in\ndata-driven models is prohibitive. Compared with popular ML methods, the\nphysics-informed neural network (PINN) can avoid generating enormous amounts of\ndata when solving forward problems and additionally provide better inference.\nTo this end, a dynamically configured PINN-based topology optimization\n(DCPINN-TO) method is proposed. The DCPINN is composed of two subnetworks,\nnamely the backbone neural network (NN) and the coefficient NN, where the\ncoefficient NN has fewer trainable parameters. The designed architecture aims\nto dynamically configure trainable parameters; that is, an inexpensive NN is\nused to replace an expensive one at certain optimization cycles. Furthermore,\nan active sampling strategy is proposed to selectively sample collocations\ndepending on the pseudo-densities at each optimization cycle. In this manner,\nthe number of collocations will decrease with the optimization process but will\nhardly affect it. The Gaussian integral is used to calculate the strain energy\nof elements, which yields a byproduct of decoupling the mapping of the material\nat the collocations. Several examples with different resolutions validate the\nfeasibility of the DCPINN-TO method, and multiload and multiconstraint problems\nare employed to illustrate its generalization. In addition, compared to finite\nelement analysis-based TO (FEA-TO), the accuracy of the displacement prediction\nand optimization results indicate that the DCPINN-TO method is effective and\nefficient.\n","authors":["Jichao Yin","Ziming Wen","Shuhao Li","Yaya Zhanga","Hu Wang"],"pdf_url":"https://arxiv.org/pdf/2312.06993v1.pdf","comment":"31 pages, 22 figures"},{"id":"http://arxiv.org/abs/2308.09604v2","updated":"2023-12-12T05:28:51Z","published":"2023-08-18T14:57:21Z","title":"Faster Stochastic Variance Reduction Methods for Compositional MiniMax\n  Optimization","summary":"  This paper delves into the realm of stochastic optimization for compositional\nminimax optimization - a pivotal challenge across various machine learning\ndomains, including deep AUC and reinforcement learning policy evaluation.\nDespite its significance, the problem of compositional minimax optimization is\nstill under-explored. Adding to the complexity, current methods of\ncompositional minimax optimization are plagued by sub-optimal complexities or\nheavy reliance on sizable batch sizes. To respond to these constraints, this\npaper introduces a novel method, called Nested STOchastic Recursive Momentum\n(NSTORM), which can achieve the optimal sample complexity of $O(\\kappa^3\n/\\epsilon^3 )$ to obtain the $\\epsilon$-accuracy solution. We also demonstrate\nthat NSTORM can achieve the same sample complexity under the Polyak-\\L\nojasiewicz (PL)-condition - an insightful extension of its capabilities. Yet,\nNSTORM encounters an issue with its requirement for low learning rates,\npotentially constraining its real-world applicability in machine learning. To\novercome this hurdle, we present ADAptive NSTORM (ADA-NSTORM) with adaptive\nlearning rates. We demonstrate that ADA-NSTORM can achieve the same sample\ncomplexity but the experimental results show its more effectiveness. All the\nproposed complexities indicate that our proposed methods can match lower bounds\nto existing minimax optimizations, without requiring a large batch size in each\niteration. Extensive experiments support the efficiency of our proposed\nmethods.\n","authors":["Jin Liu","Xiaokang Pan","Junwen Duan","Hongdong Li","Youqi Li","Zhe Qu"],"pdf_url":"https://arxiv.org/pdf/2308.09604v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05725v2","updated":"2023-12-12T05:21:40Z","published":"2023-12-10T02:14:34Z","title":"FP8-BERT: Post-Training Quantization for Transformer","summary":"  Transformer-based models, such as BERT, have been widely applied in a wide\nrange of natural language processing tasks. However, one inevitable side effect\nis that they require massive memory storage and inference cost when deployed in\nproduction. Quantization is one of the popularized ways to alleviate the cost.\nHowever, the previous 8-bit quantization strategy based on INT8 data format\neither suffers from the degradation of accuracy in a Post-Training Quantization\n(PTQ) fashion or requires an expensive Quantization-Aware Training (QAT)\nprocess. Recently, a new numeric format FP8 (i.e. floating-point of 8-bits) has\nbeen proposed and supported in commercial AI computing platforms such as H100.\nIn this paper, we empirically validate the effectiveness of FP8 as a way to do\nPost-Training Quantization without significant loss of accuracy, with a simple\ncalibration and format conversion process. We adopt the FP8 standard proposed\nby NVIDIA Corp. (2022) in our extensive experiments of BERT variants on GLUE\nand SQuAD v1.1 datasets, and show that PTQ with FP8 can significantly improve\nthe accuracy upon that with INT8, to the extent of the full-precision model.\n","authors":["Jianwei Li","Tianchi Zhang","Ian En-Hsu Yen","Dongkuan Xu"],"pdf_url":"https://arxiv.org/pdf/2312.05725v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.12496v4","updated":"2023-12-12T05:18:50Z","published":"2022-10-22T17:01:05Z","title":"Bayesian Optimization with Conformal Prediction Sets","summary":"  Bayesian optimization is a coherent, ubiquitous approach to decision-making\nunder uncertainty, with applications including multi-arm bandits, active\nlearning, and black-box optimization. Bayesian optimization selects decisions\n(i.e. objective function queries) with maximal expected utility with respect to\nthe posterior distribution of a Bayesian model, which quantifies reducible,\nepistemic uncertainty about query outcomes. In practice, subjectively\nimplausible outcomes can occur regularly for two reasons: 1) model\nmisspecification and 2) covariate shift. Conformal prediction is an uncertainty\nquantification method with coverage guarantees even for misspecified models and\na simple mechanism to correct for covariate shift. We propose conformal\nBayesian optimization, which directs queries towards regions of search space\nwhere the model predictions have guaranteed validity, and investigate its\nbehavior on a suite of black-box optimization tasks and tabular ranking tasks.\nIn many cases we find that query coverage can be significantly improved without\nharming sample-efficiency.\n","authors":["Samuel Stanton","Wesley Maddox","Andrew Gordon Wilson"],"pdf_url":"https://arxiv.org/pdf/2210.12496v4.pdf","comment":"For code, see\n  https://www.github.com/samuelstanton/conformal-bayesopt.git"},{"id":"http://arxiv.org/abs/2305.20009v2","updated":"2023-12-12T05:09:38Z","published":"2023-05-31T16:31:24Z","title":"Protein Design with Guided Discrete Diffusion","summary":"  A popular approach to protein design is to combine a generative model with a\ndiscriminative model for conditional sampling. The generative model samples\nplausible sequences while the discriminative model guides a search for\nsequences with high fitness. Given its broad success in conditional sampling,\nclassifier-guided diffusion modeling is a promising foundation for protein\ndesign, leading many to develop guided diffusion models for structure with\ninverse folding to recover sequences. In this work, we propose diffusioN\nOptimized Sampling (NOS), a guidance method for discrete diffusion models that\nfollows gradients in the hidden states of the denoising network. NOS makes it\npossible to perform design directly in sequence space, circumventing\nsignificant limitations of structure-based methods, including scarce data and\nchallenging inverse design. Moreover, we use NOS to generalize LaMBO, a\nBayesian optimization procedure for sequence design that facilitates multiple\nobjectives and edit-based constraints. The resulting method, LaMBO-2, enables\ndiscrete diffusions and stronger performance with limited edits through a novel\napplication of saliency maps. We apply LaMBO-2 to a real-world protein design\ntask, optimizing antibodies for higher expression yield and binding affinity to\nseveral therapeutic targets under locality and developability constraints,\nattaining a 99% expression rate and 40% binding rate in exploratory in vitro\nexperiments.\n","authors":["Nate Gruver","Samuel Stanton","Nathan C. Frey","Tim G. J. Rudner","Isidro Hotzel","Julien Lafrance-Vanasse","Arvind Rajpal","Kyunghyun Cho","Andrew Gordon Wilson"],"pdf_url":"https://arxiv.org/pdf/2305.20009v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16808v2","updated":"2023-12-12T05:02:59Z","published":"2023-09-28T19:30:26Z","title":"Granularity at Scale: Estimating Neighborhood Socioeconomic Indicators\n  from High-Resolution Orthographic Imagery and Hybrid Learning","summary":"  Many areas of the world are without basic information on the socioeconomic\nwell-being of the residing population due to limitations in existing data\ncollection methods. Overhead images obtained remotely, such as from satellite\nor aircraft, can help serve as windows into the state of life on the ground and\nhelp \"fill in the gaps\" where community information is sparse, with estimates\nat smaller geographic scales requiring higher resolution sensors. Concurrent\nwith improved sensor resolutions, recent advancements in machine learning and\ncomputer vision have made it possible to quickly extract features from and\ndetect patterns in image data, in the process correlating these features with\nother information. In this work, we explore how well two approaches, a\nsupervised convolutional neural network and semi-supervised clustering based on\nbag-of-visual-words, estimate population density, median household income, and\neducational attainment of individual neighborhoods from publicly available\nhigh-resolution imagery of cities throughout the United States. Results and\nanalyses indicate that features extracted from the imagery can accurately\nestimate the density (R$^2$ up to 0.81) of neighborhoods, with the supervised\napproach able to explain about half the variation in a population's income and\neducation. In addition to the presented approaches serving as a basis for\nfurther geographic generalization, the novel semi-supervised approach provides\na foundation for future work seeking to estimate fine-scale information from\naerial imagery without the need for label data.\n","authors":["Ethan Brewer","Giovani Valdrighi","Parikshit Solunke","Joao Rulff","Yurii Piadyk","Zhonghui Lv","Jorge Poco","Claudio Silva"],"pdf_url":"https://arxiv.org/pdf/2309.16808v2.pdf","comment":"Updating after a round of revisions with IEEE J-STARS"},{"id":"http://arxiv.org/abs/2312.00258v2","updated":"2023-12-12T04:56:24Z","published":"2023-11-29T11:35:50Z","title":"Precipitation Nowcasting With Spatial And Temporal Transfer Learning\n  Using Swin-UNETR","summary":"  Climate change has led to an increase in frequency of extreme weather events.\nEarly warning systems can prevent disasters and loss of life. Managing such\nevents remain a challenge for both public and private institutions.\nPrecipitation nowcasting can help relevant institutions to better prepare for\nsuch events. Numerical weather prediction (NWP) has traditionally been used to\nmake physics based forecasting, and recently deep learning based approaches\nhave been used to reduce turn-around time for nowcasting. In this work,\nrecently proposed Swin-UNETR (Swin UNEt TRansformer) is used for precipitation\nnowcasting for ten different regions of Europe. Swin-UNETR utilizes a U-shaped\nnetwork within which a swin transformer-based encoder extracts multi-scale\nfeatures from multiple input channels of satellite image, while CNN-based\ndecoder makes the prediction. Trained model is capable of nowcasting not only\nfor the regions for which data is available, but can also be used for new\nregions for which data is not available.\n","authors":["Ajitabh Kumar"],"pdf_url":"https://arxiv.org/pdf/2312.00258v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2311.17961;\n  NeurIPS 2023"},{"id":"http://arxiv.org/abs/2312.06979v1","updated":"2023-12-12T04:41:20Z","published":"2023-12-12T04:41:20Z","title":"On the notion of Hallucinations from the lens of Bias and Validity in\n  Synthetic CXR Images","summary":"  Medical imaging has revolutionized disease diagnosis, yet the potential is\nhampered by limited access to diverse and privacy-conscious datasets.\nOpen-source medical datasets, while valuable, suffer from data quality and\nclinical information disparities. Generative models, such as diffusion models,\naim to mitigate these challenges. At Stanford, researchers explored the utility\nof a fine-tuned Stable Diffusion model (RoentGen) for medical imaging data\naugmentation. Our work examines specific considerations to expand the Stanford\nresearch question, Could Stable Diffusion Solve a Gap in Medical Imaging Data?\nfrom the lens of bias and validity of the generated outcomes. We leveraged\nRoentGen to produce synthetic Chest-XRay (CXR) images and conducted assessments\non bias, validity, and hallucinations. Diagnostic accuracy was evaluated by a\ndisease classifier, while a COVID classifier uncovered latent hallucinations.\nThe bias analysis unveiled disparities in classification performance among\nvarious subgroups, with a pronounced impact on the Female Hispanic subgroup.\nFurthermore, incorporating race and gender into input prompts exacerbated\nfairness issues in the generated images. The quality of synthetic images\nexhibited variability, particularly in certain disease classes, where there was\nmore significant uncertainty compared to the original images. Additionally, we\nobserved latent hallucinations, with approximately 42% of the images\nincorrectly indicating COVID, hinting at the presence of hallucinatory\nelements. These identifications provide new research directions towards\ninterpretability of synthetic CXR images, for further understanding of\nassociated risks and patient safety in medical applications.\n","authors":["Gauri Bhardwaj","Yuvaraj Govindarajulu","Sundaraparipurnan Narayanan","Pavan Kulkarni","Manojkumar Parmar"],"pdf_url":"https://arxiv.org/pdf/2312.06979v1.pdf","comment":"Accepted at 37th Conference on Neural Information Processing Systems\n  (NeurIPS 2023) - \"Medical Imaging Meets NeurIPS\" Workshop"},{"id":"http://arxiv.org/abs/2312.06973v1","updated":"2023-12-12T04:24:05Z","published":"2023-12-12T04:24:05Z","title":"Anytime Approximate Formal Feature Attribution","summary":"  Widespread use of artificial intelligence (AI) algorithms and machine\nlearning (ML) models on the one hand and a number of crucial issues pertaining\nto them warrant the need for explainable artificial intelligence (XAI). A key\nexplainability question is: given this decision was made, what are the input\nfeatures which contributed to the decision? Although a range of XAI approaches\nexist to tackle this problem, most of them have significant limitations.\nHeuristic XAI approaches suffer from the lack of quality guarantees, and often\ntry to approximate Shapley values, which is not the same as explaining which\nfeatures contribute to a decision. A recent alternative is so-called formal\nfeature attribution (FFA), which defines feature importance as the fraction of\nformal abductive explanations (AXp's) containing the given feature. This\nmeasures feature importance from the view of formally reasoning about the\nmodel's behavior. It is challenging to compute FFA using its definition because\nthat involves counting AXp's, although one can approximate it. Based on these\nresults, this paper makes several contributions. First, it gives compelling\nevidence that computing FFA is intractable, even if the set of contrastive\nformal explanations (CXp's) is provided, by proving that the problem is\n#P-hard. Second, by using the duality between AXp's and CXp's, it proposes an\nefficient heuristic to switch from CXp enumeration to AXp enumeration\non-the-fly resulting in an adaptive explanation enumeration algorithm\neffectively approximating FFA in an anytime fashion. Finally, experimental\nresults obtained on a range of widely used datasets demonstrate the\neffectiveness of the proposed FFA approximation approach in terms of the error\nof FFA approximation as well as the number of explanations computed and their\ndiversity given a fixed time limit.\n","authors":["Jinqiang Yu","Graham Farr","Alexey Ignatiev","Peter J. Stuckey"],"pdf_url":"https://arxiv.org/pdf/2312.06973v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05978v2","updated":"2023-12-12T04:14:51Z","published":"2023-12-10T19:42:18Z","title":"Neural Architecture Codesign for Fast Bragg Peak Analysis","summary":"  We develop an automated pipeline to streamline neural architecture codesign\nfor fast, real-time Bragg peak analysis in high-energy diffraction microscopy.\nTraditional approaches, notably pseudo-Voigt fitting, demand significant\ncomputational resources, prompting interest in deep learning models for more\nefficient solutions. Our method employs neural architecture search and AutoML\nto enhance these models, including hardware costs, leading to the discovery of\nmore hardware-efficient neural architectures. Our results match the\nperformance, while achieving a 13$\\times$ reduction in bit operations compared\nto the previous state-of-the-art. We show further speedup through model\ncompression techniques such as quantization-aware-training and neural network\npruning. Additionally, our hierarchical search space provides greater\nflexibility in optimization, which can easily extend to other tasks and\ndomains.\n","authors":["Luke McDermott","Jason Weitz","Dmitri Demler","Daniel Cummings","Nhan Tran","Javier Duarte"],"pdf_url":"https://arxiv.org/pdf/2312.05978v2.pdf","comment":"To appear in 3rd Annual AAAI Workshop on AI to Accelerate Science and\n  Engineering (AI2ASE)"},{"id":"http://arxiv.org/abs/2310.17087v2","updated":"2023-12-12T04:06:02Z","published":"2023-10-26T01:11:17Z","title":"Good regularity creates large learning rate implicit biases: edge of\n  stability, balancing, and catapult","summary":"  Large learning rates, when applied to gradient descent for nonconvex\noptimization, yield various implicit biases including the edge of stability\n(Cohen et al., 2021), balancing (Wang et al., 2022), and catapult (Lewkowycz et\nal., 2020). These phenomena cannot be well explained by classical optimization\ntheory. Though significant theoretical progress has been made in understanding\nthese implicit biases, it remains unclear for which objective functions would\nthey be more likely. This paper provides an initial step in answering this\nquestion and also shows that these implicit biases are in fact various tips of\nthe same iceberg. To establish these results, we develop a global convergence\ntheory under large learning rates, for a family of nonconvex functions without\nglobally Lipschitz continuous gradient, which was typically assumed in existing\nconvergence analysis. Specifically, these phenomena are more likely to occur\nwhen the optimization objective function has good regularity. This regularity,\ntogether with gradient descent using a large learning rate that favors flatter\nregions, results in these nontrivial dynamical behaviors. Another corollary is\nthe first non-asymptotic convergence rate bound for large-learning-rate\ngradient descent optimization of nonconvex functions. Although our theory only\napplies to specific functions so far, the possibility of extrapolating it to\nneural networks is also experimentally validated, for which different choices\nof loss, activation functions, and other techniques such as batch normalization\ncan all affect regularity significantly and lead to very different training\ndynamics.\n","authors":["Yuqing Wang","Zhenghao Xu","Tuo Zhao","Molei Tao"],"pdf_url":"https://arxiv.org/pdf/2310.17087v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.09895v4","updated":"2023-12-12T04:00:31Z","published":"2023-08-19T03:19:01Z","title":"Knowledge Transfer from High-Resource to Low-Resource Programming\n  Languages for Code LLMs","summary":"  Over the past few years, Large Language Models of Code (Code LLMs) have\nstarted to have a significant impact on programming practice. Code LLMs are\nalso emerging as building blocks for research in programming languages and\nsoftware engineering. However, Code LLMs produce impressive results on\nprogramming languages that are well represented in their training data (e.g.,\nJava, Python, or JavaScript), but struggle with low-resource languages that\nhave limited training data available. Low resource languages include OCaml,\nRacket, and several others.\n  This paper presents an effective approach for boosting the performance of\nCode LLMs on low-resource languages using semi-synthetic data. Our approach,\nMultiPL-T, translates training data from high-resource languages into training\ndata for low-resource languages in the following way. 1) We use a Code LLM to\nsynthesize tests for commented code from a high-resource language, filtering\nout faulty tests and code with low test coverage. 2) We use a Code LLM to\ntranslate Python code to a target low-resource language, and use tests to\nvalidate the translation. We apply this approach to generate tens of thousands\nof validated training items for Julia, Lua, OCaml, R, and Racket. Furthermore,\nwe use an open model (StarCoderBase) with open training data (The Stack), which\nallows us to decontaminate benchmarks, train models without violating licenses,\nand run experiments that could not otherwise be done.\n  With MultiPL-T generated data, we present fine-tuned versions of\nStarCoderBase and Code Llama for Julia, Lua, OCaml, R, and Racket. On\nestablished benchmarks (MultiPL-E), these models outperform other open Code\nLLMs. The MultiPL-T approach is easy to apply to new languages, and is\nsignificantly more efficient and effective than alternatives such as training\nlonger.\n","authors":["Federico Cassano","John Gouwar","Francesca Lucchetti","Claire Schlesinger","Carolyn Jane Anderson","Michael Greenberg","Abhinav Jangda","Arjun Guha"],"pdf_url":"https://arxiv.org/pdf/2308.09895v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05743v2","updated":"2023-12-12T03:56:33Z","published":"2023-12-10T03:46:01Z","title":"Building Variable-sized Models via Learngene Pool","summary":"  Recently, Stitchable Neural Networks (SN-Net) is proposed to stitch some\npre-trained networks for quickly building numerous networks with different\ncomplexity and performance trade-offs. In this way, the burdens of designing or\ntraining the variable-sized networks, which can be used in application\nscenarios with diverse resource constraints, are alleviated. However, SN-Net\nstill faces a few challenges. 1) Stitching from multiple independently\npre-trained anchors introduces high storage resource consumption. 2) SN-Net\nfaces challenges to build smaller models for low resource constraints. 3).\nSN-Net uses an unlearned initialization method for stitch layers, limiting the\nfinal performance. To overcome these challenges, motivated by the recently\nproposed Learngene framework, we propose a novel method called Learngene Pool.\nBriefly, Learngene distills the critical knowledge from a large pre-trained\nmodel into a small part (termed as learngene) and then expands this small part\ninto a few variable-sized models. In our proposed method, we distill one\npretrained large model into multiple small models whose network blocks are used\nas learngene instances to construct the learngene pool. Since only one large\nmodel is used, we do not need to store more large models as SN-Net and after\ndistilling, smaller learngene instances can be created to build small models to\nsatisfy low resource constraints. We also insert learnable transformation\nmatrices between the instances to stitch them into variable-sized models to\nimprove the performance of these models. Exhaustive experiments have been\nimplemented and the results validate the effectiveness of the proposed\nLearngene Pool compared with SN-Net.\n","authors":["Boyu Shi","Shiyu Xia","Xu Yang","Haokun Chen","Zhiqiang Kou","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2312.05743v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.13108v3","updated":"2023-12-12T03:52:00Z","published":"2022-11-23T16:49:26Z","title":"Integral Continual Learning Along the Tangent Vector Field of Tasks","summary":"  We propose a lightweight continual learning method which incorporates\ninformation from specialized datasets incrementally, by integrating it along\nthe vector field of \"generalist\" models. The tangent plane to the specialist\nmodel acts as a generalist guide and avoids the kind of over-fitting that leads\nto catastrophic forgetting, while exploiting the convexity of the optimization\nlandscape in the tangent plane. It maintains a small fixed-size memory buffer,\nas low as 0.4% of the source datasets, which is updated by simple resampling.\nOur method achieves strong performance across various buffer sizes for\ndifferent datasets. Specifically, in the class-incremental setting we\noutperform the existing methods that do not require distillation by an average\nof 18.77% and 28.48%, for Seq-CIFAR-10 and Seq-TinyImageNet respectively. Our\nmethod can easily be used in conjunction with existing replay-based continual\nlearning methods. When memory buffer constraints are relaxed to allow storage\nof metadata such as logits, we attain an error reduction of 17.84% towards the\nparagon performance on Seq-CIFAR-10.\n","authors":["Tian Yu Liu","Aditya Golatkar","Stefano Soatto","Alessandro Achille"],"pdf_url":"https://arxiv.org/pdf/2211.13108v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06348v2","updated":"2023-12-12T03:47:38Z","published":"2023-12-11T12:53:30Z","title":"DiffAIL: Diffusion Adversarial Imitation Learning","summary":"  Imitation learning aims to solve the problem of defining reward functions in\nreal-world decision-making tasks. The current popular approach is the\nAdversarial Imitation Learning (AIL) framework, which matches expert\nstate-action occupancy measures to obtain a surrogate reward for forward\nreinforcement learning. However, the traditional discriminator is a simple\nbinary classifier and doesn't learn an accurate distribution, which may result\nin failing to identify expert-level state-action pairs induced by the policy\ninteracting with the environment. To address this issue, we propose a method\nnamed diffusion adversarial imitation learning (DiffAIL), which introduces the\ndiffusion model into the AIL framework. Specifically, DiffAIL models the\nstate-action pairs as unconditional diffusion models and uses diffusion loss as\npart of the discriminator's learning objective, which enables the discriminator\nto capture better expert demonstrations and improve generalization.\nExperimentally, the results show that our method achieves state-of-the-art\nperformance and significantly surpasses expert demonstration on two benchmark\ntasks, including the standard state-action setting and state-only settings. Our\ncode can be available at the link https://github.com/ML-Group-SDU/DiffAIL.\n","authors":["Bingzheng Wang","Guoqiang Wu","Teng Pang","Yan Zhang","Yilong Yin"],"pdf_url":"https://arxiv.org/pdf/2312.06348v2.pdf","comment":"Accepted at AAAI 2024"},{"id":"http://arxiv.org/abs/2311.06062v2","updated":"2023-12-12T03:44:04Z","published":"2023-11-10T13:55:05Z","title":"Practical Membership Inference Attacks against Fine-tuned Large Language\n  Models via Self-prompt Calibration","summary":"  Membership Inference Attacks (MIA) aim to infer whether a target data record\nhas been utilized for model training or not. Prior attempts have quantified the\nprivacy risks of language models (LMs) via MIAs, but there is still no\nconsensus on whether existing MIA algorithms can cause remarkable privacy\nleakage on practical Large Language Models (LLMs). Existing MIAs designed for\nLMs can be classified into two categories: reference-free and reference-based\nattacks. They are both based on the hypothesis that training records\nconsistently strike a higher probability of being sampled. Nevertheless, this\nhypothesis heavily relies on the overfitting of target models, which will be\nmitigated by multiple regularization methods and the generalization of LLMs.\nThe reference-based attack seems to achieve promising effectiveness in LLMs,\nwhich measures a more reliable membership signal by comparing the probability\ndiscrepancy between the target model and the reference model. However, the\nperformance of reference-based attack is highly dependent on a reference\ndataset that closely resembles the training dataset, which is usually\ninaccessible in the practical scenario. Overall, existing MIAs are unable to\neffectively unveil privacy leakage over practical fine-tuned LLMs that are\noverfitting-free and private. We propose a Membership Inference Attack based on\nSelf-calibrated Probabilistic Variation (SPV-MIA). Specifically, since\nmemorization in LLMs is inevitable during the training process and occurs\nbefore overfitting, we introduce a more reliable membership signal,\nprobabilistic variation, which is based on memorization rather than\noverfitting. Furthermore, we introduce a self-prompt approach, which constructs\nthe dataset to fine-tune the reference model by prompting the target LLM\nitself. In this manner, the adversary can collect a dataset with a similar\ndistribution from public APIs.\n","authors":["Wenjie Fu","Huandong Wang","Chen Gao","Guanghua Liu","Yong Li","Tao Jiang"],"pdf_url":"https://arxiv.org/pdf/2311.06062v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06960v1","updated":"2023-12-12T03:39:07Z","published":"2023-12-12T03:39:07Z","title":"Remote Sensing Vision-Language Foundation Models without Annotations via\n  Ground Remote Alignment","summary":"  We introduce a method to train vision-language models for remote-sensing\nimages without using any textual annotations. Our key insight is to use\nco-located internet imagery taken on the ground as an intermediary for\nconnecting remote-sensing images and language. Specifically, we train an image\nencoder for remote sensing images to align with the image encoder of CLIP using\na large amount of paired internet and satellite images. Our unsupervised\napproach enables the training of a first-of-its-kind large-scale vision\nlanguage model (VLM) for remote sensing images at two different resolutions. We\nshow that these VLMs enable zero-shot, open-vocabulary image classification,\nretrieval, segmentation and visual question answering for satellite images. On\neach of these tasks, our VLM trained without textual annotations outperforms\nexisting VLMs trained with supervision, with gains of up to 20% for\nclassification and 80% for segmentation.\n","authors":["Utkarsh Mall","Cheng Perng Phoo","Meilin Kelsey Liu","Carl Vondrick","Bharath Hariharan","Kavita Bala"],"pdf_url":"https://arxiv.org/pdf/2312.06960v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06958v1","updated":"2023-12-12T03:37:57Z","published":"2023-12-12T03:37:57Z","title":"PatchMorph: A Stochastic Deep Learning Approach for Unsupervised 3D\n  Brain Image Registration with Small Patches","summary":"  We introduce \"PatchMorph,\" an new stochastic deep learning algorithm tailored\nfor unsupervised 3D brain image registration. Unlike other methods, our method\nuses compact patches of a constant small size to derive solutions that can\ncombine global transformations with local deformations. This approach minimizes\nthe memory footprint of the GPU during training, but also enables us to operate\non numerous amounts of randomly overlapping small patches during inference to\nmitigate image and patch boundary problems. PatchMorph adeptly handles world\ncoordinate transformations between two input images, accommodating variances in\nattributes such as spacing, array sizes, and orientations. The spatial\nresolution of patches transitions from coarse to fine, addressing both global\nand local attributes essential for aligning the images. Each patch offers a\nunique perspective, together converging towards a comprehensive solution.\nExperiments on human T1 MRI brain images and marmoset brain images from serial\n2-photon tomography affirm PatchMorph's superior performance.\n","authors":["Henrik Skibbe","Michal Byra","Akiya Watakabe","Tetsuo Yamamori","Marco Reisert"],"pdf_url":"https://arxiv.org/pdf/2312.06958v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2312.06957v1","updated":"2023-12-12T03:34:09Z","published":"2023-12-12T03:34:09Z","title":"Online Saddle Point Problem and Online Convex-Concave Optimization","summary":"  Centered around solving the Online Saddle Point problem, this paper\nintroduces the Online Convex-Concave Optimization (OCCO) framework, which\ninvolves a sequence of two-player time-varying convex-concave games. We propose\nthe generalized duality gap (Dual-Gap) as the performance metric and establish\nthe parallel relationship between OCCO with Dual-Gap and Online Convex\nOptimization (OCO) with regret. To demonstrate the natural extension of OCCO\nfrom OCO, we develop two algorithms, the implicit online mirror descent-ascent\nand its optimistic variant. Analysis reveals that their duality gaps share\nsimilar expression forms with the corresponding dynamic regrets arising from\nimplicit updates in OCO. Empirical results further substantiate the\neffectiveness of our algorithms. Simultaneously, we unveil that the dynamic\nNash equilibrium regret, which was initially introduced in a recent paper, has\ninherent defects.\n","authors":["Qing-xin Meng","Jian-wei Liu"],"pdf_url":"https://arxiv.org/pdf/2312.06957v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16512v2","updated":"2023-12-12T03:28:40Z","published":"2023-09-28T15:19:30Z","title":"From Complexity to Clarity: Analytical Expressions of Deep Neural\n  Network Weights via Clifford's Geometric Algebra and Convexity","summary":"  In this paper, we introduce a novel analysis of neural networks based on\ngeometric (Clifford) algebra and convex optimization. We show that optimal\nweights of deep ReLU neural networks are given by the wedge product of training\nsamples when trained with standard regularized loss. Furthermore, the training\nproblem reduces to convex optimization over wedge product features, which\nencode the geometric structure of the training dataset. This structure is given\nin terms of signed volumes of triangles and parallelotopes generated by data\nvectors. The convex problem finds a small subset of samples via $\\ell_1$\nregularization to discover only relevant wedge product features. Our analysis\nprovides a novel perspective on the inner workings of deep neural networks and\nsheds light on the role of the hidden layers.\n","authors":["Mert Pilanci"],"pdf_url":"https://arxiv.org/pdf/2309.16512v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04815v2","updated":"2023-12-12T03:16:33Z","published":"2023-12-08T03:05:42Z","title":"Not All Negatives Are Worth Attending to: Meta-Bootstrapping Negative\n  Sampling Framework for Link Prediction","summary":"  The rapid development of graph neural networks (GNNs) encourages the rising\nof link prediction, achieving promising performance with various applications.\nUnfortunately, through a comprehensive analysis, we surprisingly find that\ncurrent link predictors with dynamic negative samplers (DNSs) suffer from the\nmigration phenomenon between \"easy\" and \"hard\" samples, which goes against the\npreference of DNS of choosing \"hard\" negatives, thus severely hindering\ncapability. Towards this end, we propose the MeBNS framework, serving as a\ngeneral plugin that can potentially improve current negative sampling based\nlink predictors. In particular, we elaborately devise a Meta-learning Supported\nTeacher-student GNN (MST-GNN) that is not only built upon teacher-student\narchitecture for alleviating the migration between \"easy\" and \"hard\" samples\nbut also equipped with a meta learning based sample re-weighting module for\nhelping the student GNN distinguish \"hard\" samples in a fine-grained manner. To\neffectively guide the learning of MST-GNN, we prepare a Structure enhanced\nTraining Data Generator (STD-Generator) and an Uncertainty based Meta Data\nCollector (UMD-Collector) for supporting the teacher and student GNN,\nrespectively. Extensive experiments show that the MeBNS achieves remarkable\nperformance across six link prediction benchmark datasets.\n","authors":["Yakun Wang","Binbin Hu","Shuo Yang","Meiqi Zhu","Zhiqiang Zhang","Qiyang Zhang","Jun Zhou","Guo Ye","Huimei He"],"pdf_url":"https://arxiv.org/pdf/2312.04815v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06951v1","updated":"2023-12-12T03:09:37Z","published":"2023-12-12T03:09:37Z","title":"Feature Norm Regularized Federated Learning: Transforming Skewed\n  Distributions into Global Insights","summary":"  In the field of federated learning, addressing non-independent and\nidentically distributed (non-i.i.d.) data remains a quintessential challenge\nfor improving global model performance. This work introduces the Feature Norm\nRegularized Federated Learning (FNR-FL) algorithm, which uniquely incorporates\nclass average feature norms to enhance model accuracy and convergence in\nnon-i.i.d. scenarios. Our comprehensive analysis reveals that FNR-FL not only\naccelerates convergence but also significantly surpasses other contemporary\nfederated learning algorithms in test accuracy, particularly under feature\ndistribution skew scenarios. The novel modular design of FNR-FL facilitates\nseamless integration with existing federated learning frameworks, reinforcing\nits adaptability and potential for widespread application. We substantiate our\nclaims through rigorous empirical evaluations, demonstrating FNR-FL's\nexceptional performance across various skewed data distributions. Relative to\nFedAvg, FNR-FL exhibits a substantial 66.24\\% improvement in accuracy and a\nsignificant 11.40\\% reduction in training time, underscoring its enhanced\neffectiveness and efficiency.\n","authors":["Ke Hu","WeiDong Qiu","Peng Tang"],"pdf_url":"https://arxiv.org/pdf/2312.06951v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13225v2","updated":"2023-12-12T02:36:51Z","published":"2023-11-22T08:26:42Z","title":"NeutronOrch: Rethinking Sample-based GNN Training under CPU-GPU\n  Heterogeneous Environments","summary":"  Graph Neural Networks (GNNs) have demonstrated outstanding performance in\nvarious applications. Existing frameworks utilize CPU-GPU heterogeneous\nenvironments to train GNN models and integrate mini-batch and sampling\ntechniques to overcome the GPU memory limitation. In CPU-GPU heterogeneous\nenvironments, we can divide sample-based GNN training into three steps: sample,\ngather, and train. Existing GNN systems use different task orchestrating\nmethods to employ each step on CPU or GPU. After extensive experiments and\nanalysis, we find that existing task orchestrating methods fail to fully\nutilize the heterogeneous resources, limited by inefficient CPU processing or\nGPU resource contention. In this paper, we propose NeutronOrch, a system for\nsample-based GNN training that incorporates a layer-based task orchestrating\nmethod and ensures balanced utilization of the CPU and GPU. NeutronOrch\ndecouples the training process by layer and pushes down the training task of\nthe bottom layer to the CPU. This significantly reduces the computational load\nand memory footprint of GPU training. To avoid inefficient CPU processing,\nNeutronOrch only offloads the training of frequently accessed vertices to the\nCPU and lets GPU reuse their embeddings with bounded staleness. Furthermore,\nNeutronOrch provides a fine-grained pipeline design for the layer-based task\norchestrating method, fully overlapping different tasks on heterogeneous\nresources while strictly guaranteeing bounded staleness. The experimental\nresults show that compared with the state-of-the-art GNN systems, NeutronOrch\ncan achieve up to 11.51x performance speedup.\n","authors":["Xin Ai","Qiange Wang","Chunyu Cao","Yanfeng Zhang","Chaoyi Chen","Hao Yuan","Yu Gu","Ge Yu"],"pdf_url":"https://arxiv.org/pdf/2311.13225v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06942v1","updated":"2023-12-12T02:34:06Z","published":"2023-12-12T02:34:06Z","title":"AI Control: Improving Safety Despite Intentional Subversion","summary":"  As large language models (LLMs) become more powerful and are deployed more\nautonomously, it will be increasingly important to prevent them from causing\nharmful outcomes. Researchers have investigated a variety of safety techniques\nfor this purpose, e.g. using models to review the outputs of other models, or\nred-teaming techniques to surface subtle failure modes. However, researchers\nhave not evaluated whether such techniques still ensure safety if the model is\nitself intentionally trying to subvert them. In this paper, we develop and\nevaluate pipelines of safety techniques (\"protocols\") that are robust to\nintentional subversion.\n  We investigate a scenario in which we want to solve a sequence of programming\nproblems, using access to a powerful but untrusted model (in our case, GPT-4),\naccess to a less powerful trusted model (in our case, GPT-3.5), and limited\naccess to human contractors who provide high-quality trusted labor. We\ninvestigate protocols that aim to never submit solutions containing backdoors,\nwhich we operationalize here as logical errors that are not caught by test\ncases.\n  We investigate a range of protocols and test each against strategies that the\nuntrusted model could use to subvert them. One protocol is what we call trusted\nediting. This protocol first asks GPT-4 to write code, and then asks GPT-3.5 to\nrate the suspiciousness of that code. If the code is below some suspiciousness\nthreshold, it is submitted. Otherwise, GPT-3.5 edits the solution to remove\nparts that seem suspicious and then submits the edited code. Another protocol\nis untrusted monitoring. This protocol asks GPT-4 to write code, and then asks\nanother instance of GPT-4 whether the code is backdoored, using various\ntechniques to prevent the GPT-4 instances from colluding. These protocols\nimprove substantially on simple baselines.\n","authors":["Ryan Greenblatt","Buck Shlegeris","Kshitij Sachan","Fabien Roger"],"pdf_url":"https://arxiv.org/pdf/2312.06942v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06941v1","updated":"2023-12-12T02:28:12Z","published":"2023-12-12T02:28:12Z","title":"Humans vs Large Language Models: Judgmental Forecasting in an Era of\n  Advanced AI","summary":"  This study investigates the forecasting accuracy of human experts versus\nLarge Language Models (LLMs) in the retail sector, particularly during standard\nand promotional sales periods. Utilizing a controlled experimental setup with\n123 human forecasters and five LLMs, including ChatGPT4, ChatGPT3.5, Bard,\nBing, and Llama2, we evaluated forecasting precision through Mean Absolute\nPercentage Error. Our analysis centered on the effect of the following factors\non forecasters performance: the supporting statistical model (baseline and\nadvanced), whether the product was on promotion, and the nature of external\nimpact. The findings indicate that LLMs do not consistently outperform humans\nin forecasting accuracy and that advanced statistical forecasting models do not\nuniformly enhance the performance of either human forecasters or LLMs. Both\nhuman and LLM forecasters exhibited increased forecasting errors, particularly\nduring promotional periods and under the influence of positive external\nimpacts. Our findings call for careful consideration when integrating LLMs into\npractical forecasting processes.\n","authors":["MAhdi Abolghasemi","Odkhishig Ganbold","Kristian Rotaru"],"pdf_url":"https://arxiv.org/pdf/2312.06941v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06940v1","updated":"2023-12-12T02:20:39Z","published":"2023-12-12T02:20:39Z","title":"Benchmarking Deep Learning Classifiers for SAR Automatic Target\n  Recognition","summary":"  Synthetic Aperture Radar SAR Automatic Target Recognition ATR is a key\ntechnique of remote-sensing image recognition which can be supported by deep\nneural networks The existing works of SAR ATR mostly focus on improving the\naccuracy of the target recognition while ignoring the systems performance in\nterms of speed and storage which is critical to real-world applications of SAR\nATR For decision-makers aiming to identify a proper deep learning model to\ndeploy in a SAR ATR system it is important to understand the performance of\ndifferent candidate deep learning models and determine the best model\naccordingly This paper comprehensively benchmarks several advanced deep\nlearning models for SAR ATR with multiple distinct SAR imagery datasets\nSpecifically we train and test five SAR image classifiers based on Residual\nNeural Networks ResNet18 ResNet34 ResNet50 Graph Neural Network GNN and Vision\nTransformer for Small-Sized Datasets (SS-ViT) We select three datasets MSTAR\nGBSAR and SynthWakeSAR that offer heterogeneity We evaluate and compare the\nfive classifiers concerning their classification accuracy runtime performance\nin terms of inference throughput and analytical performance in terms of number\nof parameters number of layers model size and number of operations Experimental\nresults show that the GNN classifier outperforms with respect to throughput and\nlatency However it is also shown that no clear model winner emerges from all of\nour chosen metrics and a one model rules all case is doubtful in the domain of\nSAR ATR\n","authors":["Jacob Fein-Ashley","Tian Ye","Rajgopal Kannan","Viktor Prasanna","Carl Busart"],"pdf_url":"https://arxiv.org/pdf/2312.06940v1.pdf","comment":"6 Pages"},{"id":"http://arxiv.org/abs/2312.06937v1","updated":"2023-12-12T02:13:50Z","published":"2023-12-12T02:13:50Z","title":"Can a Transformer Represent a Kalman Filter?","summary":"  Transformers are a class of autoregressive deep learning architectures which\nhave recently achieved state-of-the-art performance in various vision,\nlanguage, and robotics tasks. We revisit the problem of Kalman Filtering in\nlinear dynamical systems and show that Transformers can approximate the Kalman\nFilter in a strong sense. Specifically, for any observable LTI system we\nconstruct an explicit causally-masked Transformer which implements the Kalman\nFilter, up to a small additive error which is bounded uniformly in time; we\ncall our construction the Transformer Filter. Our construction is based on a\ntwo-step reduction. We first show that a softmax self-attention block can\nexactly represent a certain Gaussian kernel smoothing estimator. We then show\nthat this estimator closely approximates the Kalman Filter. We also investigate\nhow the Transformer Filter can be used for measurement-feedback control and\nprove that the resulting nonlinear controllers closely approximate the\nperformance of standard optimal control policies such as the LQG controller.\n","authors":["Gautam Goel","Peter Bartlett"],"pdf_url":"https://arxiv.org/pdf/2312.06937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06932v1","updated":"2023-12-12T02:06:50Z","published":"2023-12-12T02:06:50Z","title":"Predictive variational autoencoder for learning robust representations\n  of time-series data","summary":"  Variational autoencoders (VAEs) have been used extensively to discover\nlow-dimensional latent factors governing neural activity and animal behavior.\nHowever, without careful model selection, the uncovered latent factors may\nreflect noise in the data rather than true underlying features, rendering such\nrepresentations unsuitable for scientific interpretation. Existing solutions to\nthis problem involve introducing additional measured variables or data\naugmentations specific to a particular data type. We propose a VAE architecture\nthat predicts the next point in time and show that it mitigates the learning of\nspurious features. In addition, we introduce a model selection metric based on\nsmoothness over time in the latent space. We show that together these two\nconstraints on VAEs to be smooth over time produce robust latent\nrepresentations and faithfully recover latent factors on synthetic datasets.\n","authors":["Julia Huiming Wang","Dexter Tsin","Tatiana Engel"],"pdf_url":"https://arxiv.org/pdf/2312.06932v1.pdf","comment":"16 pages, 4 main figures, 4 supplemental figures, accepted for\n  publication at Unireps Workshop in 37th Conference on Neural Information\n  Processing Systems (NeurIPS 2023)"},{"id":"http://arxiv.org/abs/2312.06925v1","updated":"2023-12-12T01:40:14Z","published":"2023-12-12T01:40:14Z","title":"Facial Emotion Recognition in VR Games","summary":"  Emotion detection is a crucial component of Games User Research (GUR), as it\nallows game developers to gain insights into players' emotional experiences and\ntailor their games accordingly. However, detecting emotions in Virtual Reality\n(VR) games is challenging due to the Head-Mounted Display (HMD) that covers the\ntop part of the player's face, namely, their eyes and eyebrows, which provide\ncrucial information for recognizing the impression. To tackle this we used a\nConvolutional Neural Network (CNN) to train a model to predict emotions in\nfull-face images where the eyes and eyebrows are covered. We used the FER2013\ndataset, which we modified to cover eyes and eyebrows in images. The model in\nthese images can accurately recognize seven different emotions which are anger,\nhappiness, disgust, fear, impartiality, sadness and surprise.\n  We assessed the model's performance by testing it on two VR games and using\nit to detect players' emotions. We collected self-reported emotion data from\nthe players after the gameplay sessions. We analyzed the data collected from\nour experiment to understand which emotions players experience during the\ngameplay. We found that our approach has the potential to enhance gameplay\nanalysis by enabling the detection of players' emotions in VR games, which can\nhelp game developers create more engaging and immersive game experiences.\n","authors":["Fatemeh Dehghani","Loutfouz Zaman"],"pdf_url":"https://arxiv.org/pdf/2312.06925v1.pdf","comment":"IEEE Conference on Games 2023"},{"id":"http://arxiv.org/abs/2304.13107v2","updated":"2023-12-12T01:00:20Z","published":"2023-04-25T19:21:47Z","title":"Time-Selective RNN for Device-Free Multi-Room Human Presence Detection\n  Using WiFi CSI","summary":"  Device-free human presence detection is a crucial technology for various\napplications, including home automation, security, and healthcare. While\ncamera-based systems have traditionally been used for this purpose, they raise\nprivacy concerns. To address this issue, recent research has explored the use\nof wireless channel state information (CSI) extracted from commercial WiFi\naccess points (APs) to provide detailed channel characteristics. In this paper,\nwe propose a device-free human presence detection system for multi-room\nscenarios using a time-selective conditional dual feature extract recurrent\nnetwork (TCD-FERN). Our system is designed to capture significant time features\non current human features using a dynamic and static data preprocessing\ntechnique. We extract both moving and spatial features of people and\ndifferentiate between line-of-sight (LoS) and non-line-of-sight (NLoS) cases.\nSubcarrier fusion is carried out in order to provide more objective variation\nof each sample while reducing the computational complexity. A voting scheme is\nfurther adopted to mitigate the feature attenuation problem caused by room\npartitions, with around 3% improvement of human presence detection accuracy.\nExperimental results have revealed the significant improvement of leveraging\nsubcarrier fusion, dual-feature recurrent network, time selection and condition\nmechanisms. Compared to the existing works in open literature, our proposed\nTCD-FERN system can achieve above 97% of human presence detection accuracy for\nmulti-room scenarios with the adoption of fewer WiFi APs.\n","authors":["Li-Hsiang Shen","An-Hung Hsiao","Fang-Yu Chu","Kai-Ten Feng"],"pdf_url":"https://arxiv.org/pdf/2304.13107v2.pdf","comment":"Accepted by IEEE Transactions on Instrumentation & Measurement"},{"id":"http://arxiv.org/abs/2312.06914v1","updated":"2023-12-12T00:54:39Z","published":"2023-12-12T00:54:39Z","title":"Exploring Novel Object Recognition and Spontaneous Location Recognition\n  Machine Learning Analysis Techniques in Alzheimer's Mice","summary":"  Understanding object recognition patterns in mice is crucial for advancing\nbehavioral neuroscience and has significant implications for human health,\nparticularly in the realm of Alzheimer's research. This study is centered on\nthe development, application, and evaluation of a state-of-the-art\ncomputational pipeline designed to analyze such behaviors, specifically\nfocusing on Novel Object Recognition (NOR) and Spontaneous Location Recognition\n(SLR) tasks. The pipeline integrates three advanced computational models:\nAny-Maze for initial data collection, DeepLabCut for detailed pose estimation,\nand Convolutional Neural Networks (CNNs) for nuanced behavioral classification.\nEmployed across four distinct mouse groups, this pipeline demonstrated high\nlevels of accuracy and robustness. Despite certain challenges like video\nquality limitations and the need for manual calculations, the results affirm\nthe pipeline's efficacy and potential for scalability. The study serves as a\nproof of concept for a multidimensional computational approach to behavioral\nneuroscience, emphasizing the pipeline's versatility and readiness for future,\nmore complex analyses.\n","authors":["Soham Bafana","Radha Raghuraman","S. Abid Hussaini"],"pdf_url":"https://arxiv.org/pdf/2312.06914v1.pdf","comment":"10 Pages. All code used in this research can be found at\n  https://github.com/bafanaS/DLC-Object-Recognition-Analysis.git"},{"id":"http://arxiv.org/abs/2303.16852v3","updated":"2023-12-12T00:49:53Z","published":"2023-03-29T16:59:22Z","title":"Diffusion Schrödinger Bridge Matching","summary":"  Solving transport problems, i.e. finding a map transporting one given\ndistribution to another, has numerous applications in machine learning. Novel\nmass transport methods motivated by generative modeling have recently been\nproposed, e.g. Denoising Diffusion Models (DDMs) and Flow Matching Models\n(FMMs) implement such a transport through a Stochastic Differential Equation\n(SDE) or an Ordinary Differential Equation (ODE). However, while it is\ndesirable in many applications to approximate the deterministic dynamic Optimal\nTransport (OT) map which admits attractive properties, DDMs and FMMs are not\nguaranteed to provide transports close to the OT map. In contrast,\nSchr\\\"odinger bridges (SBs) compute stochastic dynamic mappings which recover\nentropy-regularized versions of OT. Unfortunately, existing numerical methods\napproximating SBs either scale poorly with dimension or accumulate errors\nacross iterations. In this work, we introduce Iterative Markovian Fitting\n(IMF), a new methodology for solving SB problems, and Diffusion Schr\\\"odinger\nBridge Matching (DSBM), a novel numerical algorithm for computing IMF iterates.\nDSBM significantly improves over previous SB numerics and recovers as\nspecial/limiting cases various recent transport methods. We demonstrate the\nperformance of DSBM on a variety of problems.\n","authors":["Yuyang Shi","Valentin De Bortoli","Andrew Campbell","Arnaud Doucet"],"pdf_url":"https://arxiv.org/pdf/2303.16852v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.19302v2","updated":"2023-12-12T00:20:01Z","published":"2023-05-30T15:26:43Z","title":"Smooth, exact rotational symmetrization for deep learning on point\n  clouds","summary":"  Point clouds are versatile representations of 3D objects and have found\nwidespread application in science and engineering. Many successful\ndeep-learning models have been proposed that use them as input. The domain of\nchemical and materials modeling is especially challenging because exact\ncompliance with physical constraints is highly desirable for a model to be\nusable in practice. These constraints include smoothness and invariance with\nrespect to translations, rotations, and permutations of identical atoms. If\nthese requirements are not rigorously fulfilled, atomistic simulations might\nlead to absurd outcomes even if the model has excellent accuracy. Consequently,\ndedicated architectures, which achieve invariance by restricting their design\nspace, have been developed. General-purpose point-cloud models are more varied\nbut often disregard rotational symmetry. We propose a general symmetrization\nmethod that adds rotational equivariance to any given model while preserving\nall the other requirements. Our approach simplifies the development of better\natomic-scale ML schemes by relaxing the constraints on the design space and\nmaking it possible to incorporate ideas that proved effective in other domains.\nWe demonstrate this idea by introducing the Point Edge Transformer (PET)\narchitecture, which is not intrinsically equivariant but achieves\nstate-of-the-art performance on several benchmark datasets of molecules and\nsolids. A-posteriori application of our general protocol makes PET exactly\nequivariant, with minimal changes to its accuracy.\n","authors":["Sergey N. Pozdnyakov","Michele Ceriotti"],"pdf_url":"https://arxiv.org/pdf/2305.19302v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06902v1","updated":"2023-12-12T00:16:18Z","published":"2023-12-12T00:16:18Z","title":"Perseus: Removing Energy Bloat from Large Model Training","summary":"  Training large AI models on numerous GPUs consumes a massive amount of\nenergy. We observe that not all energy consumed during training directly\ncontributes to end-to-end training throughput, and a significant portion can be\nremoved without slowing down training, which we call energy bloat.\n  In this work, we identify two independent sources of energy bloat in large\nmodel training, intrinsic and extrinsic, and propose Perseus, a unified\noptimization framework that mitigates both. Perseus obtains the \"iteration\ntime-energy\" Pareto frontier of any large model training job using an efficient\niterative graph cut-based algorithm and schedules energy consumption of its\nforward and backward computations across time to remove intrinsic and extrinsic\nenergy bloat. Evaluation on large models like GPT-3 and Bloom shows that\nPerseus reduces energy consumption of large model training by up to 30%,\nenabling savings otherwise unobtainable before.\n","authors":["Jae-Won Chung","Yile Gu","Insu Jang","Luoxi Meng","Nikhil Bansal","Mosharaf Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2312.06902v1.pdf","comment":"Open-source at https://ml.energy/zeus/perseus/"},{"id":"http://arxiv.org/abs/2312.06899v1","updated":"2023-12-12T00:01:47Z","published":"2023-12-12T00:01:47Z","title":"LoRA-Enhanced Distillation on Guided Diffusion Models","summary":"  Diffusion models, such as Stable Diffusion (SD), offer the ability to\ngenerate high-resolution images with diverse features, but they come at a\nsignificant computational and memory cost. In classifier-free guided diffusion\nmodels, prolonged inference times are attributed to the necessity of computing\ntwo separate diffusion models at each denoising step. Recent work has shown\npromise in improving inference time through distillation techniques, teaching\nthe model to perform similar denoising steps with reduced computations.\nHowever, the application of distillation introduces additional memory overhead\nto these already resource-intensive diffusion models, making it less practical.\n  To address these challenges, our research explores a novel approach that\ncombines Low-Rank Adaptation (LoRA) with model distillation to efficiently\ncompress diffusion models. This approach not only reduces inference time but\nalso mitigates memory overhead, and notably decreases memory consumption even\nbefore applying distillation. The results are remarkable, featuring a\nsignificant reduction in inference time due to the distillation process and a\nsubstantial 50% reduction in memory consumption. Our examination of the\ngenerated images underscores that the incorporation of LoRA-enhanced\ndistillation maintains image quality and alignment with the provided prompts.\nIn summary, while conventional distillation tends to increase memory\nconsumption, LoRA-enhanced distillation offers optimization without any\ntrade-offs or compromises in quality.\n","authors":["Pareesa Ameneh Golnari"],"pdf_url":"https://arxiv.org/pdf/2312.06899v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.01559v4","updated":"2023-12-12T23:49:49Z","published":"2021-07-04T06:55:45Z","title":"Smoothed Differential Privacy","summary":"  Differential privacy (DP) is a widely-accepted and widely-applied notion of\nprivacy based on worst-case analysis. Often, DP classifies most mechanisms\nwithout additive noise as non-private (Dwork et al., 2014). Thus, additive\nnoises are added to improve privacy (to achieve DP). However, in many\nreal-world applications, adding additive noise is undesirable (Bagdasaryan et\nal., 2019) and sometimes prohibited (Liu et al., 2020).\n  In this paper, we propose a natural extension of DP following the worst\naverage-case idea behind the celebrated smoothed analysis (Spielman & Teng, May\n2004). Our notion, smoothed DP, can effectively measure the privacy leakage of\nmechanisms without additive noises under realistic settings. We prove that any\ndiscrete mechanism with sampling procedures is more private than what DP\npredicts, while many continuous mechanisms with sampling procedures are still\nnon-private under smoothed DP. In addition, we prove several desirable\nproperties of smoothed DP, including composition, robustness to\npost-processing, and distribution reduction. Based on those properties, we\npropose an efficient algorithm to calculate the privacy parameters for smoothed\nDP. Experimentally, we verify that, according to smoothed DP, the discrete\nsampling mechanisms are private in real-world elections, and some discrete\nneural networks can be private without adding any additive noise. We believe\nthat these results contribute to the theoretical foundation of realistic\nprivacy measures beyond worst-case analysis.\n","authors":["Ao Liu","Yu-Xiang Wang","Lirong Xia"],"pdf_url":"https://arxiv.org/pdf/2107.01559v4.pdf","comment":"16 Page main text + Appendix"},{"id":"http://arxiv.org/abs/2312.07802v1","updated":"2023-12-12T23:41:59Z","published":"2023-12-12T23:41:59Z","title":"Estimation of embedding vectors in high dimensions","summary":"  Embeddings are a basic initial feature extraction step in many machine\nlearning models, particularly in natural language processing. An embedding\nattempts to map data tokens to a low-dimensional space where similar tokens are\nmapped to vectors that are close to one another by some metric in the embedding\nspace. A basic question is how well can such embedding be learned? To study\nthis problem, we consider a simple probability model for discrete data where\nthere is some \"true\" but unknown embedding where the correlation of random\nvariables is related to the similarity of the embeddings. Under this model, it\nis shown that the embeddings can be learned by a variant of low-rank\napproximate message passing (AMP) method. The AMP approach enables precise\npredictions of the accuracy of the estimation in certain high-dimensional\nlimits. In particular, the methodology provides insight on the relations of key\nparameters such as the number of samples per value, the frequency of the terms,\nand the strength of the embedding correlation on the probability distribution.\nOur theoretical findings are validated by simulations on both synthetic data\nand real text data.\n","authors":["Golara Ahmadi Azar","Melika Emami","Alyson Fletcher","Sundeep Rangan"],"pdf_url":"https://arxiv.org/pdf/2312.07802v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2309.10441v2","updated":"2023-12-12T23:24:56Z","published":"2023-09-19T08:59:46Z","title":"Coreset selection can accelerate quantum machine learning models with\n  provable generalization","summary":"  Quantum neural networks (QNNs) and quantum kernels stand as prominent figures\nin the realm of quantum machine learning, poised to leverage the nascent\ncapabilities of near-term quantum computers to surmount classical machine\nlearning challenges. Nonetheless, the training efficiency challenge poses a\nlimitation on both QNNs and quantum kernels, curbing their efficacy when\napplied to extensive datasets. To confront this concern, we present a unified\napproach: coreset selection, aimed at expediting the training of QNNs and\nquantum kernels by distilling a judicious subset from the original training\ndataset. Furthermore, we analyze the generalization error bounds of QNNs and\nquantum kernels when trained on such coresets, unveiling the comparable\nperformance with those training on the complete original dataset. Through\nsystematic numerical simulations, we illuminate the potential of coreset\nselection in expediting tasks encompassing synthetic data classification,\nidentification of quantum correlations, and quantum compiling. Our work offers\na useful way to improve diverse quantum machine learning models with a\ntheoretical guarantee while reducing the training cost.\n","authors":["Yiming Huang","Huiyuan Wang","Yuxuan Du","Xiao Yuan"],"pdf_url":"https://arxiv.org/pdf/2309.10441v2.pdf","comment":"25 pages, 7 figures"},{"id":"http://arxiv.org/abs/2312.07795v1","updated":"2023-12-12T23:21:57Z","published":"2023-12-12T23:21:57Z","title":"Traffic Signal Control Using Lightweight Transformers: An\n  Offline-to-Online RL Approach","summary":"  Efficient traffic signal control is critical for reducing traffic congestion\nand improving overall transportation efficiency. The dynamic nature of traffic\nflow has prompted researchers to explore Reinforcement Learning (RL) for\ntraffic signal control (TSC). Compared with traditional methods, RL-based\nsolutions have shown preferable performance. However, the application of\nRL-based traffic signal controllers in the real world is limited by the low\nsample efficiency and high computational requirements of these solutions. In\nthis work, we propose DTLight, a simple yet powerful lightweight Decision\nTransformer-based TSC method that can learn policy from easily accessible\noffline datasets. DTLight novelly leverages knowledge distillation to learn a\nlightweight controller from a well-trained larger teacher model to reduce\nimplementation computation. Additionally, it integrates adapter modules to\nmitigate the expenses associated with fine-tuning, which makes DTLight\npractical for online adaptation with minimal computation and only a few\nfine-tuning steps during real deployment. Moreover, DTLight is further enhanced\nto be more applicable to real-world TSC problems. Extensive experiments on\nsynthetic and real-world scenarios show that DTLight pre-trained purely on\noffline datasets can outperform state-of-the-art online RL-based methods in\nmost scenarios. Experiment results also show that online fine-tuning further\nimproves the performance of DTLight by up to 42.6% over the best online RL\nbaseline methods. In this work, we also introduce Datasets specifically\ndesigned for TSC with offline RL (referred to as DTRL). Our datasets and code\nare publicly available.\n","authors":["Xingshuai Huang","Di Wu","Benoit Boulet"],"pdf_url":"https://arxiv.org/pdf/2312.07795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07792v1","updated":"2023-12-12T23:17:29Z","published":"2023-12-12T23:17:29Z","title":"Differentially private projection-depth-based medians","summary":"  We develop $(\\epsilon,\\delta)$-differentially private projection-depth-based\nmedians using the propose-test-release (PTR) and exponential mechanisms. Under\ngeneral conditions on the input parameters and the population measure, (e.g. we\ndo not assume any moment bounds), we quantify the probability the test in PTR\nfails, as well as the cost of privacy via finite sample deviation bounds. We\ndemonstrate our main result on the canonical projection-depth-based median. In\nthe Gaussian setting, we show that the resulting deviation bound matches the\nknown lower bound for private Gaussian mean estimation, up to a polynomial\nfunction of the condition number of the covariance matrix. In the Cauchy\nsetting, we show that the ``outlier error amplification'' effect resulting from\nthe heavy tails outweighs the cost of privacy. This result is then verified via\nnumerical simulations. Additionally, we present results on general PTR\nmechanisms and a uniform concentration result on the projected spacings of\norder statistics.\n","authors":["Kelly Ramsay","Dylan Spicker"],"pdf_url":"https://arxiv.org/pdf/2312.07792v1.pdf","comment":"39 pages, 1 figure"},{"id":"http://arxiv.org/abs/2312.06585v2","updated":"2023-12-12T23:16:16Z","published":"2023-12-11T18:17:43Z","title":"Beyond Human Data: Scaling Self-Training for Problem-Solving with\n  Language Models","summary":"  Fine-tuning language models~(LMs) on human-generated data remains a prevalent\npractice. However, the performance of such models is often limited by the\nquantity and diversity of high-quality human data. In this paper, we explore\nwhether we can go beyond human data on tasks where we have access to scalar\nfeedback, for example, on math problems where one can verify correctness. To do\nso, we investigate a simple self-training method based on\nexpectation-maximization, which we call ReST$^{EM}$, where we (1) generate\nsamples from the model and filter them using binary feedback, (2) fine-tune the\nmodel on these samples, and (3) repeat this process a few times. Testing on\nadvanced MATH reasoning and APPS coding benchmarks using PaLM-2 models, we find\nthat ReST$^{EM}$ scales favorably with model size and significantly surpasses\nfine-tuning only on human data. Overall, our findings suggest self-training\nwith feedback can substantially reduce dependence on human-generated data.\n","authors":["Avi Singh","John D. Co-Reyes","Rishabh Agarwal","Ankesh Anand","Piyush Patil","Peter J. Liu","James Harrison","Jaehoon Lee","Kelvin Xu","Aaron Parisi","Abhishek Kumar","Alex Alemi","Alex Rizkowsky","Azade Nova","Ben Adlam","Bernd Bohnet","Gamaleldin Elsayed","Hanie Sedghi","Igor Mordatch","Isabelle Simpson","Izzeddin Gur","Jasper Snoek","Jeffrey Pennington","Jiri Hron","Kathleen Kenealy","Kevin Swersky","Kshiteej Mahajan","Laura Culp","Lechao Xiao","Maxwell L. Bileschi","Noah Constant","Roman Novak","Rosanne Liu","Tris Warkentin","Yundi Qian","Ethan Dyer","Behnam Neyshabur","Jascha Sohl-Dickstein","Noah Fiedel"],"pdf_url":"https://arxiv.org/pdf/2312.06585v2.pdf","comment":"First three authors contributed equally"},{"id":"http://arxiv.org/abs/2312.07790v1","updated":"2023-12-12T23:15:07Z","published":"2023-12-12T23:15:07Z","title":"Characteristic Circuits","summary":"  In many real-world scenarios, it is crucial to be able to reliably and\nefficiently reason under uncertainty while capturing complex relationships in\ndata. Probabilistic circuits (PCs), a prominent family of tractable\nprobabilistic models, offer a remedy to this challenge by composing simple,\ntractable distributions into a high-dimensional probability distribution.\nHowever, learning PCs on heterogeneous data is challenging and densities of\nsome parametric distributions are not available in closed form, limiting their\npotential use. We introduce characteristic circuits (CCs), a family of\ntractable probabilistic models providing a unified formalization of\ndistributions over heterogeneous data in the spectral domain. The one-to-one\nrelationship between characteristic functions and probability measures enables\nus to learn high-dimensional distributions on heterogeneous data domains and\nfacilitates efficient probabilistic inference even when no closed-form density\nfunction is available. We show that the structure and parameters of CCs can be\nlearned efficiently from the data and find that CCs outperform state-of-the-art\ndensity estimators for heterogeneous data domains on common benchmark data\nsets.\n","authors":["Zhongjie Yu","Martin Trapp","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2312.07790v1.pdf","comment":"Published at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2312.07784v1","updated":"2023-12-12T22:57:14Z","published":"2023-12-12T22:57:14Z","title":"Robust MRI Reconstruction by Smoothed Unrolling (SMUG)","summary":"  As the popularity of deep learning (DL) in the field of magnetic resonance\nimaging (MRI) continues to rise, recent research has indicated that DL-based\nMRI reconstruction models might be excessively sensitive to minor input\ndisturbances, including worst-case additive perturbations. This sensitivity\noften leads to unstable, aliased images. This raises the question of how to\ndevise DL techniques for MRI reconstruction that can be robust to train-test\nvariations. To address this problem, we propose a novel image reconstruction\nframework, termed Smoothed Unrolling (SMUG), which advances a deep\nunrolling-based MRI reconstruction model using a randomized smoothing\n(RS)-based robust learning approach. RS, which improves the tolerance of a\nmodel against input noises, has been widely used in the design of adversarial\ndefense approaches for image classification tasks. Yet, we find that the\nconventional design that applies RS to the entire DL-based MRI model is\nineffective. In this paper, we show that SMUG and its variants address the\nabove issue by customizing the RS process based on the unrolling architecture\nof a DL-based MRI reconstruction model. Compared to the vanilla RS approach, we\nshow that SMUG improves the robustness of MRI reconstruction with respect to a\ndiverse set of instability sources, including worst-case and random noise\nperturbations to input measurements, varying measurement sampling rates, and\ndifferent numbers of unrolling steps. Furthermore, we theoretically analyze the\nrobustness of our method in the presence of perturbations.\n","authors":["Shijun Liang","Van Hoang Minh Nguyen","Jinghan Jia","Ismail Alkhouri","Sijia Liu","Saiprasad Ravishankar"],"pdf_url":"https://arxiv.org/pdf/2312.07784v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.01881v6","updated":"2023-12-12T22:56:44Z","published":"2022-03-03T17:48:23Z","title":"Measuring Self-Supervised Representation Quality for Downstream\n  Classification using Discriminative Features","summary":"  Self-supervised learning (SSL) has shown impressive results in downstream\nclassification tasks. However, there is limited work in understanding their\nfailure modes and interpreting their learned representations. In this paper, we\nstudy the representation space of state-of-the-art self-supervised models\nincluding SimCLR, SwaV, MoCo, BYOL, DINO, SimSiam, VICReg and Barlow Twins.\nWithout the use of class label information, we discover discriminative features\nthat correspond to unique physical attributes in images, present mostly in\ncorrectly-classified representations. Using these features, we can compress the\nrepresentation space by up to 40% without significantly affecting linear\nclassification performance. We then propose Self-Supervised Representation\nQuality Score (or Q-Score), an unsupervised score that can reliably predict if\na given sample is likely to be mis-classified during linear evaluation,\nachieving AUPRC of 91.45 on ImageNet-100 and 78.78 on ImageNet-1K. Q-Score can\nalso be used as a regularization term on pre-trained encoders to remedy\nlow-quality representations. Fine-tuning with Q-Score regularization can boost\nthe linear probing accuracy of SSL models by up to 5.8% on ImageNet-100 and\n3.7% on ImageNet-1K compared to their baselines. Finally, using gradient\nheatmaps and Salient ImageNet masks, we define a metric to quantify the\ninterpretability of each representation. We show that discriminative features\nare strongly correlated to core attributes and, enhancing these features\nthrough Q-score regularization makes SSL representations more interpretable.\n","authors":["Neha Kalibhat","Kanika Narang","Hamed Firooz","Maziar Sanjabi","Soheil Feizi"],"pdf_url":"https://arxiv.org/pdf/2203.01881v6.pdf","comment":"Published at AAAI 2024"},{"id":"http://arxiv.org/abs/2310.20545v2","updated":"2023-12-12T22:52:50Z","published":"2023-10-31T15:26:33Z","title":"Optimizing accuracy and diversity: a multi-task approach to forecast\n  combinations","summary":"  Forecast combination involves using multiple forecasts to create a single,\nmore accurate prediction. Recently, feature-based forecasting has been employed\nto either select the most appropriate forecasting models or to optimize the\nweights of their combination. In this paper, we present a multi-task\noptimization paradigm that focuses on solving both problems simultaneously and\nenriches current operational research approaches to forecasting. In essence, it\nincorporates an additional learning and optimization task into the standard\nfeature-based forecasting approach, focusing on the identification of an\noptimal set of forecasting methods. During the training phase, an optimization\nmodel with linear constraints and quadratic objective function is employed to\nidentify accurate and diverse methods for each time series. Moreover, within\nthe training phase, a neural network is used to learn the behavior of that\noptimization model. Once training is completed the candidate set of methods is\nidentified using the network. The proposed approach elicits the essential role\nof diversity in feature-based forecasting and highlights the interplay between\nmodel combination and model selection when optimizing forecasting ensembles.\nExperimental results on a large set of series from the M4 competition dataset\nshow that our proposal enhances point forecast accuracy compared to\nstate-of-the-art methods.\n","authors":["Giovanni Felici","Antonio M. Sudoso"],"pdf_url":"https://arxiv.org/pdf/2310.20545v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07781v1","updated":"2023-12-12T22:49:24Z","published":"2023-12-12T22:49:24Z","title":"Combining propensity score methods with variational autoencoders for\n  generating synthetic data in presence of latent sub-groups","summary":"  In settings requiring synthetic data generation based on a clinical cohort,\ne.g., due to data protection regulations, heterogeneity across individuals\nmight be a nuisance that we need to control or faithfully preserve. The sources\nof such heterogeneity might be known, e.g., as indicated by sub-groups labels,\nor might be unknown and thus reflected only in properties of distributions,\nsuch as bimodality or skewness. We investigate how such heterogeneity can be\npreserved and controlled when obtaining synthetic data from variational\nautoencoders (VAEs), i.e., a generative deep learning technique that utilizes a\nlow-dimensional latent representation. To faithfully reproduce unknown\nheterogeneity reflected in marginal distributions, we propose to combine VAEs\nwith pre-transformations. For dealing with known heterogeneity due to\nsub-groups, we complement VAEs with models for group membership, specifically\nfrom propensity score regression. The evaluation is performed with a realistic\nsimulation design that features sub-groups and challenging marginal\ndistributions. The proposed approach faithfully recovers the latter, compared\nto synthetic data approaches that focus purely on marginal distributions.\nPropensity scores add complementary information, e.g., when visualized in the\nlatent space, and enable sampling of synthetic data with or without sub-group\nspecific characteristics. We also illustrate the proposed approach with real\ndata from an international stroke trial that exhibits considerable distribution\ndifferences between study sites, in addition to bimodality. These results\nindicate that describing heterogeneity by statistical approaches, such as\npropensity score regression, might be more generally useful for complementing\ngenerative deep learning for obtaining synthetic data that faithfully reflects\nstructure from clinical cohorts.\n","authors":["Kiana Farhadyar","Federico Bonofiglio","Maren Hackenberg","Daniela Zoeller","Harald Binder"],"pdf_url":"https://arxiv.org/pdf/2312.07781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03149v3","updated":"2023-12-12T22:42:29Z","published":"2023-10-04T20:26:59Z","title":"Attributing Learned Concepts in Neural Networks to Training Data","summary":"  By now there is substantial evidence that deep learning models learn certain\nhuman-interpretable features as part of their internal representations of data.\nAs having the right (or wrong) concepts is critical to trustworthy machine\nlearning systems, it is natural to ask which inputs from the model's original\ntraining set were most important for learning a concept at a given layer. To\nanswer this, we combine data attribution methods with methods for probing the\nconcepts learned by a model. Training network and probe ensembles for two\nconcept datasets on a range of network layers, we use the recently developed\nTRAK method for large-scale data attribution. We find some evidence for\nconvergence, where removing the 10,000 top attributing images for a concept and\nretraining the model does not change the location of the concept in the network\nnor the probing sparsity of the concept. This suggests that rather than being\nhighly dependent on a few specific examples, the features that inform the\ndevelopment of a concept are spread in a more diffuse manner across its\nexemplars, implying robustness in concept formation.\n","authors":["Nicholas Konz","Charles Godfrey","Madelyn Shapiro","Jonathan Tu","Henry Kvinge","Davis Brown"],"pdf_url":"https://arxiv.org/pdf/2310.03149v3.pdf","comment":"ATTRIB Workshop at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2309.09866v3","updated":"2023-12-12T22:37:06Z","published":"2023-09-18T15:28:09Z","title":"Domain Generalization with Fourier Transform and Soft Thresholding","summary":"  Domain generalization aims to train models on multiple source domains so that\nthey can generalize well to unseen target domains. Among many domain\ngeneralization methods, Fourier-transform-based domain generalization methods\nhave gained popularity primarily because they exploit the power of Fourier\ntransformation to capture essential patterns and regularities in the data,\nmaking the model more robust to domain shifts. The mainstream\nFourier-transform-based domain generalization swaps the Fourier amplitude\nspectrum while preserving the phase spectrum between the source and the target\nimages. However, it neglects background interference in the amplitude spectrum.\nTo overcome this limitation, we introduce a soft-thresholding function in the\nFourier domain. We apply this newly designed algorithm to retinal fundus image\nsegmentation, which is important for diagnosing ocular diseases but the neural\nnetwork's performance can degrade across different sources due to domain\nshifts. The proposed technique basically enhances fundus image augmentation by\neliminating small values in the Fourier domain and providing better\ngeneralization. The innovative nature of the soft thresholding fused with\nFourier-transform-based domain generalization improves neural network models'\nperformance by reducing the target images' background interference\nsignificantly. Experiments on public data validate our approach's effectiveness\nover conventional and state-of-the-art methods with superior segmentation\nmetrics.\n","authors":["Hongyi Pan","Bin Wang","Zheyuan Zhang","Xin Zhu","Debesh Jha","Ahmet Enis Cetin","Concetto Spampinato","Ulas Bagci"],"pdf_url":"https://arxiv.org/pdf/2309.09866v3.pdf","comment":"This paper was accepted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.00429v3","updated":"2023-12-12T22:34:13Z","published":"2023-09-30T16:41:04Z","title":"On the Stability of Iterative Retraining of Generative Models on their\n  own Data","summary":"  Deep generative models have made tremendous progress in modeling complex\ndata, often exhibiting generation quality that surpasses a typical human's\nability to discern the authenticity of samples. Undeniably, a key driver of\nthis success is enabled by the massive amounts of web-scale data consumed by\nthese models. Due to these models' striking performance and ease of\navailability, the web will inevitably be increasingly populated with synthetic\ncontent. Such a fact directly implies that future iterations of generative\nmodels must contend with the reality that their training is curated from both\nclean data and artificially generated data from past models. In this paper, we\ndevelop a framework to rigorously study the impact of training generative\nmodels on mixed datasets (of real and synthetic data) on their stability. We\nfirst prove the stability of iterative training under the condition that the\ninitial generative models approximate the data distribution well enough and the\nproportion of clean training data (w.r.t. synthetic data) is large enough. We\nempirically validate our theory on both synthetic and natural images by\niteratively training normalizing flows and state-of-the-art diffusion models on\nCIFAR10 and FFHQ.\n","authors":["Quentin Bertrand","Avishek Joey Bose","Alexandre Duplessis","Marco Jiralerspong","Gauthier Gidel"],"pdf_url":"https://arxiv.org/pdf/2310.00429v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07769v1","updated":"2023-12-12T22:27:29Z","published":"2023-12-12T22:27:29Z","title":"Incremental hierarchical text clustering methods: a review","summary":"  The growth in Internet usage has contributed to a large volume of\ncontinuously available data, and has created the need for automatic and\nefficient organization of the data. In this context, text clustering techniques\nare significant because they aim to organize documents according to their\ncharacteristics. More specifically, hierarchical and incremental clustering\ntechniques can organize dynamic data in a hierarchical form, thus guaranteeing\nthat this organization is updated and its exploration is facilitated. Based on\nthe relevance and contemporary nature of the field, this study aims to analyze\nvarious hierarchical and incremental clustering techniques; the main\ncontribution of this research is the organization and comparison of the\ntechniques used by studies published between 2010 and 2018 that aimed to texts\ndocuments clustering. We describe the principal concepts related to the\nchallenge and the different characteristics of these published works in order\nto provide a better understanding of the research in this field.\n","authors":["Fernando Simeone","Maik Olher Chaves","Ahmed Esmin"],"pdf_url":"https://arxiv.org/pdf/2312.07769v1.pdf","comment":"26 pages, 2 figures"},{"id":"http://arxiv.org/abs/2301.10814v2","updated":"2023-12-12T22:17:35Z","published":"2023-01-25T20:33:51Z","title":"Unsupervised Protein-Ligand Binding Energy Prediction via Neural Euler's\n  Rotation Equation","summary":"  Protein-ligand binding prediction is a fundamental problem in AI-driven drug\ndiscovery. Prior work focused on supervised learning methods using a large set\nof binding affinity data for small molecules, but it is hard to apply the same\nstrategy to other drug classes like antibodies as labelled data is limited. In\nthis paper, we explore unsupervised approaches and reformulate binding energy\nprediction as a generative modeling task. Specifically, we train an\nenergy-based model on a set of unlabelled protein-ligand complexes using SE(3)\ndenoising score matching and interpret its log-likelihood as binding affinity.\nOur key contribution is a new equivariant rotation prediction network called\nNeural Euler's Rotation Equations (NERE) for SE(3) score matching. It predicts\na rotation by modeling the force and torque between protein and ligand atoms,\nwhere the force is defined as the gradient of an energy function with respect\nto atom coordinates. We evaluate NERE on protein-ligand and antibody-antigen\nbinding affinity prediction benchmarks. Our model outperforms all unsupervised\nbaselines (physics-based and statistical potentials) and matches supervised\nlearning methods in the antibody case.\n","authors":["Wengong Jin","Siranush Sarkizova","Xun Chen","Nir Hacohen","Caroline Uhler"],"pdf_url":"https://arxiv.org/pdf/2301.10814v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2008.07324v4","updated":"2023-12-12T22:14:43Z","published":"2020-08-13T15:47:04Z","title":"Intelligence Primer","summary":"  Intelligence is a fundamental part of all living things, as well as the\nfoundation for Artificial Intelligence. In this primer we explore the ideas\nassociated with intelligence and, by doing so, understand the implications and\nconstraints and potentially outline the capabilities of future systems.\nArtificial Intelligence, in the form of Machine Learning, has already had a\nsignificant impact on our lives. As an exploration, we journey into different\nparts of intelligence that appear essential. We hope that people find this\nhelpful in determining the future. Also, during the exploration, we hope to\ncreate new thought-provoking questions. Intelligence is not a single weighable\nquantity but a subject that spans Biology, Physics, Philosophy, Cognitive\nScience, Neuroscience, Psychology, and Computer Science. The historian Yuval\nNoah Harari pointed out that engineers and scientists in the future will have\nto broaden their understandings to include disciplines such as Psychology,\nPhilosophy, and Ethics. Fiction writers have long portrayed engineers and\nscientists as deficient in these areas. Today, in modern society, the emergence\nof Artificial Intelligence and legal requirements act as forcing functions to\npush these broader subjects into the foreground. We start with an introduction\nto intelligence and move quickly to more profound thoughts and ideas. We call\nthis a Life, the Universe, and Everything primer, after the famous science\nfiction book by Douglas Adams. Forty-two may be the correct answer, but what\nare the questions?\n","authors":["Karl Fezer","Andrew Sloss"],"pdf_url":"https://arxiv.org/pdf/2008.07324v4.pdf","comment":"18 pages, 12 Figures"},{"id":"http://arxiv.org/abs/2310.05718v2","updated":"2023-12-12T22:13:57Z","published":"2023-10-09T13:39:26Z","title":"EdVAE: Mitigating Codebook Collapse with Evidential Discrete Variational\n  Autoencoders","summary":"  Codebook collapse is a common problem in training deep generative models with\ndiscrete representation spaces like Vector Quantized Variational Autoencoders\n(VQ-VAEs). We observe that the same problem arises for the alternatively\ndesigned discrete variational autoencoders (dVAEs) whose encoder directly\nlearns a distribution over the codebook embeddings to represent the data. We\nhypothesize that using the softmax function to obtain a probability\ndistribution causes the codebook collapse by assigning overconfident\nprobabilities to the best matching codebook elements. In this paper, we propose\na novel way to incorporate evidential deep learning (EDL) instead of softmax to\ncombat the codebook collapse problem of dVAE. We evidentially monitor the\nsignificance of attaining the probability distribution over the codebook\nembeddings, in contrast to softmax usage. Our experiments using various\ndatasets show that our model, called EdVAE, mitigates codebook collapse while\nimproving the reconstruction performance, and enhances the codebook usage\ncompared to dVAE and VQ-VAE based models. Our code can be found at\nhttps://github.com/ituvisionlab/EdVAE .\n","authors":["Gulcin Baykal","Melih Kandemir","Gozde Unal"],"pdf_url":"https://arxiv.org/pdf/2310.05718v2.pdf","comment":"submitted to Pattern Recognition"},{"id":"http://arxiv.org/abs/2312.07762v1","updated":"2023-12-12T22:10:38Z","published":"2023-12-12T22:10:38Z","title":"Interpretable factorization of clinical questionnaires to identify\n  latent factors of psychopathology","summary":"  Psychiatry research seeks to understand the manifestations of psychopathology\nin behavior, as measured in questionnaire data, by identifying a small number\nof latent factors that explain them. While factor analysis is the traditional\ntool for this purpose, the resulting factors may not be interpretable, and may\nalso be subject to confounding variables. Moreover, missing data are common,\nand explicit imputation is often required. To overcome these limitations, we\nintroduce interpretability constrained questionnaire factorization (ICQF), a\nnon-negative matrix factorization method with regularization tailored for\nquestionnaire data. Our method aims to promote factor interpretability and\nsolution stability. We provide an optimization procedure with theoretical\nconvergence guarantees, and an automated procedure to detect latent\ndimensionality accurately. We validate these procedures using realistic\nsynthetic data. We demonstrate the effectiveness of our method in a widely used\ngeneral-purpose questionnaire, in two independent datasets (the Healthy Brain\nNetwork and Adolescent Brain Cognitive Development studies). Specifically, we\nshow that ICQF improves interpretability, as defined by domain experts, while\npreserving diagnostic information across a range of disorders, and outperforms\ncompeting methods for smaller dataset sizes. This suggests that the\nregularization in our method matches domain characteristics. The python\nimplementation for ICQF is available at\n\\url{https://github.com/jefferykclam/ICQF}.\n","authors":["Ka Chun Lam","Bridget W Mahony","Armin Raznahan","Francisco Pereira"],"pdf_url":"https://arxiv.org/pdf/2312.07762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07759v1","updated":"2023-12-12T22:02:57Z","published":"2023-12-12T22:02:57Z","title":"IDKM: Memory Efficient Neural Network Quantization via Implicit,\n  Differentiable $k$-Means","summary":"  Compressing large neural networks with minimal performance loss is crucial to\nenabling their deployment on edge devices. (Cho et al., 2022) proposed a weight\nquantization method that uses an attention-based clustering algorithm called\ndifferentiable $k$-means (DKM). Despite achieving state-of-the-art results,\nDKM's performance is constrained by its heavy memory dependency. We propose an\nimplicit, differentiable $k$-means algorithm (IDKM), which eliminates the major\nmemory restriction of DKM. Let $t$ be the number of $k$-means iterations, $m$\nbe the number of weight-vectors, and $b$ be the number of bits per cluster\naddress. IDKM reduces the overall memory complexity of a single $k$-means layer\nfrom $\\mathcal{O}(t \\cdot m \\cdot 2^b)$ to $\\mathcal{O}( m \\cdot 2^b)$. We also\nintroduce a variant, IDKM with Jacobian-Free-Backpropagation (IDKM-JFB), for\nwhich the time complexity of the gradient calculation is independent of $t$ as\nwell. We provide a proof of concept of our methods by showing that, under the\nsame settings, IDKM achieves comparable performance to DKM with less compute\ntime and less memory. We also use IDKM and IDKM-JFB to quantize a large neural\nnetwork, Resnet18, on hardware where DKM cannot train at all.\n","authors":["Sean Jaffe","Ambuj K. Singh","Francesco Bullo"],"pdf_url":"https://arxiv.org/pdf/2312.07759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.10697v2","updated":"2023-12-12T21:47:08Z","published":"2023-05-18T04:18:59Z","title":"The Blessing of Heterogeneity in Federated Q-Learning: Linear Speedup\n  and Beyond","summary":"  When the data used for reinforcement learning (RL) are collected by multiple\nagents in a distributed manner, federated versions of RL algorithms allow\ncollaborative learning without the need for agents to share their local data.\nIn this paper, we consider federated Q-learning, which aims to learn an optimal\nQ-function by periodically aggregating local Q-estimates trained on local data\nalone. Focusing on infinite-horizon tabular Markov decision processes, we\nprovide sample complexity guarantees for both the synchronous and asynchronous\nvariants of federated Q-learning. In both cases, our bounds exhibit a linear\nspeedup with respect to the number of agents and near-optimal dependencies on\nother salient problem parameters.\n  In the asynchronous setting, existing analyses of federated Q-learning, which\nadopt an equally weighted averaging of local Q-estimates, require that every\nagent covers the entire state-action space. In contrast, our improved sample\ncomplexity scales inverse proportionally to the minimum entry of the average\nstationary state-action occupancy distribution of all agents, thus only\nrequiring the agents to collectively cover the entire state-action space,\nunveiling the blessing of heterogeneity in enabling collaborative learning by\nrelaxing the coverage requirement of the single-agent case. However, its sample\ncomplexity still suffers when the local trajectories are highly heterogeneous.\nIn response, we propose a novel federated Q-learning algorithm with importance\naveraging, giving larger weights to more frequently visited state-action pairs,\nwhich achieves a robust linear speedup as if all trajectories are centrally\nprocessed, regardless of the heterogeneity of local behavior policies.\n","authors":["Jiin Woo","Gauri Joshi","Yuejie Chi"],"pdf_url":"https://arxiv.org/pdf/2305.10697v2.pdf","comment":"Short version at ICML 2023"}],"Multimedia":[{"id":"http://arxiv.org/abs/2312.07294v1","updated":"2023-12-12T14:12:05Z","published":"2023-12-12T14:12:05Z","title":"Probing Commonsense Reasoning Capability of Text-to-Image Generative\n  Models via Non-visual Description","summary":"  Commonsense reasoning, the ability to make logical assumptions about daily\nscenes, is one core intelligence of human beings. In this work, we present a\nnovel task and dataset for evaluating the ability of text-to-image generative\nmodels to conduct commonsense reasoning, which we call PAINTaboo. Given a\ndescription with few visual clues of one object, the goal is to generate images\nillustrating the object correctly. The dataset was carefully hand-curated and\ncovered diverse object categories to analyze model performance comprehensively.\nOur investigation of several prevalent text-to-image generative models reveals\nthat these models are not proficient in commonsense reasoning, as anticipated.\nWe trust that PAINTaboo can improve our understanding of the reasoning\nabilities of text-to-image generative models.\n","authors":["Mianzhi Pan","Jianfei Li","Mingyue Yu","Zheng Ma","Kanzhi Cheng","Jianbing Zhang","Jiajun Chen"],"pdf_url":"https://arxiv.org/pdf/2312.07294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07212v1","updated":"2023-12-12T12:22:36Z","published":"2023-12-12T12:22:36Z","title":"More than Vanilla Fusion: a Simple, Decoupling-free, Attention Module\n  for Multimodal Fusion Based on Signal Theory","summary":"  The vanilla fusion methods still dominate a large percentage of mainstream\naudio-visual tasks. However, the effectiveness of vanilla fusion from a\ntheoretical perspective is still worth discussing. Thus, this paper reconsiders\nthe signal fused in the multimodal case from a bionics perspective and proposes\na simple, plug-and-play, attention module for vanilla fusion based on\nfundamental signal theory and uncertainty theory. In addition, previous work on\nmultimodal dynamic gradient modulation still relies on decoupling the\nmodalities. So, a decoupling-free gradient modulation scheme has been designed\nin conjunction with the aforementioned attention module, which has various\nadvantages over the decoupled one. Experiment results show that just a few\nlines of code can achieve up to 2.0% performance improvements to several\nmultimodal classification methods. Finally, quantitative evaluation of other\nfusion tasks reveals the potential for additional application scenarios.\n","authors":["Peiwen Sun","Yifan Zhang","Zishan Liu","Donghao Chen","Honggang Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.07212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16318v2","updated":"2023-12-12T10:42:46Z","published":"2023-05-25T17:59:47Z","title":"Referred by Multi-Modality: A Unified Temporal Transformer for Video\n  Object Segmentation","summary":"  Recently, video object segmentation (VOS) referred by multi-modal signals,\ne.g., language and audio, has evoked increasing attention in both industry and\nacademia. It is challenging for exploring the semantic alignment within\nmodalities and the visual correspondence across frames. However, existing\nmethods adopt separate network architectures for different modalities, and\nneglect the inter-frame temporal interaction with references. In this paper, we\npropose MUTR, a Multi-modal Unified Temporal transformer for Referring video\nobject segmentation. With a unified framework for the first time, MUTR adopts a\nDETR-style transformer and is capable of segmenting video objects designated by\neither text or audio reference. Specifically, we introduce two strategies to\nfully explore the temporal relations between videos and multi-modal signals.\nFirstly, for low-level temporal aggregation before the transformer, we enable\nthe multi-modal references to capture multi-scale visual cues from consecutive\nvideo frames. This effectively endows the text or audio signals with temporal\nknowledge and boosts the semantic alignment between modalities. Secondly, for\nhigh-level temporal interaction after the transformer, we conduct inter-frame\nfeature communication for different object embeddings, contributing to better\nobject-wise correspondence for tracking along the video. On Ref-YouTube-VOS and\nAVSBench datasets with respective text and audio references, MUTR achieves\n+4.2% and +8.7% J&F improvements to state-of-the-art methods, demonstrating our\nsignificance for unified multi-modal VOS. Code is released at\nhttps://github.com/OpenGVLab/MUTR.\n","authors":["Shilin Yan","Renrui Zhang","Ziyu Guo","Wenchao Chen","Wei Zhang","Hongyang Li","Yu Qiao","Hao Dong","Zhongjiang He","Peng Gao"],"pdf_url":"https://arxiv.org/pdf/2305.16318v2.pdf","comment":"Accepted by AAAI 2024. Code is released at\n  https://github.com/OpenGVLab/MUTR"},{"id":"http://arxiv.org/abs/2312.07132v1","updated":"2023-12-12T10:07:16Z","published":"2023-12-12T10:07:16Z","title":"Image Content Generation with Causal Reasoning","summary":"  The emergence of ChatGPT has once again sparked research in generative\nartificial intelligence (GAI). While people have been amazed by the generated\nresults, they have also noticed the reasoning potential reflected in the\ngenerated textual content. However, this current ability for causal reasoning\nis primarily limited to the domain of language generation, such as in models\nlike GPT-3. In visual modality, there is currently no equivalent research.\nConsidering causal reasoning in visual content generation is significant. This\nis because visual information contains infinite granularity. Particularly,\nimages can provide more intuitive and specific demonstrations for certain\nreasoning tasks, especially when compared to coarse-grained text. Hence, we\npropose a new image generation task called visual question answering with image\n(VQAI) and establish a dataset of the same name based on the classic\n\\textit{Tom and Jerry} animated series. Additionally, we develop a new paradigm\nfor image generation to tackle the challenges of this task. Finally, we perform\nextensive experiments and analyses, including visualizations of the generated\ncontent and discussions on the potentials and limitations. The code and data\nare publicly available under the license of CC BY-NC-SA 4.0 for academic and\nnon-commercial usage. The code and dataset are publicly available at:\nhttps://github.com/IEIT-AGI/MIX-Shannon/blob/main/projects/VQAI/lgd_vqai.md.\n","authors":["Xiaochuan Li","Baoyu Fan","Runze Zhang","Liang Jin","Di Wang","Zhenhua Guo","Yaqian Zhao","Rengang Li"],"pdf_url":"https://arxiv.org/pdf/2312.07132v1.pdf","comment":"Accepted by the 38th Annual AAAI Conference on Artificial\n  Intelligence (AAAI 2024) in December 2023"},{"id":"http://arxiv.org/abs/2312.07661v1","updated":"2023-12-12T19:00:04Z","published":"2023-12-12T19:00:04Z","title":"CLIP as RNN: Segment Countless Visual Concepts without Training Endeavor","summary":"  Existing open-vocabulary image segmentation methods require a fine-tuning\nstep on mask annotations and/or image-text datasets. Mask labels are\nlabor-intensive, which limits the number of categories in segmentation\ndatasets. As a result, the open-vocabulary capacity of pre-trained VLMs is\nseverely reduced after fine-tuning. However, without fine-tuning, VLMs trained\nunder weak image-text supervision tend to make suboptimal mask predictions when\nthere are text queries referring to non-existing concepts in the image. To\nalleviate these issues, we introduce a novel recurrent framework that\nprogressively filters out irrelevant texts and enhances mask quality without\ntraining efforts. The recurrent unit is a two-stage segmenter built upon a VLM\nwith frozen weights. Thus, our model retains the VLM's broad vocabulary space\nand strengthens its segmentation capability. Experimental results show that our\nmethod outperforms not only the training-free counterparts, but also those\nfine-tuned with millions of additional data samples, and sets new\nstate-of-the-art records for both zero-shot semantic and referring image\nsegmentation tasks. Specifically, we improve the current record by 28.8, 16.0,\nand 6.9 mIoU on Pascal VOC, COCO Object, and Pascal Context.\n","authors":["Shuyang Sun","Runjia Li","Philip Torr","Xiuye Gu","Siyang Li"],"pdf_url":"https://arxiv.org/pdf/2312.07661v1.pdf","comment":"Project page: https://torrvision.com/clip_as_rnn/"}]},"2023-12-13T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2305.18290v2","updated":"2023-12-13T18:48:48Z","published":"2023-05-29T17:57:46Z","title":"Direct Preference Optimization: Your Language Model is Secretly a Reward\n  Model","summary":"  While large-scale unsupervised language models (LMs) learn broad world\nknowledge and some reasoning skills, achieving precise control of their\nbehavior is difficult due to the completely unsupervised nature of their\ntraining. Existing methods for gaining such steerability collect human labels\nof the relative quality of model generations and fine-tune the unsupervised LM\nto align with these preferences, often with reinforcement learning from human\nfeedback (RLHF). However, RLHF is a complex and often unstable procedure, first\nfitting a reward model that reflects the human preferences, and then\nfine-tuning the large unsupervised LM using reinforcement learning to maximize\nthis estimated reward without drifting too far from the original model. In this\npaper we introduce a new parameterization of the reward model in RLHF that\nenables extraction of the corresponding optimal policy in closed form, allowing\nus to solve the standard RLHF problem with only a simple classification loss.\nThe resulting algorithm, which we call Direct Preference Optimization (DPO), is\nstable, performant, and computationally lightweight, eliminating the need for\nsampling from the LM during fine-tuning or performing significant\nhyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align\nwith human preferences as well as or better than existing methods. Notably,\nfine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of\ngenerations, and matches or improves response quality in summarization and\nsingle-turn dialogue while being substantially simpler to implement and train.\n","authors":["Rafael Rafailov","Archit Sharma","Eric Mitchell","Stefano Ermon","Christopher D. Manning","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2305.18290v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.03061v2","updated":"2023-12-13T18:18:38Z","published":"2023-06-05T17:32:35Z","title":"Structured Voronoi Sampling","summary":"  Gradient-based sampling algorithms have demonstrated their effectiveness in\ntext generation, especially in the context of controlled text generation.\nHowever, there exists a lack of theoretically grounded and principled\napproaches for this task. In this paper, we take an important step toward\nbuilding a principled approach for sampling from language models with\ngradient-based methods. We use discrete distributions given by language models\nto define densities and develop an algorithm based on Hamiltonian Monte Carlo\nto sample from them. We name our gradient-based technique Structured Voronoi\nSampling (SVS). In an experimental setup where the reference distribution is\nknown, we show that the empirical distribution of SVS samples is closer to the\nreference distribution compared to alternative sampling schemes. Furthermore,\nin a controlled generation task, SVS is able to generate fluent and diverse\nsamples while following the control targets significantly better than other\nmethods.\n","authors":["Afra Amini","Li Du","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2306.03061v2.pdf","comment":"Accepted at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.05608v2","updated":"2023-12-13T17:54:16Z","published":"2023-11-09T18:59:11Z","title":"FigStep: Jailbreaking Large Vision-language Models via Typographic\n  Visual Prompts","summary":"  Ensuring the safety of artificial intelligence-generated content (AIGC) is a\nlongstanding topic in the artificial intelligence (AI) community, and the\nsafety concerns associated with Large Language Models (LLMs) have been widely\ninvestigated. Recently, large vision-language models (VLMs) represent an\nunprecedented revolution, as they are built upon LLMs but can incorporate\nadditional modalities (e.g., images). However, the safety of VLMs lacks\nsystematic evaluation, and there may be an overconfidence in the safety\nguarantees provided by their underlying LLMs. In this paper, to demonstrate\nthat introducing additional modality modules leads to unforeseen AI safety\nissues, we propose FigStep, a straightforward yet effective jailbreaking\nalgorithm against VLMs. Instead of feeding textual harmful instructions\ndirectly, FigStep converts the harmful content into images through typography\nto bypass the safety alignment within the textual module of the VLMs, inducing\nVLMs to output unsafe responses that violate common AI safety policies. In our\nevaluation, we manually review 46,500 model responses generated by 3 families\nof the promising open-source VLMs, i.e., LLaVA, MiniGPT4, and CogVLM (a total\nof 6 VLMs). The experimental results show that FigStep can achieve an average\nattack success rate of 82.50% on 500 harmful queries in 10 topics. Moreover, we\ndemonstrate that the methodology of FigStep can even jailbreak GPT-4V, which\nalready leverages an OCR detector to filter harmful queries. Above all, our\nwork reveals that VLMs are vulnerable to jailbreaking attacks, which highlights\nthe necessity of novel safety alignments between visual and textual modalities.\n","authors":["Yichen Gong","Delong Ran","Jinyuan Liu","Conglei Wang","Tianshuo Cong","Anyu Wang","Sisi Duan","Xiaoyun Wang"],"pdf_url":"https://arxiv.org/pdf/2311.05608v2.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2310.10594v2","updated":"2023-12-13T17:29:15Z","published":"2023-10-16T17:16:32Z","title":"Motion2Language, unsupervised learning of synchronized semantic motion\n  segmentation","summary":"  In this paper, we investigate building a sequence to sequence architecture\nfor motion to language translation and synchronization. The aim is to translate\nmotion capture inputs into English natural-language descriptions, such that the\ndescriptions are generated synchronously with the actions performed, enabling\nsemantic segmentation as a byproduct, but without requiring synchronized\ntraining data. We propose a new recurrent formulation of local attention that\nis suited for synchronous/live text generation, as well as an improved motion\nencoder architecture better suited to smaller data and for synchronous\ngeneration. We evaluate both contributions in individual experiments, using the\nstandard BLEU4 metric, as well as a simple semantic equivalence measure, on the\nKIT motion language dataset. In a follow-up experiment, we assess the quality\nof the synchronization of generated text in our proposed approaches through\nmultiple evaluation metrics. We find that both contributions to the attention\nmechanism and the encoder architecture additively improve the quality of\ngenerated text (BLEU and semantic equivalence), but also of synchronization.\nOur code is available at\nhttps://github.com/rd20karim/M2T-Segmentation/tree/main\n","authors":["Karim Radouane","Andon Tchechmedjiev","Julien Lagarde","Sylvie Ranwez"],"pdf_url":"https://arxiv.org/pdf/2310.10594v2.pdf","comment":"Published at Neural Computing and Applications"},{"id":"http://arxiv.org/abs/2312.08303v1","updated":"2023-12-13T17:22:19Z","published":"2023-12-13T17:22:19Z","title":"Efficient Toxic Content Detection by Bootstrapping and Distilling Large\n  Language Models","summary":"  Toxic content detection is crucial for online services to remove\ninappropriate content that violates community standards. To automate the\ndetection process, prior works have proposed varieties of machine learning (ML)\napproaches to train Language Models (LMs) for toxic content detection. However,\nboth their accuracy and transferability across datasets are limited. Recently,\nLarge Language Models (LLMs) have shown promise in toxic content detection due\nto their superior zero-shot and few-shot in-context learning ability as well as\nbroad transferability on ML tasks. However, efficiently designing prompts for\nLLMs remains challenging. Moreover, the high run-time cost of LLMs may hinder\ntheir deployments in production. To address these challenges, in this work, we\npropose BD-LLM, a novel and efficient approach to Bootstrapping and Distilling\nLLMs for toxic content detection. Specifically, we design a novel prompting\nmethod named Decision-Tree-of-Thought (DToT) to bootstrap LLMs' detection\nperformance and extract high-quality rationales. DToT can automatically select\nmore fine-grained context to re-prompt LLMs when their responses lack\nconfidence. Additionally, we use the rationales extracted via DToT to fine-tune\nstudent LMs. Our experimental results on various datasets demonstrate that DToT\ncan improve the accuracy of LLMs by up to 4.6%. Furthermore, student LMs\nfine-tuned with rationales extracted via DToT outperform baselines on all\ndatasets with up to 16.9\\% accuracy improvement, while being more than 60x\nsmaller than conventional LLMs. Finally, we observe that student LMs fine-tuned\nwith rationales exhibit better cross-dataset transferability.\n","authors":["Jiang Zhang","Qiong Wu","Yiming Xu","Cheng Cao","Zheng Du","Konstantinos Psounis"],"pdf_url":"https://arxiv.org/pdf/2312.08303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08299v1","updated":"2023-12-13T17:15:12Z","published":"2023-12-13T17:15:12Z","title":"Conceptualizing Suicidal Behavior: Utilizing Explanations of Predicted\n  Outcomes to Analyze Longitudinal Social Media Data","summary":"  The COVID-19 pandemic has escalated mental health crises worldwide, with\nsocial isolation and economic instability contributing to a rise in suicidal\nbehavior. Suicide can result from social factors such as shame, abuse,\nabandonment, and mental health conditions like depression, Post-Traumatic\nStress Disorder (PTSD), Attention-Deficit/Hyperactivity Disorder (ADHD),\nanxiety disorders, and bipolar disorders. As these conditions develop, signs of\nsuicidal ideation may manifest in social media interactions. Analyzing social\nmedia data using artificial intelligence (AI) techniques can help identify\npatterns of suicidal behavior, providing invaluable insights for suicide\nprevention agencies, professionals, and broader community awareness\ninitiatives. Machine learning algorithms for this purpose require large volumes\nof accurately labeled data. Previous research has not fully explored the\npotential of incorporating explanations in analyzing and labeling longitudinal\nsocial media data. In this study, we employed a model explanation method, Layer\nIntegrated Gradients, on top of a fine-tuned state-of-the-art language model,\nto assign each token from Reddit users' posts an attribution score for\npredicting suicidal ideation. By extracting and analyzing attributions of\ntokens from the data, we propose a methodology for preliminary screening of\nsocial media posts for suicidal ideation without using large language models\nduring inference.\n","authors":["Van Minh Nguyen","Nasheen Nur","William Stern","Thomas Mercer","Chiradeep Sen","Siddhartha Bhattacharyya","Victor Tumbiolo","Seng Jhing Goh"],"pdf_url":"https://arxiv.org/pdf/2312.08299v1.pdf","comment":"Accepted for ICMLA 2023"},{"id":"http://arxiv.org/abs/2312.08282v1","updated":"2023-12-13T16:57:31Z","published":"2023-12-13T16:57:31Z","title":"Prompting LLMs with content plans to enhance the summarization of\n  scientific articles","summary":"  This paper presents novel prompting techniques to improve the performance of\nautomatic summarization systems for scientific articles. Scientific article\nsummarization is highly challenging due to the length and complexity of these\ndocuments. We conceive, implement, and evaluate prompting techniques that\nprovide additional contextual information to guide summarization systems.\nSpecifically, we feed summarizers with lists of key terms extracted from\narticles, such as author keywords or automatically generated keywords. Our\ntechniques are tested with various summarization models and input texts.\nResults show performance gains, especially for smaller models summarizing\nsections separately. This evidences that prompting is a promising approach to\novercoming the limitations of less powerful systems. Our findings introduce a\nnew research direction of using prompts to aid smaller models.\n","authors":["Aldan Creo","Manuel Lama","Juan C. Vidal"],"pdf_url":"https://arxiv.org/pdf/2312.08282v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2312.08274v1","updated":"2023-12-13T16:43:41Z","published":"2023-12-13T16:43:41Z","title":"High-throughput Biomedical Relation Extraction for Semi-Structured Web\n  Articles Empowered by Large Language Models","summary":"  Objective: To develop a high-throughput biomedical relation extraction system\nthat takes advantage of the large language models' (LLMs) reading comprehension\nability and biomedical world knowledge in a scalable and evidential manner.\nMethods: We formulate the relation extraction task as a simple binary\nclassification problem for large language models such as ChatGPT. Specifically,\nLLMs make the decision based on the external corpus and its world knowledge,\ngiving the reason for the judgment to factual verification. This method is\ntailored for semi-structured web articles, wherein we designate the main title\nas the tail entity and explicitly incorporate it into the context, and the\npotential head entities are matched based on a biomedical thesaurus. Moreover,\nlengthy contents are sliced into text chunks, embedded, and retrieved with\nadditional embedding models, ensuring compatibility with the context window\nsize constraints of available open-source LLMs. Results: Using an open-source\nLLM, we extracted 304315 relation triplets of three distinct relation types\nfrom four reputable biomedical websites. To assess the efficacy of the basic\npipeline employed for biomedical relation extraction, we curated a benchmark\ndataset annotated by a medical expert. Evaluation results indicate that the\npipeline exhibits performance comparable to that of GPT-4. Case studies further\nilluminate challenges faced by contemporary LLMs in the context of biomedical\nrelation extraction for semi-structured web articles. Conclusion: The proposed\nmethod has demonstrated its effectiveness in leveraging the strengths of LLMs\nfor high-throughput biomedical relation extraction. Its adaptability is\nevident, as it can be seamlessly extended to diverse semi-structured biomedical\nwebsites, facilitating the extraction of various types of biomedical relations\nwith ease.\n","authors":["Songchi Zhou","Sheng Yu"],"pdf_url":"https://arxiv.org/pdf/2312.08274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.09512v3","updated":"2023-12-13T16:27:16Z","published":"2022-12-19T14:48:08Z","title":"Rethinking Label Smoothing on Multi-hop Question Answering","summary":"  Multi-Hop Question Answering (MHQA) is a significant area in question\nanswering, requiring multiple reasoning components, including document\nretrieval, supporting sentence prediction, and answer span extraction. In this\nwork, we analyze the primary factors limiting the performance of multi-hop\nreasoning and introduce label smoothing into the MHQA task. This is aimed at\nenhancing the generalization capabilities of MHQA systems and mitigating\noverfitting of answer spans and reasoning paths in training set. We propose a\nnovel label smoothing technique, F1 Smoothing, which incorporates uncertainty\ninto the learning process and is specifically tailored for Machine Reading\nComprehension (MRC) tasks. Inspired by the principles of curriculum learning,\nwe introduce the Linear Decay Label Smoothing Algorithm (LDLA), which\nprogressively reduces uncertainty throughout the training process. Experiment\non the HotpotQA dataset demonstrates the effectiveness of our methods in\nenhancing performance and generalizability in multi-hop reasoning, achieving\nnew state-of-the-art results on the leaderboard.\n","authors":["Zhangyue Yin","Yuxin Wang","Xiannian Hu","Yiguang Wu","Hang Yan","Xinyu Zhang","Zhao Cao","Xuanjing Huang","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2212.09512v3.pdf","comment":"13 pages, 8 figures, accepted by CCL2023"},{"id":"http://arxiv.org/abs/2312.08198v1","updated":"2023-12-13T15:03:27Z","published":"2023-12-13T15:03:27Z","title":"Towards Model-Based Data Acquisition for Subjective Multi-Task NLP\n  Problems","summary":"  Data annotated by humans is a source of knowledge by describing the\npeculiarities of the problem and therefore fueling the decision process of the\ntrained model. Unfortunately, the annotation process for subjective natural\nlanguage processing (NLP) problems like offensiveness or emotion detection is\noften very expensive and time-consuming. One of the inevitable risks is to\nspend some of the funds and annotator effort on annotations that do not provide\nany additional knowledge about the specific task. To minimize these costs, we\npropose a new model-based approach that allows the selection of tasks annotated\nindividually for each text in a multi-task scenario. The experiments carried\nout on three datasets, dozens of NLP tasks, and thousands of annotations show\nthat our method allows up to 40% reduction in the number of annotations with\nnegligible loss of knowledge. The results also emphasize the need to collect a\ndiverse amount of data required to efficiently train a model, depending on the\nsubjectivity of the annotation task. We also focused on measuring the relation\nbetween subjective tasks by evaluating the model in single-task and multi-task\nscenarios. Moreover, for some datasets, training only on the labels predicted\nby our model improved the efficiency of task selection as a self-supervised\nlearning regularization technique.\n","authors":["Kamil Kanclerz","Julita Bielaniewicz","Marcin Gruza","Jan Kocon","Stanisław Woźniak","Przemysław Kazienko"],"pdf_url":"https://arxiv.org/pdf/2312.08198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.06161v2","updated":"2023-12-13T14:44:10Z","published":"2023-05-09T08:16:42Z","title":"StarCoder: may the source be with you!","summary":"  The BigCode community, an open-scientific collaboration working on the\nresponsible development of Large Language Models for Code (Code LLMs),\nintroduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context\nlength, infilling capabilities and fast large-batch inference enabled by\nmulti-query attention. StarCoderBase is trained on 1 trillion tokens sourced\nfrom The Stack, a large collection of permissively licensed GitHub repositories\nwith inspection tools and an opt-out process. We fine-tuned StarCoderBase on\n35B Python tokens, resulting in the creation of StarCoder. We perform the most\ncomprehensive evaluation of Code LLMs to date and show that StarCoderBase\noutperforms every open Code LLM that supports multiple programming languages\nand matches or outperforms the OpenAI code-cushman-001 model. Furthermore,\nStarCoder outperforms every model that is fine-tuned on Python, can be prompted\nto achieve 40\\% pass@1 on HumanEval, and still retains its performance on other\nprogramming languages. We take several important steps towards a safe\nopen-access model release, including an improved PII redaction pipeline and a\nnovel attribution tracing tool, and make the StarCoder models publicly\navailable under a more commercially viable version of the Open Responsible AI\nModel license.\n","authors":["Raymond Li","Loubna Ben Allal","Yangtian Zi","Niklas Muennighoff","Denis Kocetkov","Chenghao Mou","Marc Marone","Christopher Akiki","Jia Li","Jenny Chim","Qian Liu","Evgenii Zheltonozhskii","Terry Yue Zhuo","Thomas Wang","Olivier Dehaene","Mishig Davaadorj","Joel Lamy-Poirier","João Monteiro","Oleh Shliazhko","Nicolas Gontier","Nicholas Meade","Armel Zebaze","Ming-Ho Yee","Logesh Kumar Umapathi","Jian Zhu","Benjamin Lipkin","Muhtasham Oblokulov","Zhiruo Wang","Rudra Murthy","Jason Stillerman","Siva Sankalp Patel","Dmitry Abulkhanov","Marco Zocca","Manan Dey","Zhihan Zhang","Nour Fahmy","Urvashi Bhattacharyya","Wenhao Yu","Swayam Singh","Sasha Luccioni","Paulo Villegas","Maxim Kunakov","Fedor Zhdanov","Manuel Romero","Tony Lee","Nadav Timor","Jennifer Ding","Claire Schlesinger","Hailey Schoelkopf","Jan Ebert","Tri Dao","Mayank Mishra","Alex Gu","Jennifer Robinson","Carolyn Jane Anderson","Brendan Dolan-Gavitt","Danish Contractor","Siva Reddy","Daniel Fried","Dzmitry Bahdanau","Yacine Jernite","Carlos Muñoz Ferrandis","Sean Hughes","Thomas Wolf","Arjun Guha","Leandro von Werra","Harm de Vries"],"pdf_url":"https://arxiv.org/pdf/2305.06161v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.11871v2","updated":"2023-12-13T14:39:29Z","published":"2022-09-23T21:41:10Z","title":"Cem Mil Podcasts: A Spoken Portuguese Document Corpus For Multi-modal,\n  Multi-lingual and Multi-Dialect Information Access Research","summary":"  In this paper we describe the Portuguese-language podcast dataset we have\nreleased for academic research purposes. We give an overview of how the data\nwas sampled, descriptive statistics over the collection, as well as information\nabout the distribution over Brazilian and Portuguese dialects. We give results\nfrom experiments on multi-lingual summarization, showing that summarizing\npodcast transcripts can be performed well by a system supporting both English\nand Portuguese. We also show experiments on Portuguese podcast genre\nclassification using text metadata. Combining this collection with previously\nreleased English-language collection opens up the potential for multi-modal,\nmulti-lingual and multi-dialect podcast information access research.\n","authors":["Ekaterina Garmash","Edgar Tanaka","Ann Clifton","Joana Correia","Sharmistha Jat","Winstead Zhu","Rosie Jones","Jussi Karlgren"],"pdf_url":"https://arxiv.org/pdf/2209.11871v2.pdf","comment":"12 pages, 1 figure"},{"id":"http://arxiv.org/abs/2310.01217v2","updated":"2023-12-13T14:09:02Z","published":"2023-10-02T14:01:36Z","title":"ScaLearn: Simple and Highly Parameter-Efficient Task Transfer by\n  Learning to Scale","summary":"  Multi-task learning (MTL) has shown considerable practical benefits,\nparticularly when using pre-trained language models (PLMs). While this is\ncommonly achieved by simultaneously learning $n$ tasks under a joint\noptimization procedure, recent methods such as AdapterFusion structure the\nproblem into two distinct stages: (i) task learning, where knowledge specific\nto a task is encapsulated within sets of parameters (e.g., adapters), and (ii)\ntransfer, where this already learned knowledge is leveraged for a target task.\nThis separation of concerns provides numerous benefits, such as promoting\nreusability, and addressing cases involving data privacy and societal concerns;\non the flip side, current two-stage MTL methods come with the cost of\nintroducing a substantial number of additional parameters. In this work, we\naddress this issue by leveraging the usefulness of linearly scaling the output\nrepresentations of source adapters for transfer learning. We introduce\nScaLearn, a simple and highly parameter-efficient two-stage MTL method that\ncapitalizes on the knowledge of the source tasks by learning a minimal set of\nscaling parameters that enable effective knowledge transfer to a target task.\nOur experiments on three benchmarks (GLUE, SuperGLUE, and HumSet) show that our\nScaLearn, in addition to facilitating the benefits of two-stage MTL,\nconsistently outperforms strong baselines with only a small number of transfer\nparameters - roughly 0.35% of those of AdapterFusion. Remarkably, we observe\nthat ScaLearn maintains its strong abilities even when further reducing\nparameters through uniform scaling and layer-sharing, achieving similarly\ncompetitive results with only $8$ transfer parameters for each target task. Our\nproposed approach thus demonstrates the power of simple scaling as a promise\nfor more efficient task transfer.\n","authors":["Markus Frohmann","Carolin Holtermann","Shahed Masoudian","Anne Lauscher","Navid Rekabsaz"],"pdf_url":"https://arxiv.org/pdf/2310.01217v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.02784v2","updated":"2023-12-13T13:29:29Z","published":"2023-09-06T06:51:15Z","title":"Norm Tweaking: High-performance Low-bit Quantization of Large Language\n  Models","summary":"  As the size of large language models (LLMs) continues to grow, model\ncompression without sacrificing accuracy has become a crucial challenge for\ndeployment. While some quantization methods, such as GPTQ, have made progress\nin achieving acceptable 4-bit weight-only quantization, attempts at lower-bit\nquantization often result in severe performance degradation. In this paper, we\nintroduce a technique called norm tweaking, which can be used as a plugin in\ncurrent PTQ methods to achieve high precision while being cost-efficient. Our\napproach is inspired by the observation that rectifying the quantized\nactivation distribution to match its float counterpart can readily restore\naccuracy for LLMs. To achieve this, we carefully design a tweaking strategy\nthat includes calibration data generation and channel-wise distance constraint\nto update the weights of normalization layers for better generalization. We\nconduct extensive experiments on various datasets using several open-sourced\nLLMs. Our method demonstrates significant improvements in both weight-only\nquantization and joint quantization of weights and activations, surpassing\nexisting PTQ methods. On GLM-130B and OPT-66B, our method even achieves the\nsame level of accuracy at 2-bit quantization as their float ones. Our simple\nand effective approach makes it more practical for real-world applications.\n","authors":["Liang Li","Qingyuan Li","Bo Zhang","Xiangxiang Chu"],"pdf_url":"https://arxiv.org/pdf/2309.02784v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.08742v3","updated":"2023-12-13T12:00:26Z","published":"2023-08-17T02:33:43Z","title":"PMET: Precise Model Editing in a Transformer","summary":"  Model editing techniques modify a minor proportion of knowledge in Large\nLanguage Models (LLMs) at a relatively low cost, which have demonstrated\nnotable success. Existing methods assume Transformer Layer (TL) hidden states\nare values of key-value memories of the Feed-Forward Network (FFN). They\nusually optimize the TL hidden states to memorize target knowledge and use it\nto update the weights of the FFN in LLMs. However, the information flow of TL\nhidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN,\nand residual connections. Existing methods neglect the fact that the TL hidden\nstates contains information not specifically required for FFN. Consequently,\nthe performance of model editing decreases. To achieve more precise model\nediting, we analyze hidden states of MHSA and FFN, finding that MHSA encodes\ncertain general knowledge extraction patterns. This implies that MHSA weights\ndo not require updating when new knowledge is introduced. Based on above\nfindings, we introduce PMET, which simultaneously optimizes Transformer\nComponent (TC, namely MHSA and FFN) hidden states, while only using the\noptimized TC hidden states of FFN to precisely update FFN weights. Our\nexperiments demonstrate that PMET exhibits state-of-the-art performance on both\nthe COUNTERFACT and zsRE datasets. Our ablation experiments substantiate the\neffectiveness of our enhancements, further reinforcing the finding that the\nMHSA encodes certain general knowledge extraction patterns and indicating its\nstorage of a small amount of factual knowledge. Our code is available at\nhttps://github.com/xpq-tech/PMET.\n","authors":["Xiaopeng Li","Shasha Li","Shezheng Song","Jing Yang","Jun Ma","Jie Yu"],"pdf_url":"https://arxiv.org/pdf/2308.08742v3.pdf","comment":"Accepted in AAAI24"},{"id":"http://arxiv.org/abs/2312.08079v1","updated":"2023-12-13T11:49:16Z","published":"2023-12-13T11:49:16Z","title":"Extending Whisper with prompt tuning to target-speaker ASR","summary":"  Target-speaker automatic speech recognition (ASR) aims to transcribe the\ndesired speech of a target speaker from multi-talker overlapped utterances.\nMost of the existing target-speaker ASR (TS-ASR) methods involve either\ntraining from scratch or fully fine-tuning a pre-trained model, leading to\nsignificant training costs and becoming inapplicable to large foundation\nmodels. This work leverages prompt tuning, a parameter-efficient fine-tuning\napproach, to extend Whisper, a large-scale single-talker ASR model, to TS-ASR.\nExperimental results show that prompt tuning can achieve performance comparable\nto state-of-the-art full fine-tuning approaches while only requiring about 1%\nof task-specific model parameters. Notably, the original Whisper's features,\nsuch as inverse text normalization and timestamp prediction, are retained in\ntarget-speaker ASR, keeping the generated transcriptions natural and\ninformative.\n","authors":["Hao Ma","Zhiyuan Peng","Mingjie Shao","Jing Li","Ju Liu"],"pdf_url":"https://arxiv.org/pdf/2312.08079v1.pdf","comment":"ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.08078v1","updated":"2023-12-13T11:47:28Z","published":"2023-12-13T11:47:28Z","title":"Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic\n  Image-Report Generation","summary":"  To address these issues, we propose a novel Adaptive patch-word Matching\n(AdaMatch) model to correlate chest X-ray (CXR) image regions with words in\nmedical reports and apply it to CXR-report generation to provide explainability\nfor the generation process. AdaMatch exploits the fine-grained relation between\nadaptive patches and words to provide explanations of specific image regions\nwith corresponding words. To capture the abnormal regions of varying sizes and\npositions, we introduce the Adaptive Patch extraction (AdaPatch) module to\nacquire the adaptive patches for these regions adaptively. In order to provide\nexplicit explainability for CXR-report generation task, we propose an\nAdaMatch-based bidirectional large language model for Cyclic CXR-report\ngeneration (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywords\nfor CXR images and `keypatches' for medical reports as hints to guide\nCXR-report generation. Extensive experiments on two publicly available CXR\ndatasets prove the effectiveness of our method and its superior performance to\nexisting methods.\n","authors":["Wenting Chen","Xiang Li","Linlin Shen","Yixuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2312.08078v1.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2312.08063v1","updated":"2023-12-13T11:17:27Z","published":"2023-12-13T11:17:27Z","title":"Estimation of Concept Explanations Should be Uncertainty Aware","summary":"  Model explanations are very valuable for interpreting and debugging\nprediction models. We study a specific kind of global explanations called\nConcept Explanations, where the goal is to interpret a model using\nhuman-understandable concepts. Recent advances in multi-modal learning\nrekindled interest in concept explanations and led to several label-efficient\nproposals for estimation. However, existing estimation methods are unstable to\nthe choice of concepts or dataset that is used for computing explanations. We\nobserve that instability in explanations is due to high variance in point\nestimation of importance scores. We propose an uncertainty aware Bayesian\nestimation method, which readily improved reliability of the concept\nexplanations. We demonstrate with theoretical analysis and empirical evaluation\nthat explanations computed by our method are more reliable while also being\nlabel-efficient and faithful.\n","authors":["Vihari Piratla","Juyeon Heo","Sukriti Singh","Adrian Weller"],"pdf_url":"https://arxiv.org/pdf/2312.08063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.04118v2","updated":"2023-12-13T10:40:49Z","published":"2022-11-08T09:29:45Z","title":"ConsPrompt: Exploiting Contrastive Samples for Fewshot Prompt Learning","summary":"  Prompt recently have become an effective linguistic tool on utilizing the\npre-trained language models. However, in few-shot scenarios, subtle changes of\nprompt's design always make the result widely different, and the prompt design\nis also easy to overfit the current limited samples. To alleviate this, we\nexplore how to utilize suitable contrastive samples and multiple contrastive\nlearning methods to realize a more robust prompt's representation. Therefore,\nthe contrastive prompt model ConsPrompt combining with prompt encoding network,\ncontrastive sampling modules, and contrastive scoring modules are introduced to\nrealize differential contrastive learning. Our results exhibit the\nstate-of-the-art performance in different few-shot settings, and the ablation\nexperiments also certificate the effectiveness in utilizing multi-degree\ncontrastive learning in prompt-based fine-tuning process.\n","authors":["Jinta Weng","Yifan Deng","d Donghao Li","Hao You","Yue Hu","Heyan Huang"],"pdf_url":"https://arxiv.org/pdf/2211.04118v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08036v1","updated":"2023-12-13T10:29:34Z","published":"2023-12-13T10:29:34Z","title":"CoRTEx: Contrastive Learning for Representing Terms via Explanations\n  with Applications on Constructing Biomedical Knowledge Graphs","summary":"  Objective: Biomedical Knowledge Graphs play a pivotal role in various\nbiomedical research domains. Concurrently, term clustering emerges as a crucial\nstep in constructing these knowledge graphs, aiming to identify synonymous\nterms. Due to a lack of knowledge, previous contrastive learning models trained\nwith Unified Medical Language System (UMLS) synonyms struggle at clustering\ndifficult terms and do not generalize well beyond UMLS terms. In this work, we\nleverage the world knowledge from Large Language Models (LLMs) and propose\nContrastive Learning for Representing Terms via Explanations (CoRTEx) to\nenhance term representation and significantly improves term clustering.\nMaterials and Methods: The model training involves generating explanations for\na cleaned subset of UMLS terms using ChatGPT. We employ contrastive learning,\nconsidering term and explanation embeddings simultaneously, and progressively\nintroduce hard negative samples. Additionally, a ChatGPT-assisted BIRCH\nalgorithm is designed for efficient clustering of a new ontology. Results: We\nestablished a clustering test set and a hard negative test set, where our model\nconsistently achieves the highest F1 score. With CoRTEx embeddings and the\nmodified BIRCH algorithm, we grouped 35,580,932 terms from the Biomedical\nInformatics Ontology System (BIOS) into 22,104,559 clusters with O(N) queries\nto ChatGPT. Case studies highlight the model's efficacy in handling challenging\nsamples, aided by information from explanations. Conclusion: By aligning terms\nto their explanations, CoRTEx demonstrates superior accuracy over benchmark\nmodels and robustness beyond its training set, and it is suitable for\nclustering terms for large-scale biomedical ontologies.\n","authors":["Huaiyuan Ying","Zhengyun Zhao","Yang Zhao","Sihang Zeng","Sheng Yu"],"pdf_url":"https://arxiv.org/pdf/2312.08036v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.09515v3","updated":"2023-12-13T10:24:00Z","published":"2023-05-16T15:10:22Z","title":"AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation","summary":"  Diffusion models have gained significant attention in the realm of image\ngeneration due to their exceptional performance. Their success has been\nrecently expanded to text generation via generating all tokens within a\nsequence concurrently. However, natural language exhibits a far more pronounced\nsequential dependency in comparison to images, and the majority of existing\nlanguage models are trained with a left-to-right auto-regressive approach. To\naccount for the inherent sequential characteristic of natural language, we\nintroduce Auto-Regressive Diffusion (AR-Diffusion). AR-Diffusion ensures that\nthe generation of tokens on the right depends on the generated ones on the\nleft, a mechanism achieved through employing a dynamic number of denoising\nsteps that vary based on token position. This results in tokens on the left\nundergoing fewer denoising steps than those on the right, thereby enabling them\nto generate earlier and subsequently influence the generation of tokens on the\nright. In a series of experiments on various text generation tasks, including\ntext summarization, machine translation, and common sense generation,\nAR-Diffusion clearly demonstrated its superiority over existing diffusion\nlanguage models and that it can be $100\\times\\sim600\\times$ faster when\nachieving comparable results. Our code is available at\nhttps://github.com/microsoft/ProphetNet/tree/master/AR-diffusion.\n","authors":["Tong Wu","Zhihao Fan","Xiao Liu","Yeyun Gong","Yelong Shen","Jian Jiao","Hai-Tao Zheng","Juntao Li","Zhongyu Wei","Jian Guo","Nan Duan","Weizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2305.09515v3.pdf","comment":"Accept By NIPS 2023"},{"id":"http://arxiv.org/abs/2312.08027v1","updated":"2023-12-13T10:00:44Z","published":"2023-12-13T10:00:44Z","title":"Helping Language Models Learn More: Multi-dimensional Task Prompt for\n  Few-shot Tuning","summary":"  Large language models (LLMs) can be used as accessible and intelligent\nchatbots by constructing natural language queries and directly inputting the\nprompt into the large language model. However, different prompt' constructions\noften lead to uncertainty in the answers and thus make it hard to utilize the\nspecific knowledge of LLMs (like ChatGPT). To alleviate this, we use an\ninterpretable structure to explain the prompt learning principle in LLMs, which\ncertificates that the effectiveness of language models is determined by\nposition changes of the task's related tokens. Therefore, we propose MTPrompt,\na multi-dimensional task prompt learning method consisting based on\ntask-related object, summary, and task description information. By\nautomatically building and searching for appropriate prompts, our proposed\nMTPrompt achieves the best results on few-shot samples setting and five\ndifferent datasets. In addition, we demonstrate the effectiveness and stability\nof our method in different experimental settings and ablation experiments. In\ninteraction with large language models, embedding more task-related information\ninto prompts will make it easier to stimulate knowledge embedded in large\nlanguage models.\n","authors":["Jinta Weng","Jiarui Zhang","Yue Hu","Daidong Fa","Xiaofeng Xuand","Heyan Huang"],"pdf_url":"https://arxiv.org/pdf/2312.08027v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2210.16489"},{"id":"http://arxiv.org/abs/2312.07987v1","updated":"2023-12-13T09:00:21Z","published":"2023-12-13T09:00:21Z","title":"SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention","summary":"  The costly self-attention layers in modern Transformers require memory and\ncompute quadratic in sequence length. Existing approximation methods usually\nunderperform and fail to obtain significant speedups in practice. Here we\npresent SwitchHead - a novel method that reduces both compute and memory\nrequirements and achieves wall-clock speedup, while matching the language\nmodeling performance of baseline Transformers with the same parameter budget.\nSwitchHead uses Mixture-of-Experts (MoE) layers for the value and output\nprojections and requires 4 to 8 times fewer attention matrices than standard\nTransformers. Our novel attention can also be combined with MoE MLP layers,\nresulting in an efficient fully-MoE \"SwitchHead\" Transformer model. Our code is\npublic.\n","authors":["Róbert Csordás","Piotr Piękos","Kazuki Irie"],"pdf_url":"https://arxiv.org/pdf/2312.07987v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07979v1","updated":"2023-12-13T08:50:02Z","published":"2023-12-13T08:50:02Z","title":"SLJP: Semantic Extraction based Legal Judgment Prediction","summary":"  Legal Judgment Prediction (LJP) is a judicial assistance system that\nrecommends the legal components such as applicable statues, prison term and\npenalty term by analyzing the given input case document. Indian legal system is\nin the need of technical assistance such as artificial intelligence to solve\nthe crores of pending cases in various courts for years and its being increased\nday to day. Most of the existing Indian models did not adequately concentrate\non the semantics embedded in the fact description (FD) that impacts the\ndecision. The proposed semantic extraction based LJP (SLJP) model provides the\nadvantages of pretrained transformers for complex unstructured legal case\ndocument understanding and to generate embeddings. The model draws the in-depth\nsemantics of the given FD at multiple levels i.e., chunk and case document\nlevel by following the divide and conquer approach. It creates the concise view\nof the given fact description using the extracted semantics as per the original\ncourt case document structure and predicts judgment using attention mechanism.\nWe tested the model performance on two available Indian datasets Indian Legal\nDocuments corpus (ILDC) and Indian Legal Statue Identification (ILSI) and got\npromising results. Also shown the highest performance and less performance\ndegradation for increased epochs than base models on ILDC dataset.\n","authors":["Prameela Madambakam","Shathanaa Rajmohan","Himangshu Sharma","Tummepalli Anka Chandrahas Purushotham Gupta"],"pdf_url":"https://arxiv.org/pdf/2312.07979v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07961v1","updated":"2023-12-13T08:17:00Z","published":"2023-12-13T08:17:00Z","title":"Robust Few-Shot Named Entity Recognition with Boundary Discrimination\n  and Correlation Purification","summary":"  Few-shot named entity recognition (NER) aims to recognize novel named\nentities in low-resource domains utilizing existing knowledge. However, the\npresent few-shot NER models assume that the labeled data are all clean without\nnoise or outliers, and there are few works focusing on the robustness of the\ncross-domain transfer learning ability to textual adversarial attacks in\nFew-shot NER. In this work, we comprehensively explore and assess the\nrobustness of few-shot NER models under textual adversarial attack scenario,\nand found the vulnerability of existing few-shot NER models. Furthermore, we\npropose a robust two-stage few-shot NER method with Boundary Discrimination and\nCorrelation Purification (BDCP). Specifically, in the span detection stage, the\nentity boundary discriminative module is introduced to provide a highly\ndistinguishing boundary representation space to detect entity spans. In the\nentity typing stage, the correlations between entities and contexts are\npurified by minimizing the interference information and facilitating\ncorrelation generalization to alleviate the perturbations caused by textual\nadversarial attacks. In addition, we construct adversarial examples for\nfew-shot NER based on public datasets Few-NERD and Cross-Dataset. Comprehensive\nevaluations on those two groups of few-shot NER datasets containing adversarial\nexamples demonstrate the robustness and superiority of the proposed method.\n","authors":["Xiaojun Xue","Chunxia Zhang","Tianxiang Xu","Zhendong Niu"],"pdf_url":"https://arxiv.org/pdf/2312.07961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07950v1","updated":"2023-12-13T07:56:27Z","published":"2023-12-13T07:56:27Z","title":"CBQ: Cross-Block Quantization for Large Language Models","summary":"  Post-training quantization (PTQ) has driven attention to producing efficient\nlarge language models (LLMs) with ultra-low costs. Since hand-craft\nquantization parameters lead to low performance in low-bit quantization, recent\nmethods optimize the quantization parameters through block-wise reconstruction\nbetween the floating-point and quantized models. However, these methods suffer\nfrom two challenges: accumulated errors from independent one-by-one block\nquantization and reconstruction difficulties from extreme weight and activation\noutliers. To address these two challenges, we propose CBQ, a cross-block\nreconstruction-based PTQ method for LLMs. To reduce error accumulation, we\nintroduce a cross-block dependency with the aid of a homologous reconstruction\nscheme to build the long-range dependency between adjacent multi-blocks with\noverlapping. To reduce reconstruction difficulty, we design a coarse-to-fine\npre-processing (CFP) to truncate weight outliers and dynamically scale\nactivation outliers before optimization, and an adaptive rounding scheme,\ncalled LoRA-Rounding, with two low-rank learnable matrixes to further rectify\nweight quantization errors. Extensive experiments demonstrate that: (1) CBQ\npushes both activation and weight quantization to low-bit settings W4A4, W4A8,\nand W2A16. (2) CBQ achieves better performance than the existing\nstate-of-the-art methods on various LLMs and benchmark datasets.\n","authors":["Xin Ding","Xiaoyu Liu","Yun Zhang","Zhijun Tu","Wei Li","Jie Hu","Hanting Chen","Yehui Tang","Zhiwei Xiong","Baoqun Yin","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2312.07950v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.03520v4","updated":"2023-12-13T07:13:19Z","published":"2023-05-05T13:50:04Z","title":"Context-Aware Semantic Similarity Measurement for Unsupervised Word\n  Sense Disambiguation","summary":"  The issue of word sense ambiguity poses a significant challenge in natural\nlanguage processing due to the scarcity of annotated data to feed machine\nlearning models to face the challenge. Therefore, unsupervised word sense\ndisambiguation methods have been developed to overcome that challenge without\nrelying on annotated data. This research proposes a new context-aware approach\nto unsupervised word sense disambiguation, which provides a flexible mechanism\nfor incorporating contextual information into the similarity measurement\nprocess. We experiment with a popular benchmark dataset to evaluate the\nproposed strategy and compare its performance with state-of-the-art\nunsupervised word sense disambiguation techniques. The experimental results\nindicate that our approach substantially enhances disambiguation accuracy and\nsurpasses the performance of several existing techniques. Our findings\nunderscore the significance of integrating contextual information in semantic\nsimilarity measurements to manage word sense ambiguity in unsupervised\nscenarios effectively.\n","authors":["Jorge Martinez-Gil"],"pdf_url":"https://arxiv.org/pdf/2305.03520v4.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2312.07930v1","updated":"2023-12-13T06:57:00Z","published":"2023-12-13T06:57:00Z","title":"Towards Optimal Statistical Watermarking","summary":"  We study statistical watermarking by formulating it as a hypothesis testing\nproblem, a general framework which subsumes all previous statistical\nwatermarking methods. Key to our formulation is a coupling of the output tokens\nand the rejection region, realized by pseudo-random generators in practice,\nthat allows non-trivial trade-off between the Type I error and Type II error.\nWe characterize the Uniformly Most Powerful (UMP) watermark in this context. In\nthe most common scenario where the output is a sequence of $n$ tokens, we\nestablish matching upper and lower bounds on the number of i.i.d. tokens\nrequired to guarantee small Type I and Type II errors. Our rate scales as\n$\\Theta(h^{-1} \\log (1/h))$ with respect to the average entropy per token $h$\nand thus greatly improves the $O(h^{-2})$ rate in the previous works. For\nscenarios where the detector lacks knowledge of the model's distribution, we\nintroduce the concept of model-agnostic watermarking and establish the minimax\nbounds for the resultant increase in Type II error. Moreover, we formulate the\nrobust watermarking problem where user is allowed to perform a class of\nperturbation on the generated texts, and characterize the optimal type II error\nof robust UMP tests via a linear programming problem. To the best of our\nknowledge, this is the first systematic statistical treatment on the\nwatermarking problem with near-optimal rates in the i.i.d. setting, and might\nbe of interest for future works.\n","authors":["Baihe Huang","Banghua Zhu","Hanlin Zhu","Jason D. Lee","Jiantao Jiao","Michael I. Jordan"],"pdf_url":"https://arxiv.org/pdf/2312.07930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07913v1","updated":"2023-12-13T06:11:42Z","published":"2023-12-13T06:11:42Z","title":"A Survey of Text Watermarking in the Era of Large Language Models","summary":"  In recent years, significant advancements have been made in the text\ngeneration capabilities of Large Language Models (LLMs), demonstrating\nexceptional performance in downstream tasks such as abstract summarization,\ndialogue generation, and data-to-text conversion. However, their generative\nabilities also pose risks such as the rapid spread of fake news, infringement\nof datasets/LLM copyrights, and challenges to academic integrity. Text\nwatermarking technology emerges as a potential solution. By embedding invisible\nyet detectable patterns in generated texts, it helps in tracking and verifying\ntext origins, thus preventing misuse and piracy.\n  This survey aims to comprehensively summarize current text watermarking\ntechnologies, covering three main aspects: (1) an overview and comparison of\ndifferent text watermarking techniques; (2) evaluation methods for text\nwatermarking algorithms, including their success rate, impact on text quality,\nrobustness, and unforgeability; (3) potential applications of text watermarking\ntechnologys. This survey aims to help researchers thoroughly understanding the\ntext watermarking technologies, thereby fostering further development.\n","authors":["Aiwei Liu","Leyi Pan","Yijian Lu","Jingjing Li","Xuming Hu","Lijie Wen","Irwin King","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2312.07913v1.pdf","comment":"33 pages, 5 figures"},{"id":"http://arxiv.org/abs/2312.07910v1","updated":"2023-12-13T05:58:34Z","published":"2023-12-13T05:58:34Z","title":"PromptBench: A Unified Library for Evaluation of Large Language Models","summary":"  The evaluation of large language models (LLMs) is crucial to assess their\nperformance and mitigate potential security risks. In this paper, we introduce\nPromptBench, a unified library to evaluate LLMs. It consists of several key\ncomponents that are easily used and extended by researchers: prompt\nconstruction, prompt engineering, dataset and model loading, adversarial prompt\nattack, dynamic evaluation protocols, and analysis tools. PromptBench is\ndesigned to be an open, general, and flexible codebase for research purposes\nthat can facilitate original study in creating new benchmarks, deploying\ndownstream applications, and designing new evaluation protocols. The code is\navailable at: https://github.com/microsoft/promptbench and will be continuously\nsupported.\n","authors":["Kaijie Zhu","Qinlin Zhao","Hao Chen","Jindong Wang","Xing Xie"],"pdf_url":"https://arxiv.org/pdf/2312.07910v1.pdf","comment":"An extension to PromptBench (arXiv:2306.04528) for unified evaluation\n  of LLMs using the same name; code: https://github.com/microsoft/promptbench"},{"id":"http://arxiv.org/abs/2302.10915v2","updated":"2023-12-13T04:31:44Z","published":"2023-02-17T01:31:55Z","title":"Conformers are All You Need for Visual Speech Recognition","summary":"  Visual speech recognition models extract visual features in a hierarchical\nmanner. At the lower level, there is a visual front-end with a limited temporal\nreceptive field that processes the raw pixels depicting the lips or faces. At\nthe higher level, there is an encoder that attends to the embeddings produced\nby the front-end over a large temporal receptive field. Previous work has\nfocused on improving the visual front-end of the model to extract more useful\nfeatures for speech recognition. Surprisingly, our work shows that complex\nvisual front-ends are not necessary. Instead of allocating resources to a\nsophisticated visual front-end, we find that a linear visual front-end paired\nwith a larger Conformer encoder results in lower latency, more efficient memory\nusage, and improved WER performance. We achieve a new state-of-the-art of 12.8%\nWER for visual speech recognition on the TED LRS3 dataset, which rivals the\nperformance of audio-only models from just four years ago.\n","authors":["Oscar Chang","Hank Liao","Dmitriy Serdyuk","Ankit Shah","Olivier Siohan"],"pdf_url":"https://arxiv.org/pdf/2302.10915v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.06152v3","updated":"2023-12-13T04:21:23Z","published":"2023-05-06T03:57:05Z","title":"Structure-CLIP: Towards Scene Graph Knowledge to Enhance Multi-modal\n  Structured Representations","summary":"  Large-scale vision-language pre-training has achieved significant performance\nin multi-modal understanding and generation tasks. However, existing methods\noften perform poorly on image-text matching tasks that require structured\nrepresentations, i.e., representations of objects, attributes, and relations.\nAs illustrated in Fig.~reffig:case (a), the models cannot make a distinction\nbetween ``An astronaut rides a horse\" and ``A horse rides an astronaut\". This\nis because they fail to fully leverage structured knowledge when learning\nrepresentations in multi-modal scenarios. In this paper, we present an\nend-to-end framework Structure-CLIP, which integrates Scene Graph Knowledge\n(SGK) to enhance multi-modal structured representations. Firstly, we use scene\ngraphs to guide the construction of semantic negative examples, which results\nin an increased emphasis on learning structured representations. Moreover, a\nKnowledge-Enhance Encoder (KEE) is proposed to leverage SGK as input to further\nenhance structured representations. To verify the effectiveness of the proposed\nframework, we pre-train our model with the aforementioned approaches and\nconduct experiments on downstream tasks. Experimental results demonstrate that\nStructure-CLIP achieves state-of-the-art (SOTA) performance on VG-Attribution\nand VG-Relation datasets, with 12.5% and 4.1% ahead of the multi-modal SOTA\nmodel respectively. Meanwhile, the results on MSCOCO indicate that\nStructure-CLIP significantly enhances the structured representations while\nmaintaining the ability of general representations. Our code is available at\nhttps://github.com/zjukg/Structure-CLIP.\n","authors":["Yufeng Huang","Jiji Tang","Zhuo Chen","Rongsheng Zhang","Xinfeng Zhang","Weijie Chen","Zeng Zhao","Zhou Zhao","Tangjie Lv","Zhipeng Hu","Wen Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.06152v3.pdf","comment":"AAAI 2024, https://github.com/zjukg/Structure-CLIP"},{"id":"http://arxiv.org/abs/2311.08836v2","updated":"2023-12-13T04:15:26Z","published":"2023-11-15T10:25:14Z","title":"Evaluating Gender Bias in the Translation of Gender-Neutral Languages\n  into English","summary":"  Machine Translation (MT) continues to improve in quality and adoption, yet\nthe inadvertent perpetuation of gender bias remains a significant concern.\nDespite numerous studies into gender bias in translations from gender-neutral\nlanguages such as Turkish into more strongly gendered languages like English,\nthere are no benchmarks for evaluating this phenomenon or for assessing\nmitigation strategies. To address this gap, we introduce GATE X-E, an extension\nto the GATE (Rarrick et al., 2023) corpus, that consists of human translations\nfrom Turkish, Hungarian, Finnish, and Persian into English. Each translation is\naccompanied by feminine, masculine, and neutral variants for each possible\ngender interpretation. The dataset, which contains between 1250 and 1850\ninstances for each of the four language pairs, features natural sentences with\na wide range of sentence lengths and domains, challenging translation rewriters\non various linguistic phenomena. Additionally, we present an English gender\nrewriting solution built on GPT-3.5 Turbo and use GATE X-E to evaluate it. We\nopen source our contributions to encourage further research on gender\ndebiasing.\n","authors":["Spencer Rarrick","Ranjita Naik","Sundar Poudel","Vishal Chowdhary"],"pdf_url":"https://arxiv.org/pdf/2311.08836v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07887v1","updated":"2023-12-13T04:14:22Z","published":"2023-12-13T04:14:22Z","title":"Learn or Recall? Revisiting Incremental Learning with Pre-trained\n  Language Models","summary":"  Incremental Learning (IL) has been a long-standing problem in both vision and\nNatural Language Processing (NLP) communities. In recent years, as Pre-trained\nLanguage Models (PLMs) have achieved remarkable progress in various NLP\ndownstream tasks, utilizing PLMs as backbones has become a common practice in\nrecent research of IL in NLP. Most assume that catastrophic forgetting is the\nbiggest obstacle to achieving superior IL performance and propose various\ntechniques to overcome this issue. However, we find that this assumption is\nproblematic. Specifically, we revisit more than 20 methods on four\nclassification tasks (Text Classification, Intent Classification, Relation\nExtraction, and Named Entity Recognition) under the two most popular IL\nsettings (Class-Incremental and Task-Incremental) and reveal that most of them\nseverely underestimate the inherent anti-forgetting ability of PLMs. Based on\nthe observation, we propose a frustratingly easy method called SEQ* for IL with\nPLMs. The results show that SEQ* has competitive or superior performance\ncompared to state-of-the-art (SOTA) IL methods and requires considerably less\ntrainable parameters and training time. These findings urge us to revisit the\nIL with PLMs and encourage future studies to have a fundamental understanding\nof the catastrophic forgetting in PLMs. The data, code and scripts are publicly\navailable at\nhttps://github.com/zzz47zzz/pretrained-lm-for-incremental-learning.\n","authors":["Junhao Zheng","Shengjie Qiu","Qianli Ma"],"pdf_url":"https://arxiv.org/pdf/2312.07887v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07886v1","updated":"2023-12-13T04:08:59Z","published":"2023-12-13T04:08:59Z","title":"Modality Plug-and-Play: Elastic Modality Adaptation in Multimodal LLMs\n  for Embodied AI","summary":"  Large Language Models (LLMs) are capable of reasoning over diverse input data\nmodalities through pre-trained encoders. However, the growing diversity of\ninput data modalities prevents incorporating all modalities into LLMs,\nespecially when LLMs are deployed on resource-constrained edge devices for\nembodied AI applications. Instead, a better option is to adaptively involve\nonly the useful modalities at runtime, depending on the current environmental\ncontexts and task requirements. For such modality adaptation, existing work\nadopts fixed connections between encoders and the LLM's input layer, leading to\nhigh training cost at runtime and ineffective cross-modal interaction. In this\npaper, we address these limitations by presenting mPnP-LLM, a new technique\nthat allows fully elastic, automated and prompt runtime modality adaptation, by\nconnecting unimodal encoders to a flexible set of last LLM blocks and making\nsuch latent connections fully trainable at runtime. Experiments over the\nnuScenes-QA dataset show that mPnP-LLM can achieve up to 3.7x FLOPs reduction\nand 30% GPU memory usage reduction, while retaining on-par accuracy with the\nexisting schemes. Under the same compute budget, mPnP-LLM improves the task\naccuracy by up to 4% compared to the best existing scheme.\n","authors":["Kai Huang","Boyuan Yang","Wei Gao"],"pdf_url":"https://arxiv.org/pdf/2312.07886v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07868v1","updated":"2023-12-13T03:16:33Z","published":"2023-12-13T03:16:33Z","title":"Graph vs. Sequence: An Empirical Study on Knowledge Forms for\n  Knowledge-Grounded Dialogue","summary":"  Knowledge-grounded dialogue is a task of generating an informative response\nbased on both the dialogue history and external knowledge source. In general,\nthere are two forms of knowledge: manually annotated knowledge graphs and\nknowledge text from website. From various evaluation viewpoints, each type of\nknowledge has advantages and downsides. To further distinguish the principles\nand determinants from the intricate factors, we conduct a thorough experiment\nand study on the task to answer three essential questions. The questions\ninvolve the choice of appropriate knowledge form, the degree of mutual effects\nbetween knowledge and the model selection, and the few-shot performance of\nknowledge. Supported by statistical shreds of evidence, we offer conclusive\nsolutions and sensible suggestions for directions and standards of future\nresearch.\n","authors":["Yizhe Yang","Heyan Huang","Yihang Liu","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2312.07868v1.pdf","comment":"Accepted in EMNLP2023"},{"id":"http://arxiv.org/abs/2312.07867v1","updated":"2023-12-13T03:08:48Z","published":"2023-12-13T03:08:48Z","title":"BESTMVQA: A Benchmark Evaluation System for Medical Visual Question\n  Answering","summary":"  Medical Visual Question Answering (Med-VQA) is a very important task in\nhealthcare industry, which answers a natural language question with a medical\nimage. Existing VQA techniques in information systems can be directly applied\nto solving the task. However, they often suffer from (i) the data insufficient\nproblem, which makes it difficult to train the state of the arts (SOTAs) for\nthe domain-specific task, and (ii) the reproducibility problem, that many\nexisting models have not been thoroughly evaluated in a unified experimental\nsetup. To address these issues, this paper develops a Benchmark Evaluation\nSysTem for Medical Visual Question Answering, denoted by BESTMVQA. Given\nself-collected clinical data, our system provides a useful tool for users to\nautomatically build Med-VQA datasets, which helps overcoming the data\ninsufficient problem. Users also can conveniently select a wide spectrum of\nSOTA models from our model library to perform a comprehensive empirical study.\nWith simple configurations, our system automatically trains and evaluates the\nselected models over a benchmark dataset, and reports the comprehensive results\nfor users to develop new techniques or perform medical practice. Limitations of\nexisting work are overcome (i) by the data generation tool, which automatically\nconstructs new datasets from unstructured clinical data, and (ii) by evaluating\nSOTAs on benchmark datasets in a unified experimental setup. The demonstration\nvideo of our system can be found at https://youtu.be/QkEeFlu1x4A. Our code and\ndata will be available soon.\n","authors":["Xiaojie Hong","Zixin Song","Liangzhi Li","Xiaoli Wang","Feiyan Liu"],"pdf_url":"https://arxiv.org/pdf/2312.07867v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07848v1","updated":"2023-12-13T02:32:01Z","published":"2023-12-13T02:32:01Z","title":"Finetuning an LLM on Contextual Knowledge of Classics for Q&A","summary":"  The open-source publishing of large language models (LLMs) has created many\npossibilities for how anyone who understands language and has access to a\ncomputer can interact with significant tools of artificial intelligence,\nparticularly in the context of learning and knowledge dissemination. However,\nthe utility of these models in specialized fields like Classics is still\nlargely unexplored. This project is an attempt to merge the knowledge of\nClassics with the capabilities of artificial intelligence by finetuning an LLM\nto cater to the specific needs of learners and professionals. The goal of this\nproject is to develop an LLM that not only reproduces contextual knowledge\naccurately but also exhibits a consistent \"personality\" - and, indeed, has\nconsistent propriety - to appeal to a diverse audience who possess differing\nlevels of knowledge. A significant portion of this project was dedicated to\nrefining the dataset, following the principle of \"garbage in, garbage out,\" to\nensure the model generates relevant, useful, and creative responses when given\na prompt (a statement, question, or single word). After training and\nevaluation, my model's ability to handle a vast array of different types of\ninputs and prompting exceeded expectations for a 355M parameter model, though\nits occasional hallucinations (especially when set with a high temperature),\nparticularly in its assertions about historical events or its own identity,\nmake it seem somewhat capricious and more work in the form of continuous\nfinetuning will be undertaken.\n","authors":["Shane Storm Strachan"],"pdf_url":"https://arxiv.org/pdf/2312.07848v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2304.10464v4","updated":"2023-12-13T02:08:50Z","published":"2023-04-20T17:09:12Z","title":"Learning to Plan with Natural Language","summary":"  Large Language Models (LLMs) have shown remarkable performance in various\nbasic natural language tasks. For completing the complex task, we still need a\nplan for the task to guide LLMs to generate the specific solutions step by\nstep. LLMs can directly generate task plans, but these plans may still contain\nfactual errors or are incomplete. A high-quality task plan contains correct\nstep-by-step solutions for solving all situations and behavioral instructions\nfor avoiding mistakes. To obtain it, we propose the Learning to Plan method,\nwhich involves two phases: (1) In the first learning task plan phase, it\niteratively updates the task plan with new step-by-step solutions and\nbehavioral instructions, which are obtained by prompting LLMs to derive from\ntraining error feedback. (2) In the subsequent test phase, the LLM uses the\nlearned task plan to guide the inference of LLM on the test set. We demonstrate\nthe effectiveness of our method on the five different reasoning type tasks (8\ndatasets). Further, our analysis experiment shows that the task plan learned by\none LLM can directly guide another LLM to improve its performance, which\nreveals a new transfer learning paradigm. We release the code at\n\\url{https://github.com/Eureka6174/LearnNLPlan}\n","authors":["Yiduo Guo","Yaobo Liang","Chenfei Wu","Wenshan Wu","Dongyan Zhao","Nan Duan"],"pdf_url":"https://arxiv.org/pdf/2304.10464v4.pdf","comment":"Large Language Model, Learning from feedback, Planning and Reasoning"},{"id":"http://arxiv.org/abs/2304.07699v3","updated":"2023-12-13T01:39:20Z","published":"2023-04-16T05:30:42Z","title":"A Clustering Framework for Unsupervised and Semi-supervised New Intent\n  Discovery","summary":"  New intent discovery is of great value to natural language processing,\nallowing for a better understanding of user needs and providing friendly\nservices. However, most existing methods struggle to capture the complicated\nsemantics of discrete text representations when limited or no prior knowledge\nof labeled data is available. To tackle this problem, we propose a novel\nclustering framework, USNID, for unsupervised and semi-supervised new intent\ndiscovery, which has three key technologies. First, it fully utilizes\nunsupervised or semi-supervised data to mine shallow semantic similarity\nrelations and provide well-initialized representations for clustering. Second,\nit designs a centroid-guided clustering mechanism to address the issue of\ncluster allocation inconsistency and provide high-quality self-supervised\ntargets for representation learning. Third, it captures high-level semantics in\nunsupervised or semi-supervised data to discover fine-grained intent-wise\nclusters by optimizing both cluster-level and instance-level objectives. We\nalso propose an effective method for estimating the cluster number in\nopen-world scenarios without knowing the number of new intents beforehand.\nUSNID performs exceptionally well on several benchmark intent datasets,\nachieving new state-of-the-art results in unsupervised and semi-supervised new\nintent discovery and demonstrating robust performance with different cluster\nnumbers.\n","authors":["Hanlei Zhang","Hua Xu","Xin Wang","Fei Long","Kai Gao"],"pdf_url":"https://arxiv.org/pdf/2304.07699v3.pdf","comment":"Accepted by IEEE TKDE"},{"id":"http://arxiv.org/abs/2312.07831v1","updated":"2023-12-13T01:36:18Z","published":"2023-12-13T01:36:18Z","title":"Abusive Span Detection for Vietnamese Narrative Texts","summary":"  Abuse in its various forms, including physical, psychological, verbal,\nsexual, financial, and cultural, has a negative impact on mental health.\nHowever, there are limited studies on applying natural language processing\n(NLP) in this field in Vietnam. Therefore, we aim to contribute by building a\nhuman-annotated Vietnamese dataset for detecting abusive content in Vietnamese\nnarrative texts. We sourced these texts from VnExpress, Vietnam's popular\nonline newspaper, where readers often share stories containing abusive content.\nIdentifying and categorizing abusive spans in these texts posed significant\nchallenges during dataset creation, but it also motivated our research. We\nexperimented with lightweight baseline models by freezing PhoBERT and\nXLM-RoBERTa and using their hidden states in a BiLSTM to assess the complexity\nof the dataset. According to our experimental results, PhoBERT outperforms\nother models in both labeled and unlabeled abusive span detection tasks. These\nresults indicate that it has the potential for future improvements.\n","authors":["Nhu-Thanh Nguyen","Khoa Thi-Kim Phan","Duc-Vu Nguyen","Ngan Luu-Thuy Nguyen"],"pdf_url":"https://arxiv.org/pdf/2312.07831v1.pdf","comment":"Accepted at SoICT 2023"},{"id":"http://arxiv.org/abs/2312.07824v1","updated":"2023-12-13T01:18:10Z","published":"2023-12-13T01:18:10Z","title":"A Deep Learning-Based System for Automatic Case Summarization","summary":"  This paper presents a deep learning-based system for efficient automatic case\nsummarization. Leveraging state-of-the-art natural language processing\ntechniques, the system offers both supervised and unsupervised methods to\ngenerate concise and relevant summaries of lengthy legal case documents. The\nuser-friendly interface allows users to browse the system's database of legal\ncase documents, select their desired case, and choose their preferred\nsummarization method. The system generates comprehensive summaries for each\nsubsection of the legal text as well as an overall summary. This demo\nstreamlines legal case document analysis, potentially benefiting legal\nprofessionals by reducing workload and increasing efficiency. Future work will\nfocus on refining summarization techniques and exploring the application of our\nmethods to other types of legal texts.\n","authors":["Minh Duong","Long Nguyen","Yen Vuong","Trong Le","Ha-Thanh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2312.07824v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07819v1","updated":"2023-12-13T00:52:15Z","published":"2023-12-13T00:52:15Z","title":"Native Language Identification with Large Language Models","summary":"  We present the first experiments on Native Language Identification (NLI)\nusing LLMs such as GPT-4. NLI is the task of predicting a writer's first\nlanguage by analyzing their writings in a second language, and is used in\nsecond language acquisition and forensic linguistics. Our results show that GPT\nmodels are proficient at NLI classification, with GPT-4 setting a new\nperformance record of 91.7% on the benchmark TOEFL11 test set in a zero-shot\nsetting. We also show that unlike previous fully-supervised settings, LLMs can\nperform NLI without being limited to a set of known classes, which has\npractical implications for real-world applications. Finally, we also show that\nLLMs can provide justification for their choices, providing reasoning based on\nspelling errors, syntactic patterns, and usage of directly translated\nlinguistic patterns.\n","authors":["Wei Zhang","Alexandre Salle"],"pdf_url":"https://arxiv.org/pdf/2312.07819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17728v2","updated":"2023-12-13T00:18:46Z","published":"2023-03-30T22:06:10Z","title":"Evaluation of GPT and BERT-based models on identifying protein-protein\n  interactions in biomedical text","summary":"  Detecting protein-protein interactions (PPIs) is crucial for understanding\ngenetic mechanisms, disease pathogenesis, and drug design. However, with the\nfast-paced growth of biomedical literature, there is a growing need for\nautomated and accurate extraction of PPIs to facilitate scientific knowledge\ndiscovery. Pre-trained language models, such as generative pre-trained\ntransformers (GPT) and bidirectional encoder representations from transformers\n(BERT), have shown promising results in natural language processing (NLP)\ntasks. We evaluated the performance of PPI identification of multiple GPT and\nBERT models using three manually curated gold-standard corpora: Learning\nLanguage in Logic (LLL) with 164 PPIs in 77 sentences, Human Protein Reference\nDatabase with 163 PPIs in 145 sentences, and Interaction Extraction Performance\nAssessment with 335 PPIs in 486 sentences. BERT-based models achieved the best\noverall performance, with BioBERT achieving the highest recall (91.95%) and\nF1-score (86.84%) and PubMedBERT achieving the highest precision (85.25%).\nInterestingly, despite not being explicitly trained for biomedical texts, GPT-4\nachieved commendable performance, comparable to the top-performing BERT models.\nIt achieved a precision of 88.37%, a recall of 85.14%, and an F1-score of\n86.49% on the LLL dataset. These results suggest that GPT models can\neffectively detect PPIs from text data, offering promising avenues for\napplication in biomedical literature mining. Further research could explore how\nthese models might be fine-tuned for even more specialized tasks within the\nbiomedical domain.\n","authors":["Hasin Rehana","Nur Bengisu Çam","Mert Basmaci","Jie Zheng","Christianah Jemiyo","Yongqun He","Arzucan Özgür","Junguk Hur"],"pdf_url":"https://arxiv.org/pdf/2303.17728v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07492v2","updated":"2023-12-13T20:34:23Z","published":"2023-12-12T18:27:44Z","title":"SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in\n  Generative Language Models","summary":"  Current datasets for unwanted social bias auditing are limited to studying\nprotected demographic features such as race and gender. In this work, we\nintroduce a comprehensive benchmark that is meant to capture the amplification\nof social bias, via stigmas, in generative language models. We start with a\ncomprehensive list of 93 stigmas documented in social science literature and\ncurate a question-answering (QA) dataset which involves simple social\nsituations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts, with a\nvariety of prompt styles, carefully constructed to systematically test for both\nsocial bias and model robustness. We present results for SocialStigmaQA with\ntwo widely used open source generative language models and we demonstrate that\nthe output generated by these models considerably amplifies existing social\nbias against stigmatized groups. Specifically, we find that the proportion of\nsocially biased output ranges from 45% to 59% across a variety of decoding\nstrategies and prompting styles. We discover that the deliberate design of the\ntemplates in our benchmark (e.g., by adding biasing text to the prompt or\nvarying the answer that indicates bias) impact the model tendencies to generate\nsocially biased output. Additionally, we report on patterns in the generated\nchain-of-thought output, finding a variety of problems from subtle bias to\nevidence of a lack of reasoning.\n  Warning: This paper contains examples of text which is toxic, biased, and\nharmful.\n","authors":["Manish Nagireddy","Lamogha Chiazor","Moninder Singh","Ioana Baldini"],"pdf_url":"https://arxiv.org/pdf/2312.07492v2.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2312.08566v1","updated":"2023-12-13T23:35:31Z","published":"2023-12-13T23:35:31Z","title":"Learning adaptive planning representations with natural language\n  guidance","summary":"  Effective planning in the real world requires not only world knowledge, but\nthe ability to leverage that knowledge to build the right representation of the\ntask at hand. Decades of hierarchical planning techniques have used\ndomain-specific temporal action abstractions to support efficient and accurate\nplanning, almost always relying on human priors and domain knowledge to\ndecompose hard tasks into smaller subproblems appropriate for a goal or set of\ngoals. This paper describes Ada (Action Domain Acquisition), a framework for\nautomatically constructing task-specific planning representations using\ntask-general background knowledge from language models (LMs). Starting with a\ngeneral-purpose hierarchical planner and a low-level goal-conditioned policy,\nAda interactively learns a library of planner-compatible high-level action\nabstractions and low-level controllers adapted to a particular domain of\nplanning tasks. On two language-guided interactive planning benchmarks (Mini\nMinecraft and ALFRED Household Tasks), Ada strongly outperforms other\napproaches that use LMs for sequential decision-making, offering more accurate\nplans and better generalization to complex tasks.\n","authors":["Lionel Wong","Jiayuan Mao","Pratyusha Sharma","Zachary S. Siegel","Jiahai Feng","Noa Korneev","Joshua B. Tenenbaum","Jacob Andreas"],"pdf_url":"https://arxiv.org/pdf/2312.08566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08495v1","updated":"2023-12-13T20:15:29Z","published":"2023-12-13T20:15:29Z","title":"Beyond Accuracy: Automated De-Identification of Large Real-World\n  Clinical Text Datasets","summary":"  Recent research advances achieve human-level accuracy for de-identifying\nfree-text clinical notes on research datasets, but gaps remain in reproducing\nthis in large real-world settings. This paper summarizes lessons learned from\nbuilding a system used to de-identify over one billion real clinical notes, in\na fully automated way, that was independently certified by multiple\norganizations for production use. A fully automated solution requires a very\nhigh level of accuracy that does not require manual review. A hybrid\ncontext-based model architecture is described, which outperforms a Named Entity\nRecogniton (NER) - only model by 10% on the i2b2-2014 benchmark. The proposed\nsystem makes 50%, 475%, and 575% fewer errors than the comparable AWS, Azure,\nand GCP services respectively while also outperforming ChatGPT by 33%. It\nexceeds 98% coverage of sensitive data across 7 European languages, without a\nneed for fine tuning. A second set of described models enable data obfuscation\n-- replacing sensitive data with random surrogates -- while retaining name,\ndate, gender, clinical, and format consistency. Both the practical need and the\nsolution architecture that provides for reliable & linked anonymized documents\nare described.\n","authors":["Veysel Kocaman","Hasham Ul Haq","David Talby"],"pdf_url":"https://arxiv.org/pdf/2312.08495v1.pdf","comment":"Extended Abstract presented at Machine Learning for Health (ML4H)\n  symposium 2023, December 10th, 2023, New Orleans, United States, 13 pages"},{"id":"http://arxiv.org/abs/2312.03664v2","updated":"2023-12-13T20:13:25Z","published":"2023-12-06T18:33:50Z","title":"Generative agent-based modeling with actions grounded in physical,\n  social, or digital space using Concordia","summary":"  Agent-based modeling has been around for decades, and applied widely across\nthe social and natural sciences. The scope of this research method is now\npoised to grow dramatically as it absorbs the new affordances provided by Large\nLanguage Models (LLM)s. Generative Agent-Based Models (GABM) are not just\nclassic Agent-Based Models (ABM)s where the agents talk to one another. Rather,\nGABMs are constructed using an LLM to apply common sense to situations, act\n\"reasonably\", recall common semantic knowledge, produce API calls to control\ndigital technologies like apps, and communicate both within the simulation and\nto researchers viewing it from the outside. Here we present Concordia, a\nlibrary to facilitate constructing and working with GABMs. Concordia makes it\neasy to construct language-mediated simulations of physically- or\ndigitally-grounded environments. Concordia agents produce their behavior using\na flexible component system which mediates between two fundamental operations:\nLLM calls and associative memory retrieval. A special agent called the Game\nMaster (GM), which was inspired by tabletop role-playing games, is responsible\nfor simulating the environment where the agents interact. Agents take actions\nby describing what they want to do in natural language. The GM then translates\ntheir actions into appropriate implementations. In a simulated physical world,\nthe GM checks the physical plausibility of agent actions and describes their\neffects. In digital environments simulating technologies such as apps and\nservices, the GM may handle API calls to integrate with external tools such as\ngeneral AI assistants (e.g., Bard, ChatGPT), and digital apps (e.g., Calendar,\nEmail, Search, etc.). Concordia was designed to support a wide array of\napplications both in scientific research and for evaluating performance of real\ndigital services by simulating users and/or generating synthetic data.\n","authors":["Alexander Sasha Vezhnevets","John P. Agapiou","Avia Aharon","Ron Ziv","Jayd Matyas","Edgar A. Duéñez-Guzmán","William A. Cunningham","Simon Osindero","Danny Karmon","Joel Z. Leibo"],"pdf_url":"https://arxiv.org/pdf/2312.03664v2.pdf","comment":"32 pages, 5 figures"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2312.08372v1","updated":"2023-12-13T18:59:58Z","published":"2023-12-13T18:59:58Z","title":"SAM-guided Graph Cut for 3D Instance Segmentation","summary":"  This paper addresses the challenge of 3D instance segmentation by\nsimultaneously leveraging 3D geometric and multi-view image information. Many\nprevious works have applied deep learning techniques to 3D point clouds for\ninstance segmentation. However, these methods often failed to generalize to\nvarious types of scenes due to the scarcity and low-diversity of labeled 3D\npoint cloud data. Some recent works have attempted to lift 2D instance\nsegmentations to 3D within a bottom-up framework. The inconsistency in 2D\ninstance segmentations among views can substantially degrade the performance of\n3D segmentation. In this work, we introduce a novel 3D-to-2D query framework to\neffectively exploit 2D segmentation models for 3D instance segmentation.\nSpecifically, we pre-segment the scene into several superpoints in 3D,\nformulating the task into a graph cut problem. The superpoint graph is\nconstructed based on 2D segmentation models, where node features are obtained\nfrom multi-view image features and edge weights are computed based on\nmulti-view segmentation results, enabling the better generalization ability. To\nprocess the graph, we train a graph neural network using pseudo 3D labels from\n2D segmentation models. Experimental results on the ScanNet, ScanNet++ and\nKITTI-360 datasets demonstrate that our method achieves robust segmentation\nperformance and can generalize across different types of scenes. Our project\npage is available at https://zju3dv.github.io/sam_graph.\n","authors":["Haoyu Guo","He Zhu","Sida Peng","Yuang Wang","Yujun Shen","Ruizhen Hu","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2312.08372v1.pdf","comment":"Project page: https://zju3dv.github.io/sam_graph"},{"id":"http://arxiv.org/abs/2312.08371v1","updated":"2023-12-13T18:59:13Z","published":"2023-12-13T18:59:13Z","title":"PTT: Point-Trajectory Transformer for Efficient Temporal 3D Object\n  Detection","summary":"  Recent temporal LiDAR-based 3D object detectors achieve promising performance\nbased on the two-stage proposal-based approach. They generate 3D box candidates\nfrom the first-stage dense detector, followed by different temporal aggregation\nmethods. However, these approaches require per-frame objects or whole point\nclouds, posing challenges related to memory bank utilization. Moreover, point\nclouds and trajectory features are combined solely based on concatenation,\nwhich may neglect effective interactions between them. In this paper, we\npropose a point-trajectory transformer with long short-term memory for\nefficient temporal 3D object detection. To this end, we only utilize point\nclouds of current-frame objects and their historical trajectories as input to\nminimize the memory bank storage requirement. Furthermore, we introduce modules\nto encode trajectory features, focusing on long short-term and future-aware\nperspectives, and then effectively aggregate them with point cloud features. We\nconduct extensive experiments on the large-scale Waymo dataset to demonstrate\nthat our approach performs well against state-of-the-art methods. Code and\nmodels will be made publicly available at https://github.com/kuanchihhuang/PTT.\n","authors":["Kuan-Chih Huang","Weijie Lyu","Ming-Hsuan Yang","Yi-Hsuan Tsai"],"pdf_url":"https://arxiv.org/pdf/2312.08371v1.pdf","comment":"Project page: https://github.com/kuanchihhuang/PTT"},{"id":"http://arxiv.org/abs/2312.08367v1","updated":"2023-12-13T18:58:15Z","published":"2023-12-13T18:58:15Z","title":"VLAP: Efficient Video-Language Alignment via Frame Prompting and\n  Distilling for Video Question Answering","summary":"  In this work, we propose an efficient Video-Language Alignment via\nFrame-Prompting and Distilling (VLAP) network. Our VLAP model addresses both\nefficient frame sampling and effective cross-modal alignment in a unified way.\nIn our VLAP network, we design a new learnable question-aware Frame-Prompter\ntogether with a new cross-modal distillation (QFormer-Distiller) module.\nPre-trained large image-language models have shown promising results on\nproblems such as visual question answering. However, how to efficiently and\neffectively sample image frames when adapting pre-trained large image-language\nmodel to video-language alignment is still the major challenge. Compared with\nprior work, our VLAP model demonstrates the capability of selecting key frames\nwith critical contents, thus improving the video-language alignment accuracy\nwhile reducing the inference latency (+3.3% on NExT-QA Temporal with 3.0X speed\nup). Overall, our VLAP network outperforms (e.g. +4.6% on STAR Interaction and\n+2.2% on STAR average with 3.0X speed up, ours 2-frames out-perform SeViLA\n4-frames on VLEP with 4.2X speed up) the state-of-the-art methods on the video\nquestion-answering benchmarks.\n","authors":["Xijun Wang","Junbang Liang","Chun-Kai Wang","Kenan Deng","Yu Lou","Ming Lin","Shan Yang"],"pdf_url":"https://arxiv.org/pdf/2312.08367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08366v1","updated":"2023-12-13T18:58:04Z","published":"2023-12-13T18:58:04Z","title":"See, Say, and Segment: Teaching LMMs to Overcome False Premises","summary":"  Current open-source Large Multimodal Models (LMMs) excel at tasks such as\nopen-vocabulary language grounding and segmentation but can suffer under false\npremises when queries imply the existence of something that is not actually\npresent in the image. We observe that existing methods that fine-tune an LMM to\nsegment images significantly degrade their ability to reliably determine\n(\"see\") if an object is present and to interact naturally with humans (\"say\"),\na form of catastrophic forgetting. In this work, we propose a cascading and\njoint training approach for LMMs to solve this task, avoiding catastrophic\nforgetting of previous skills. Our resulting model can \"see\" by detecting\nwhether objects are present in an image, \"say\" by telling the user if they are\nnot, proposing alternative queries or correcting semantic errors in the query,\nand finally \"segment\" by outputting the mask of the desired objects if they\nexist. Additionally, we introduce a novel False Premise Correction benchmark\ndataset, an extension of existing RefCOCO(+/g) referring segmentation datasets\n(which we call FP-RefCOCO(+/g)). The results show that our method not only\ndetects false premises up to 55% better than existing approaches, but under\nfalse premise conditions produces relative cIOU improvements of more than 31%\nover baselines, and produces natural language feedback judged helpful up to 67%\nof the time.\n","authors":["Tsung-Han Wu","Giscard Biamby","David Chan","Lisa Dunlap","Ritwik Gupta","Xudong Wang","Joseph E. Gonzalez","Trevor Darrell"],"pdf_url":"https://arxiv.org/pdf/2312.08366v1.pdf","comment":"Project Page: https://see-say-segment.github.io"},{"id":"http://arxiv.org/abs/2312.08364v1","updated":"2023-12-13T18:56:13Z","published":"2023-12-13T18:56:13Z","title":"View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for\n  Procedural Synthetic Data","summary":"  Procedural synthetic data generation has received increasing attention in\ncomputer vision. Procedural signed distance functions (SDFs) are a powerful\ntool for modeling large-scale detailed scenes, but existing mesh extraction\nmethods have artifacts or performance profiles that limit their use for\nsynthetic data. We propose OcMesher, a mesh extraction algorithm that\nefficiently handles high-detail unbounded scenes with perfect view-consistency,\nwith easy export to downstream real-time engines. The main novelty of our\nsolution is an algorithm to construct an octree based on a given SDF and\nmultiple camera views. We performed extensive experiments, and show our\nsolution produces better synthetic data for training and evaluation of computer\nvision models.\n","authors":["Zeyu Ma","Alexander Raistrick","Lahav Lipson","Jia Deng"],"pdf_url":"https://arxiv.org/pdf/2312.08364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08344v1","updated":"2023-12-13T18:28:09Z","published":"2023-12-13T18:28:09Z","title":"FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects","summary":"  We present FoundationPose, a unified foundation model for 6D object pose\nestimation and tracking, supporting both model-based and model-free setups. Our\napproach can be instantly applied at test-time to a novel object without\nfine-tuning, as long as its CAD model is given, or a small number of reference\nimages are captured. We bridge the gap between these two setups with a neural\nimplicit representation that allows for effective novel view synthesis, keeping\nthe downstream pose estimation modules invariant under the same unified\nframework. Strong generalizability is achieved via large-scale synthetic\ntraining, aided by a large language model (LLM), a novel transformer-based\narchitecture, and contrastive learning formulation. Extensive evaluation on\nmultiple public datasets involving challenging scenarios and objects indicate\nour unified approach outperforms existing methods specialized for each task by\na large margin. In addition, it even achieves comparable results to\ninstance-level methods despite the reduced assumptions. Project page:\nhttps://nvlabs.github.io/FoundationPose/\n","authors":["Bowen Wen","Wei Yang","Jan Kautz","Stan Birchfield"],"pdf_url":"https://arxiv.org/pdf/2312.08344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08343v1","updated":"2023-12-13T18:22:38Z","published":"2023-12-13T18:22:38Z","title":"Ehancing CT Image synthesis from multi-modal MRI data based on a\n  multi-task neural network framework","summary":"  Image segmentation, real-value prediction, and cross-modal translation are\ncritical challenges in medical imaging. In this study, we propose a versatile\nmulti-task neural network framework, based on an enhanced Transformer U-Net\narchitecture, capable of simultaneously, selectively, and adaptively addressing\nthese medical image tasks. Validation is performed on a public repository of\nhuman brain MR and CT images. We decompose the traditional problem of\nsynthesizing CT images into distinct subtasks, which include skull\nsegmentation, Hounsfield unit (HU) value prediction, and image sequential\nreconstruction. To enhance the framework's versatility in handling multi-modal\ndata, we expand the model with multiple image channels. Comparisons between\nsynthesized CT images derived from T1-weighted and T2-Flair images were\nconducted, evaluating the model's capability to integrate multi-modal\ninformation from both morphological and pixel value perspectives.\n","authors":["Zhuoyao Xin","Christopher Wu","Dong Liu","Chunming Gu","Jia Guo","Jun Hua"],"pdf_url":"https://arxiv.org/pdf/2312.08343v1.pdf","comment":"4 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2312.08338v1","updated":"2023-12-13T18:14:13Z","published":"2023-12-13T18:14:13Z","title":"Global Latent Neural Rendering","summary":"  A recent trend among generalizable novel view synthesis methods is to learn a\nrendering operator acting over single camera rays. This approach is promising\nbecause it removes the need for explicit volumetric rendering, but it\neffectively treats target images as collections of independent pixels. Here, we\npropose to learn a global rendering operator acting over all camera rays\njointly. We show that the right representation to enable such rendering is the\n5-dimensional plane sweep volume, consisting of the projection of the input\nimages on a set of planes facing the target camera. Based on this\nunderstanding, we introduce our Convolutional Global Latent Renderer (ConvGLR),\nan efficient convolutional architecture that performs the rendering operation\nglobally in a low-resolution latent space. Experiments on various datasets\nunder sparse and generalizable setups show that our approach consistently\noutperforms existing methods by significant margins.\n","authors":["Thomas Tanay","Matteo Maggioni"],"pdf_url":"https://arxiv.org/pdf/2312.08338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08334v1","updated":"2023-12-13T18:11:37Z","published":"2023-12-13T18:11:37Z","title":"LD-SDM: Language-Driven Hierarchical Species Distribution Modeling","summary":"  We focus on the problem of species distribution modeling using global-scale\npresence-only data. Most previous studies have mapped the range of a given\nspecies using geographical and environmental features alone. To capture a\nstronger implicit relationship between species, we encode the taxonomic\nhierarchy of species using a large language model. This enables range mapping\nfor any taxonomic rank and unseen species without additional supervision.\nFurther, we propose a novel proximity-aware evaluation metric that enables\nevaluating species distribution models using any pixel-level representation of\nground-truth species range map. The proposed metric penalizes the predictions\nof a model based on its proximity to the ground truth. We describe the\neffectiveness of our model by systematically evaluating on the task of species\nrange prediction, zero-shot prediction and geo-feature regression against the\nstate-of-the-art. Results show our model outperforms the strong baselines when\ntrained with a variety of multi-label learning losses.\n","authors":["Srikumar Sastry","Xin Xing","Aayush Dhakal","Subash Khanal","Adeel Ahmad","Nathan Jacobs"],"pdf_url":"https://arxiv.org/pdf/2312.08334v1.pdf","comment":"17 pages, 9 figures"},{"id":"http://arxiv.org/abs/2308.09065v2","updated":"2023-12-13T18:01:23Z","published":"2023-08-17T15:54:11Z","title":"Discretization-Induced Dirichlet Posterior for Robust Uncertainty\n  Quantification on Regression","summary":"  Uncertainty quantification is critical for deploying deep neural networks\n(DNNs) in real-world applications. An Auxiliary Uncertainty Estimator (AuxUE)\nis one of the most effective means to estimate the uncertainty of the main task\nprediction without modifying the main task model. To be considered robust, an\nAuxUE must be capable of maintaining its performance and triggering higher\nuncertainties while encountering Out-of-Distribution (OOD) inputs, i.e., to\nprovide robust aleatoric and epistemic uncertainty. However, for vision\nregression tasks, current AuxUE designs are mainly adopted for aleatoric\nuncertainty estimates, and AuxUE robustness has not been explored. In this\nwork, we propose a generalized AuxUE scheme for more robust uncertainty\nquantification on regression tasks. Concretely, to achieve a more robust\naleatoric uncertainty estimation, different distribution assumptions are\nconsidered for heteroscedastic noise, and Laplace distribution is finally\nchosen to approximate the prediction error. For epistemic uncertainty, we\npropose a novel solution named Discretization-Induced Dirichlet pOsterior\n(DIDO), which models the Dirichlet posterior on the discretized prediction\nerror. Extensive experiments on age estimation, monocular depth estimation, and\nsuper-resolution tasks show that our proposed method can provide robust\nuncertainty estimates in the face of noisy inputs and that it can be scalable\nto both image-level and pixel-wise tasks. Code is available at\nhttps://github.com/ENSTA-U2IS/DIDO .\n","authors":["Xuanlong Yu","Gianni Franchi","Jindong Gu","Emanuel Aldea"],"pdf_url":"https://arxiv.org/pdf/2308.09065v2.pdf","comment":"23 pages with main paper and supplymentary material. Accepted at AAAI\n  2024"},{"id":"http://arxiv.org/abs/2303.11420v3","updated":"2023-12-13T17:50:32Z","published":"2023-03-21T13:31:15Z","title":"ADCNet: Learning from Raw Radar Data via Distillation","summary":"  As autonomous vehicles and advanced driving assistance systems have entered\nwider deployment, there is an increased interest in building robust perception\nsystems using radars. Radar-based systems are lower cost and more robust to\nadverse weather conditions than their LiDAR-based counterparts; however the\npoint clouds produced are typically noisy and sparse by comparison. In order to\ncombat these challenges, recent research has focused on consuming the raw radar\ndata, instead of the final radar point cloud. We build on this line of work and\ndemonstrate that by bringing elements of the signal processing pipeline into\nour network and then pre-training on the signal processing task, we are able to\nachieve state of the art detection performance on the RADIal dataset. Our\nmethod uses expensive offline signal processing algorithms to pseudo-label data\nand trains a network to distill this information into a fast convolutional\nbackbone, which can then be finetuned for perception tasks. Extensive\nexperiment results corroborate the effectiveness of the proposed techniques.\n","authors":["Bo Yang","Ishan Khatri","Michael Happold","Chulong Chen"],"pdf_url":"https://arxiv.org/pdf/2303.11420v3.pdf","comment":"Update 12/13/2023: upgrade organization and presentation of the\n  paper, adding appendix"},{"id":"http://arxiv.org/abs/2312.08323v1","updated":"2023-12-13T17:50:31Z","published":"2023-12-13T17:50:31Z","title":"PnPNet: Pull-and-Push Networks for Volumetric Segmentation with Boundary\n  Confusion","summary":"  Precise boundary segmentation of volumetric images is a critical task for\nimage-guided diagnosis and computer-assisted intervention, especially for\nboundary confusion in clinical practice. However, U-shape networks cannot\neffectively resolve this challenge due to the lack of boundary shape\nconstraints. Besides, existing methods of refining boundaries overemphasize the\nslender structure, which results in the overfitting phenomenon due to networks'\nlimited abilities to model tiny objects. In this paper, we reconceptualize the\nmechanism of boundary generation by encompassing the interaction dynamics with\nadjacent regions. Moreover, we propose a unified network termed PnPNet to model\nshape characteristics of the confused boundary region. Core ingredients of\nPnPNet contain the pushing and pulling branches. Specifically, based on\ndiffusion theory, we devise the semantic difference module (SDM) from the\npushing branch to squeeze the boundary region. Explicit and implicit\ndifferential information inside SDM significantly boost representation\nabilities for inter-class boundaries. Additionally, motivated by the K-means\nalgorithm, the class clustering module (CCM) from the pulling branch is\nintroduced to stretch the intersected boundary region. Thus, pushing and\npulling branches will shrink and enlarge the boundary uncertainty respectively.\nThey furnish two adversarial forces to promote models to output a more precise\ndelineation of boundaries. We carry out experiments on three challenging public\ndatasets and one in-house dataset, containing three types of boundary confusion\nin model predictions. Experimental results demonstrate the superiority of\nPnPNet over other segmentation networks, especially on evaluation metrics of HD\nand ASSD. Besides, pushing and pulling branches can serve as plug-and-play\nmodules to enhance classic U-shape baseline models. Codes are available.\n","authors":["Xin You","Ming Ding","Minghui Zhang","Hanxiao Zhang","Yi Yu","Jie Yang","Yun Gu"],"pdf_url":"https://arxiv.org/pdf/2312.08323v1.pdf","comment":"13 Figures, 8 Tables"},{"id":"http://arxiv.org/abs/2304.14365v3","updated":"2023-12-13T17:41:17Z","published":"2023-04-27T17:40:08Z","title":"Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous\n  Driving","summary":"  Robotic perception requires the modeling of both 3D geometry and semantics.\nExisting methods typically focus on estimating 3D bounding boxes, neglecting\nfiner geometric details and struggling to handle general, out-of-vocabulary\nobjects. 3D occupancy prediction, which estimates the detailed occupancy states\nand semantics of a scene, is an emerging task to overcome these limitations. To\nsupport 3D occupancy prediction, we develop a label generation pipeline that\nproduces dense, visibility-aware labels for any given scene. This pipeline\ncomprises three stages: voxel densification, occlusion reasoning, and\nimage-guided voxel refinement. We establish two benchmarks, derived from the\nWaymo Open Dataset and the nuScenes Dataset, namely Occ3D-Waymo and\nOcc3D-nuScenes benchmarks. Furthermore, we provide an extensive analysis of the\nproposed dataset with various baseline models. Lastly, we propose a new model,\ndubbed Coarse-to-Fine Occupancy (CTF-Occ) network, which demonstrates superior\nperformance on the Occ3D benchmarks. The code, data, and benchmarks are\nreleased at https://tsinghua-mars-lab.github.io/Occ3D/.\n","authors":["Xiaoyu Tian","Tao Jiang","Longfei Yun","Yucheng Mao","Huitong Yang","Yue Wang","Yilun Wang","Hang Zhao"],"pdf_url":"https://arxiv.org/pdf/2304.14365v3.pdf","comment":"Accepted to NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.10594v2","updated":"2023-12-13T17:29:15Z","published":"2023-10-16T17:16:32Z","title":"Motion2Language, unsupervised learning of synchronized semantic motion\n  segmentation","summary":"  In this paper, we investigate building a sequence to sequence architecture\nfor motion to language translation and synchronization. The aim is to translate\nmotion capture inputs into English natural-language descriptions, such that the\ndescriptions are generated synchronously with the actions performed, enabling\nsemantic segmentation as a byproduct, but without requiring synchronized\ntraining data. We propose a new recurrent formulation of local attention that\nis suited for synchronous/live text generation, as well as an improved motion\nencoder architecture better suited to smaller data and for synchronous\ngeneration. We evaluate both contributions in individual experiments, using the\nstandard BLEU4 metric, as well as a simple semantic equivalence measure, on the\nKIT motion language dataset. In a follow-up experiment, we assess the quality\nof the synchronization of generated text in our proposed approaches through\nmultiple evaluation metrics. We find that both contributions to the attention\nmechanism and the encoder architecture additively improve the quality of\ngenerated text (BLEU and semantic equivalence), but also of synchronization.\nOur code is available at\nhttps://github.com/rd20karim/M2T-Segmentation/tree/main\n","authors":["Karim Radouane","Andon Tchechmedjiev","Julien Lagarde","Sylvie Ranwez"],"pdf_url":"https://arxiv.org/pdf/2310.10594v2.pdf","comment":"Published at Neural Computing and Applications"},{"id":"http://arxiv.org/abs/2311.13435v2","updated":"2023-12-13T17:24:10Z","published":"2023-11-22T14:48:30Z","title":"PG-Video-LLaVA: Pixel Grounding Large Video-Language Models","summary":"  Extending image-based Large Multimodal Models (LMMs) to videos is challenging\ndue to the inherent complexity of video data. The recent approaches extending\nimage-based LMMs to videos either lack the grounding capabilities (e.g.,\nVideoChat, Video-ChatGPT, Video-LLaMA) or do not utilize the audio-signals for\nbetter video understanding (e.g., Video-ChatGPT). Addressing these gaps, we\npropose PG-Video-LLaVA, the first LMM with pixel-level grounding capability,\nintegrating audio cues by transcribing them into text to enrich video-context\nunderstanding. Our framework uses an off-the-shelf tracker and a novel\ngrounding module, enabling it to spatially localize objects in videos following\nuser instructions. We evaluate PG-Video-LLaVA using video-based generative and\nquestion-answering benchmarks and introduce new benchmarks specifically\ndesigned to measure prompt-based object grounding performance in videos.\nFurther, we propose the use of Vicuna over GPT-3.5, as utilized in\nVideo-ChatGPT, for video-based conversation benchmarking, ensuring\nreproducibility of results which is a concern with the proprietary nature of\nGPT-3.5. Our framework builds on SoTA image-based LLaVA model and extends its\nadvantages to the video domain, delivering promising gains on video-based\nconversation and grounding tasks. Project Page:\nhttps://github.com/mbzuai-oryx/Video-LLaVA\n","authors":["Shehan Munasinghe","Rusiru Thushara","Muhammad Maaz","Hanoona Abdul Rasheed","Salman Khan","Mubarak Shah","Fahad Khan"],"pdf_url":"https://arxiv.org/pdf/2311.13435v2.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2312.08291v1","updated":"2023-12-13T17:08:38Z","published":"2023-12-13T17:08:38Z","title":"VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent\n  Space","summary":"  Human Pose and Shape Estimation (HPSE) from RGB images can be broadly\ncategorized into two main groups: parametric and non-parametric approaches.\nParametric techniques leverage a low-dimensional statistical body model for\nrealistic results, whereas recent non-parametric methods achieve higher\nprecision by directly regressing the 3D coordinates of the human body. Despite\ntheir strengths, both approaches face limitations: the parameters of\nstatistical body models pose challenges as regression targets, and predicting\n3D coordinates introduces computational complexities and issues related to\nsmoothness. In this work, we take a novel approach to address the HPSE problem.\nWe introduce a unique method involving a low-dimensional discrete latent\nrepresentation of the human mesh, framing HPSE as a classification task.\nInstead of predicting body model parameters or 3D vertex coordinates, our focus\nis on forecasting the proposed discrete latent representation, which can be\ndecoded into a registered human mesh. This innovative paradigm offers two key\nadvantages: firstly, predicting a low-dimensional discrete representation\nconfines our predictions to the space of anthropomorphic poses and shapes;\nsecondly, by framing the problem as a classification task, we can harness the\ndiscriminative power inherent in neural networks. Our proposed model, VQ-HPS, a\ntransformer-based architecture, forecasts the discrete latent representation of\nthe mesh, trained through minimizing a cross-entropy loss. Our results\ndemonstrate that VQ-HPS outperforms the current state-of-the-art non-parametric\napproaches while yielding results as realistic as those produced by parametric\nmethods. This highlights the significant potential of the classification\napproach for HPSE.\n","authors":["Guénolé Fiche","Simon Leglaive","Xavier Alameda-Pineda","Antonio Agudo","Francesc Moreno-Noguer"],"pdf_url":"https://arxiv.org/pdf/2312.08291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08288v1","updated":"2023-12-13T17:04:16Z","published":"2023-12-13T17:04:16Z","title":"Hybrid Sample Synthesis-based Debiasing of Classifier in Limited Data\n  Setting","summary":"  Deep learning models are known to suffer from the problem of bias, and\nresearchers have been exploring methods to address this issue. However, most of\nthese methods require prior knowledge of the bias and are not always practical.\nIn this paper, we focus on a more practical setting with no prior information\nabout the bias. Generally, in this setting, there are a large number of\nbias-aligned samples that cause the model to produce biased predictions and a\nfew bias-conflicting samples that do not conform to the bias. If the training\ndata is limited, the influence of the bias-aligned samples may become even\nstronger on the model predictions, and we experimentally demonstrate that\nexisting debiasing techniques suffer severely in such cases. In this paper, we\nexamine the effects of unknown bias in small dataset regimes and present a\nnovel approach to mitigate this issue. The proposed approach directly addresses\nthe issue of the extremely low occurrence of bias-conflicting samples in\nlimited data settings through the synthesis of hybrid samples that can be used\nto reduce the effect of bias. We perform extensive experiments on several\nbenchmark datasets and experimentally demonstrate the effectiveness of our\nproposed approach in addressing any unknown bias in the presence of limited\ndata. Specifically, our approach outperforms the vanilla, LfF, LDD, and DebiAN\ndebiasing methods by absolute margins of 10.39%, 9.08%, 8.07%, and 9.67% when\nonly 10% of the Corrupted CIFAR-10 Type 1 dataset is available with a\nbias-conflicting sample ratio of 0.05.\n","authors":["Piyush Arora","Pratik Mazumder"],"pdf_url":"https://arxiv.org/pdf/2312.08288v1.pdf","comment":"Accepted in WACV 2024"},{"id":"http://arxiv.org/abs/2311.03426v2","updated":"2023-12-13T16:57:19Z","published":"2023-11-06T17:29:24Z","title":"GQKVA: Efficient Pre-training of Transformers by Grouping Queries, Keys,\n  and Values","summary":"  Massive transformer-based models face several challenges, including slow and\ncomputationally intensive pre-training and over-parametrization. This paper\naddresses these challenges by proposing a versatile method called GQKVA, which\ngeneralizes query, key, and value grouping techniques. GQKVA is designed to\nspeed up transformer pre-training while reducing the model size. Our\nexperiments with various GQKVA variants highlight a clear trade-off between\nperformance and model size, allowing for customized choices based on resource\nand time limitations. Our findings also indicate that the conventional\nmulti-head attention approach is not always the best choice, as there are\nlighter and faster alternatives available. We tested our method on ViT, which\nachieved an approximate 0.3% increase in accuracy while reducing the model size\nby about 4% in the task of image classification. Additionally, our most\naggressive model reduction experiment resulted in a reduction of approximately\n15% in model size, with only around a 1% drop in accuracy.\n","authors":["Farnoosh Javadi","Walid Ahmed","Habib Hajimolahoseini","Foozhan Ataiefard","Mohammad Hassanpour","Saina Asani","Austin Wen","Omar Mohamed Awad","Kangling Liu","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2311.03426v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08268v1","updated":"2023-12-13T16:30:00Z","published":"2023-12-13T16:30:00Z","title":"Efficient Multi-Object Pose Estimation using Multi-Resolution Deformable\n  Attention and Query Aggregation","summary":"  Object pose estimation is a long-standing problem in computer vision.\nRecently, attention-based vision transformer models have achieved\nstate-of-the-art results in many computer vision applications. Exploiting the\npermutation-invariant nature of the attention mechanism, a family of vision\ntransformer models formulate multi-object pose estimation as a set prediction\nproblem. However, existing vision transformer models for multi-object pose\nestimation rely exclusively on the attention mechanism. Convolutional neural\nnetworks, on the other hand, hard-wire various inductive biases into their\narchitecture. In this paper, we investigate incorporating inductive biases in\nvision transformer models for multi-object pose estimation, which facilitates\nlearning long-range dependencies while circumventing the costly global\nattention. In particular, we use multi-resolution deformable attention, where\nthe attention operation is performed only between a few deformed reference\npoints. Furthermore, we propose a query aggregation mechanism that enables\nincreasing the number of object queries without increasing the computational\ncomplexity. We evaluate the proposed model on the challenging YCB-Video dataset\nand report state-of-the-art results.\n","authors":["Arul Selvam Periyasamy","Vladimir Tsaturyan","Sven Behnke"],"pdf_url":"https://arxiv.org/pdf/2312.08268v1.pdf","comment":"IEEE International Conference on Robotic Computing (IRC), Laguna\n  Hills, USA, December 2023"},{"id":"http://arxiv.org/abs/2312.08267v1","updated":"2023-12-13T16:29:28Z","published":"2023-12-13T16:29:28Z","title":"TABSurfer: a Hybrid Deep Learning Architecture for Subcortical\n  Segmentation","summary":"  Subcortical segmentation remains challenging despite its important\napplications in quantitative structural analysis of brain MRI scans. The most\naccurate method, manual segmentation, is highly labor intensive, so automated\ntools like FreeSurfer have been adopted to handle this task. However, these\ntraditional pipelines are slow and inefficient for processing large datasets.\nIn this study, we propose TABSurfer, a novel 3D patch-based CNN-Transformer\nhybrid deep learning model designed for superior subcortical segmentation\ncompared to existing state-of-the-art tools. To evaluate, we first demonstrate\nTABSurfer's consistent performance across various T1w MRI datasets with\nsignificantly shorter processing times compared to FreeSurfer. Then, we\nvalidate against manual segmentations, where TABSurfer outperforms FreeSurfer\nbased on the manual ground truth. In each test, we also establish TABSurfer's\nadvantage over a leading deep learning benchmark, FastSurferVINN. Together,\nthese studies highlight TABSurfer's utility as a powerful tool for fully\nautomated subcortical segmentation with high fidelity.\n","authors":["Aaron Cao","Vishwanatha M. Rao","Kejia Liu","Xinru Liu","Andrew F. Laine","Jia Guo"],"pdf_url":"https://arxiv.org/pdf/2312.08267v1.pdf","comment":"5 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2312.08256v1","updated":"2023-12-13T16:18:45Z","published":"2023-12-13T16:18:45Z","title":"A Compact and Semantic Latent Space for Disentangled and Controllable\n  Image Editing","summary":"  Recent advances in the field of generative models and in particular\ngenerative adversarial networks (GANs) have lead to substantial progress for\ncontrolled image editing, especially compared with the pre-deep learning era.\nDespite their powerful ability to apply realistic modifications to an image,\nthese methods often lack properties like disentanglement (the capacity to edit\nattributes independently). In this paper, we propose an auto-encoder which\nre-organizes the latent space of StyleGAN, so that each attribute which we wish\nto edit corresponds to an axis of the new latent space, and furthermore that\nthe latent axes are decorrelated, encouraging disentanglement. We work in a\ncompressed version of the latent space, using Principal Component Analysis,\nmeaning that the parameter complexity of our autoencoder is reduced, leading to\nshort training times ($\\sim$ 45 mins). Qualitative and quantitative results\ndemonstrate the editing capabilities of our approach, with greater\ndisentanglement than competing methods, while maintaining fidelity to the\noriginal image with respect to identity. Our autoencoder architecture simple\nand straightforward, facilitating implementation.\n","authors":["Gwilherm Lesné","Yann Gousseau","Saïd Ladjal","Alasdair Newson"],"pdf_url":"https://arxiv.org/pdf/2312.08256v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08255v1","updated":"2023-12-13T16:18:40Z","published":"2023-12-13T16:18:40Z","title":"OCTDL: Optical Coherence Tomography Dataset for Image-Based Deep\n  Learning Methods","summary":"  Optical coherence tomography (OCT) is a non-invasive imaging technique with\nextensive clinical applications in ophthalmology. OCT enables the visualization\nof the retinal layers, playing a vital role in the early detection and\nmonitoring of retinal diseases. OCT uses the principle of light wave\ninterference to create detailed images of the retinal microstructures, making\nit a valuable tool for diagnosing ocular conditions. This work presents an\nopen-access OCT dataset (OCTDL) comprising over 1600 high-resolution OCT images\nlabeled according to disease group and retinal pathology. The dataset consists\nof OCT records of patients with Age-related Macular Degeneration (AMD),\nDiabetic Macular Edema (DME), Epiretinal Membrane (ERM), Retinal Artery\nOcclusion (RAO), Retinal Vein Occlusion (RVO), and Vitreomacular Interface\nDisease (VID). The images were acquired with an Optovue Avanti RTVue XR using\nraster scanning protocols with dynamic scan length and image resolution. Each\nretinal b-scan was acquired by centering on the fovea and interpreted and\ncataloged by an experienced retinal specialist. In this work, we applied Deep\nLearning classification techniques to this new open-access dataset.\n","authors":["Mikhail Kulyabin","Aleksei Zhdanov","Anastasia Nikiforova","Andrey Stepichev","Anna Kuznetsova","Mikhail Ronkin","Vasilii Borisov","Alexander Bogachev","Sergey Korotkich","Paul A Constable","Andreas Maier"],"pdf_url":"https://arxiv.org/pdf/2312.08255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08240v1","updated":"2023-12-13T16:01:50Z","published":"2023-12-13T16:01:50Z","title":"CenterGrasp: Object-Aware Implicit Representation Learning for\n  Simultaneous Shape Reconstruction and 6-DoF Grasp Estimation","summary":"  Reliable object grasping is a crucial capability for autonomous robots.\nHowever, many existing grasping approaches focus on general clutter removal\nwithout explicitly modeling objects and thus only relying on the visible local\ngeometry. We introduce CenterGrasp, a novel framework that combines object\nawareness and holistic grasping. CenterGrasp learns a general object prior by\nencoding shapes and valid grasps in a continuous latent space. It consists of\nan RGB-D image encoder that leverages recent advances to detect objects and\ninfer their pose and latent code, and a decoder to predict shape and grasps for\neach object in the scene. We perform extensive experiments on simulated as well\nas real-world cluttered scenes and demonstrate strong scene reconstruction and\n6-DoF grasp-pose estimation performance. Compared to the state of the art,\nCenterGrasp achieves an improvement of 38.5 mm in shape reconstruction and 33\npercentage points on average in grasp success. We make the code and trained\nmodels publicly available at http://centergrasp.cs.uni-freiburg.de.\n","authors":["Eugenio Chisari","Nick Heppert","Tim Welschehold","Wolfram Burgard","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2312.08240v1.pdf","comment":"Submitted to RA-L. Video, code and models available at\n  http://centergrasp.cs.uni-freiburg.de"},{"id":"http://arxiv.org/abs/2312.08234v1","updated":"2023-12-13T15:56:24Z","published":"2023-12-13T15:56:24Z","title":"Beyond the Label Itself: Latent Labels Enhance Semi-supervised Point\n  Cloud Panoptic Segmentation","summary":"  As the exorbitant expense of labeling autopilot datasets and the growing\ntrend of utilizing unlabeled data, semi-supervised segmentation on point clouds\nbecomes increasingly imperative. Intuitively, finding out more ``unspoken\nwords'' (i.e., latent instance information) beyond the label itself should be\nhelpful to improve performance. In this paper, we discover two types of latent\nlabels behind the displayed label embedded in LiDAR and image data. First, in\nthe LiDAR Branch, we propose a novel augmentation, Cylinder-Mix, which is able\nto augment more yet reliable samples for training. Second, in the Image Branch,\nwe propose the Instance Position-scale Learning (IPSL) Module to learn and fuse\nthe information of instance position and scale, which is from a 2D pre-trained\ndetector and a type of latent label obtained from 3D to 2D projection. Finally,\nthe two latent labels are embedded into the multi-modal panoptic segmentation\nnetwork. The ablation of the IPSL module demonstrates its robust adaptability,\nand the experiments evaluated on SemanticKITTI and nuScenes demonstrate that\nour model outperforms the state-of-the-art method, LaserMix.\n","authors":["Yujun Chen","Xin Tan","Zhizhong Zhang","Yanyun Qu","Yuan Xie"],"pdf_url":"https://arxiv.org/pdf/2312.08234v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2312.08230v1","updated":"2023-12-13T15:48:50Z","published":"2023-12-13T15:48:50Z","title":"Partial Symmetry Detection for 3D Geometry using Contrastive Learning\n  with Geodesic Point Cloud Patches","summary":"  Symmetry detection, especially partial and extrinsic symmetry, is essential\nfor various downstream tasks, like 3D geometry completion, segmentation,\ncompression and structure-aware shape encoding or generation. In order to\ndetect partial extrinsic symmetries, we propose to learn rotation, reflection,\ntranslation and scale invariant local shape features for geodesic point cloud\npatches via contrastive learning, which are robust across multiple classes and\ngeneralize over different datasets. We show that our approach is able to\nextract multiple valid solutions for this ambiguous problem. Furthermore, we\nintroduce a novel benchmark test for partial extrinsic symmetry detection to\nevaluate our method. Lastly, we incorporate the detected symmetries together\nwith a region growing algorithm to demonstrate a downstream task with the goal\nof computing symmetry-aware partitions of 3D shapes. To our knowledge, we are\nthe first to propose a self-supervised data-driven method for partial extrinsic\nsymmetry detection.\n","authors":["Gregor Kobsik","Isaak Lim","Leif Kobbelt"],"pdf_url":"https://arxiv.org/pdf/2312.08230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08223v1","updated":"2023-12-13T15:45:19Z","published":"2023-12-13T15:45:19Z","title":"Patch-wise Graph Contrastive Learning for Image Translation","summary":"  Recently, patch-wise contrastive learning is drawing attention for the image\ntranslation by exploring the semantic correspondence between the input and\noutput images. To further explore the patch-wise topology for high-level\nsemantic understanding, here we exploit the graph neural network to capture the\ntopology-aware features. Specifically, we construct the graph based on the\npatch-wise similarity from a pretrained encoder, whose adjacency matrix is\nshared to enhance the consistency of patch-wise relation between the input and\nthe output. Then, we obtain the node feature from the graph neural network, and\nenhance the correspondence between the nodes by increasing mutual information\nusing the contrastive loss. In order to capture the hierarchical semantic\nstructure, we further propose the graph pooling. Experimental results\ndemonstrate the state-of-art results for the image translation thanks to the\nsemantic encoding by the constructed graphs.\n","authors":["Chanyong Jung","Gihyun Kwon","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2312.08223v1.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2312.08220v1","updated":"2023-12-13T15:42:04Z","published":"2023-12-13T15:42:04Z","title":"EventAid: Benchmarking Event-aided Image/Video Enhancement Algorithms\n  with Real-captured Hybrid Dataset","summary":"  Event cameras are emerging imaging technology that offers advantages over\nconventional frame-based imaging sensors in dynamic range and sensing speed.\nComplementing the rich texture and color perception of traditional image\nframes, the hybrid camera system of event and frame-based cameras enables\nhigh-performance imaging. With the assistance of event cameras, high-quality\nimage/video enhancement methods make it possible to break the limits of\ntraditional frame-based cameras, especially exposure time, resolution, dynamic\nrange, and frame rate limits. This paper focuses on five event-aided image and\nvideo enhancement tasks (i.e., event-based video reconstruction, event-aided\nhigh frame rate video reconstruction, image deblurring, image super-resolution,\nand high dynamic range image reconstruction), provides an analysis of the\neffects of different event properties, a real-captured and ground truth labeled\nbenchmark dataset, a unified benchmarking of state-of-the-art methods, and an\nevaluation for two mainstream event simulators. In detail, this paper collects\na real-captured evaluation dataset EventAid for five event-aided image/video\nenhancement tasks, by using \"Event-RGB\" multi-camera hybrid system, taking into\naccount scene diversity and spatiotemporal synchronization. We further perform\nquantitative and visual comparisons for state-of-the-art algorithms, provide a\ncontrolled experiment to analyze the performance limit of event-aided image\ndeblurring methods, and discuss open problems to inspire future research.\n","authors":["Peiqi Duan","Boyu Li","Yixin Yang","Hanyue Lou","Minggui Teng","Yi Ma","Boxin Shi"],"pdf_url":"https://arxiv.org/pdf/2312.08220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10410v2","updated":"2023-12-13T15:30:38Z","published":"2023-10-16T13:53:37Z","title":"Loci-Segmented: Improving Scene Segmentation Learning","summary":"  Slot-oriented approaches for compositional scene segmentation from images and\nvideos still depend on provided background information or slot assignments. We\npresent Loci-Segmented (Loci-s) building on the slot-based location and\nidentity tracking architecture Loci (Traub et al., ICLR 2023). Loci-s enables\ndynamic (i) background processing by means of a foreground identifying module\nand a background re-generator; (ii) top-down modified object-focused bottom-up\nprocessing; and (iii) depth estimate generation. We also improve automatic slot\nassignment via a slot-location-entity regularization mechanism and a prior\nsegmentation network. The results reveal superior video decomposition\nperformance in the MOVi datasets and in another established dataset collection\ntargeting scene segmentation. Loci-s outperforms the state-of-the-art with\nrespect to the intersection over union (IoU) score in the multi-object video\ndataset MOVi-E by a large margin and even without supervised slot assignments\nand without the provision of background information. We furthermore show that\nLoci-s generates well-interpretable latent representations. These\nrepresentations may serve as a foundation-model-like interpretable basis for\nsolving downstream tasks, such as grounding language, forming compositional\nrules, or solving one-shot reinforcement learning tasks.\n","authors":["Manuel Traub","Frederic Becker","Adrian Sauter","Sebastian Otte","Martin V. Butz"],"pdf_url":"https://arxiv.org/pdf/2310.10410v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08213v1","updated":"2023-12-13T15:30:29Z","published":"2023-12-13T15:30:29Z","title":"Accelerated Event-Based Feature Detection and Compression for\n  Surveillance Video Systems","summary":"  The strong temporal consistency of surveillance video enables compelling\ncompression performance with traditional methods, but downstream vision\napplications operate on decoded image frames with a high data rate. Since it is\nnot straightforward for applications to extract information on temporal\nredundancy from the compressed video representations, we propose a novel system\nwhich conveys temporal redundancy within a sparse decompressed representation.\nWe leverage a video representation framework called ADDER to transcode framed\nvideos to sparse, asynchronous intensity samples. We introduce mechanisms for\ncontent adaptation, lossy compression, and asynchronous forms of classical\nvision algorithms. We evaluate our system on the VIRAT surveillance video\ndataset, and we show a median 43.7% speed improvement in FAST feature detection\ncompared to OpenCV. We run the same algorithm as OpenCV, but only process\npixels that receive new asynchronous events, rather than process every pixel in\nan image frame. Our work paves the way for upcoming neuromorphic sensors and is\namenable to future applications with spiking neural networks.\n","authors":["Andrew C. Freeman","Ketan Mayer-Patel","Montek Singh"],"pdf_url":"https://arxiv.org/pdf/2312.08213v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08212v1","updated":"2023-12-13T15:29:52Z","published":"2023-12-13T15:29:52Z","title":"LAMM: Label Alignment for Multi-Modal Prompt Learning","summary":"  With the success of pre-trained visual-language (VL) models such as CLIP in\nvisual representation tasks, transferring pre-trained models to downstream\ntasks has become a crucial paradigm. Recently, the prompt tuning paradigm,\nwhich draws inspiration from natural language processing (NLP), has made\nsignificant progress in VL field. However, preceding methods mainly focus on\nconstructing prompt templates for text and visual inputs, neglecting the gap in\nclass label representations between the VL models and downstream tasks. To\naddress this challenge, we introduce an innovative label alignment method named\n\\textbf{LAMM}, which can dynamically adjust the category embeddings of\ndownstream datasets through end-to-end training. Moreover, to achieve a more\nappropriate label distribution, we propose a hierarchical loss, encompassing\nthe alignment of the parameter space, feature space, and logits space. We\nconduct experiments on 11 downstream vision datasets and demonstrate that our\nmethod significantly improves the performance of existing multi-modal prompt\nlearning models in few-shot scenarios, exhibiting an average accuracy\nimprovement of 2.31(\\%) compared to the state-of-the-art methods on 16 shots.\nMoreover, our methodology exhibits the preeminence in continual learning\ncompared to other prompt tuning methods. Importantly, our method is synergistic\nwith existing prompt tuning methods and can boost the performance on top of\nthem. Our code and dataset will be publicly available at\nhttps://github.com/gaojingsheng/LAMM.\n","authors":["Jingsheng Gao","Jiacheng Ruan","Suncheng Xiang","Zefang Yu","Ke Ji","Mingye Xie","Ting Liu","Yuzhuo Fu"],"pdf_url":"https://arxiv.org/pdf/2312.08212v1.pdf","comment":"Accepted at AAAI 2024 Main Conference"},{"id":"http://arxiv.org/abs/2312.04334v2","updated":"2023-12-13T15:00:56Z","published":"2023-12-07T14:51:12Z","title":"Towards a Perceptual Evaluation Framework for Lighting Estimation","summary":"  Progress in lighting estimation is tracked by computing existing image\nquality assessment (IQA) metrics on images from standard datasets. While this\nmay appear to be a reasonable approach, we demonstrate that doing so does not\ncorrelate to human preference when the estimated lighting is used to relight a\nvirtual scene into a real photograph. To study this, we design a controlled\npsychophysical experiment where human observers must choose their preference\namongst rendered scenes lit using a set of lighting estimation algorithms\nselected from the recent literature, and use it to analyse how these algorithms\nperform according to human perception. Then, we demonstrate that none of the\nmost popular IQA metrics from the literature, taken individually, correctly\nrepresent human perception. Finally, we show that by learning a combination of\nexisting IQA metrics, we can more accurately represent human preference. This\nprovides a new perceptual framework to help evaluate future lighting estimation\nalgorithms.\n","authors":["Justine Giroux","Mohammad Reza Karimi Dastjerdi","Yannick Hold-Geoffroy","Javier Vazquez-Corral","Jean-François Lalonde"],"pdf_url":"https://arxiv.org/pdf/2312.04334v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08195v1","updated":"2023-12-13T14:59:49Z","published":"2023-12-13T14:59:49Z","title":"Concept-centric Personalization with Large-scale Diffusion Priors","summary":"  Despite large-scale diffusion models being highly capable of generating\ndiverse open-world content, they still struggle to match the photorealism and\nfidelity of concept-specific generators. In this work, we present the task of\ncustomizing large-scale diffusion priors for specific concepts as\nconcept-centric personalization. Our goal is to generate high-quality\nconcept-centric images while maintaining the versatile controllability inherent\nto open-world models, enabling applications in diverse tasks such as\nconcept-centric stylization and image translation. To tackle these challenges,\nwe identify catastrophic forgetting of guidance prediction from diffusion\npriors as the fundamental issue. Consequently, we develop a guidance-decoupled\npersonalization framework specifically designed to address this task. We\npropose Generalized Classifier-free Guidance (GCFG) as the foundational theory\nfor our framework. This approach extends Classifier-free Guidance (CFG) to\naccommodate an arbitrary number of guidances, sourced from a variety of\nconditions and models. Employing GCFG enables us to separate conditional\nguidance into two distinct components: concept guidance for fidelity and\ncontrol guidance for controllability. This division makes it feasible to train\na specialized model for concept guidance, while ensuring both control and\nunconditional guidance remain intact. We then present a null-text\nConcept-centric Diffusion Model as a concept-specific generator to learn\nconcept guidance without the need for text annotations. Code will be available\nat https://github.com/PRIV-Creation/Concept-centric-Personalization.\n","authors":["Pu Cao","Lu Yang","Feng Zhou","Tianrui Huang","Qing Song"],"pdf_url":"https://arxiv.org/pdf/2312.08195v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08194v1","updated":"2023-12-13T14:58:25Z","published":"2023-12-13T14:58:25Z","title":"SVInvNet: A Densely Connected Encoder-Decoder Architecture for Seismic\n  Velocity Inversion","summary":"  This study presents a deep learning-based approach to seismic velocity\ninversion problem, focusing on both noisy and noiseless training datasets of\nvarying sizes. Our Seismic Velocity Inversion Network (SVInvNet) introduces a\nnovel architecture that contains a multi-connection encoder-decoder structure\nenhanced with dense blocks. This design is specifically tuned to effectively\nprocess complex information, crucial for addressing the challenges of\nnon-linear seismic velocity inversion. For training and testing, we created\ndiverse seismic velocity models, including multi-layered, faulty, and salt dome\ncategories. We also investigated how different kinds of ambient noise, both\ncoherent and stochastic, and the size of the training dataset affect learning\noutcomes. SVInvNet is trained on datasets ranging from 750 to 6,000 samples and\nis tested using a large benchmark dataset of 12,000 samples. Despite its fewer\nparameters compared to the baseline, SVInvNet achieves superior performance\nwith this dataset. The outcomes of the SVInvNet are additionally compared to\nthose of the Full Waveform Inversion (FWI) method. The comparative analysis\nclearly reveals the effectiveness of the proposed model.\n","authors":["Mojtaba Najafi Khatounabad","Hacer Yalim Keles","Selma Kadioglu"],"pdf_url":"https://arxiv.org/pdf/2312.08194v1.pdf","comment":"14 pages, 11 figures, submitted to IEEE Transactions on Geoscience\n  and Remote Sensing"},{"id":"http://arxiv.org/abs/2312.08193v1","updated":"2023-12-13T14:58:17Z","published":"2023-12-13T14:58:17Z","title":"Universal Adversarial Framework to Improve Adversarial Robustness for\n  Diabetic Retinopathy Detection","summary":"  Diabetic Retinopathy (DR) is a prevalent illness associated with Diabetes\nwhich, if left untreated, can result in irreversible blindness. Deep Learning\nbased systems are gradually being introduced as automated support for clinical\ndiagnosis. Since healthcare has always been an extremely important domain\ndemanding error-free performance, any adversaries could pose a big threat to\nthe applicability of such systems. In this work, we use Universal Adversarial\nPerturbations (UAPs) to quantify the vulnerability of Medical Deep Neural\nNetworks (DNNs) for detecting DR. To the best of our knowledge, this is the\nvery first attempt that works on attacking complete fine-grained classification\nof DR images using various UAPs. Also, as a part of this work, we use UAPs to\nfine-tune the trained models to defend against adversarial samples. We\nexperiment on several models and observe that the performance of such models\ntowards unseen adversarial attacks gets boosted on average by $3.41$\nCohen-kappa value and maximum by $31.92$ Cohen-kappa value. The performance\ndegradation on normal data upon ensembling the fine-tuned models was found to\nbe statistically insignificant using t-test, highlighting the benefits of\nUAP-based adversarial fine-tuning.\n","authors":["Samrat Mukherjee","Dibyanayan Bandyopadhyay","Baban Gain","Asif Ekbal"],"pdf_url":"https://arxiv.org/pdf/2312.08193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08192v1","updated":"2023-12-13T14:57:28Z","published":"2023-12-13T14:57:28Z","title":"PAD: Self-Supervised Pre-Training with Patchwise-Scale Adapter for\n  Infrared Images","summary":"  Self-supervised learning (SSL) for RGB images has achieved significant\nsuccess, yet there is still limited research on SSL for infrared images,\nprimarily due to three prominent challenges: 1) the lack of a suitable\nlarge-scale infrared pre-training dataset, 2) the distinctiveness of non-iconic\ninfrared images rendering common pre-training tasks like masked image modeling\n(MIM) less effective, and 3) the scarcity of fine-grained textures making it\nparticularly challenging to learn general image features. To address these\nissues, we construct a Multi-Scene Infrared Pre-training (MSIP) dataset\ncomprising 178,756 images, and introduce object-sensitive random RoI cropping,\nan image preprocessing method, to tackle the challenge posed by non-iconic\nimages. To alleviate the impact of weak textures on feature learning, we\npropose a pre-training paradigm called Pre-training with ADapter (PAD), which\nuses adapters to learn domain-specific features while freezing parameters\npre-trained on ImageNet to retain the general feature extraction capability.\nThis new paradigm is applicable to any transformer-based SSL method.\nFurthermore, to achieve more flexible coordination between pre-trained and\nnewly-learned features in different layers and patches, a patchwise-scale\nadapter with dynamically learnable scale factors is introduced. Extensive\nexperiments on three downstream tasks show that PAD, with only 1.23M\npre-trainable parameters, outperforms other baseline paradigms including\ncontinual full pre-training on MSIP. Our code and dataset are available at\nhttps://github.com/casiatao/PAD.\n","authors":["Tao Zhang","Kun Ding","Jinyong Wen","Yu Xiong","Zeyu Zhang","Shiming Xiang","Chunhong Pan"],"pdf_url":"https://arxiv.org/pdf/2312.08192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08177v1","updated":"2023-12-13T14:36:16Z","published":"2023-12-13T14:36:16Z","title":"Advanced Image Segmentation Techniques for Neural Activity Detection via\n  C-fos Immediate Early Gene Expression","summary":"  This paper investigates the application of advanced image segmentation\ntechniques to analyze C-fos immediate early gene expression, a crucial marker\nfor neural activity. Due to the complexity and high variability of neural\ncircuits, accurate segmentation of C-fos images is paramount for the\ndevelopment of new insights into neural function. Amidst this backdrop, this\nresearch aims to improve accuracy and minimize manual intervention in C-fos\nimage segmentation by leveraging the capabilities of CNNs and the Unet model.\nWe describe the development of a novel workflow for the segmentation process\ninvolving Convolutional Neural Networks (CNNs) and the Unet model,\ndemonstrating their efficiency in various image segmentation tasks. Our\nworkflow incorporates pre-processing steps such as cropping, image feature\nextraction, and clustering for the training dataset selection. We used an\nAutoEncoder model to extract features and implement constrained clustering to\nidentify similarities and differences in image types. Additionally, we utilized\nmanual and automatic labeling approaches to enhance the performance of our\nmodel. We demonstrated the effectiveness of our method in distinguishing areas\nwith significant C-fos expression from normal tissue areas. Lastly, we\nimplemented a modified Unet network for the detection of C-fos expressions.\nThis research contributes to the development of more efficient and automated\nimage segmentation methods, advancing the understanding of neural function in\nneuroscience research.\n","authors":["Peilin Cai"],"pdf_url":"https://arxiv.org/pdf/2312.08177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08176v1","updated":"2023-12-13T14:36:08Z","published":"2023-12-13T14:36:08Z","title":"ASC: Adaptive Scale Feature Map Compression for Deep Neural Network","summary":"  Deep-learning accelerators are increasingly in demand; however, their\nperformance is constrained by the size of the feature map, leading to high\nbandwidth requirements and large buffer sizes. We propose an adaptive scale\nfeature map compression technique leveraging the unique properties of the\nfeature map. This technique adopts independent channel indexing given the weak\nchannel correlation and utilizes a cubical-like block shape to benefit from\nstrong local correlations. The method further optimizes compression using a\nswitchable endpoint mode and adaptive scale interpolation to handle unimodal\ndata distributions, both with and without outliers. This results in 4$\\times$\nand up to 7.69$\\times$ compression rates for 16-bit data in constant and\nvariable bitrates, respectively. Our hardware design minimizes area cost by\nadjusting interpolation scales, which facilitates hardware sharing among\ninterpolation points. Additionally, we introduce a threshold concept for\nstraightforward interpolation, preventing the need for intricate hardware. The\nTSMC 28nm implementation showcases an equivalent gate count of 6135 for the\n8-bit version. Furthermore, the hardware architecture scales effectively, with\nonly a sublinear increase in area cost. Achieving a 32$\\times$ throughput\nincrease meets the theoretical bandwidth of DDR5-6400 at just 7.65$\\times$ the\nhardware cost.\n","authors":["Yuan Yao","Tian-Sheuan Chang"],"pdf_url":"https://arxiv.org/pdf/2312.08176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08168v1","updated":"2023-12-13T14:27:45Z","published":"2023-12-13T14:27:45Z","title":"Chat-3D v2: Bridging 3D Scene and Large Language Models with Object\n  Identifiers","summary":"  Recent research has evidenced the significant potentials of Large Language\nModels (LLMs) in handling challenging tasks within 3D scenes. However, current\nmodels are constrained to addressing object-centric tasks, where each\nquestion-answer pair focuses solely on an individual object. In real-world\napplications, users may pose queries involving multiple objects or expect for\nanswers that precisely reference various objects. We introduce the use of\nobject identifiers to freely reference objects during a conversation. While\nthis solution appears straightforward, it presents two main challenges: 1) How\nto establish a reliable one-to-one correspondence between each object and its\nidentifier? 2) How to incorporate complex spatial relationships among dozens of\nobjects into the embedding space of the LLM? To address these challenges, we\npropose a two-stage alignment method, which involves learning an\nattribute-aware token and a relation-aware token for each object. These tokens\ncapture the object's attributes and spatial relationships with surrounding\nobjects in the 3D scene. Once the alignment is established, we can fine-tune\nour model on various downstream tasks using instruction tuning. Experiments\nconducted on traditional datasets like ScanQA, ScanRefer, and Nr3D/Sr3D\nshowcase the effectiveness of our proposed method. Additionally, we create a 3D\nscene captioning dataset annotated with rich object identifiers, with the\nassistant of GPT-4. This dataset aims to further explore the capability of\nobject identifiers in effective object referencing and precise scene\nunderstanding.\n","authors":["Haifeng Huang","Zehan Wang","Rongjie Huang","Luping Liu","Xize Cheng","Yang Zhao","Tao Jin","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2312.08168v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08146v1","updated":"2023-12-13T13:55:36Z","published":"2023-12-13T13:55:36Z","title":"High-accuracy Vision-Based Attitude Estimation System for Air-Bearing\n  Spacecraft Simulators","summary":"  Air-bearing platforms for simulating the rotational dynamics of satellites\nrequire highly precise ground truth systems. Unfortunately, commercial motion\ncapture systems used for this scope are complex and expensive. This paper shows\na novel and versatile method to compute the attitude of rotational air-bearing\nplatforms using a monocular camera and sets of fiducial markers. The work\nproposes a geometry-based iterative algorithm that is significantly more\naccurate than other literature methods that involve the solution of the\nPerspective-n-Point problem. Additionally, auto-calibration procedures to\nperform a preliminary estimation of the system parameters are shown. The\ndeveloped methodology is deployed onto a Raspberry Pi 4 micro-computer and\ntested with a set of LED markers. Data obtained with this setup are compared\nagainst computer simulations of the same system to understand and validate the\nattitude estimation performances. Simulation results show expected 1-sigma\naccuracies in the order of $\\sim$ 12 arcsec and $\\sim$ 37 arcsec for about- and\ncross-boresight rotations of the platform, and average latency times of 6 ms.\n","authors":["Fabio Ornati","Gianfranco Di Domenico","Paolo Panicucci","Francesco Topputo"],"pdf_url":"https://arxiv.org/pdf/2312.08146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2103.13389v5","updated":"2023-12-13T13:44:40Z","published":"2021-03-24T17:59:07Z","title":"Generating Novel Scene Compositions from Single Images and Videos","summary":"  Given a large dataset for training, generative adversarial networks (GANs)\ncan achieve remarkable performance for the image synthesis task. However,\ntraining GANs in extremely low data regimes remains a challenge, as overfitting\noften occurs, leading to memorization or training divergence. In this work, we\nintroduce SIV-GAN, an unconditional generative model that can generate new\nscene compositions from a single training image or a single video clip. We\npropose a two-branch discriminator architecture, with content and layout\nbranches designed to judge internal content and scene layout realism separately\nfrom each other. This discriminator design enables synthesis of visually\nplausible, novel compositions of a scene, with varying content and layout,\nwhile preserving the context of the original sample. Compared to previous\nsingle image GANs, our model generates more diverse, higher quality images,\nwhile not being restricted to a single image setting. We further introduce a\nnew challenging task of learning from a few frames of a single video. In this\ntraining setup the training images are highly similar to each other, which\nmakes it difficult for prior GAN models to achieve a synthesis of both high\nquality and diversity.\n","authors":["Vadim Sushko","Dan Zhang","Juergen Gall","Anna Khoreva"],"pdf_url":"https://arxiv.org/pdf/2103.13389v5.pdf","comment":"Accepted for publication in Computer Vision and Image Understanding:\n  https://www.sciencedirect.com/science/article/pii/S1077314223002680. Code\n  repository: https://github.com/boschresearch/one-shot-synthesis"},{"id":"http://arxiv.org/abs/2312.08136v1","updated":"2023-12-13T13:37:32Z","published":"2023-12-13T13:37:32Z","title":"ProNeRF: Learning Efficient Projection-Aware Ray Sampling for\n  Fine-Grained Implicit Neural Radiance Fields","summary":"  Recent advances in neural rendering have shown that, albeit slow, implicit\ncompact models can learn a scene's geometries and view-dependent appearances\nfrom multiple views. To maintain such a small memory footprint but achieve\nfaster inference times, recent works have adopted `sampler' networks that\nadaptively sample a small subset of points along each ray in the implicit\nneural radiance fields. Although these methods achieve up to a 10$\\times$\nreduction in rendering time, they still suffer from considerable quality\ndegradation compared to the vanilla NeRF. In contrast, we propose ProNeRF,\nwhich provides an optimal trade-off between memory footprint (similar to NeRF),\nspeed (faster than HyperReel), and quality (better than K-Planes). ProNeRF is\nequipped with a novel projection-aware sampling (PAS) network together with a\nnew training strategy for ray exploration and exploitation, allowing for\nefficient fine-grained particle sampling. Our ProNeRF yields state-of-the-art\nmetrics, being 15-23x faster with 0.65dB higher PSNR than NeRF and yielding\n0.95dB higher PSNR than the best published sampler-based method, HyperReel. Our\nexploration and exploitation training strategy allows ProNeRF to learn the full\nscenes' color and density distributions while also learning efficient ray\nsampling focused on the highest-density regions. We provide extensive\nexperimental results that support the effectiveness of our method on the widely\nadopted forward-facing and 360 datasets, LLFF and Blender, respectively.\n","authors":["Juan Luis Gonzalez Bello","Minh-Quan Viet Bui","Munchurl Kim"],"pdf_url":"https://arxiv.org/pdf/2312.08136v1.pdf","comment":"Visit our project website at\n  https://kaist-viclab.github.io/pronerf-site/"},{"id":"http://arxiv.org/abs/2312.08128v1","updated":"2023-12-13T13:30:27Z","published":"2023-12-13T13:30:27Z","title":"Clockwork Diffusion: Efficient Generation With Model-Step Distillation","summary":"  This work aims to improve the efficiency of text-to-image diffusion models.\nWhile diffusion models use computationally expensive UNet-based denoising\noperations in every generation step, we identify that not all operations are\nequally relevant for the final output quality. In particular, we observe that\nUNet layers operating on high-res feature maps are relatively sensitive to\nsmall perturbations. In contrast, low-res feature maps influence the semantic\nlayout of the final image and can often be perturbed with no noticeable change\nin the output. Based on this observation, we propose Clockwork Diffusion, a\nmethod that periodically reuses computation from preceding denoising steps to\napproximate low-res feature maps at one or more subsequent steps. For multiple\nbaselines, and for both text-to-image generation and image editing, we\ndemonstrate that Clockwork leads to comparable or improved perceptual scores\nwith drastically reduced computational complexity. As an example, for Stable\nDiffusion v1.5 with 8 DPM++ steps we save 32% of FLOPs with negligible FID and\nCLIP change.\n","authors":["Amirhossein Habibian","Amir Ghodrati","Noor Fathima","Guillaume Sautiere","Risheek Garrepalli","Fatih Porikli","Jens Petersen"],"pdf_url":"https://arxiv.org/pdf/2312.08128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14521v3","updated":"2023-12-13T13:29:42Z","published":"2023-11-24T14:46:59Z","title":"GaussianEditor: Swift and Controllable 3D Editing with Gaussian\n  Splatting","summary":"  3D editing plays a crucial role in many areas such as gaming and virtual\nreality. Traditional 3D editing methods, which rely on representations like\nmeshes and point clouds, often fall short in realistically depicting complex\nscenes. On the other hand, methods based on implicit 3D representations, like\nNeural Radiance Field (NeRF), render complex scenes effectively but suffer from\nslow processing speeds and limited control over specific scene areas. In\nresponse to these challenges, our paper presents GaussianEditor, an innovative\nand efficient 3D editing algorithm based on Gaussian Splatting (GS), a novel 3D\nrepresentation. GaussianEditor enhances precision and control in editing\nthrough our proposed Gaussian semantic tracing, which traces the editing target\nthroughout the training process. Additionally, we propose Hierarchical Gaussian\nsplatting (HGS) to achieve stabilized and fine results under stochastic\ngenerative guidance from 2D diffusion models. We also develop editing\nstrategies for efficient object removal and integration, a challenging task for\nexisting methods. Our comprehensive experiments demonstrate GaussianEditor's\nsuperior control, efficacy, and rapid performance, marking a significant\nadvancement in 3D editing. Project Page:\nhttps://buaacyw.github.io/gaussian-editor/\n","authors":["Yiwen Chen","Zilong Chen","Chi Zhang","Feng Wang","Xiaofeng Yang","Yikai Wang","Zhongang Cai","Lei Yang","Huaping Liu","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2311.14521v3.pdf","comment":"Project Page: https://buaacyw.github.io/gaussian-editor/"},{"id":"http://arxiv.org/abs/2312.08118v1","updated":"2023-12-13T13:15:19Z","published":"2023-12-13T13:15:19Z","title":"Neural Radiance Fields for Transparent Object Using Visual Hull","summary":"  Unlike opaque object, novel view synthesis of transparent object is a\nchallenging task, because transparent object refracts light of background\ncausing visual distortions on the transparent object surface along the\nviewpoint change. Recently introduced Neural Radiance Fields (NeRF) is a view\nsynthesis method. Thanks to its remarkable performance improvement, lots of\nfollowing applications based on NeRF in various topics have been developed.\nHowever, if an object with a different refractive index is included in a scene\nsuch as transparent object, NeRF shows limited performance because refracted\nlight ray at the surface of the transparent object is not appropriately\nconsidered. To resolve the problem, we propose a NeRF-based method consisting\nof the following three steps: First, we reconstruct a three-dimensional shape\nof a transparent object using visual hull. Second, we simulate the refraction\nof the rays inside of the transparent object according to Snell's law. Last, we\nsample points through refracted rays and put them into NeRF. Experimental\nevaluation results demonstrate that our method addresses the limitation of\nconventional NeRF with transparent objects.\n","authors":["Heechan Yoon","Seungkyu Lee"],"pdf_url":"https://arxiv.org/pdf/2312.08118v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12004v3","updated":"2023-12-13T13:08:29Z","published":"2023-10-18T14:39:25Z","title":"Image Super-resolution Via Latent Diffusion: A Sampling-space Mixture Of\n  Experts And Frequency-augmented Decoder Approach","summary":"  The recent use of diffusion prior, enhanced by pre-trained text-image models,\nhas markedly elevated the performance of image super-resolution (SR). To\nalleviate the huge computational cost required by pixel-based diffusion SR,\nlatent-based methods utilize a feature encoder to transform the image and then\nimplement the SR image generation in a compact latent space. Nevertheless,\nthere are two major issues that limit the performance of latent-based\ndiffusion. First, the compression of latent space usually causes reconstruction\ndistortion. Second, huge computational cost constrains the parameter scale of\nthe diffusion model. To counteract these issues, we first propose a frequency\ncompensation module that enhances the frequency components from latent space to\npixel space. The reconstruction distortion (especially for high-frequency\ninformation) can be significantly decreased. Then, we propose to use\nSample-Space Mixture of Experts (SS-MoE) to achieve more powerful latent-based\nSR, which steadily improves the capacity of the model without a significant\nincrease in inference costs. These carefully crafted designs contribute to\nperformance improvements in largely explored 4x blind super-resolution\nbenchmarks and extend to large magnification factors, i.e., 8x image SR\nbenchmarks. The code is available at https://github.com/amandaluof/moe_sr.\n","authors":["Feng Luo","Jinxi Xiang","Jun Zhang","Xiao Han","Wei Yang"],"pdf_url":"https://arxiv.org/pdf/2310.12004v3.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2312.07180v2","updated":"2023-12-13T13:01:25Z","published":"2023-12-12T11:27:13Z","title":"Context-Aware Iteration Policy Network for Efficient Optical Flow\n  Estimation","summary":"  Existing recurrent optical flow estimation networks are computationally\nexpensive since they use a fixed large number of iterations to update the flow\nfield for each sample. An efficient network should skip iterations when the\nflow improvement is limited. In this paper, we develop a Context-Aware\nIteration Policy Network for efficient optical flow estimation, which\ndetermines the optimal number of iterations per sample. The policy network\nachieves this by learning contextual information to realize whether flow\nimprovement is bottlenecked or minimal. On the one hand, we use iteration\nembedding and historical hidden cell, which include previous iterations\ninformation, to convey how flow has changed from previous iterations. On the\nother hand, we use the incremental loss to make the policy network implicitly\nperceive the magnitude of optical flow improvement in the subsequent iteration.\nFurthermore, the computational complexity in our dynamic network is\ncontrollable, allowing us to satisfy various resource preferences with a single\ntrained model. Our policy network can be easily integrated into\nstate-of-the-art optical flow networks. Extensive experiments show that our\nmethod maintains performance while reducing FLOPs by about 40%/20% for the\nSintel/KITTI datasets.\n","authors":["Ri Cheng","Ruian He","Xuhao Jiang","Shili Zhou","Weimin Tan","Bo Yan"],"pdf_url":"https://arxiv.org/pdf/2312.07180v2.pdf","comment":"2024, Association for the Advancement of Artificial Intelligence"},{"id":"http://arxiv.org/abs/2312.07424v2","updated":"2023-12-13T13:00:57Z","published":"2023-12-12T16:48:07Z","title":"How Well Does GPT-4V(ision) Adapt to Distribution Shifts? A Preliminary\n  Investigation","summary":"  In machine learning, generalization against distribution shifts -- where\ndeployment conditions diverge from the training scenarios -- is crucial,\nparticularly in fields like climate modeling, biomedicine, and autonomous\ndriving. The emergence of foundation models, distinguished by their extensive\npretraining and task versatility, has led to an increased interest in their\nadaptability to distribution shifts. GPT-4V(ision) acts as the most advanced\npublicly accessible multimodal foundation model, with extensive applications\nacross various domains, including anomaly detection, video understanding, image\ngeneration, and medical diagnosis. However, its robustness against data\ndistributions remains largely underexplored. Addressing this gap, this study\nrigorously evaluates GPT-4V's adaptability and generalization capabilities in\ndynamic environments, benchmarking against prominent models like CLIP and\nLLaVA. We delve into GPT-4V's zero-shot generalization across 13 diverse\ndatasets spanning natural, medical, and molecular domains. We further\ninvestigate its adaptability to controlled data perturbations and examine the\nefficacy of in-context learning as a tool to enhance its adaptation. Our\nfindings delineate GPT-4V's capability boundaries in distribution shifts,\nshedding light on its strengths and limitations across various scenarios.\nImportantly, this investigation contributes to our understanding of how AI\nfoundation models generalize to distribution shifts, offering pivotal insights\ninto their adaptability and robustness. Code is publicly available at\nhttps://github.com/jameszhou-gl/gpt-4v-distribution-shift.\n","authors":["Zhongyi Han","Guanglin Zhou","Rundong He","Jindong Wang","Tailin Wu","Yilong Yin","Salman Khan","Lina Yao","Tongliang Liu","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.07424v2.pdf","comment":"62 pages, 39 figures, preprint"},{"id":"http://arxiv.org/abs/2312.08111v1","updated":"2023-12-13T12:58:30Z","published":"2023-12-13T12:58:30Z","title":"Towards Better Morphed Face Images without Ghosting Artifacts","summary":"  Automatic generation of morphed face images often produces ghosting artifacts\ndue to poorly aligned structures in the input images. Manual processing can\nmitigate these artifacts. However, this is not feasible for the generation of\nlarge datasets, which are required for training and evaluating robust morphing\nattack detectors. In this paper, we propose a method for automatic prevention\nof ghosting artifacts based on a pixel-wise alignment during morph generation.\nWe evaluate our proposed method on state-of-the-art detectors and show that our\nmorphs are harder to detect, particularly, when combined with\nstyle-transfer-based improvement of low-level image characteristics.\nFurthermore, we show that our approach does not impair the biometric quality,\nwhich is essential for high quality morphs.\n","authors":["Clemens Seibold","Anna Hilsmann","Peter Eisert"],"pdf_url":"https://arxiv.org/pdf/2312.08111v1.pdf","comment":"Accepted at VISAPP 2024"},{"id":"http://arxiv.org/abs/2308.14075v2","updated":"2023-12-13T12:29:20Z","published":"2023-08-27T11:38:42Z","title":"FaceCoresetNet: Differentiable Coresets for Face Set Recognition","summary":"  In set-based face recognition, we aim to compute the most discriminative\ndescriptor from an unbounded set of images and videos showing a single person.\nA discriminative descriptor balances two policies when aggregating information\nfrom a given set. The first is a quality-based policy: emphasizing high-quality\nand down-weighting low-quality images. The second is a diversity-based policy:\nemphasizing unique images in the set and down-weighting multiple occurrences of\nsimilar images as found in video clips which can overwhelm the set\nrepresentation. This work frames face-set representation as a differentiable\ncoreset selection problem. Our model learns how to select a small coreset of\nthe input set that balances quality and diversity policies using a learned\nmetric parameterized by the face quality, optimized end-to-end. The selection\nprocess is a differentiable farthest-point sampling (FPS) realized by\napproximating the non-differentiable Argmax operation with differentiable\nsampling from the Gumbel-Softmax distribution of distances. The small coreset\nis later used as queries in a self and cross-attention architecture to enrich\nthe descriptor with information from the whole set. Our model is\norder-invariant and linear in the input set size. We set a new SOTA to set face\nverification on the IJB-B and IJB-C datasets. Our code is publicly available.\n","authors":["Gil Shapira","Yosi Keller"],"pdf_url":"https://arxiv.org/pdf/2308.14075v2.pdf","comment":"Accepted to AAAI-24"},{"id":"http://arxiv.org/abs/2312.08094v1","updated":"2023-12-13T12:24:34Z","published":"2023-12-13T12:24:34Z","title":"3DGEN: A GAN-based approach for generating novel 3D models from image\n  data","summary":"  The recent advances in text and image synthesis show a great promise for the\nfuture of generative models in creative fields. However, a less explored area\nis the one of 3D model generation, with a lot of potential applications to game\ndesign, video production, and physical product design. In our paper, we present\n3DGEN, a model that leverages the recent work on both Neural Radiance Fields\nfor object reconstruction and GAN-based image generation. We show that the\nproposed architecture can generate plausible meshes for objects of the same\ncategory as the training images and compare the resulting meshes with the\nstate-of-the-art baselines, leading to visible uplifts in generation quality.\n","authors":["Antoine Schnepf","Flavian Vasile","Ugo Tanielian"],"pdf_url":"https://arxiv.org/pdf/2312.08094v1.pdf","comment":"Submitted to NeurIPS 2022 Machine Learning for Creativity and Design\n  Workshop"},{"id":"http://arxiv.org/abs/2312.08078v1","updated":"2023-12-13T11:47:28Z","published":"2023-12-13T11:47:28Z","title":"Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic\n  Image-Report Generation","summary":"  To address these issues, we propose a novel Adaptive patch-word Matching\n(AdaMatch) model to correlate chest X-ray (CXR) image regions with words in\nmedical reports and apply it to CXR-report generation to provide explainability\nfor the generation process. AdaMatch exploits the fine-grained relation between\nadaptive patches and words to provide explanations of specific image regions\nwith corresponding words. To capture the abnormal regions of varying sizes and\npositions, we introduce the Adaptive Patch extraction (AdaPatch) module to\nacquire the adaptive patches for these regions adaptively. In order to provide\nexplicit explainability for CXR-report generation task, we propose an\nAdaMatch-based bidirectional large language model for Cyclic CXR-report\ngeneration (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywords\nfor CXR images and `keypatches' for medical reports as hints to guide\nCXR-report generation. Extensive experiments on two publicly available CXR\ndatasets prove the effectiveness of our method and its superior performance to\nexisting methods.\n","authors":["Wenting Chen","Xiang Li","Linlin Shen","Yixuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2312.08078v1.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.09797v2","updated":"2023-12-13T11:34:54Z","published":"2023-03-17T06:43:08Z","title":"MMFace4D: A Large-Scale Multi-Modal 4D Face Dataset for Audio-Driven 3D\n  Face Animation","summary":"  Audio-Driven Face Animation is an eagerly anticipated technique for\napplications such as VR/AR, games, and movie making. With the rapid development\nof 3D engines, there is an increasing demand for driving 3D faces with audio.\nHowever, currently available 3D face animation datasets are either\nscale-limited or quality-unsatisfied, which hampers further developments of\naudio-driven 3D face animation. To address this challenge, we propose MMFace4D,\na large-scale multi-modal 4D (3D sequence) face dataset consisting of 431\nidentities, 35,904 sequences, and 3.9 million frames. MMFace4D exhibits two\ncompelling characteristics: 1) a remarkably diverse set of subjects and corpus,\nencompassing actors spanning ages 15 to 68, and recorded sentences with\ndurations ranging from 0.7 to 11.4 seconds. 2) It features synchronized audio\nand 3D mesh sequences with high-resolution face details. To capture the subtle\nnuances of 3D facial expressions, we leverage three synchronized RGBD cameras\nduring the recording process. Upon MMFace4D, we construct a non-autoregressive\nframework for audio-driven 3D face animation. Our framework considers the\nregional and composite natures of facial animations, and surpasses contemporary\nstate-of-the-art approaches both qualitatively and quantitatively. The code,\nmodel, and dataset will be publicly available.\n","authors":["Haozhe Wu","Jia Jia","Junliang Xing","Hongwei Xu","Xiangyuan Wang","Jelo Wang"],"pdf_url":"https://arxiv.org/pdf/2303.09797v2.pdf","comment":"10 pages, 8 figures. This paper has been submitted to IEEE\n  Transaction on MultiMedia, which is the extension of our MM2023 paper\n  arXiv:2308.05428. The dataset is now publicly available, see Project page at\n  https://wuhaozhe.github.io/mmface4d/"},{"id":"http://arxiv.org/abs/2312.08071v1","updated":"2023-12-13T11:29:47Z","published":"2023-12-13T11:29:47Z","title":"Novel View Synthesis with View-Dependent Effects from a Single Image","summary":"  In this paper, we firstly consider view-dependent effects into single\nimage-based novel view synthesis (NVS) problems. For this, we propose to\nexploit the camera motion priors in NVS to model view-dependent appearance or\neffects (VDE) as the negative disparity in the scene. By recognizing\nspecularities \"follow\" the camera motion, we infuse VDEs into the input images\nby aggregating input pixel colors along the negative depth region of the\nepipolar lines. Also, we propose a `relaxed volumetric rendering' approximation\nthat allows computing the densities in a single pass, improving efficiency for\nNVS from single images. Our method can learn single-image NVS from image\nsequences only, which is a completely self-supervised learning method, for the\nfirst time requiring neither depth nor camera pose annotations. We present\nextensive experiment results and show that our proposed method can learn NVS\nwith VDEs, outperforming the SOTA single-view NVS methods on the RealEstate10k\nand MannequinChallenge datasets.\n","authors":["Juan Luis Gonzalez Bello","Munchurl Kim"],"pdf_url":"https://arxiv.org/pdf/2312.08071v1.pdf","comment":"Visit our website https://kaist-viclab.github.io/monovde-site"},{"id":"http://arxiv.org/abs/2312.08060v1","updated":"2023-12-13T11:14:57Z","published":"2023-12-13T11:14:57Z","title":"C-BEV: Contrastive Bird's Eye View Training for Cross-View Image\n  Retrieval and 3-DoF Pose Estimation","summary":"  To find the geolocation of a street-view image, cross-view geolocalization\n(CVGL) methods typically perform image retrieval on a database of georeferenced\naerial images and determine the location from the visually most similar match.\nRecent approaches focus mainly on settings where street-view and aerial images\nare preselected to align w.r.t. translation or orientation, but struggle in\nchallenging real-world scenarios where varying camera poses have to be matched\nto the same aerial image. We propose a novel trainable retrieval architecture\nthat uses bird's eye view (BEV) maps rather than vectors as embedding\nrepresentation, and explicitly addresses the many-to-one ambiguity that arises\nin real-world scenarios. The BEV-based retrieval is trained using the same\ncontrastive setting and loss as classical retrieval.\n  Our method C-BEV surpasses the state-of-the-art on the retrieval task on\nmultiple datasets by a large margin. It is particularly effective in\nchallenging many-to-one scenarios, e.g. increasing the top-1 recall on VIGOR's\ncross-area split with unknown orientation from 31.1% to 65.0%. Although the\nmodel is supervised only through a contrastive objective applied on image\npairings, it additionally learns to infer the 3-DoF camera pose on the matching\naerial image, and even yields a lower mean pose error than recent methods that\nare explicitly trained with metric groundtruth.\n","authors":["Florian Fervers","Sebastian Bullinger","Christoph Bodensteiner","Michael Arens","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2312.08060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08056v1","updated":"2023-12-13T11:03:07Z","published":"2023-12-13T11:03:07Z","title":"Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and\n  Multi-Source Supervision","summary":"  Ancient artifacts are an important medium for cultural preservation and\nrestoration. However, many physical copies of artifacts are either damaged or\nlost, leaving a blank space in archaeological and historical studies that calls\nfor artifact image generation techniques. Despite the significant advancements\nin open-domain text-to-image synthesis, existing approaches fail to capture the\nimportant domain knowledge presented in the textual description, resulting in\nerrors in recreated images such as incorrect shapes and patterns. In this\npaper, we propose a novel knowledge-aware artifact image synthesis approach\nthat brings lost historical objects accurately into their visual forms. We use\na pretrained diffusion model as backbone and introduce three key techniques to\nenhance the text-to-image generation framework: 1) we construct prompts with\nexplicit archaeological knowledge elicited from large language models (LLMs);\n2) we incorporate additional textual guidance to correlated historical\nexpertise in a contrastive manner; 3) we introduce further visual-semantic\nconstraints on edge and perceptual features that enable our model to learn more\nintricate visual details of the artifacts. Compared to existing approaches, our\nproposed model produces higher-quality artifact images that align better with\nthe implicit details and historical knowledge contained within written\ndocuments, thus achieving significant improvements across automatic metrics and\nin human evaluation. Our code and data are available at\nhttps://github.com/danielwusg/artifact_diffusion.\n","authors":["Shengguang Wu","Zhenglun Chen","Qi Su"],"pdf_url":"https://arxiv.org/pdf/2312.08056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08054v1","updated":"2023-12-13T11:01:40Z","published":"2023-12-13T11:01:40Z","title":"Semantic Complete Scene Forecasting from a 4D Dynamic Point Cloud\n  Sequence","summary":"  We study a new problem of semantic complete scene forecasting (SCSF) in this\nwork. Given a 4D dynamic point cloud sequence, our goal is to forecast the\ncomplete scene corresponding to the future next frame along with its semantic\nlabels. To tackle this challenging problem, we properly model the synergetic\nrelationship between future forecasting and semantic scene completion through a\nnovel network named SCSFNet. SCSFNet leverages a hybrid geometric\nrepresentation for high-resolution complete scene forecasting. To leverage\nmulti-frame observation as well as the understanding of scene dynamics to ease\nthe completion task, SCSFNet introduces an attention-based skip connection\nscheme. To ease the need to model occlusion variations and to better focus on\nthe occluded part, SCSFNet utilizes auxiliary visibility grids to guide the\nforecasting task. To evaluate the effectiveness of SCSFNet, we conduct\nexperiments on various benchmarks including two large-scale indoor benchmarks\nwe contributed and the outdoor SemanticKITTI benchmark. Extensive experiments\nshow SCSFNet outperforms baseline methods on multiple metrics by a large\nmargin, and also prove the synergy between future forecasting and semantic\nscene completion.\n","authors":["Zifan Wang","Zhuorui Ye","Haoran Wu","Junyu Chen","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2312.08054v1.pdf","comment":"AAAI 2024, see https://scsfnet.github.io/"},{"id":"http://arxiv.org/abs/2312.08048v1","updated":"2023-12-13T10:57:46Z","published":"2023-12-13T10:57:46Z","title":"Compositional Inversion for Stable Diffusion Models","summary":"  Inversion methods, such as Textual Inversion, generate personalized images by\nincorporating concepts of interest provided by user images. However, existing\nmethods often suffer from overfitting issues, where the dominant presence of\ninverted concepts leads to the absence of other desired concepts. It stems from\nthe fact that during inversion, the irrelevant semantics in the user images are\nalso encoded, forcing the inverted concepts to occupy locations far from the\ncore distribution in the embedding space. To address this issue, we propose a\nmethod that guides the inversion process towards the core distribution for\ncompositional embeddings. Additionally, we introduce a spatial regularization\napproach to balance the attention on the concepts being composed. Our method is\ndesigned as a post-training approach and can be seamlessly integrated with\nother inversion methods. Experimental results demonstrate the effectiveness of\nour proposed approach in mitigating the overfitting problem and generating more\ndiverse and balanced compositions of concepts in the synthesized images. The\nsource code is available at\nhttps://github.com/zhangxulu1996/Compositional-Inversion.\n","authors":["Xu-Lu Zhang","Xiao-Yong Wei","Jin-Lin Wu","Tian-Yi Zhang","Zhao-Xiang Zhang","Zhen Lei","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2312.08048v1.pdf","comment":"This paper was accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.05799v3","updated":"2023-12-13T10:47:08Z","published":"2023-12-10T07:17:06Z","title":"SGNet: Structure Guided Network via Gradient-Frequency Awareness for\n  Depth Map Super-Resolution","summary":"  Depth super-resolution (DSR) aims to restore high-resolution (HR) depth from\nlow-resolution (LR) one, where RGB image is often used to promote this task.\nRecent image guided DSR approaches mainly focus on spatial domain to rebuild\ndepth structure. However, since the structure of LR depth is usually blurry,\nonly considering spatial domain is not very sufficient to acquire satisfactory\nresults. In this paper, we propose structure guided network (SGNet), a method\nthat pays more attention to gradient and frequency domains, both of which have\nthe inherent ability to capture high-frequency structure. Specifically, we\nfirst introduce the gradient calibration module (GCM), which employs the\naccurate gradient prior of RGB to sharpen the LR depth structure. Then we\npresent the Frequency Awareness Module (FAM) that recursively conducts multiple\nspectrum differencing blocks (SDB), each of which propagates the precise\nhigh-frequency components of RGB into the LR depth. Extensive experimental\nresults on both real and synthetic datasets demonstrate the superiority of our\nSGNet, reaching the state-of-the-art. Codes and pre-trained models are\navailable at https://github.com/yanzq95/SGNet.\n","authors":["Zhengxue Wang","Zhiqiang Yan","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2312.05799v3.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2307.10705v5","updated":"2023-12-13T10:43:45Z","published":"2023-07-20T08:53:47Z","title":"TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and\n  Lane Segmentation in Self-Driving Cars","summary":"  Semantic segmentation is a common task in autonomous driving to understand\nthe surrounding environment. Driveable Area Segmentation and Lane Detection are\nparticularly important for safe and efficient navigation on the road. However,\noriginal semantic segmentation models are computationally expensive and require\nhigh-end hardware, which is not feasible for embedded systems in autonomous\nvehicles. This paper proposes a lightweight model for the driveable area and\nlane line segmentation. TwinLiteNet is designed cheaply but achieves accurate\nand efficient segmentation results. We evaluate TwinLiteNet on the BDD100K\ndataset and compare it with modern models. Experimental results show that our\nTwinLiteNet performs similarly to existing approaches, requiring significantly\nfewer computational resources. Specifically, TwinLiteNet achieves a mIoU score\nof 91.3% for the Drivable Area task and 31.08% IoU for the Lane Detection task\nwith only 0.4 million parameters and achieves 415 FPS on GPU RTX A5000.\nFurthermore, TwinLiteNet can run in real-time on embedded devices with limited\ncomputing power, especially since it achieves 60FPS on Jetson Xavier NX, making\nit an ideal solution for self-driving vehicles. Code is available:\nurl{https://github.com/chequanghuy/TwinLiteNet}.\n","authors":["Quang Huy Che","Dinh Phuc Nguyen","Minh Quan Pham","Duc Khai Lam"],"pdf_url":"https://arxiv.org/pdf/2307.10705v5.pdf","comment":"Accepted by MAPR 2023"},{"id":"http://arxiv.org/abs/2312.04008v3","updated":"2023-12-13T10:38:13Z","published":"2023-12-07T02:55:46Z","title":"Natural-language-driven Simulation Benchmark and Copilot for Efficient\n  Production of Object Interactions in Virtual Road Scenes","summary":"  We advocate the idea of the natural-language-driven(NLD) simulation to\nefficiently produce the object interactions between multiple objects in the\nvirtual road scenes, for teaching and testing the autonomous driving systems\nthat should take quick action to avoid collision with obstacles with\nunpredictable motions. The NLD simulation allows the brief natural-language\ndescription to control the object interactions, significantly reducing the\nhuman efforts for creating a large amount of interaction data. To facilitate\nthe research of NLD simulation, we collect the Language-to-Interaction(L2I)\nbenchmark dataset with 120,000 natural-language descriptions of object\ninteractions in 6 common types of road topologies. Each description is\nassociated with the programming code, which the graphic render can use to\nvisually reconstruct the object interactions in the virtual scenes. As a\nmethodology contribution, we design SimCopilot to translate the interaction\ndescriptions to the renderable code. We use the L2I dataset to evaluate\nSimCopilot's abilities to control the object motions, generate complex\ninteractions, and generalize interactions across road topologies. The L2I\ndataset and the evaluation results motivate the relevant research of the NLD\nsimulation.\n","authors":["Kairui Yang","Zihao Guo","Gengjie Lin","Haotian Dong","Die Zuo","Jibin Peng","Zhao Huang","Zhecheng Xu","Fupeng Li","Ziyun Bai","Di Lin"],"pdf_url":"https://arxiv.org/pdf/2312.04008v3.pdf","comment":"Section 7-9 have some formatting errors"},{"id":"http://arxiv.org/abs/2312.08034v1","updated":"2023-12-13T10:21:00Z","published":"2023-12-13T10:21:00Z","title":"Individualized Deepfake Detection Exploiting Traces Due to Double\n  Neural-Network Operations","summary":"  In today's digital landscape, journalists urgently require tools to verify\nthe authenticity of facial images and videos depicting specific public figures\nbefore incorporating them into news stories. Existing deepfake detectors are\nnot optimized for this detection task when an image is associated with a\nspecific and identifiable individual. This study focuses on the deepfake\ndetection of facial images of individual public figures. We propose to\ncondition the proposed detector on the identity of the identified individual\ngiven the advantages revealed by our theory-driven simulations. While most\ndetectors in the literature rely on perceptible or imperceptible artifacts\npresent in deepfake facial images, we demonstrate that the detection\nperformance can be improved by exploiting the idempotency property of neural\nnetworks. In our approach, the training process involves double neural-network\noperations where we pass an authentic image through a deepfake simulating\nnetwork twice. Experimental results show that the proposed method improves the\narea under the curve (AUC) from 0.92 to 0.94 and reduces its standard deviation\nby 17\\%. For evaluating the detection performance of individual public figures,\na facial image dataset with individuals' names is required, a criterion not met\nby the current deepfake datasets. To address this, we curated a dataset\ncomprising 32k images featuring 45 public figures, which we intend to release\nto the public after the paper is published.\n","authors":["Mushfiqur Rahman","Runze Liu","Chau-Wai Wong","Huaiyu Dai"],"pdf_url":"https://arxiv.org/pdf/2312.08034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.04037v3","updated":"2023-12-13T10:12:47Z","published":"2023-01-10T15:39:02Z","title":"ROBUSfT: Robust Real-Time Shape-from-Template, a C++ Library","summary":"  Tracking the 3D shape of a deforming object using only monocular 2D vision is\na challenging problem. This is because one should (i) infer the 3D shape from a\n2D image, which is a severely underconstrained problem, and (ii) implement the\nwhole solution pipeline in real-time. The pipeline typically requires feature\ndetection and matching, mismatch filtering, 3D shape inference and feature\ntracking algorithms. We propose ROBUSfT, a conventional pipeline based on a\ntemplate containing the object's rest shape, texturemap and deformation law.\nROBUSfT is ready-to-use, wide-baseline, capable of handling large deformations,\nfast up to 30 fps, free of training, and robust against partial occlusions and\ndiscontinuity in video frames. It outperforms the state-of-the-art methods in\nchallenging datasets. ROBUSfT is implemented as a publicly available C++\nlibrary and we provide a tutorial on how to use it in\nhttps://github.com/mrshetab/ROBUSfT\n","authors":["Mohammadreza Shetab-Bushehri","Miguel Aranda","Youcef Mezouar","Adrien Bartoli","Erol Ozgur"],"pdf_url":"https://arxiv.org/pdf/2301.04037v3.pdf","comment":"This is the arXiv version of an article published in Image and Vision\n  Computing. Please cite the accepted version: M. Shetab-Bushehri, M. Aranda,\n  E. Ozgur, Y. Mezouar and Adrien Bartoli \"ROBUSfT: Robust Real-Time\n  Shape-from-Template, a C++ Library,\" in Image and Vision Computing, doi:\n  10.1016/j.imavis.2023.104867"},{"id":"http://arxiv.org/abs/2312.08029v1","updated":"2023-12-13T10:04:06Z","published":"2023-12-13T10:04:06Z","title":"ClusterDDPM: An EM clustering framework with Denoising Diffusion\n  Probabilistic Models","summary":"  Variational autoencoder (VAE) and generative adversarial networks (GAN) have\nfound widespread applications in clustering and have achieved significant\nsuccess. However, the potential of these approaches may be limited due to VAE's\nmediocre generation capability or GAN's well-known instability during\nadversarial training. In contrast, denoising diffusion probabilistic models\n(DDPMs) represent a new and promising class of generative models that may\nunlock fresh dimensions in clustering. In this study, we introduce an\ninnovative expectation-maximization (EM) framework for clustering using DDPMs.\nIn the E-step, we aim to derive a mixture of Gaussian priors for the subsequent\nM-step. In the M-step, our focus lies in learning clustering-friendly latent\nrepresentations for the data by employing the conditional DDPM and matching the\ndistribution of latent representations to the mixture of Gaussian priors. We\npresent a rigorous theoretical analysis of the optimization process in the\nM-step, proving that the optimizations are equivalent to maximizing the lower\nbound of the Q function within the vanilla EM framework under certain\nconstraints. Comprehensive experiments validate the advantages of the proposed\nframework, showcasing superior performance in clustering, unsupervised\nconditional generation and latent representation learning.\n","authors":["Jie Yan","Jing Liu","Zhong-yuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.08029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08022v1","updated":"2023-12-13T09:49:59Z","published":"2023-12-13T09:49:59Z","title":"Mono3DVG: 3D Visual Grounding in Monocular Images","summary":"  We introduce a novel task of 3D visual grounding in monocular RGB images\nusing language descriptions with both appearance and geometry information.\nSpecifically, we build a large-scale dataset, Mono3DRefer, which contains 3D\nobject targets with their corresponding geometric text descriptions, generated\nby ChatGPT and refined manually. To foster this task, we propose Mono3DVG-TR,\nan end-to-end transformer-based network, which takes advantage of both the\nappearance and geometry information in text embeddings for multi-modal learning\nand 3D object localization. Depth predictor is designed to explicitly learn\ngeometry features. The dual text-guided adapter is proposed to refine\nmultiscale visual and geometry features of the referred object. Based on\ndepth-text-visual stacking attention, the decoder fuses object-level geometric\ncues and visual appearance into a learnable query. Comprehensive benchmarks and\nsome insightful analyses are provided for Mono3DVG. Extensive comparisons and\nablation studies show that our method significantly outperforms all baselines.\nThe dataset and code will be publicly available at:\nhttps://github.com/ZhanYang-nwpu/Mono3DVG.\n","authors":["Yang Zhan","Yuan Yuan","Zhitong Xiong"],"pdf_url":"https://arxiv.org/pdf/2312.08022v1.pdf","comment":"Accepted by the Thirty-Eighth AAAI Conference on Artificial\n  Intelligence (AAAI 2024)"},{"id":"http://arxiv.org/abs/2312.08020v1","updated":"2023-12-13T09:49:15Z","published":"2023-12-13T09:49:15Z","title":"Generalized Deepfakes Detection with Reconstructed-Blended Images and\n  Multi-scale Feature Reconstruction Network","summary":"  The growing diversity of digital face manipulation techniques has led to an\nurgent need for a universal and robust detection technology to mitigate the\nrisks posed by malicious forgeries. We present a blended-based detection\napproach that has robust applicability to unseen datasets. It combines a method\nfor generating synthetic training samples, i.e., reconstructed blended images,\nthat incorporate potential deepfake generator artifacts and a detection model,\na multi-scale feature reconstruction network, for capturing the generic\nboundary artifacts and noise distribution anomalies brought about by digital\nface manipulations. Experiments demonstrated that this approach results in\nbetter performance in both cross-manipulation detection and cross-dataset\ndetection on unseen data.\n","authors":["Yuyang Sun","Huy H. Nguyen","Chun-Shien Lu","ZhiYong Zhang","Lu Sun","Isao Echizen"],"pdf_url":"https://arxiv.org/pdf/2312.08020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08059v3","updated":"2023-12-13T09:47:22Z","published":"2023-11-14T10:32:17Z","title":"FS-Net: Full Scale Network and Adaptive Threshold for Improving\n  Extraction of Micro-Retinal Vessel Structures","summary":"  Retinal vascular segmentation, is a widely researched subject in biomedical\nimage processing, aims to relieve ophthalmologists' workload when treating and\ndetecting retinal disorders. However, segmenting retinal vessels has its own\nset of challenges, with prior techniques failing to generate adequate results\nwhen segmenting branches and microvascular structures. The neural network\napproaches used recently are characterized by the inability to keep local and\nglobal properties together and the failure to capture tiny end vessels make it\nchallenging to attain the desired result. To reduce this retinal vessel\nsegmentation problem, we propose a full-scale micro-vessel extraction mechanism\nbased on an encoder-decoder neural network architecture, sigmoid smoothing, and\nan adaptive threshold method. The network consists of of residual, encoder\nbooster, bottleneck enhancement, squeeze, and excitation building blocks. All\nof these blocks together help to improve the feature extraction and prediction\nof the segmentation map. The proposed solution has been evaluated using the\nDRIVE, CHASE-DB1, and STARE datasets, and competitive results are obtained when\ncompared with previous studies. The AUC and accuracy on the DRIVE dataset are\n0.9884 and 0.9702, respectively. On the CHASE-DB1 dataset, the scores are\n0.9903 and 0.9755, respectively. On the STARE dataset, the scores are 0.9916\nand 0.9750, respectively. The performance achieved is one step ahead of what\nhas been done in previous studies, and this results in a higher chance of\nhaving this solution in real-life diagnostic centers that seek ophthalmologists\nattention.\n","authors":["Melaku N. Getahun","Oleg Y. Rogov","Dmitry V. Dylov","Andrey Somov","Ahmed Bouridane","Rifat Hamoudi"],"pdf_url":"https://arxiv.org/pdf/2311.08059v3.pdf","comment":"7 pages, 2 figures, under consideration at Pattern Recognition\n  Letters"},{"id":"http://arxiv.org/abs/2312.08019v1","updated":"2023-12-13T09:45:58Z","published":"2023-12-13T09:45:58Z","title":"AdapEdit: Spatio-Temporal Guided Adaptive Editing Algorithm for\n  Text-Based Continuity-Sensitive Image Editing","summary":"  With the great success of text-conditioned diffusion models in creative\ntext-to-image generation, various text-driven image editing approaches have\nattracted the attentions of many researchers. However, previous works mainly\nfocus on discreteness-sensitive instructions such as adding, removing or\nreplacing specific objects, background elements or global styles (i.e., hard\nediting), while generally ignoring subject-binding but semantically\nfine-changing continuity-sensitive instructions such as actions, poses or\nadjectives, and so on (i.e., soft editing), which hampers generative AI from\ngenerating user-customized visual contents. To mitigate this predicament, we\npropose a spatio-temporal guided adaptive editing algorithm AdapEdit, which\nrealizes adaptive image editing by introducing a soft-attention strategy to\ndynamically vary the guiding degree from the editing conditions to visual\npixels from both temporal and spatial perspectives. Note our approach has a\nsignificant advantage in preserving model priors and does not require model\ntraining, fine-tuning, extra data, or optimization. We present our results over\na wide variety of raw images and editing instructions, demonstrating\ncompetitive performance and showing it significantly outperforms the previous\napproaches.\n","authors":["Zhiyuan Ma","Guoli Jia","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2312.08019v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08012v1","updated":"2023-12-13T09:34:01Z","published":"2023-12-13T09:34:01Z","title":"uSF: Learning Neural Semantic Field with Uncertainty","summary":"  Recently, there has been an increased interest in NeRF methods which\nreconstruct differentiable representation of three-dimensional scenes. One of\nthe main limitations of such methods is their inability to assess the\nconfidence of the model in its predictions. In this paper, we propose a new\nneural network model for the formation of extended vector representations,\ncalled uSF, which allows the model to predict not only color and semantic label\nof each point, but also estimate the corresponding values of uncertainty. We\nshow that with a small number of images available for training, a model\nquantifying uncertainty performs better than a model without such\nfunctionality. Code of the uSF approach is publicly available at\nhttps://github.com/sevashasla/usf/.\n","authors":["Vsevolod Skorokhodov","Darya Drozdova","Dmitry Yudin"],"pdf_url":"https://arxiv.org/pdf/2312.08012v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2312.08010v1","updated":"2023-12-13T09:33:08Z","published":"2023-12-13T09:33:08Z","title":"EZ-CLIP: Efficient Zeroshot Video Action Recognition","summary":"  Recent advancements in large-scale pre-training of visual-language models on\npaired image-text data have demonstrated impressive generalization capabilities\nfor zero-shot tasks. Building on this success, efforts have been made to adapt\nthese image-based visual-language models, such as CLIP, for videos extending\ntheir zero-shot capabilities to the video domain. While these adaptations have\nshown promising results, they come at a significant computational cost and\nstruggle with effectively modeling the crucial temporal aspects inherent to the\nvideo domain. In this study, we present EZ-CLIP, a simple and efficient\nadaptation of CLIP that addresses these challenges. EZ-CLIP leverages temporal\nvisual prompting for seamless temporal adaptation, requiring no fundamental\nalterations to the core CLIP architecture while preserving its remarkable\ngeneralization abilities. Moreover, we introduce a novel learning objective\nthat guides the temporal visual prompts to focus on capturing motion, thereby\nenhancing its learning capabilities from video data. We conducted extensive\nexperiments on five different benchmark datasets, thoroughly evaluating EZ-CLIP\nfor zero-shot learning and base-to-novel video action recognition, and also\ndemonstrating its potential for few-shot generalization.Impressively, with a\nmere 5.2 million learnable parameters (as opposed to the 71.1 million in the\nprior best model), EZ-CLIP can be efficiently trained on a single GPU,\noutperforming existing approaches in several evaluations.\n","authors":["Shahzad Ahmad","Sukalpa Chanda","Yogesh S Rawat"],"pdf_url":"https://arxiv.org/pdf/2312.08010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06968v2","updated":"2023-12-13T09:32:59Z","published":"2023-12-12T04:05:15Z","title":"Hallucination Augmented Contrastive Learning for Multimodal Large\n  Language Model","summary":"  Multi-modal large language models (MLLMs) have been shown to efficiently\nintegrate natural language with visual information to handle multi-modal tasks.\nHowever, MLLMs still face a fundamental limitation of hallucinations, where\nthey tend to generate erroneous or fabricated information. In this paper, we\naddress hallucinations in MLLMs from a novel perspective of representation\nlearning. We first analyzed the representation distribution of textual and\nvisual tokens in MLLM, revealing two important findings: 1) there is a\nsignificant gap between textual and visual representations, indicating\nunsatisfactory cross-modal representation alignment; 2) representations of\ntexts that contain and do not contain hallucinations are entangled, making it\nchallenging to distinguish them. These two observations inspire us with a\nsimple yet effective method to mitigate hallucinations. Specifically, we\nintroduce contrastive learning into MLLMs and use text with hallucination as\nhard negative examples, naturally bringing representations of non-hallucinative\ntext and visual samples closer while pushing way representations of\nnon-hallucinating and hallucinative text. We evaluate our method quantitatively\nand qualitatively, showing its effectiveness in reducing hallucination\noccurrences and improving performance across multiple benchmarks. On the\nMMhal-Bench benchmark, our method obtains a 34.66% /29.5% improvement over the\nbaseline MiniGPT-4/LLaVA.\n","authors":["Chaoya Jiang","Haiyang Xu","Mengfan Dong","Jiaxing Chen","Wei Ye","Ming Yan","Qinghao Ye","Ji Zhang","Fei Huang","Shikun Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.06968v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08009v1","updated":"2023-12-13T09:32:50Z","published":"2023-12-13T09:32:50Z","title":"Semi-Supervised Class-Agnostic Motion Prediction with Pseudo Label\n  Regeneration and BEVMix","summary":"  Class-agnostic motion prediction methods aim to comprehend motion within\nopen-world scenarios, holding significance for autonomous driving systems.\nHowever, training a high-performance model in a fully-supervised manner always\nrequires substantial amounts of manually annotated data, which can be both\nexpensive and time-consuming to obtain. To address this challenge, our study\nexplores the potential of semi-supervised learning (SSL) for class-agnostic\nmotion prediction. Our SSL framework adopts a consistency-based self-training\nparadigm, enabling the model to learn from unlabeled data by generating pseudo\nlabels through test-time inference. To improve the quality of pseudo labels, we\npropose a novel motion selection and re-generation module. This module\neffectively selects reliable pseudo labels and re-generates unreliable ones.\nFurthermore, we propose two data augmentation strategies: temporal sampling and\nBEVMix. These strategies facilitate consistency regularization in SSL.\nExperiments conducted on nuScenes demonstrate that our SSL method can surpass\nthe self-supervised approach by a large margin by utilizing only a tiny\nfraction of labeled data. Furthermore, our method exhibits comparable\nperformance to weakly and some fully supervised methods. These results\nhighlight the ability of our method to strike a favorable balance between\nannotation costs and performance. Code will be available at\nhttps://github.com/kwwcv/SSMP.\n","authors":["Kewei Wang","Yizheng Wu","Zhiyu Pan","Xingyi Li","Ke Xian","Zhe Wang","Zhiguo Cao","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2312.08009v1.pdf","comment":"This paper is accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2312.08007v1","updated":"2023-12-13T09:29:45Z","published":"2023-12-13T09:29:45Z","title":"Unveiling Parts Beyond Objects:Towards Finer-Granularity Referring\n  Expression Segmentation","summary":"  Referring expression segmentation (RES) aims at segmenting the foreground\nmasks of the entities that match the descriptive natural language expression.\nPrevious datasets and methods for classic RES task heavily rely on the prior\nassumption that one expression must refer to object-level targets. In this\npaper, we take a step further to finer-grained part-level RES task. To promote\nthe object-level RES task towards finer-grained vision-language understanding,\nwe put forward a new multi-granularity referring expression segmentation (MRES)\ntask and construct an evaluation benchmark called RefCOCOm by manual\nannotations. By employing our automatic model-assisted data engine, we build\nthe largest visual grounding dataset namely MRES-32M, which comprises over\n32.2M high-quality masks and captions on the provided 1M images. Besides, a\nsimple yet strong model named UniRES is designed to accomplish the unified\nobject-level and part-level grounding task. Extensive experiments on our\nRefCOCOm for MRES and three datasets (i.e., RefCOCO(+/g) for classic RES task\ndemonstrate the superiority of our method over previous state-of-the-art\nmethods. To foster future research into fine-grained visual grounding, our\nbenchmark RefCOCOm, the MRES-32M dataset and model UniRES will be publicly\navailable at https://github.com/Rubics-Xuan/MRES\n","authors":["Wenxuan Wang","Tongtian Yue","Yisi Zhang","Longteng Guo","Xingjian He","Xinlong Wang","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2312.08007v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08004v1","updated":"2023-12-13T09:24:42Z","published":"2023-12-13T09:24:42Z","title":"Instance-aware Multi-Camera 3D Object Detection with Structural Priors\n  Mining and Self-Boosting Learning","summary":"  Camera-based bird-eye-view (BEV) perception paradigm has made significant\nprogress in the autonomous driving field. Under such a paradigm, accurate BEV\nrepresentation construction relies on reliable depth estimation for\nmulti-camera images. However, existing approaches exhaustively predict depths\nfor every pixel without prioritizing objects, which are precisely the entities\nrequiring detection in the 3D space. To this end, we propose IA-BEV, which\nintegrates image-plane instance awareness into the depth estimation process\nwithin a BEV-based detector. First, a category-specific structural priors\nmining approach is proposed for enhancing the efficacy of monocular depth\ngeneration. Besides, a self-boosting learning strategy is further proposed to\nencourage the model to place more emphasis on challenging objects in\ncomputation-expensive temporal stereo matching. Together they provide advanced\ndepth estimation results for high-quality BEV features construction, benefiting\nthe ultimate 3D detection. The proposed method achieves state-of-the-art\nperformances on the challenging nuScenes benchmark, and extensive experimental\nresults demonstrate the effectiveness of our designs.\n","authors":["Yang Jiao","Zequn Jie","Shaoxiang Chen","Lechao Cheng","Jingjing Chen","Lin Ma","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2312.08004v1.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2312.06978v2","updated":"2023-12-13T09:21:52Z","published":"2023-12-12T04:38:30Z","title":"CLASS-M: Adaptive stain separation-based contrastive learning with\n  pseudo-labeling for histopathological image classification","summary":"  Histopathological image classification is one of the critical aspects in\nmedical image analysis. Due to the high expense associated with the labeled\ndata in model training, semi-supervised learning methods have been proposed to\nalleviate the need of extensively labeled datasets. In this work, we propose a\nmodel for semi-supervised classification tasks on digital histopathological\nHematoxylin and Eosin (H&E) images. We call the new model Contrastive Learning\nwith Adaptive Stain Separation and MixUp (CLASS-M). Our model is formed by two\nmain parts: contrastive learning between adaptively stain separated Hematoxylin\nimages and Eosin images, and pseudo-labeling using MixUp. We compare our model\nwith other state-of-the-art models on clear cell renal cell carcinoma (ccRCC)\ndatasets from our institution and The Cancer Genome Atlas Program (TCGA). We\ndemonstrate that our CLASS-M model has the best performance on both datasets.\nThe contributions of different parts in our model are also analyzed.\n","authors":["Bodong Zhang","Hamid Manoochehri","Man Minh Ho","Fahimeh Fooladgar","Yosep Chong","Beatrice S. Knudsen","Deepika Sirohi","Tolga Tasdizen"],"pdf_url":"https://arxiv.org/pdf/2312.06978v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.01808v2","updated":"2023-12-13T09:19:13Z","published":"2023-06-02T01:52:35Z","title":"Morphology Edge Attention Network and Optimal Geometric Matching\n  Connection model for vascular segmentation","summary":"  There are many unsolved problems in vascular image segmentation, including\nvascular structural connectivity, scarce branches and missing small vessels.\nObtaining vessels that preserve their correct topological structures is\ncurrently a crucial research issue, as it provides an overall view of one\nvascular system. In order to preserve the topology and accuracy of vessel\nsegmentation, we proposed a novel Morphology Edge Attention Network (MEA-Net)\nfor the segmentation of vessel-like structures, and an Optimal Geometric\nMatching Connection (OGMC) model to connect the broken vessel segments. The\nMEA-Net has an edge attention module that improves the segmentation of edges\nand small objects by morphology operation extracting boundary voxels on\nmulti-scale. The OGMC model uses the concept of curve touching from\ndifferential geometry to filter out fragmented vessel endpoints, and then\nemploys minimal surfaces to determine the optimal connection order between\nblood vessels. Finally, we calculate the geodesic to repair missing vessels\nunder a given Riemannian metric. Our method achieves superior or competitive\nresults compared to state-of-the-art methods on four datasets of 3D vascular\nsegmentation tasks, both effectively reducing vessel broken and increasing\nvessel branch richness, yielding blood vessels with a more precise topological\nstructure.\n","authors":["Yuntao Zhu","Yuxuan Qiao","Xiaoping Yang"],"pdf_url":"https://arxiv.org/pdf/2306.01808v2.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2310.06470v2","updated":"2023-12-13T09:17:13Z","published":"2023-10-10T09:41:13Z","title":"Focus on Local Regions for Query-based Object Detection","summary":"  Query-based methods have garnered significant attention in object detection\nsince the advent of DETR, the pioneering query-based detector. However, these\nmethods face challenges like slow convergence and suboptimal performance.\nNotably, self-attention in object detection often hampers convergence due to\nits global focus. To address these issues, we propose FoLR, a transformer-like\narchitecture with only decoders. We improve the self-attention by isolating\nconnections between irrelevant objects that makes it focus on local regions but\nnot global regions. We also design the adaptive sampling method to extract\neffective features based on queries' local regions from feature maps.\nAdditionally, we employ a look-back strategy for decoders to retain previous\ninformation, followed by the Feature Mixer module to fuse features and queries.\nExperimental results demonstrate FoLR's state-of-the-art performance in\nquery-based detectors, excelling in convergence speed and computational\nefficiency.\n","authors":["Hongbin Xu","Yamei Xia","Shuai Zhao","Bo Cheng"],"pdf_url":"https://arxiv.org/pdf/2310.06470v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.02367v6","updated":"2023-12-13T08:55:27Z","published":"2023-02-05T12:13:27Z","title":"FastPillars: A Deployment-friendly Pillar-based 3D Detector","summary":"  The deployment of 3D detectors strikes one of the major challenges in\nreal-world self-driving scenarios. Existing BEV-based (i.e., Bird Eye View)\ndetectors favor sparse convolutions (known as SPConv) to speed up training and\ninference, which puts a hard barrier for deployment, especially for on-device\napplications. In this paper, to tackle the challenge of efficient 3D object\ndetection from an industry perspective, we devise a deployment-friendly\npillar-based 3D detector, termed FastPillars. First, we introduce a novel\nlightweight Max-and-Attention Pillar Encoding (MAPE) module specially for\nenhancing small 3D objects. Second, we propose a simple yet effective principle\nfor designing a backbone in pillar-based 3D detection. We construct FastPillars\nbased on these designs, achieving high performance and low latency without\nSPConv. Extensive experiments on two large-scale datasets demonstrate the\neffectiveness and efficiency of FastPillars for on-device 3D detection\nregarding both performance and speed. Specifically, FastPillars delivers\nstate-of-the-art accuracy on Waymo Open Dataset with 1.8X speed up and 3.8\nmAPH/L2 improvement over CenterPoint (SPConv-based). Our code is publicly\navailable at: https://github.com/StiphyJay/FastPillars.\n","authors":["Sifan Zhou","Zhi Tian","Xiangxiang Chu","Xinyu Zhang","Bo Zhang","Xiaobo Lu","Chengjian Feng","Zequn Jie","Patrick Yin Chiang","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2302.02367v6.pdf","comment":"Submitted to AAAI2024"},{"id":"http://arxiv.org/abs/2312.07976v1","updated":"2023-12-13T08:45:57Z","published":"2023-12-13T08:45:57Z","title":"Challenges of YOLO Series for Object Detection in Extremely Heavy Rain:\n  CALRA Simulator based Synthetic Evaluation Dat a set","summary":"  Recently, as many studies of autonomous vehicles have been achieved for\nlevels 4 and 5, there has been also increasing interest in the advancement of\nperception, decision, and control technologies, which are the three major\naspects of autonomous vehicles. As for the perception technologies achieving\nreliable maneuvering of autonomous vehicles, object detection by using diverse\nsensors (e.g., LiDAR, radar, and camera) should be prioritized. These sensors\nrequire to detect objects accurately and quickly in diverse weather conditions,\nbut they tend to have challenges to consistently detect objects in bad weather\nconditions with rain, snow, or fog. Thus, in this study, based on the\nexperimentally obtained raindrop data from precipitation conditions, we\nconstructed a novel dataset that could test diverse network model in various\nprecipitation conditions through the CARLA simulator. Consequently, based on\nour novel dataset, YOLO series, a one-stage-detector, was used to\nquantitatively verify how much object detection performance could be decreased\nunder various precipitation conditions from normal to extreme heavy rain\nsituations.\n","authors":["T. Kim","H. Jeon","Y. Lim"],"pdf_url":"https://arxiv.org/pdf/2312.07976v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.10756v4","updated":"2023-12-13T08:44:16Z","published":"2021-07-22T15:42:25Z","title":"Semantic Text-to-Face GAN -ST^2FG","summary":"  Faces generated using generative adversarial networks (GANs) have reached\nunprecedented realism. These faces, also known as \"Deep Fakes\", appear as\nrealistic photographs with very little pixel-level distortions. While some work\nhas enabled the training of models that lead to the generation of specific\nproperties of the subject, generating a facial image based on a natural\nlanguage description has not been fully explored. For security and criminal\nidentification, the ability to provide a GAN-based system that works like a\nsketch artist would be incredibly useful. In this paper, we present a novel\napproach to generate facial images from semantic text descriptions. The learned\nmodel is provided with a text description and an outline of the type of face,\nwhich the model uses to sketch the features. Our models are trained using an\nAffine Combination Module (ACM) mechanism to combine the text embedding from\nBERT and the GAN latent space using a self-attention matrix. This avoids the\nloss of features due to inadequate \"attention\", which may happen if text\nembedding and latent vector are simply concatenated. Our approach is capable of\ngenerating images that are very accurately aligned to the exhaustive textual\ndescriptions of faces with many fine detail features of the face and helps in\ngenerating better images. The proposed method is also capable of making\nincremental changes to a previously generated image if it is provided with\nadditional textual descriptions or sentences.\n","authors":["Manan Oza","Sukalpa Chanda","David Doermann"],"pdf_url":"https://arxiv.org/pdf/2107.10756v4.pdf","comment":"Experiments needs to be redone"},{"id":"http://arxiv.org/abs/2312.07971v1","updated":"2023-12-13T08:36:51Z","published":"2023-12-13T08:36:51Z","title":"LMD: Faster Image Reconstruction with Latent Masking Diffusion","summary":"  As a class of fruitful approaches, diffusion probabilistic models (DPMs) have\nshown excellent advantages in high-resolution image reconstruction. On the\nother hand, masked autoencoders (MAEs), as popular self-supervised vision\nlearners, have demonstrated simpler and more effective image reconstruction and\ntransfer capabilities on downstream tasks. However, they all require extremely\nhigh training costs, either due to inherent high temporal-dependence (i.e.,\nexcessively long diffusion steps) or due to artificially low spatial-dependence\n(i.e., human-formulated high mask ratio, such as 0.75). To the end, this paper\npresents LMD, a faster image reconstruction framework with latent masking\ndiffusion. First, we propose to project and reconstruct images in latent space\nthrough a pre-trained variational autoencoder, which is theoretically more\nefficient than in the pixel-based space. Then, we combine the advantages of\nMAEs and DPMs to design a progressive masking diffusion model, which gradually\nincreases the masking proportion by three different schedulers and reconstructs\nthe latent features from simple to difficult, without sequentially performing\ndenoising diffusion as in DPMs or using fixed high masking ratio as in MAEs, so\nas to alleviate the high training time-consumption predicament. Our approach\nallows for learning high-capacity models and accelerate their training (by 3x\nor more) and barely reduces the original accuracy. Inference speed in\ndownstream tasks also significantly outperforms the previous approaches.\n","authors":["Zhiyuan Ma","zhihuan yu","Jianjun Li","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2312.07971v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07970v1","updated":"2023-12-13T08:33:50Z","published":"2023-12-13T08:33:50Z","title":"Divide and Conquer: Hybrid Pre-training for Person Search","summary":"  Large-scale pre-training has proven to be an effective method for improving\nperformance across different tasks. Current person search methods use ImageNet\npre-trained models for feature extraction, yet it is not an optimal solution\ndue to the gap between the pre-training task and person search task (as a\ndownstream task). Therefore, in this paper, we focus on pre-training for person\nsearch, which involves detecting and re-identifying individuals simultaneously.\nAlthough labeled data for person search is scarce, datasets for two sub-tasks\nperson detection and re-identification are relatively abundant. To this end, we\npropose a hybrid pre-training framework specifically designed for person search\nusing sub-task data only. It consists of a hybrid learning paradigm that\nhandles data with different kinds of supervisions, and an intra-task alignment\nmodule that alleviates domain discrepancy under limited resources. To the best\nof our knowledge, this is the first work that investigates how to support\nfull-task pre-training using sub-task data. Extensive experiments demonstrate\nthat our pre-trained model can achieve significant improvements across diverse\nprotocols, such as person search method, fine-tuning data, pre-training data\nand model backbone. For example, our model improves ResNet50 based NAE by 10.3%\nrelative improvement w.r.t. mAP. Our code and pre-trained models are released\nfor plug-and-play usage to the person search community.\n","authors":["Yanling Tian","Di Chen","Yunan Liu","Jian Yang","Shanshan Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.07970v1.pdf","comment":"accepted by AAAI24"},{"id":"http://arxiv.org/abs/2312.07969v1","updated":"2023-12-13T08:31:26Z","published":"2023-12-13T08:31:26Z","title":"ASLseg: Adapting SAM in the Loop for Semi-supervised Liver Tumor\n  Segmentation","summary":"  Liver tumor segmentation is essential for computer-aided diagnosis, surgical\nplanning, and prognosis evaluation. However, obtaining and maintaining a\nlarge-scale dataset with dense annotations is challenging. Semi-Supervised\nLearning (SSL) is a common technique to address these challenges. Recently,\nSegment Anything Model (SAM) has shown promising performance in some medical\nimage segmentation tasks, but it performs poorly for liver tumor segmentation.\nIn this paper, we propose a novel semi-supervised framework, named ASLseg,\nwhich can effectively adapt the SAM to the SSL setting and combine both\ndomain-specific and general knowledge of liver tumors. Specifically, the\nsegmentation model trained with a specific SSL paradigm provides the generated\npseudo-labels as prompts to the fine-tuned SAM. An adaptation network is then\nused to refine the SAM-predictions and generate higher-quality pseudo-labels.\nFinally, the reliable pseudo-labels are selected to expand the labeled set for\niterative training. Extensive experiments on the LiTS dataset demonstrate\noverwhelming performance of our ASLseg.\n","authors":["Shiyun Chen","Li Lin","Pujin Cheng","Xiaoying Tang"],"pdf_url":"https://arxiv.org/pdf/2312.07969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07965v1","updated":"2023-12-13T08:28:21Z","published":"2023-12-13T08:28:21Z","title":"Pneumonia Detection on chest X-ray images Using Ensemble of Deep\n  Convolutional Neural Networks","summary":"  Pneumonia is a life-threatening lung infection resulting from several\ndifferent viral infections. Identifying and treating pneumonia on chest X-ray\nimages can be difficult due to its similarity to other pulmonary diseases.\nThus, the existing methods for predicting pneumonia cannot attain substantial\nlevels of accuracy. Therefore, this paper presents a computer-aided\nclassification of pneumonia, coined as Ensemble Learning (EL), to simplify the\ndiagnosis process on chest X-ray images. Our proposal is based on Convolutional\nNeural Network (CNN) models, which are pre-trained CNN models that have been\nrecently employed to enhance the performance of many medical tasks instead of\ntraining CNN models from scratch. We propose to use three well-known CNN\npre-trained (DenseNet169, MobileNetV2 and Vision Transformer) using the\nImageNet database. Then, these models are trained on the chest X-ray data set\nusing fine-tuning. Finally, the results are obtained by combining the extracted\nfeatures from these three models during the experimental phase. The proposed EL\napproach outperforms other existing state-of-the-art methods, and it obtains an\naccuracy of 93.91% and a F1-Score of 93.88% on the testing phase.\n","authors":["Alhassan Mabrouk","Rebeca P. Díaz Redondo","Abdelghani Dahou","Mohamed Abd Elaziz","Mohammed Kayed"],"pdf_url":"https://arxiv.org/pdf/2312.07965v1.pdf","comment":"14 pages, 4 figures, journal"},{"id":"http://arxiv.org/abs/2312.07964v1","updated":"2023-12-13T08:18:49Z","published":"2023-12-13T08:18:49Z","title":"Three-Filters-to-Normal+: Revisiting Discontinuity Discrimination in\n  Depth-to-Normal Translation","summary":"  This article introduces three-filters-to-normal+ (3F2N+), an extension of our\nprevious work three-filters-to-normal (3F2N), with a specific focus on\nincorporating discontinuity discrimination capability into surface normal\nestimators (SNEs). 3F2N+ achieves this capability by utilizing a novel\ndiscontinuity discrimination module (DDM), which combines depth curvature\nminimization and correlation coefficient maximization through conditional\nrandom fields (CRFs). To evaluate the robustness of SNEs on noisy data, we\ncreate a large-scale synthetic surface normal (SSN) dataset containing 20\nscenarios (ten indoor scenarios and ten outdoor scenarios with and without\nrandom Gaussian noise added to depth images). Extensive experiments demonstrate\nthat 3F2N+ achieves greater performance than all other geometry-based surface\nnormal estimators, with average angular errors of 7.85$^\\circ$, 8.95$^\\circ$,\n9.25$^\\circ$, and 11.98$^\\circ$ on the clean-indoor, clean-outdoor,\nnoisy-indoor, and noisy-outdoor datasets, respectively. We conduct three\nadditional experiments to demonstrate the effectiveness of incorporating our\nproposed 3F2N+ into downstream robot perception tasks, including freespace\ndetection, 6D object pose estimation, and point cloud completion. Our source\ncode and datasets are publicly available at https://mias.group/3F2Nplus.\n","authors":["Jingwei Yang","Bohuan Xue","Yi Feng","Deming Wang","Rui Fan","Qijun Chen"],"pdf_url":"https://arxiv.org/pdf/2312.07964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07955v1","updated":"2023-12-13T08:01:15Z","published":"2023-12-13T08:01:15Z","title":"Erasing Self-Supervised Learning Backdoor by Cluster Activation Masking","summary":"  Researchers have recently found that Self-Supervised Learning (SSL) is\nvulnerable to backdoor attacks. The attacker can embed hidden SSL backdoors via\na few poisoned examples in the training dataset and maliciously manipulate the\nbehavior of downstream models. To defend against SSL backdoor attacks, a\nfeasible route is to detect and remove the poisonous samples in the training\nset. However, the existing SSL backdoor defense method fails to detect the\npoisonous samples precisely. In this paper, we propose to erase the SSL\nbackdoor by cluster activation masking and propose a novel PoisonCAM method.\nAfter obtaining the threat model trained on the poisoned dataset, our method\ncan precisely detect poisonous samples based on the assumption that masking the\nbackdoor trigger can effectively change the activation of a downstream\nclustering model. In experiments, our PoisonCAM achieves 96% accuracy for\nbackdoor trigger detection compared to 3% of the state-of-the-art method on\npoisoned ImageNet-100. Moreover, our proposed PoisonCAM significantly improves\nthe performance of the trained SSL model under backdoor attacks compared to the\nstate-of-the-art method. Our code will be available at\nhttps://github.com/LivXue/PoisonCAM.\n","authors":["Shengsheng Qian","Yifei Wang","Dizhan Xue","Shengjie Zhang","Huaiwen Zhang","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2312.07955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07951v1","updated":"2023-12-13T07:57:40Z","published":"2023-12-13T07:57:40Z","title":"Semantic-aware Data Augmentation for Text-to-image Synthesis","summary":"  Data augmentation has been recently leveraged as an effective regularizer in\nvarious vision-language deep neural networks. However, in text-to-image\nsynthesis (T2Isyn), current augmentation wisdom still suffers from the semantic\nmismatch between augmented paired data. Even worse, semantic collapse may occur\nwhen generated images are less semantically constrained. In this paper, we\ndevelop a novel Semantic-aware Data Augmentation (SADA) framework dedicated to\nT2Isyn. In particular, we propose to augment texts in the semantic space via an\nImplicit Textual Semantic Preserving Augmentation ($ITA$), in conjunction with\na specifically designed Image Semantic Regularization Loss ($L_r$) as Generated\nImage Semantic Conservation, to cope well with semantic mismatch and collapse.\nAs one major contribution, we theoretically show that $ITA$ can certify better\ntext-image consistency while $L_r$ regularizing the semantics of generated\nimages would avoid semantic collapse and enhance image quality. Extensive\nexperiments validate that SADA enhances text-image consistency and improves\nimage quality significantly in T2Isyn models across various backbones.\nEspecially, incorporating SADA during the tuning process of Stable Diffusion\nmodels also yields performance improvements.\n","authors":["Zhaorui Tan","Xi Yang","Kaizhu Huang"],"pdf_url":"https://arxiv.org/pdf/2312.07951v1.pdf","comment":"Accepted by AAAI24"},{"id":"http://arxiv.org/abs/2312.07943v1","updated":"2023-12-13T07:40:39Z","published":"2023-12-13T07:40:39Z","title":"ReFusion: Learning Image Fusion from Reconstruction with Learnable Loss\n  via Meta-Learning","summary":"  Image fusion aims to combine information from multiple source images into a\nsingle and more informative image. A major challenge for deep learning-based\nimage fusion algorithms is the absence of a definitive ground truth and\ndistance measurement. Thus, the manually specified loss functions aiming to\nsteer the model learning, include hyperparameters that need to be manually\nthereby limiting the model's flexibility and generalizability to unseen tasks.\nTo overcome the limitations of designing loss functions for specific fusion\ntasks, we propose a unified meta-learning based fusion framework named\nReFusion, which learns optimal fusion loss from reconstructing source images.\nReFusion consists of a fusion module, a loss proposal module, and a\nreconstruction module. Compared with the conventional methods with fixed loss\nfunctions, ReFusion employs a parameterized loss function, which is dynamically\nadapted by the loss proposal module based on the specific fusion scene and\ntask. To ensure that the fusion network preserves maximal information from the\nsource images, makes it possible to reconstruct the original images from the\nfusion image, a meta-learning strategy is used to make the reconstruction loss\ncontinually refine the parameters of the loss proposal module. Adaptive\nupdating is achieved by alternating between inter update, outer update, and\nfusion update, where the training of the three components facilitates each\nother. Extensive experiments affirm that our method can successfully adapt to\ndiverse fusion tasks, including infrared-visible, multi-focus, multi-exposure,\nand medical image fusion problems. The code will be released.\n","authors":["Haowen Bai","Zixiang Zhao","Jiangshe Zhang","Yichen Wu","Lilun Deng","Yukun Cui","Shuang Xu","Baisong Jiang"],"pdf_url":"https://arxiv.org/pdf/2312.07943v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14585v3","updated":"2023-12-13T07:39:47Z","published":"2023-09-26T00:15:13Z","title":"DifAttack: Query-Efficient Black-Box Attack via Disentangled Feature\n  Space","summary":"  This work investigates efficient score-based black-box adversarial attacks\nwith a high Attack Success Rate (ASR) and good generalizability. We design a\nnovel attack method based on a Disentangled Feature space, called DifAttack,\nwhich differs significantly from the existing ones operating over the entire\nfeature space. Specifically, DifAttack firstly disentangles an image's latent\nfeature into an adversarial feature and a visual feature, where the former\ndominates the adversarial capability of an image, while the latter largely\ndetermines its visual appearance. We train an autoencoder for the\ndisentanglement by using pairs of clean images and their Adversarial Examples\n(AEs) generated from available surrogate models via white-box attack methods.\nEventually, DifAttack iteratively optimizes the adversarial feature according\nto the query feedback from the victim model until a successful AE is generated,\nwhile keeping the visual feature unaltered. In addition, due to the avoidance\nof using surrogate models' gradient information when optimizing AEs for\nblack-box models, our proposed DifAttack inherently possesses better attack\ncapability in the open-set scenario, where the training dataset of the victim\nmodel is unknown. Extensive experimental results demonstrate that our method\nachieves significant improvements in ASR and query efficiency simultaneously,\nespecially in the targeted attack and open-set scenarios. The code is available\nat https://github.com/csjunjun/DifAttack.git.\n","authors":["Liu Jun","Zhou Jiantao","Zeng Jiandian","Jinyu Tian"],"pdf_url":"https://arxiv.org/pdf/2309.14585v3.pdf","comment":"Accepted in AAAI'24"},{"id":"http://arxiv.org/abs/2312.07937v1","updated":"2023-12-13T07:30:19Z","published":"2023-12-13T07:30:19Z","title":"BOTH2Hands: Inferring 3D Hands from Both Text Prompts and Body Dynamics","summary":"  The recently emerging text-to-motion advances have spired numerous attempts\nfor convenient and interactive human motion generation. Yet, existing methods\nare largely limited to generating body motions only without considering the\nrich two-hand motions, let alone handling various conditions like body dynamics\nor texts. To break the data bottleneck, we propose BOTH57M, a novel multi-modal\ndataset for two-hand motion generation. Our dataset includes accurate motion\ntracking for the human body and hands and provides pair-wised finger-level hand\nannotations and body descriptions. We further provide a strong baseline method,\nBOTH2Hands, for the novel task: generating vivid two-hand motions from both\nimplicit body dynamics and explicit text prompts. We first warm up two parallel\nbody-to-hand and text-to-hand diffusion models and then utilize the\ncross-attention transformer for motion blending. Extensive experiments and\ncross-validations demonstrate the effectiveness of our approach and dataset for\ngenerating convincing two-hand motions from the hybrid body-and-textual\nconditions. Our dataset and code will be disseminated to the community for\nfuture research.\n","authors":["Wenqian Zhang","Molin Huang","Yuxuan Zhou","Juze Zhang","Jingyi Yu","Jingya Wang","Lan Xu"],"pdf_url":"https://arxiv.org/pdf/2312.07937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07935v1","updated":"2023-12-13T07:29:24Z","published":"2023-12-13T07:29:24Z","title":"Comparing YOLOv8 and Mask RCNN for object segmentation in complex\n  orchard environments","summary":"  Instance segmentation, an important image processing operation for automation\nin agriculture, is used to precisely delineate individual objects of interest\nwithin images, which provides foundational information for various automated or\nrobotic tasks such as selective harvesting and precision pruning. This study\ncompares the one-stage YOLOv8 and the two-stage Mask R-CNN machine learning\nmodels for instance segmentation under varying orchard conditions across two\ndatasets. Dataset 1, collected in dormant season, includes images of dormant\napple trees, which were used to train multi-object segmentation models\ndelineating tree branches and trunks. Dataset 2, collected in the early growing\nseason, includes images of apple tree canopies with green foliage and immature\n(green) apples (also called fruitlet), which were used to train single-object\nsegmentation models delineating only immature green apples. The results showed\nthat YOLOv8 performed better than Mask R-CNN, achieving good precision and\nnear-perfect recall across both datasets at a confidence threshold of 0.5.\nSpecifically, for Dataset 1, YOLOv8 achieved a precision of 0.90 and a recall\nof 0.95 for all classes. In comparison, Mask R-CNN demonstrated a precision of\n0.81 and a recall of 0.81 for the same dataset. With Dataset 2, YOLOv8 achieved\na precision of 0.93 and a recall of 0.97. Mask R-CNN, in this single-class\nscenario, achieved a precision of 0.85 and a recall of 0.88. Additionally, the\ninference times for YOLOv8 were 10.9 ms for multi-class segmentation (Dataset\n1) and 7.8 ms for single-class segmentation (Dataset 2), compared to 15.6 ms\nand 12.8 ms achieved by Mask R-CNN's, respectively.\n","authors":["Ranjan Sapkota","Dawood Ahmed","Manoj Karkee"],"pdf_url":"https://arxiv.org/pdf/2312.07935v1.pdf","comment":"15 figures, 2 tables"},{"id":"http://arxiv.org/abs/2312.07934v1","updated":"2023-12-13T07:24:50Z","published":"2023-12-13T07:24:50Z","title":"Toward Real World Stereo Image Super-Resolution via Hybrid Degradation\n  Model and Discriminator for Implied Stereo Image Information","summary":"  Real-world stereo image super-resolution has a significant influence on\nenhancing the performance of computer vision systems. Although existing methods\nfor single-image super-resolution can be applied to improve stereo images,\nthese methods often introduce notable modifications to the inherent disparity,\nresulting in a loss in the consistency of disparity between the original and\nthe enhanced stereo images. To overcome this limitation, this paper proposes a\nnovel approach that integrates a implicit stereo information discriminator and\na hybrid degradation model. This combination ensures effective enhancement\nwhile preserving disparity consistency. The proposed method bridges the gap\nbetween the complex degradations in real-world stereo domain and the simpler\ndegradations in real-world single-image super-resolution domain. Our results\ndemonstrate impressive performance on synthetic and real datasets, enhancing\nvisual perception while maintaining disparity consistency. The complete code is\navailable at the following \\href{https://github.com/fzuzyb/SCGLANet}{link}.\n","authors":["Yuanbo Zhou","Yuyang Xue","Jiang Bi","Wenlin He","Xinlin Zhang","Jiajun Zhang","Wei Deng","Ruofeng Nie","Junlin Lan","Qinquan Gao","Tong Tong"],"pdf_url":"https://arxiv.org/pdf/2312.07934v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07932v1","updated":"2023-12-13T07:21:16Z","published":"2023-12-13T07:21:16Z","title":"A Novel Framework Based on Variational Quantum Algorithms:\n  Revolutionizing Image Classification","summary":"  Image classification is a crucial task in machine learning. In recent years,\nthis field has witnessed rapid development, with a series of image\nclassification models being proposed and achieving state-of-the-art (SOTA)\nresults. Parallelly, with the advancement of quantum technologies, quantum\nmachine learning has attracted a lot of interest. In particular, a class of\nalgorithms known as variational quantum algorithms (VQAs) has been extensively\nstudied to improve the performance of classical machine learning. In this\npaper, we propose a novel image classification framework using VQAs. The major\nadvantage of our framework is the elimination of the need for the global\npooling operation typically performed at the end of classical image\nclassification models. While global pooling can help to reduce computational\ncomplexity, it often results in a significant loss of information. By removing\nthe global pooling module before the output layer, our approach allows for\neffectively capturing more discriminative features and fine-grained details in\nimages, leading to improved classification performance. Moreover, employing\nVQAs enables our framework to have fewer parameters compared to the classical\nframework, even in the absence of global pooling, which makes it more\nadvantageous in preventing overfitting. We apply our method to different SOTA\nimage classification models and demonstrate the superiority of the proposed\nquantum architecture over its classical counterpart through a series of\nexperiments on public datasets.\n","authors":["Yixiong Chen"],"pdf_url":"https://arxiv.org/pdf/2312.07932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.13428v4","updated":"2023-12-13T07:01:08Z","published":"2023-01-31T05:51:05Z","title":"Contrast and Clustering: Learning Neighborhood Pair Representation for\n  Source-free Domain Adaptation","summary":"  Unsupervised domain adaptation uses source data from different distributions\nto solve the problem of classifying data from unlabeled target domains.\nHowever, conventional methods require access to source data, which often raise\nconcerns about data privacy. In this paper, we consider a more practical but\nchallenging setting where the source domain data is unavailable and the target\ndomain data is unlabeled. Specifically, we address the domain discrepancy\nproblem from the perspective of contrastive learning. The key idea of our work\nis to learn a domain-invariant feature by 1) performing clustering directly in\nthe original feature space with nearest neighbors; 2) constructing truly hard\nnegative pairs by extended neighbors without introducing additional\ncomputational complexity; and 3) combining noise-contrastive estimation theory\nto gain computational advantage. We conduct careful ablation studies and\nextensive experiments on three common benchmarks: VisDA, Office-Home, and\nOffice-31. The results demonstrate the superiority of our methods compared with\nother state-of-the-art works.\n","authors":["Yuqi Chen","Xiangbin Zhu","Yonggang Li","Yingjian Li","Haojie Fang"],"pdf_url":"https://arxiv.org/pdf/2301.13428v4.pdf","comment":"Journal articles"},{"id":"http://arxiv.org/abs/2312.07925v1","updated":"2023-12-13T06:50:30Z","published":"2023-12-13T06:50:30Z","title":"Polar-Doc: One-Stage Document Dewarping with Multi-Scope Constraints\n  under Polar Representation","summary":"  Document dewarping, aiming to eliminate geometric deformation in photographed\ndocuments to benefit text recognition, has made great progress in recent years\nbut is still far from being solved. While Cartesian coordinates are typically\nleveraged by state-of-the-art approaches to learn a group of deformation\ncontrol points, such representation is not efficient for dewarping model to\nlearn the deformation information. In this work, we explore Polar coordinates\nrepresentation for each point in document dewarping, namely Polar-Doc. In\ncontrast to most current works adopting a two-stage pipeline typically, Polar\nrepresentation enables a unified point regression framework for both\nsegmentation and dewarping network in one single stage. Such unification makes\nthe whole model more efficient to learn under an end-to-end optimization\npipeline, and also obtains a compact representation. Furthermore, we propose a\nnovel multi-scope Polar-Doc-IOU loss to constrain the relationship among\ncontrol points as a grid-based regularization under the Polar representation.\nVisual comparisons and quantitative experiments on two benchmarks show that,\nwith much fewer parameters than the other mainstream counterparts, our\none-stage model with multi-scope constraints achieves new state-of-the-art\nperformance on both pixel alignment metrics and OCR metrics. Source codes will\nbe available at \\url{*****}.\n","authors":["Weiguang Zhang","Qiufeng Wang","Kaizhu Huang"],"pdf_url":"https://arxiv.org/pdf/2312.07925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07922v1","updated":"2023-12-13T06:39:49Z","published":"2023-12-13T06:39:49Z","title":"Memory-Efficient Reversible Spiking Neural Networks","summary":"  Spiking neural networks (SNNs) are potential competitors to artificial neural\nnetworks (ANNs) due to their high energy-efficiency on neuromorphic hardware.\nHowever, SNNs are unfolded over simulation time steps during the training\nprocess. Thus, SNNs require much more memory than ANNs, which impedes the\ntraining of deeper SNN models. In this paper, we propose the reversible spiking\nneural network to reduce the memory cost of intermediate activations and\nmembrane potentials during training. Firstly, we extend the reversible\narchitecture along temporal dimension and propose the reversible spiking block,\nwhich can reconstruct the computational graph and recompute all intermediate\nvariables in forward pass with a reverse process. On this basis, we adopt the\nstate-of-the-art SNN models to the reversible variants, namely reversible\nspiking ResNet (RevSResNet) and reversible spiking transformer (RevSFormer).\nThrough experiments on static and neuromorphic datasets, we demonstrate that\nthe memory cost per image of our reversible SNNs does not increase with the\nnetwork depth. On CIFAR10 and CIFAR100 datasets, our RevSResNet37 and\nRevSFormer-4-384 achieve comparable accuracies and consume 3.79x and 3.00x\nlower GPU memory per image than their counterparts with roughly identical model\ncomplexity and parameters. We believe that this work can unleash the memory\nconstraints in SNN training and pave the way for training extremely large and\ndeep SNNs. The code is available at https://github.com/mi804/RevSNN.git.\n","authors":["Hong Zhang","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.07922v1.pdf","comment":"accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2312.07920v1","updated":"2023-12-13T06:30:51Z","published":"2023-12-13T06:30:51Z","title":"DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic\n  Autonomous Driving Scenes","summary":"  We present DrivingGaussian, an efficient and effective framework for\nsurrounding dynamic autonomous driving scenes. For complex scenes with moving\nobjects, we first sequentially and progressively model the static background of\nthe entire scene with incremental static 3D Gaussians. We then leverage a\ncomposite dynamic Gaussian graph to handle multiple moving objects,\nindividually reconstructing each object and restoring their accurate positions\nand occlusion relationships within the scene. We further use a LiDAR prior for\nGaussian Splatting to reconstruct scenes with greater details and maintain\npanoramic consistency. DrivingGaussian outperforms existing methods in driving\nscene reconstruction and enables photorealistic surround-view synthesis with\nhigh-fidelity and multi-camera consistency. The source code and trained models\nwill be released.\n","authors":["Xiaoyu Zhou","Zhiwei Lin","Xiaojun Shan","Yongtao Wang","Deqing Sun","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2312.07920v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.08716v2","updated":"2023-12-13T06:25:23Z","published":"2022-09-19T02:25:41Z","title":"GLARE: A Dataset for Traffic Sign Detection in Sun Glare","summary":"  Real-time machine learning object detection algorithms are often found within\nautonomous vehicle technology and depend on quality datasets. It is essential\nthat these algorithms work correctly in everyday conditions as well as under\nstrong sun glare. Reports indicate glare is one of the two most prominent\nenvironment-related reasons for crashes. However, existing datasets, such as\nthe Laboratory for Intelligent & Safe Automobiles Traffic Sign (LISA) Dataset\nand the German Traffic Sign Recognition Benchmark, do not reflect the existence\nof sun glare at all. This paper presents the GLARE (GLARE is available at:\nhttps://github.com/NicholasCG/GLARE_Dataset ) traffic sign dataset: a\ncollection of images with U.S-based traffic signs under heavy visual\ninterference by sunlight. GLARE contains 2,157 images of traffic signs with sun\nglare, pulled from 33 videos of dashcam footage of roads in the United States.\nIt provides an essential enrichment to the widely used LISA Traffic Sign\ndataset. Our experimental study shows that although several state-of-the-art\nbaseline architectures have demonstrated good performance on traffic sign\ndetection in conditions without sun glare in the past, they performed poorly\nwhen tested against GLARE (e.g., average mAP0.5:0.95 of 19.4). We also notice\nthat current architectures have better detection when trained on images of\ntraffic signs in sun glare performance (e.g., average mAP0.5:0.95 of 39.6), and\nperform best when trained on a mixture of conditions (e.g., average mAP0.5:0.95\nof 42.3).\n","authors":["Nicholas Gray","Megan Moraes","Jiang Bian","Alex Wang","Allen Tian","Kurt Wilson","Yan Huang","Haoyi Xiong","Zhishan Guo"],"pdf_url":"https://arxiv.org/pdf/2209.08716v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07911v1","updated":"2023-12-13T05:58:52Z","published":"2023-12-13T05:58:52Z","title":"Projective Parallel Single-Pixel Imaging: 3D Structured Light Scanning\n  Under Global Illumination","summary":"  We present projective parallel single-pixel imaging (pPSI), a 3D photography\nmethod that provides a robust and efficient way to analyze the light transport\nbehavior and enables separation of light effect due to global illumination,\nthereby achieving 3D structured light scanning under global illumination. The\nlight transport behavior is described by the light transport coefficients\n(LTC), which contain complete information for a projector camera pair, and is a\n4D data set. However, the capture of LTC is generally time consuming. The 4D\nLTC in pPSI are reduced to projection functions, thereby enabling a highly\nefficient data capture process. We introduce the local maximum constraint,\nwhich provides constraint for the location of candidate correspondence matching\npoints when projections are captured. Local slice extension (LSE) method is\nintroduced to accelerate the capture of projection functions. Optimization is\nconducted for pPSI under several situations. The number of projection functions\nrequired for pPSI is optimized and the influence of capture ratio in LSE on the\naccuracy of the correspondence matching points is investigated. Discussions and\nexperiments include two typical kinds of global illuminations:\ninter-reflections and subsurface scattering. The proposed method is validated\nwith several challenging scenarios, and outperforms the state-of-the-art\nmethods.\n","authors":["Yuxi Li","Hongzhi Jiang","Huijie Zhao","Xudong Li"],"pdf_url":"https://arxiv.org/pdf/2312.07911v1.pdf","comment":"21 pages,13 figures"},{"id":"http://arxiv.org/abs/2312.07472v2","updated":"2023-12-13T05:25:17Z","published":"2023-12-12T17:55:45Z","title":"MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active\n  Perception","summary":"  It is a long-lasting goal to design an embodied system that can solve\nlong-horizon open-world tasks in human-like ways. However, existing approaches\nusually struggle with compound difficulties caused by the logic-aware\ndecomposition and context-aware execution of these tasks. To this end, we\nintroduce MP5, an open-ended multimodal embodied system built upon the\nchallenging Minecraft simulator, which can decompose feasible sub-objectives,\ndesign sophisticated situation-aware plans, and perform embodied action\ncontrol, with frequent communication with a goal-conditioned active perception\nscheme. Specifically, MP5 is developed on top of recent advances in Multimodal\nLarge Language Models (MLLMs), and the system is modulated into functional\nmodules that can be scheduled and collaborated to ultimately solve pre-defined\ncontext- and process-dependent tasks. Extensive experiments prove that MP5 can\nachieve a 22% success rate on difficult process-dependent tasks and a 91%\nsuccess rate on tasks that heavily depend on the context. Moreover, MP5\nexhibits a remarkable ability to address many open-ended tasks that are\nentirely novel.\n","authors":["Yiran Qin","Enshen Zhou","Qichang Liu","Zhenfei Yin","Lu Sheng","Ruimao Zhang","Yu Qiao","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2312.07472v2.pdf","comment":"Project URL: https://iranqin.github.io/MP5.github.io/"},{"id":"http://arxiv.org/abs/2309.08912v2","updated":"2023-12-13T05:24:46Z","published":"2023-09-16T07:30:52Z","title":"Delving into Multimodal Prompting for Fine-grained Visual Classification","summary":"  Fine-grained visual classification (FGVC) involves categorizing fine\nsubdivisions within a broader category, which poses challenges due to subtle\ninter-class discrepancies and large intra-class variations. However, prevailing\napproaches primarily focus on uni-modal visual concepts. Recent advancements in\npre-trained vision-language models have demonstrated remarkable performance in\nvarious high-level vision tasks, yet the applicability of such models to FGVC\ntasks remains uncertain. In this paper, we aim to fully exploit the\ncapabilities of cross-modal description to tackle FGVC tasks and propose a\nnovel multimodal prompting solution, denoted as MP-FGVC, based on the\ncontrastive language-image pertaining (CLIP) model. Our MP-FGVC comprises a\nmultimodal prompts scheme and a multimodal adaptation scheme. The former\nincludes Subcategory-specific Vision Prompt (SsVP) and Discrepancy-aware Text\nPrompt (DaTP), which explicitly highlights the subcategory-specific\ndiscrepancies from the perspectives of both vision and language. The latter\naligns the vision and text prompting elements in a common semantic space,\nfacilitating cross-modal collaborative reasoning through a Vision-Language\nFusion Module (VLFM) for further improvement on FGVC. Moreover, we tailor a\ntwo-stage optimization strategy for MP-FGVC to fully leverage the pre-trained\nCLIP model and expedite efficient adaptation for FGVC. Extensive experiments\nconducted on four FGVC datasets demonstrate the effectiveness of our MP-FGVC.\n","authors":["Xin Jiang","Hao Tang","Junyao Gao","Xiaoyu Du","Shengfeng He","Zechao Li"],"pdf_url":"https://arxiv.org/pdf/2309.08912v2.pdf","comment":"This paper is accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.07905v1","updated":"2023-12-13T05:24:36Z","published":"2023-12-13T05:24:36Z","title":"Plant Disease Recognition Datasets in the Age of Deep Learning:\n  Challenges and Opportunities","summary":"  Plant disease recognition has witnessed a significant improvement with deep\nlearning in recent years. Although plant disease datasets are essential and\nmany relevant datasets are public available, two fundamental questions exist.\nFirst, how to differentiate datasets and further choose suitable public\ndatasets for specific applications? Second, what kinds of characteristics of\ndatasets are desired to achieve promising performance in real-world\napplications? To address the questions, this study explicitly propose an\ninformative taxonomy to describe potential plant disease datasets. We further\nprovide several directions for future, such as creating challenge-oriented\ndatasets and the ultimate objective deploying deep learning in real-world\napplications with satisfactory performance. In addition, existing related\npublic RGB image datasets are summarized. We believe that this study will\ncontributing making better datasets and that this study will contribute beyond\nplant disease recognition such as plant species recognition. To facilitate the\ncommunity, our project is public https://github.com/xml94/PPDRD with the\ninformation of relevant public datasets.\n","authors":["Mingle Xu","Ji Eun Park","Jaehwan Lee","Jucheng Yang","Sook Yoon"],"pdf_url":"https://arxiv.org/pdf/2312.07905v1.pdf","comment":"Submission v1 to a journal"},{"id":"http://arxiv.org/abs/2312.07899v1","updated":"2023-12-13T05:08:32Z","published":"2023-12-13T05:08:32Z","title":"Morphological Profiling for Drug Discovery in the Era of Deep Learning","summary":"  Morphological profiling is a valuable tool in phenotypic drug discovery. The\nadvent of high-throughput automated imaging has enabled the capturing of a wide\nrange of morphological features of cells or organisms in response to\nperturbations at the single-cell resolution. Concurrently, significant advances\nin machine learning and deep learning, especially in computer vision, have led\nto substantial improvements in analyzing large-scale high-content images at\nhigh-throughput. These efforts have facilitated understanding of compound\nmechanism-of-action (MOA), drug repurposing, characterization of cell\nmorphodynamics under perturbation, and ultimately contributing to the\ndevelopment of novel therapeutics. In this review, we provide a comprehensive\noverview of the recent advances in the field of morphological profiling. We\nsummarize the image profiling analysis workflow, survey a broad spectrum of\nanalysis strategies encompassing feature engineering- and deep learning-based\napproaches, and introduce publicly available benchmark datasets. We place a\nparticular emphasis on the application of deep learning in this pipeline,\ncovering cell segmentation, image representation learning, and multimodal\nlearning. Additionally, we illuminate the application of morphological\nprofiling in phenotypic drug discovery and highlight potential challenges and\nopportunities in this field.\n","authors":["Qiaosi Tang","Ranjala Ratnayake","Gustavo Seabra","Zhe Jiang","Ruogu Fang","Lina Cui","Yousong Ding","Tamer Kahveci","Jiang Bian","Chenglong Li","Hendrik Luesch","Yanjun Li"],"pdf_url":"https://arxiv.org/pdf/2312.07899v1.pdf","comment":"44 pages, 5 figure, 5 tables"},{"id":"http://arxiv.org/abs/2311.10529v2","updated":"2023-12-13T04:57:47Z","published":"2023-11-17T13:49:00Z","title":"Segment Anything Model with Uncertainty Rectification for Auto-Prompting\n  Medical Image Segmentation","summary":"  The introduction of the Segment Anything Model (SAM) has marked a significant\nadvancement in prompt-driven image segmentation. However, SAM's application to\nmedical image segmentation requires manual prompting of target structures to\nobtain acceptable performance, which is still labor-intensive. Despite attempts\nof auto-prompting to turn SAM into a fully automatic manner, it still exhibits\nsubpar performance and lacks of reliability in the field of medical imaging. In\nthis paper, we propose UR-SAM, an uncertainty rectified SAM framework to\nenhance the robustness and reliability for auto-prompting medical image\nsegmentation. Our method incorporates a prompt augmentation module to estimate\nthe distribution of predictions and generate uncertainty maps, and an\nuncertainty-based rectification module to further enhance the performance of\nSAM. Extensive experiments on two public 3D medical datasets covering the\nsegmentation of 35 organs demonstrate that without supplementary training or\nfine-tuning, our method further improves the segmentation performance with up\nto 10.7 % and 13.8 % in dice similarity coefficient, demonstrating efficiency\nand broad capabilities for medical image segmentation without manual prompting.\n","authors":["Yichi Zhang","Shiyao Hu","Chen Jiang","Yuan Cheng","Yuan Qi"],"pdf_url":"https://arxiv.org/pdf/2311.10529v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.10915v2","updated":"2023-12-13T04:31:44Z","published":"2023-02-17T01:31:55Z","title":"Conformers are All You Need for Visual Speech Recognition","summary":"  Visual speech recognition models extract visual features in a hierarchical\nmanner. At the lower level, there is a visual front-end with a limited temporal\nreceptive field that processes the raw pixels depicting the lips or faces. At\nthe higher level, there is an encoder that attends to the embeddings produced\nby the front-end over a large temporal receptive field. Previous work has\nfocused on improving the visual front-end of the model to extract more useful\nfeatures for speech recognition. Surprisingly, our work shows that complex\nvisual front-ends are not necessary. Instead of allocating resources to a\nsophisticated visual front-end, we find that a linear visual front-end paired\nwith a larger Conformer encoder results in lower latency, more efficient memory\nusage, and improved WER performance. We achieve a new state-of-the-art of 12.8%\nWER for visual speech recognition on the TED LRS3 dataset, which rivals the\nperformance of audio-only models from just four years ago.\n","authors":["Oscar Chang","Hank Liao","Dmitriy Serdyuk","Ankit Shah","Olivier Siohan"],"pdf_url":"https://arxiv.org/pdf/2302.10915v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.17462v2","updated":"2023-12-13T04:28:53Z","published":"2023-06-30T08:17:38Z","title":"CausalVLR: A Toolbox and Benchmark for Visual-Linguistic Causal\n  Reasoning","summary":"  We present CausalVLR (Causal Visual-Linguistic Reasoning), an open-source\ntoolbox containing a rich set of state-of-the-art causal relation discovery and\ncausal inference methods for various visual-linguistic reasoning tasks, such as\nVQA, image/video captioning, medical report generation, model generalization\nand robustness, etc. These methods have been included in the toolbox with\nPyTorch implementations under NVIDIA computing system. It not only includes\ntraining and inference codes, but also provides model weights. We believe this\ntoolbox is by far the most complete visual-linguitic causal reasoning toolbox.\nWe wish that the toolbox and benchmark could serve the growing research\ncommunity by providing a flexible toolkit to re-implement existing methods and\ndevelop their own new causal reasoning methods. Code and models are available\nat https://github.com/HCPLab-SYSU/CausalVLR. The project is under active\ndevelopment by HCP-Lab's contributors and we will keep this document updated.\n","authors":["Yang Liu","Weixing Chen","Guanbin Li","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2306.17462v2.pdf","comment":"CausalVLR: A Toolbox and Benchmark for Visual-Linguistic Causal\n  Reasoning. https://github.com/HCPLab-SYSU/CausalVLR"},{"id":"http://arxiv.org/abs/2312.07884v1","updated":"2023-12-13T04:06:18Z","published":"2023-12-13T04:06:18Z","title":"Mutual-Learning Knowledge Distillation for Nighttime UAV Tracking","summary":"  Nighttime unmanned aerial vehicle (UAV) tracking has been facilitated with\nindispensable plug-and-play low-light enhancers. However, the introduction of\nlow-light enhancers increases the extra computational burden for the UAV,\nsignificantly hindering the development of real-time UAV applications.\nMeanwhile, these state-of-the-art (SOTA) enhancers lack tight coupling with the\nadvanced daytime UAV tracking approach. To solve the above issues, this work\nproposes a novel mutual-learning knowledge distillation framework for nighttime\nUAV tracking, i.e., MLKD. This framework is constructed to learn a compact and\nfast nighttime tracker via knowledge transferring from the teacher and\nknowledge sharing among various students. Specifically, an advanced teacher\nbased on a SOTA enhancer and a superior tracking backbone is adopted for\nguiding the student based only on the tight coupling-aware tracking backbone to\ndirectly extract nighttime object features. To address the biased learning of a\nsingle student, diverse lightweight students with different distillation\nmethods are constructed to focus on various aspects of the teacher's knowledge.\nMoreover, an innovative mutual-learning room is designed to elect the superior\nstudent candidate to assist the remaining students frame-by-frame in the\ntraining phase. Furthermore, the final best student, i.e., MLKD-Track, is\nselected through the testing dataset. Extensive experiments demonstrate the\neffectiveness and superiority of MLKD and MLKD-Track. The practicality of the\nMLKD-Track is verified in real-world tests with different challenging\nsituations. The code is available at https://github.com/vision4robotics/MLKD.\n","authors":["Yufeng Liu","Haobo Zuo","Liangliang Yao","Kunhan Lu","Guangze Zheng","Changhong Fu"],"pdf_url":"https://arxiv.org/pdf/2312.07884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06988v2","updated":"2023-12-13T03:53:23Z","published":"2023-12-12T05:12:22Z","title":"MWSIS: Multimodal Weakly Supervised Instance Segmentation with 2D Box\n  Annotations for Autonomous Driving","summary":"  Instance segmentation is a fundamental research in computer vision,\nespecially in autonomous driving. However, manual mask annotation for instance\nsegmentation is quite time-consuming and costly. To address this problem, some\nprior works attempt to apply weakly supervised manner by exploring 2D or 3D\nboxes. However, no one has ever successfully segmented 2D and 3D instances\nsimultaneously by only using 2D box annotations, which could further reduce the\nannotation cost by an order of magnitude. Thus, we propose a novel framework\ncalled Multimodal Weakly Supervised Instance Segmentation (MWSIS), which\nincorporates various fine-grained label generation and correction modules for\nboth 2D and 3D modalities to improve the quality of pseudo labels, along with a\nnew multimodal cross-supervision approach, named Consistency Sparse Cross-modal\nSupervision (CSCS), to reduce the inconsistency of multimodal predictions by\nresponse distillation. Particularly, transferring the 3D backbone to downstream\ntasks not only improves the performance of the 3D detectors, but also\noutperforms fully supervised instance segmentation with only 5% fully\nsupervised annotations. On the Waymo dataset, the proposed framework\ndemonstrates significant improvements over the baseline, especially achieving\n2.59% mAP and 12.75% mAP increases for 2D and 3D instance segmentation tasks,\nrespectively. The code is available at\nhttps://github.com/jiangxb98/mwsis-plugin.\n","authors":["Guangfeng Jiang","Jun Liu","Yuzhi Wu","Wenlong Liao","Tao He","Pai Peng"],"pdf_url":"https://arxiv.org/pdf/2312.06988v2.pdf","comment":"AAAI2024"},{"id":"http://arxiv.org/abs/2312.07879v1","updated":"2023-12-13T03:48:45Z","published":"2023-12-13T03:48:45Z","title":"CoIE: Chain-of-Instruct Editing for Multi-Attribute Face Manipulation","summary":"  Current text-to-image editing models often encounter challenges with smoothly\nmanipulating multiple attributes using a single instruction. Taking inspiration\nfrom the Chain-of-Thought prompting technique utilized in language models, we\npresent an innovative concept known as Chain-of-Instruct Editing (CoIE), which\nenhances the capabilities of these models through step-by-step editing using a\nseries of instructions. In particular, in the context of face manipulation, we\nleverage the contextual learning abilities of a pretrained Large Language Model\n(LLM), such as GPT-4, to generate a sequence of instructions from the original\ninput, utilizing a purpose-designed 1-shot template. To further improve the\nprecision of each editing step, we conduct fine-tuning on the editing models\nusing our self-constructed instruction-guided face editing dataset,\nInstruct-CelebA. And additionally, we incorporate a super-resolution module to\nmitigate the adverse effects of editability and quality degradation.\nExperimental results across various challenging cases confirm the significant\nboost in multi-attribute facial image manipulation using chain-of-instruct\nediting. This is evident in enhanced editing success rates, measured by CLIPSim\nand Coverage metrics, improved by 17.86% and 85.45% respectively, and\nheightened controllability indicated by Preserve L1 and Quality metrics,\nimproved by 11.58% and 4.93% respectively.\n","authors":["Zhenduo Zhang","Bowen Zhang","Guang Liu"],"pdf_url":"https://arxiv.org/pdf/2312.07879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07875v1","updated":"2023-12-13T03:33:16Z","published":"2023-12-13T03:33:16Z","title":"Enhance Sketch Recognition's Explainability via Semantic Component-Level\n  Parsing","summary":"  Free-hand sketches are appealing for humans as a universal tool to depict the\nvisual world. Humans can recognize varied sketches of a category easily by\nidentifying the concurrence and layout of the intrinsic semantic components of\nthe category, since humans draw free-hand sketches based a common consensus\nthat which types of semantic components constitute each sketch category. For\nexample, an airplane should at least have a fuselage and wings. Based on this\nanalysis, a semantic component-level memory module is constructed and embedded\nin the proposed structured sketch recognition network in this paper. The memory\nkeys representing semantic components of each sketch category can be\nself-learned and enhance the recognition network's explainability. Our proposed\nnetworks can deal with different situations of sketch recognition, i.e., with\nor without semantic components labels of strokes. Experiments on the SPG and\nSketchIME datasets demonstrate the memory module's flexibility and the\nrecognition network's explainability. The code and data are available at\nhttps://github.com/GuangmingZhu/SketchESC.\n","authors":["Guangming Zhu","Siyuan Wang","Tianci Wu","Liang Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.07875v1.pdf","comment":"The paper has been accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2304.13850v3","updated":"2023-12-13T03:31:18Z","published":"2023-04-26T22:29:49Z","title":"Do SSL Models Have Déjà Vu? A Case of Unintended Memorization in\n  Self-supervised Learning","summary":"  Self-supervised learning (SSL) algorithms can produce useful image\nrepresentations by learning to associate different parts of natural images with\none another. However, when taken to the extreme, SSL models can unintendedly\nmemorize specific parts in individual training samples rather than learning\nsemantically meaningful associations. In this work, we perform a systematic\nstudy of the unintended memorization of image-specific information in SSL\nmodels -- which we refer to as d\\'ej\\`a vu memorization. Concretely, we show\nthat given the trained model and a crop of a training image containing only the\nbackground (e.g., water, sky, grass), it is possible to infer the foreground\nobject with high accuracy or even visually reconstruct it. Furthermore, we show\nthat d\\'ej\\`a vu memorization is common to different SSL algorithms, is\nexacerbated by certain design choices, and cannot be detected by conventional\ntechniques for evaluating representation quality. Our study of d\\'ej\\`a vu\nmemorization reveals previously unknown privacy risks in SSL models, as well as\nsuggests potential practical mitigation strategies. Code is available at\nhttps://github.com/facebookresearch/DejaVu.\n","authors":["Casey Meehan","Florian Bordes","Pascal Vincent","Kamalika Chaudhuri","Chuan Guo"],"pdf_url":"https://arxiv.org/pdf/2304.13850v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07871v1","updated":"2023-12-13T03:17:34Z","published":"2023-12-13T03:17:34Z","title":"MLNet: Mutual Learning Network with Neighborhood Invariance for\n  Universal Domain Adaptation","summary":"  Universal domain adaptation (UniDA) is a practical but challenging problem,\nin which information about the relation between the source and the target\ndomains is not given for knowledge transfer. Existing UniDA methods may suffer\nfrom the problems of overlooking intra-domain variations in the target domain\nand difficulty in separating between the similar known and unknown class. To\naddress these issues, we propose a novel \\textbf{Mutual Learning Network\n(MLNet)} with neighborhood invariance for UniDA. In our method,\nconfidence-guided invariant feature learning with self-adaptive neighbor\nselection is designed to reduce the intra-domain variations for more\ngeneralizable feature representation. By using the cross-domain mixup scheme\nfor better unknown-class identification, the proposed method compensates for\nthe misidentified known-class errors by mutual learning between the closed-set\nand open-set classifiers. Extensive experiments on three publicly available\nbenchmarks demonstrate that our method achieves the best results compared to\nthe state-of-the-arts in most cases and significantly outperforms the baseline\nacross all the four settings in UniDA. Code is available at\nhttps://github.com/YanzuoLu/MLNet.\n","authors":["Yanzuo Lu","Meng Shen","Andy J Ma","Xiaohua Xie","Jian-Huang Lai"],"pdf_url":"https://arxiv.org/pdf/2312.07871v1.pdf","comment":"Accepted by AAAI2024. Code is available at\n  https://github.com/YanzuoLu/MLNet"},{"id":"http://arxiv.org/abs/2312.07865v1","updated":"2023-12-13T03:04:22Z","published":"2023-12-13T03:04:22Z","title":"SimAC: A Simple Anti-Customization Method against Text-to-Image\n  Synthesis of Diffusion Models","summary":"  Despite the success of diffusion-based customization methods on visual\ncontent creation, increasing concerns have been raised about such techniques\nfrom both privacy and political perspectives. To tackle this issue, several\nanti-customization methods have been proposed in very recent months,\npredominantly grounded in adversarial attacks. Unfortunately, most of these\nmethods adopt straightforward designs, such as end-to-end optimization with a\nfocus on adversarially maximizing the original training loss, thereby\nneglecting nuanced internal properties intrinsic to the diffusion model, and\neven leading to ineffective optimization in some diffusion time steps. In this\npaper, we strive to bridge this gap by undertaking a comprehensive exploration\nof these inherent properties, to boost the performance of current\nanti-customization approaches. Two aspects of properties are investigated: 1)\nWe examine the relationship between time step selection and the model's\nperception in the frequency domain of images and find that lower time steps can\ngive much more contributions to adversarial noises. This inspires us to propose\nan adaptive greedy search for optimal time steps that seamlessly integrates\nwith existing anti-customization methods. 2) We scrutinize the roles of\nfeatures at different layers during denoising and devise a sophisticated\nfeature-based optimization framework for anti-customization. Experiments on\nfacial benchmarks demonstrate that our approach significantly increases\nidentity disruption, thereby enhancing user privacy and security.\n","authors":["Feifei Wang","Zhentao Tan","Tianyi Wei","Yue Wu","Qidong Huang"],"pdf_url":"https://arxiv.org/pdf/2312.07865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07860v1","updated":"2023-12-13T02:57:30Z","published":"2023-12-13T02:57:30Z","title":"Data-Dependent Higher-Order Clique Selection for Artery-Vein\n  Segmentation by Energy Minimization","summary":"  We propose a novel segmentation method based on energy minimization of\nhigher-order potentials. We introduce higher-order terms into the energy to\nincorporate prior knowledge on the shape of the segments. The terms encourage\ncertain sets of pixels to be entirely in one segment or the other. The sets can\nfor instance be smooth curves in order to help delineate pulmonary vessels,\nwhich are known to run in almost straight lines. The higher-order terms can be\nconverted to submodular first-order terms by adding auxiliary variables, which\ncan then be globally minimized using graph cuts. We also determine the weight\nof these terms, or the degree of the aforementioned encouragement, in a\nprincipled way by learning from training data with the ground truth. We\ndemonstrate the effectiveness of the method in a real-world application in\nfully-automatic pulmonary artery-vein segmentation in CT images.\n","authors":["Yoshiro Kitamura","Yuanzhong Li","Wataru Ito","Hiroshi Ishikawa"],"pdf_url":"https://arxiv.org/pdf/2312.07860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07856v1","updated":"2023-12-13T02:51:26Z","published":"2023-12-13T02:51:26Z","title":"DTL: Disentangled Transfer Learning for Visual Recognition","summary":"  When pre-trained models become rapidly larger, the cost of fine-tuning on\ndownstream tasks steadily increases, too. To economically fine-tune these\nmodels, parameter-efficient transfer learning (PETL) is proposed, which only\ntunes a tiny subset of trainable parameters to efficiently learn quality\nrepresentations. However, current PETL methods are facing the dilemma that\nduring training the GPU memory footprint is not effectively reduced as\ntrainable parameters. PETL will likely fail, too, if the full fine-tuning\nencounters the out-of-GPU-memory issue. This phenomenon happens because\ntrainable parameters from these methods are generally entangled with the\nbackbone, such that a lot of intermediate states have to be stored in GPU\nmemory for gradient propagation. To alleviate this problem, we introduce\nDisentangled Transfer Learning (DTL), which disentangles the trainable\nparameters from the backbone using a lightweight Compact Side Network (CSN). By\nprogressively extracting task-specific information with a few low-rank linear\nmappings and appropriately adding the information back to the backbone, CSN\neffectively realizes knowledge transfer in various downstream tasks. We\nconducted extensive experiments to validate the effectiveness of our method.\nThe proposed method not only reduces a large amount of GPU memory usage and\ntrainable parameters, but also outperforms existing PETL methods by a\nsignificant margin in accuracy, achieving new state-of-the-art on several\nstandard benchmarks.\n","authors":["Minghao Fu","Ke Zhu","Jianxin Wu"],"pdf_url":"https://arxiv.org/pdf/2312.07856v1.pdf","comment":"To appear in AAAI 2024"},{"id":"http://arxiv.org/abs/2312.07854v1","updated":"2023-12-13T02:48:11Z","published":"2023-12-13T02:48:11Z","title":"Diffusion Models Enable Zero-Shot Pose Estimation for Lower-Limb\n  Prosthetic Users","summary":"  The application of 2D markerless gait analysis has garnered increasing\ninterest and application within clinical settings. However, its effectiveness\nin the realm of lower-limb amputees has remained less than optimal. In\nresponse, this study introduces an innovative zero-shot method employing image\ngeneration diffusion models to achieve markerless pose estimation for\nlower-limb prosthetics, presenting a promising solution to gait analysis for\nthis specific population. Our approach demonstrates an enhancement in detecting\nkey points on prosthetic limbs over existing methods, and enables clinicians to\ngain invaluable insights into the kinematics of lower-limb amputees across the\ngait cycle. The outcomes obtained not only serve as a proof-of-concept for the\nfeasibility of this zero-shot approach but also underscore its potential in\nadvancing rehabilitation through gait analysis for this unique population.\n","authors":["Tianxun Zhou","Muhammad Nur Shahril Iskandar","Keng-Hwee Chiam"],"pdf_url":"https://arxiv.org/pdf/2312.07854v1.pdf","comment":"25 pages, 6 figures. Supplementary documents in source file"},{"id":"http://arxiv.org/abs/2312.07853v1","updated":"2023-12-13T02:48:03Z","published":"2023-12-13T02:48:03Z","title":"High-Order Structure Based Middle-Feature Learning for Visible-Infrared\n  Person Re-Identification","summary":"  Visible-infrared person re-identification (VI-ReID) aims to retrieve images\nof the same persons captured by visible (VIS) and infrared (IR) cameras.\nExisting VI-ReID methods ignore high-order structure information of features\nwhile being relatively difficult to learn a reasonable common feature space due\nto the large modality discrepancy between VIS and IR images. To address the\nabove problems, we propose a novel high-order structure based middle-feature\nlearning network (HOS-Net) for effective VI-ReID. Specifically, we first\nleverage a short- and long-range feature extraction (SLE) module to effectively\nexploit both short-range and long-range features. Then, we propose a high-order\nstructure learning (HSL) module to successfully model the high-order\nrelationship across different local features of each person image based on a\nwhitened hypergraph network.This greatly alleviates model collapse and enhances\nfeature representations. Finally, we develop a common feature space learning\n(CFL) module to learn a discriminative and reasonable common feature space\nbased on middle features generated by aligning features from different\nmodalities and ranges. In particular, a modality-range identity-center\ncontrastive (MRIC) loss is proposed to reduce the distances between the VIS,\nIR, and middle features, smoothing the training process. Extensive experiments\non the SYSU-MM01, RegDB, and LLCM datasets show that our HOS-Net achieves\nsuperior state-of-the-art performance. Our code is available at\n\\url{https://github.com/Jaulaucoeng/HOS-Net}.\n","authors":["Liuxiang Qiu","Si Chen","Yan Yan","Jin-Hao Xue","Da-Han Wang","Shunzhi Zhu"],"pdf_url":"https://arxiv.org/pdf/2312.07853v1.pdf","comment":"Accepted by AAAI-2024"},{"id":"http://arxiv.org/abs/2312.07849v1","updated":"2023-12-13T02:35:02Z","published":"2023-12-13T02:35:02Z","title":"Encoder-minimal and Decoder-minimal Framework for Remote Sensing Image\n  Dehazing","summary":"  Haze obscures remote sensing images, hindering valuable information\nextraction. To this end, we propose RSHazeNet, an encoder-minimal and\ndecoder-minimal framework for efficient remote sensing image dehazing.\nSpecifically, regarding the process of merging features within the same level,\nwe develop an innovative module called intra-level transposed fusion module\n(ITFM). This module employs adaptive transposed self-attention to capture\ncomprehensive context-aware information, facilitating the robust context-aware\nfeature fusion. Meanwhile, we present a cross-level multi-view interaction\nmodule (CMIM) to enable effective interactions between features from various\nlevels, mitigating the loss of information due to the repeated sampling\noperations. In addition, we propose a multi-view progressive extraction block\n(MPEB) that partitions the features into four distinct components and employs\nconvolution with varying kernel sizes, groups, and dilation factors to\nfacilitate view-progressive feature learning. Extensive experiments demonstrate\nthe superiority of our proposed RSHazeNet. We release the source code and all\npre-trained models at \\url{https://github.com/chdwyb/RSHazeNet}.\n","authors":["Yuanbo Wen","Tao Gao","Ziqi Li","Jing Zhang","Ting Chen"],"pdf_url":"https://arxiv.org/pdf/2312.07849v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07835v1","updated":"2023-12-13T01:57:11Z","published":"2023-12-13T01:57:11Z","title":"Video Dynamics Prior: An Internal Learning Approach for Robust Video\n  Enhancements","summary":"  In this paper, we present a novel robust framework for low-level vision\ntasks, including denoising, object removal, frame interpolation, and\nsuper-resolution, that does not require any external training data corpus. Our\nproposed approach directly learns the weights of neural modules by optimizing\nover the corrupted test sequence, leveraging the spatio-temporal coherence and\ninternal statistics of videos. Furthermore, we introduce a novel spatial\npyramid loss that leverages the property of spatio-temporal patch recurrence in\na video across the different scales of the video. This loss enhances robustness\nto unstructured noise in both the spatial and temporal domains. This further\nresults in our framework being highly robust to degradation in input frames and\nyields state-of-the-art results on downstream tasks such as denoising, object\nremoval, and frame interpolation. To validate the effectiveness of our\napproach, we conduct qualitative and quantitative evaluations on standard video\ndatasets such as DAVIS, UCF-101, and VIMEO90K-T.\n","authors":["Gaurav Shrivastava","Ser-Nam Lim","Abhinav Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2312.07835v1.pdf","comment":"NeurIPS 2023; Webpage - http://www.cs.umd.edu/~gauravsh/vdp.html"},{"id":"http://arxiv.org/abs/2312.02142v2","updated":"2023-12-13T01:56:59Z","published":"2023-12-04T18:58:40Z","title":"Object Recognition as Next Token Prediction","summary":"  We present an approach to pose object recognition as next token prediction.\nThe idea is to apply a language decoder that auto-regressively predicts the\ntext tokens from image embeddings to form labels. To ground this prediction\nprocess in auto-regression, we customize a non-causal attention mask for the\ndecoder, incorporating two key features: modeling tokens from different labels\nto be independent, and treating image tokens as a prefix. This masking\nmechanism inspires an efficient method - one-shot sampling - to simultaneously\nsample tokens of multiple labels in parallel and rank generated labels by their\nprobabilities during inference. To further enhance the efficiency, we propose a\nsimple strategy to construct a compact decoder by simply discarding the\nintermediate blocks of a pretrained language model. This approach yields a\ndecoder that matches the full model's performance while being notably more\nefficient. The code is available at https://github.com/kaiyuyue/nxtp\n","authors":["Kaiyu Yue","Bor-Chun Chen","Jonas Geiping","Hengduo Li","Tom Goldstein","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2312.02142v2.pdf","comment":"update the intro fig and refs; auto-regression for recognition"},{"id":"http://arxiv.org/abs/2308.03977v2","updated":"2023-12-13T01:44:58Z","published":"2023-08-08T01:33:13Z","title":"PUG: Photorealistic and Semantically Controllable Synthetic Data for\n  Representation Learning","summary":"  Synthetic image datasets offer unmatched advantages for designing and\nevaluating deep neural networks: they make it possible to (i) render as many\ndata samples as needed, (ii) precisely control each scene and yield granular\nground truth labels (and captions), (iii) precisely control distribution shifts\nbetween training and testing to isolate variables of interest for sound\nexperimentation. Despite such promise, the use of synthetic image data is still\nlimited -- and often played down -- mainly due to their lack of realism. Most\nworks therefore rely on datasets of real images, which have often been scraped\nfrom public images on the internet, and may have issues with regards to\nprivacy, bias, and copyright, while offering little control over how objects\nprecisely appear. In this work, we present a path to democratize the use of\nphotorealistic synthetic data: we develop a new generation of interactive\nenvironments for representation learning research, that offer both\ncontrollability and realism. We use the Unreal Engine, a powerful game engine\nwell known in the entertainment industry, to produce PUG (Photorealistic Unreal\nGraphics) environments and datasets for representation learning. In this paper,\nwe demonstrate the potential of PUG to enable more rigorous evaluations of\nvision models.\n","authors":["Florian Bordes","Shashank Shekhar","Mark Ibrahim","Diane Bouchacourt","Pascal Vincent","Ari S. Morcos"],"pdf_url":"https://arxiv.org/pdf/2308.03977v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07833v1","updated":"2023-12-13T01:40:21Z","published":"2023-12-13T01:40:21Z","title":"Stable Rivers: A Case Study in the Application of Text-to-Image\n  Generative Models for Earth Sciences","summary":"  Text-to-image (TTI) generative models can be used to generate photorealistic\nimages from a given text-string input. These models offer great potential to\nmitigate challenges to the uptake of machine learning in the earth sciences.\nHowever, the rapid increase in their use has raised questions about fairness\nand biases, with most research to-date focusing on social and cultural areas\nrather than domain-specific considerations. We conducted a case study for the\nearth sciences, focusing on the field of fluvial geomorphology, where we\nevaluated subject-area specific biases in the training data and downstream\nmodel performance of Stable Diffusion (v1.5). In addition to perpetuating\nWestern biases, we found that the training data over-represented scenic\nlocations, such as famous rivers and waterfalls, and showed serious under- and\nover-representation of many morphological and environmental terms. Despite\nbiased training data, we found that with careful prompting, the Stable\nDiffusion model was able to generate photorealistic synthetic river images\nreproducing many important environmental and morphological characteristics.\nFurthermore, conditional control techniques, such as the use of condition maps\nwith ControlNet were effective for providing additional constraints on output\nimages. Despite great potential for the use of TTI models in the earth sciences\nfield, we advocate for caution in sensitive applications, and advocate for\ndomain-specific reviews of training data and image generation biases to\nmitigate perpetuation of existing biases.\n","authors":["C Kupferschmidt","A. D. Binns","K. L. Kupferschmidt","G. W Taylor"],"pdf_url":"https://arxiv.org/pdf/2312.07833v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07823v1","updated":"2023-12-13T01:16:50Z","published":"2023-12-13T01:16:50Z","title":"Semantic-Lens: Instance-Centric Semantic Alignment for Video\n  Super-Resolution","summary":"  As a critical clue of video super-resolution (VSR), inter-frame alignment\nsignificantly impacts overall performance. However, accurate pixel-level\nalignment is a challenging task due to the intricate motion interweaving in the\nvideo. In response to this issue, we introduce a novel paradigm for VSR named\n\\textbf{Semantic Lens}, predicated on semantic priors drawn from degraded\nvideos. Specifically, video is modeled as instances, events, and scenes via a\nSemantic Extractor. Those semantics assist the Pixel Enhancer in understanding\nthe recovered contents and generating more realistic visual results. The\ndistilled global semantics embody the scene information of each frame, while\nthe instance-specific semantics assemble the spatial-temporal contexts related\nto each instance. Furthermore, we devise a \\textbf{S}emantics-\\textbf{P}owered\n\\textbf{A}ttention \\textbf{C}ross-\\textbf{E}mbedding (SPACE) block to bridge\nthe pixel-level features with semantic knowledge, composed of a \\textbf{G}lobal\n\\textbf{P}erspective \\textbf{S}hifter (GPS) and an \\textbf{I}nstance-Specific\n\\textbf{S}emantic \\textbf{E}mbedding \\textbf{E}ncoder (ISEE). Concretely, the\nGPS module generates pairs of affine transformation parameters for pixel-level\nfeature modulation conditioned on global semantics. After that, the ISEE module\nharnesses the attention mechanism to align the adjacent frames in the\ninstance-centric semantic space. In addition, we incorporate a simple yet\neffective pre-alignment module to alleviate the difficulty of model training.\nExtensive experiments demonstrate the superiority of our model over existing\nstate-of-the-art VSR methods.\n","authors":["Qi Tang","Yao Zhao","Meiqin Liu","Jian Jin","Chao Yao"],"pdf_url":"https://arxiv.org/pdf/2312.07823v1.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2312.07814v1","updated":"2023-12-13T00:24:37Z","published":"2023-12-13T00:24:37Z","title":"A Foundational Multimodal Vision Language AI Assistant for Human\n  Pathology","summary":"  The field of computational pathology has witnessed remarkable progress in the\ndevelopment of both task-specific predictive models and task-agnostic\nself-supervised vision encoders. However, despite the explosive growth of\ngenerative artificial intelligence (AI), there has been limited study on\nbuilding general purpose, multimodal AI assistants tailored to pathology. Here\nwe present PathChat, a vision-language generalist AI assistant for human\npathology using an in-house developed foundational vision encoder pretrained on\n100 million histology images from over 100,000 patient cases and 1.18 million\npathology image-caption pairs. The vision encoder is then combined with a\npretrained large language model and the whole system is finetuned on over\n250,000 diverse disease agnostic visual language instructions. We compare\nPathChat against several multimodal vision language AI assistants as well as\nGPT4V, which powers the commercially available multimodal general purpose AI\nassistant ChatGPT-4. When relevant clinical context is provided with the\nhistology image, PathChat achieved a diagnostic accuracy of 87% on\nmultiple-choice questions based on publicly available cases of diverse tissue\norigins and disease models. Additionally, using open-ended questions and human\nexpert evaluation, we found that overall PathChat produced more accurate and\npathologist-preferable responses to diverse queries related to pathology. As an\ninteractive and general vision language AI assistant that can flexibly handle\nboth visual and natural language inputs, PathChat can potentially find\nimpactful applications in pathology education, research, and human-in-the-loop\nclinical decision making.\n","authors":["Ming Y. Lu","Bowen Chen","Drew F. K. Williamson","Richard J. Chen","Kenji Ikamura","Georg Gerber","Ivy Liang","Long Phi Le","Tong Ding","Anil V Parwani","Faisal Mahmood"],"pdf_url":"https://arxiv.org/pdf/2312.07814v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03999v2","updated":"2023-12-13T00:06:18Z","published":"2023-09-07T20:05:39Z","title":"Adapting Self-Supervised Representations to Multi-Domain Setups","summary":"  Current state-of-the-art self-supervised approaches, are effective when\ntrained on individual domains but show limited generalization on unseen\ndomains. We observe that these models poorly generalize even when trained on a\nmixture of domains, making them unsuitable to be deployed under diverse\nreal-world setups. We therefore propose a general-purpose, lightweight Domain\nDisentanglement Module (DDM) that can be plugged into any self-supervised\nencoder to effectively perform representation learning on multiple, diverse\ndomains with or without shared classes. During pre-training according to a\nself-supervised loss, DDM enforces a disentanglement in the representation\nspace by splitting it into a domain-variant and a domain-invariant portion.\nWhen domain labels are not available, DDM uses a robust clustering approach to\ndiscover pseudo-domains. We show that pre-training with DDM can show up to 3.5%\nimprovement in linear probing accuracy on state-of-the-art self-supervised\nmodels including SimCLR, MoCo, BYOL, DINO, SimSiam and Barlow Twins on\nmulti-domain benchmarks including PACS, DomainNet and WILDS. Models trained\nwith DDM show significantly improved generalization (7.4%) to unseen domains\ncompared to baselines. Therefore, DDM can efficiently adapt self-supervised\nencoders to provide high-quality, generalizable representations for diverse\nmulti-domain data.\n","authors":["Neha Kalibhat","Sam Sharpe","Jeremy Goodsitt","Bayan Bruss","Soheil Feizi"],"pdf_url":"https://arxiv.org/pdf/2309.03999v2.pdf","comment":"Published at BMVC 2023"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2311.01314v2","updated":"2023-12-13T14:31:27Z","published":"2023-11-02T15:31:12Z","title":"Recommendations by Concise User Profiles from Review Text","summary":"  Recommender systems are most successful for popular items and users with\nample interactions (likes, ratings etc.). This work addresses the difficult and\nunderexplored case of supporting users who have very sparse interactions but\npost informative review texts. Our experimental studies address two book\ncommunities with these characteristics. We design a framework with\nTransformer-based representation learning, covering user-item interactions,\nitem content, and user-provided reviews. To overcome interaction sparseness, we\ndevise techniques for selecting the most informative cues to construct concise\nuser profiles. Comprehensive experiments, with datasets from Amazon and\nGoodreads, show that judicious selection of text snippets achieves the best\nperformance, even in comparison to LLM-generated rankings and to using LLMs to\ngenerate user profiles.\n","authors":["Ghazaleh Haratinezhad Torbati","Anna Tigunova","Andrew Yates","Gerhard Weikum"],"pdf_url":"https://arxiv.org/pdf/2311.01314v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07160v2","updated":"2023-12-13T11:29:07Z","published":"2023-12-12T10:55:34Z","title":"Audience Prospecting for Dynamic-Product-Ads in Native Advertising","summary":"  With yearly revenue exceeding one billion USD, Yahoo Gemini native\nadvertising marketplace serves more than two billion impressions daily to\nhundreds of millions of unique users. One of the fastest growing segments of\nGemini native is dynamic-product-ads (DPA), where major advertisers, such as\nAmazon and Walmart, provide catalogs with millions of products for the system\nto choose from and present to users. The subject of this work is finding and\nexpanding the right audience for each DPA ad, which is one of the many\nchallenges DPA presents. Approaches such as targeting various user groups,\ne.g., users who already visited the advertisers' websites (Retargeting), users\nthat searched for certain products (Search-Prospecting), or users that reside\nin preferred locations (Location-Prospecting), have limited audience expansion\ncapabilities. In this work we present two new approaches for audience expansion\nthat also maintain predefined performance goals. The Conversion-Prospecting\napproach predicts DPA conversion rates based on Gemini native logged data, and\ncalculates the expected cost-per-action (CPA) for determining users'\neligibility to products and optimizing DPA bids in Gemini native auctions. To\nsupport new advertisers and products, the Trending-Prospecting approach matches\ntrending products to users by learning their tendency towards products from\nadvertisers' sites logged events. The tendency scores indicate the popularity\nof the product and the similarity of the user to those who have previously\nengaged with this product. The two new prospecting approaches were tested\nonline, serving real Gemini native traffic, demonstrating impressive DPA\ndelivery and DPA revenue lifts while maintaining most traffic within the\nacceptable CPA range (i.e., performance goal). After a successful testing\nphase, the proposed approaches are currently in production and serve all Gemini\nnative traffic.\n","authors":["Eliran Abutbul","Yohay Kaplan","Naama Krasne","Oren Somekh","Or David","Omer Duvdevany","Evgeny Segal"],"pdf_url":"https://arxiv.org/pdf/2312.07160v2.pdf","comment":"In Proc. IeeeBigData'2023 (Industry and Government Program)"},{"id":"http://arxiv.org/abs/2312.08041v1","updated":"2023-12-13T10:40:15Z","published":"2023-12-13T10:40:15Z","title":"Leveraging User Simulation to Develop and Evaluate Conversational\n  Information Access Agents","summary":"  We observe a change in the way users access information, that is, the rise of\nconversational information access (CIA) agents. However, the automatic\nevaluation of these agents remains an open challenge. Moreover, the training of\nCIA agents is cumbersome as it mostly relies on conversational corpora, expert\nknowledge, and reinforcement learning. User simulation has been identified as a\npromising solution to tackle automatic evaluation and has been previously used\nin reinforcement learning. In this research, we investigate how user simulation\ncan be leveraged in the context of CIA. We organize the work in three parts. We\nbegin with the identification of requirements for user simulators for training\nand evaluating CIA agents and compare existing types of simulator regarding\nthese. Then, we plan to combine these different types of simulators into a new\nhybrid simulator. Finally, we aim to extend simulators to handle more complex\ninformation seeking scenarios.\n","authors":["Nolwenn Bernard"],"pdf_url":"https://arxiv.org/pdf/2312.08041v1.pdf","comment":"Proceedings of the 17th ACM International Conference on Web Search\n  and Data Mining (WSDM '24), 2024"},{"id":"http://arxiv.org/abs/2312.08021v1","updated":"2023-12-13T09:49:53Z","published":"2023-12-13T09:49:53Z","title":"Improving search relevance of Azure Cognitive Search by Bayesian\n  optimization","summary":"  Azure Cognitive Search (ACS) has emerged as a major contender in \"Search as a\nService\" cloud products in recent years. However, one of the major challenges\nfor ACS users is to improve the relevance of the search results for their\nspecific usecases. In this paper, we propose a novel method to find the optimal\nACS configuration that maximizes search relevance for a specific usecase\n(product search, document search...) The proposed solution improves key online\nmarketplace metrics such as click through rates (CTR) by formulating the search\nrelevance problem as hyperparameter tuning. We have observed significant\nimprovements in real-world search call to action (CTA) rate in multiple\nmarketplaces by introducing optimized weights generated from the proposed\napproach.\n","authors":["Nitin Agarwal","Ashish Kumar","Kiran R","Manish Gupta","Laurent Boué"],"pdf_url":"https://arxiv.org/pdf/2312.08021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07885v1","updated":"2023-12-13T04:07:29Z","published":"2023-12-13T04:07:29Z","title":"RAT: Reinforcement-Learning-Driven and Adaptive Testing for\n  Vulnerability Discovery in Web Application Firewalls","summary":"  Due to the increasing sophistication of web attacks, Web Application\nFirewalls (WAFs) have to be tested and updated regularly to resist the\nrelentless flow of web attacks. In practice, using a brute-force attack to\ndiscover vulnerabilities is infeasible due to the wide variety of attack\npatterns. Thus, various black-box testing techniques have been proposed in the\nliterature. However, these techniques suffer from low efficiency. This paper\npresents Reinforcement-Learning-Driven and Adaptive Testing (RAT), an automated\nblack-box testing strategy to discover injection vulnerabilities in WAFs. In\nparticular, we focus on SQL injection and Cross-site Scripting, which have been\namong the top ten vulnerabilities over the past decade. More specifically, RAT\nclusters similar attack samples together. It then utilizes a reinforcement\nlearning technique combined with a novel adaptive search algorithm to discover\nalmost all bypassing attack patterns efficiently. We compare RAT with three\nstate-of-the-art methods considering their objectives. The experiments show\nthat RAT performs 33.53% and 63.16% on average better than its counterparts in\ndiscovering the most possible bypassing payloads and reducing the number of\nattempts before finding the first bypassing payload when testing\nwell-configured WAFs, respectively.\n","authors":["Mohammadhossein Amouei","Mohsen Rezvani","Mansoor Fateh"],"pdf_url":"https://arxiv.org/pdf/2312.07885v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07855v1","updated":"2023-12-13T02:48:35Z","published":"2023-12-13T02:48:35Z","title":"Exploring Popularity Bias in Session-based Recommendation","summary":"  Existing work has revealed that large-scale offline evaluation of recommender\nsystems for user-item interactions is prone to bias caused by the deployed\nsystem itself, as a form of closed loop feedback. Many adopt the\n\\textit{propensity} concept to analyze or mitigate this empirical issue. In\nthis work, we extend the analysis to session-based setup and adapted propensity\ncalculation to the unique characteristics of session-based recommendation\ntasks. Our experiments incorporate neural models and KNN-based models, and\ncover both the music and the e-commerce domain. We study the distributions of\npropensity and different stratification techniques on different datasets and\nfind that propensity-related traits are actually dataset-specific. We then\nleverage the effect of stratification and achieve promising results compared to\nthe original models.\n","authors":["Haowen Wang"],"pdf_url":"https://arxiv.org/pdf/2312.07855v1.pdf","comment":"10pages, 9 figures"},{"id":"http://arxiv.org/abs/2310.03175v2","updated":"2023-12-13T22:57:24Z","published":"2023-10-04T21:43:16Z","title":"Impedance Leakage Vulnerability and its Utilization in\n  Reverse-engineering Embedded Software","summary":"  Discovering new vulnerabilities and implementing security and privacy\nmeasures are important to protect systems and data against physical attacks.\nOne such vulnerability is impedance, an inherent property of a device that can\nbe exploited to leak information through an unintended side channel, thereby\nposing significant security and privacy risks. Unlike traditional\nvulnerabilities, impedance is often overlooked or narrowly explored, as it is\ntypically treated as a fixed value at a specific frequency in research and\ndesign endeavors. Moreover, impedance has never been explored as a source of\ninformation leakage. This paper demonstrates that the impedance of an embedded\ndevice is not constant and directly relates to the programs executed on the\ndevice. We define this phenomenon as impedance leakage and use this as a side\nchannel to extract software instructions from protected memory. Our experiment\non the ATmega328P microcontroller and the Artix 7 FPGA indicates that the\nimpedance side channel can detect software instructions with 96.1% and 92.6%\naccuracy, respectively. Furthermore, we explore the dual nature of the\nimpedance side channel, highlighting the potential for beneficial purposes and\nthe associated risk of intellectual property theft. Finally, potential\ncountermeasures that specifically address impedance leakage are discussed.\n","authors":["Md Sadik Awal","Md Tauhidur Rahman"],"pdf_url":"https://arxiv.org/pdf/2310.03175v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10091v1","updated":"2023-12-13T18:36:43Z","published":"2023-12-13T18:36:43Z","title":"Look Before You Leap: A Universal Emergent Decomposition of Retrieval\n  Tasks in Language Models","summary":"  When solving challenging problems, language models (LMs) are able to identify\nrelevant information from long and complicated contexts. To study how LMs solve\nretrieval tasks in diverse situations, we introduce ORION, a collection of\nstructured retrieval tasks spanning six domains, from text understanding to\ncoding. Each task in ORION can be represented abstractly by a request (e.g. a\nquestion) that retrieves an attribute (e.g. the character name) from a context\n(e.g. a story). We apply causal analysis on 18 open-source language models with\nsizes ranging from 125 million to 70 billion parameters. We find that LMs\ninternally decompose retrieval tasks in a modular way: middle layers at the\nlast token position process the request, while late layers retrieve the correct\nentity from the context. After causally enforcing this decomposition, models\nare still able to solve the original task, preserving 70% of the original\ncorrect token probability in 98 of the 106 studied model-task pairs. We connect\nour macroscopic decomposition with a microscopic description by performing a\nfine-grained case study of a question-answering task on Pythia-2.8b. Building\non our high-level understanding, we demonstrate a proof of concept application\nfor scalable internal oversight of LMs to mitigate prompt-injection while\nrequiring human supervision on only a single input. Our solution improves\naccuracy drastically (from 15.5% to 97.5% on Pythia-12b). This work presents\nevidence of a universal emergent modular processing of tasks across varied\ndomains and models and is a pioneering effort in applying interpretability for\nscalable internal oversight of LMs.\n","authors":["Alexandre Variengien","Eric Winsor"],"pdf_url":"https://arxiv.org/pdf/2312.10091v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10089v1","updated":"2023-12-13T11:07:32Z","published":"2023-12-13T11:07:32Z","title":"Advancements in Content-Based Image Retrieval: A Comprehensive Survey of\n  Relevance Feedback Techniques","summary":"  Content-based image retrieval (CBIR) systems have emerged as crucial tools in\nthe field of computer vision, allowing for image search based on visual content\nrather than relying solely on metadata. This survey paper presents a\ncomprehensive overview of CBIR, emphasizing its role in object detection and\nits potential to identify and retrieve visually similar images based on content\nfeatures. Challenges faced by CBIR systems, including the semantic gap and\nscalability, are discussed, along with potential solutions. It elaborates on\nthe semantic gap, which arises from the disparity between low-level features\nand high-level semantic concepts, and explores approaches to bridge this gap.\nOne notable solution is the integration of relevance feedback (RF), empowering\nusers to provide feedback on retrieved images and refine search results\niteratively. The survey encompasses long-term and short-term learning\napproaches that leverage RF for enhanced CBIR accuracy and relevance. These\nmethods focus on weight optimization and the utilization of active learning\nalgorithms to select samples for training classifiers. Furthermore, the paper\ninvestigates machine learning techniques and the utilization of deep learning\nand convolutional neural networks to enhance CBIR performance. This survey\npaper plays a significant role in advancing the understanding of CBIR and RF\ntechniques. It guides researchers and practitioners in comprehending existing\nmethodologies, challenges, and potential solutions while fostering knowledge\ndissemination and identifying research gaps. By addressing future research\ndirections, it sets the stage for advancements in CBIR that will enhance\nretrieval accuracy, usability, and effectiveness in various application\ndomains.\n","authors":["Hamed Qazanfari","Mohammad M. AlyanNezhadi","Zohreh Nozari Khoshdaregi"],"pdf_url":"https://arxiv.org/pdf/2312.10089v1.pdf","comment":"7 pages, 2 figures"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2312.08369v1","updated":"2023-12-13T18:58:56Z","published":"2023-12-13T18:58:56Z","title":"The Effective Horizon Explains Deep RL Performance in Stochastic\n  Environments","summary":"  Reinforcement learning (RL) theory has largely focused on proving minimax\nsample complexity bounds. These require strategic exploration algorithms that\nuse relatively limited function classes for representing the policy or value\nfunction. Our goal is to explain why deep RL algorithms often perform well in\npractice, despite using random exploration and much more expressive function\nclasses like neural networks. Our work arrives at an explanation by showing\nthat many stochastic MDPs can be solved by performing only a few steps of value\niteration on the random policy's Q function and then acting greedily. When this\nis true, we find that it is possible to separate the exploration and learning\ncomponents of RL, making it much easier to analyze. We introduce a new RL\nalgorithm, SQIRL, that iteratively learns a near-optimal policy by exploring\nrandomly to collect rollouts and then performing a limited number of steps of\nfitted-Q iteration over those rollouts. Any regression algorithm that satisfies\nbasic in-distribution generalization properties can be used in SQIRL to\nefficiently solve common MDPs. This can explain why deep RL works neural\nnetworks, since it is empirically established that neural networks generalize\nwell in-distribution. Furthermore, SQIRL explains why random exploration works\nwell in practice, since we show many environments can be solved by estimating\nthe random policy's Q-function and then applying zero or a few steps of value\niteration. We leverage SQIRL to derive instance-dependent sample complexity\nbounds for RL that are exponential only in an \"effective horizon\" of lookahead\nand on the complexity of the class used for function approximation.\nEmpirically, we also find that SQIRL performance strongly correlates with PPO\nand DQN performance in a variety of stochastic environments, supporting that\nour theoretical analysis is predictive of practical performance.\n","authors":["Cassidy Laidlaw","Banghua Zhu","Stuart Russell","Anca Dragan"],"pdf_url":"https://arxiv.org/pdf/2312.08369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08365v1","updated":"2023-12-13T18:57:23Z","published":"2023-12-13T18:57:23Z","title":"An Invitation to Deep Reinforcement Learning","summary":"  Training a deep neural network to maximize a target objective has become the\nstandard recipe for successful machine learning over the last decade. These\nnetworks can be optimized with supervised learning, if the target objective is\ndifferentiable. For many interesting problems, this is however not the case.\nCommon objectives like intersection over union (IoU), bilingual evaluation\nunderstudy (BLEU) score or rewards cannot be optimized with supervised\nlearning. A common workaround is to define differentiable surrogate losses,\nleading to suboptimal solutions with respect to the actual objective.\nReinforcement learning (RL) has emerged as a promising alternative for\noptimizing deep neural networks to maximize non-differentiable objectives in\nrecent years. Examples include aligning large language models via human\nfeedback, code generation, object detection or control problems. This makes RL\ntechniques relevant to the larger machine learning audience. The subject is,\nhowever, time intensive to approach due to the large range of methods, as well\nas the often very theoretical presentation. In this introduction, we take an\nalternative approach, different from classic reinforcement learning textbooks.\nRather than focusing on tabular problems, we introduce reinforcement learning\nas a generalization of supervised learning, which we first apply to\nnon-differentiable objectives and later to temporal problems. Assuming only\nbasic knowledge of supervised learning, the reader will be able to understand\nstate-of-the-art deep RL algorithms like proximal policy optimization (PPO)\nafter reading this tutorial.\n","authors":["Bernhard Jaeger","Andreas Geiger"],"pdf_url":"https://arxiv.org/pdf/2312.08365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08361v1","updated":"2023-12-13T18:52:49Z","published":"2023-12-13T18:52:49Z","title":"Distributed Inference and Fine-tuning of Large Language Models Over The\n  Internet","summary":"  Large language models (LLMs) are useful in many NLP tasks and become more\ncapable with size, with the best open-source models having over 50 billion\nparameters. However, using these 50B+ models requires high-end hardware, making\nthem inaccessible to most researchers. In this work, we investigate methods for\ncost-efficient inference and fine-tuning of LLMs, comparing local and\ndistributed strategies. We observe that a large enough model (50B+) can run\nefficiently even on geodistributed devices in a consumer-grade network. This\ncould allow running LLM efficiently by pooling together idle compute resources\nof multiple research groups and volunteers. We address two open problems: (1)\nhow to perform inference and fine-tuning reliably if any device can disconnect\nabruptly and (2) how to partition LLMs between devices with uneven hardware,\njoining and leaving at will. In order to do that, we develop special\nfault-tolerant inference algorithms and load-balancing protocols that\nautomatically assign devices to maximize the total system throughput. We\nshowcase these algorithms in Petals - a decentralized system that runs Llama 2\n(70B) and BLOOM (176B) over the Internet up to 10x faster than offloading for\ninteractive generation. We evaluate the performance of our system in simulated\nconditions and a real-world setup spanning two continents.\n","authors":["Alexander Borzunov","Max Ryabinin","Artem Chumachenko","Dmitry Baranchuk","Tim Dettmers","Younes Belkada","Pavel Samygin","Colin Raffel"],"pdf_url":"https://arxiv.org/pdf/2312.08361v1.pdf","comment":"Accepted to Conference on Neural Information Processing Systems\n  (NeurIPS) 2023. 20 pages, 3 figures"},{"id":"http://arxiv.org/abs/2312.08358v1","updated":"2023-12-13T18:51:34Z","published":"2023-12-13T18:51:34Z","title":"Distributional Preference Learning: Understanding and Accounting for\n  Hidden Context in RLHF","summary":"  In practice, preference learning from human feedback depends on incomplete\ndata with hidden context. Hidden context refers to data that affects the\nfeedback received, but which is not represented in the data used to train a\npreference model. This captures common issues of data collection, such as\nhaving human annotators with varied preferences, cognitive processes that\nresult in seemingly irrational behavior, and combining data labeled according\nto different criteria. We prove that standard applications of preference\nlearning, including reinforcement learning from human feedback (RLHF),\nimplicitly aggregate over hidden contexts according to a well-known voting rule\ncalled Borda count. We show this can produce counter-intuitive results that are\nvery different from other methods which implicitly aggregate via expected\nutility. Furthermore, our analysis formalizes the way that preference learning\nfrom users with diverse values tacitly implements a social choice function. A\nkey implication of this result is that annotators have an incentive to\nmisreport their preferences in order to influence the learned model, leading to\nvulnerabilities in the deployment of RLHF. As a step towards mitigating these\nproblems, we introduce a class of methods called distributional preference\nlearning (DPL). DPL methods estimate a distribution of possible score values\nfor each alternative in order to better account for hidden context.\nExperimental results indicate that applying DPL to RLHF for LLM chatbots\nidentifies hidden context in the data and significantly reduces subsequent\njailbreak vulnerability. Our code and data are available at\nhttps://github.com/cassidylaidlaw/hidden-context\n","authors":["Anand Siththaranjan","Cassidy Laidlaw","Dylan Hadfield-Menell"],"pdf_url":"https://arxiv.org/pdf/2312.08358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18290v2","updated":"2023-12-13T18:48:48Z","published":"2023-05-29T17:57:46Z","title":"Direct Preference Optimization: Your Language Model is Secretly a Reward\n  Model","summary":"  While large-scale unsupervised language models (LMs) learn broad world\nknowledge and some reasoning skills, achieving precise control of their\nbehavior is difficult due to the completely unsupervised nature of their\ntraining. Existing methods for gaining such steerability collect human labels\nof the relative quality of model generations and fine-tune the unsupervised LM\nto align with these preferences, often with reinforcement learning from human\nfeedback (RLHF). However, RLHF is a complex and often unstable procedure, first\nfitting a reward model that reflects the human preferences, and then\nfine-tuning the large unsupervised LM using reinforcement learning to maximize\nthis estimated reward without drifting too far from the original model. In this\npaper we introduce a new parameterization of the reward model in RLHF that\nenables extraction of the corresponding optimal policy in closed form, allowing\nus to solve the standard RLHF problem with only a simple classification loss.\nThe resulting algorithm, which we call Direct Preference Optimization (DPO), is\nstable, performant, and computationally lightweight, eliminating the need for\nsampling from the LM during fine-tuning or performing significant\nhyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align\nwith human preferences as well as or better than existing methods. Notably,\nfine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of\ngenerations, and matches or improves response quality in summarization and\nsingle-turn dialogue while being substantially simpler to implement and train.\n","authors":["Rafael Rafailov","Archit Sharma","Eric Mitchell","Stefano Ermon","Christopher D. Manning","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2305.18290v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.01226v2","updated":"2023-12-13T18:42:58Z","published":"2023-09-03T17:19:11Z","title":"Saturn: An Optimized Data System for Large Model Deep Learning Workloads","summary":"  Large language models such as GPT-3 & ChatGPT have transformed deep learning\n(DL), powering applications that have captured the public's imagination. These\nmodels are rapidly being adopted across domains for analytics on various\nmodalities, often by finetuning pre-trained base models. Such models need\nmultiple GPUs due to both their size and computational load, driving the\ndevelopment of a bevy of \"model parallelism\" techniques & tools. Navigating\nsuch parallelism choices, however, is a new burden for end users of DL such as\ndata scientists, domain scientists, etc. who may lack the necessary systems\nknowhow. The need for model selection, which leads to many models to train due\nto hyper-parameter tuning or layer-wise finetuning, compounds the situation\nwith two more burdens: resource apportioning and scheduling. In this work, we\ntackle these three burdens for DL users in a unified manner by formalizing them\nas a joint problem that we call SPASE: Select a Parallelism, Allocate\nresources, and SchedulE. We propose a new information system architecture to\ntackle the SPASE problem holistically, representing a key step toward enabling\nwider adoption of large DL models. We devise an extensible template for\nexisting parallelism schemes and combine it with an automated empirical\nprofiler for runtime estimation. We then formulate SPASE as an MILP.\n  We find that direct use of an MILP-solver is significantly more effective\nthan several baseline heuristics. We optimize the system runtime further with\nan introspective scheduling approach. We implement all these techniques into a\nnew data system we call Saturn. Experiments with benchmark DL workloads show\nthat Saturn achieves 39-49% lower model selection runtimes than typical current\nDL practice.\n","authors":["Kabir Nagrecha","Arun Kumar"],"pdf_url":"https://arxiv.org/pdf/2309.01226v2.pdf","comment":"Accepted at VLDB '24. Code available:\n  https://github.com/knagrecha/saturn. 12 pages + 3 pages references + 2 pages\n  appendix"},{"id":"http://arxiv.org/abs/2309.01922v2","updated":"2023-12-13T18:41:44Z","published":"2023-09-05T03:22:46Z","title":"Regret Analysis of Policy Gradient Algorithm for Infinite Horizon\n  Average Reward Markov Decision Processes","summary":"  In this paper, we consider an infinite horizon average reward Markov Decision\nProcess (MDP). Distinguishing itself from existing works within this context,\nour approach harnesses the power of the general policy gradient-based\nalgorithm, liberating it from the constraints of assuming a linear MDP\nstructure. We propose a policy gradient-based algorithm and show its global\nconvergence property. We then prove that the proposed algorithm has\n$\\tilde{\\mathcal{O}}({T}^{3/4})$ regret. Remarkably, this paper marks a\npioneering effort by presenting the first exploration into regret-bound\ncomputation for the general parameterized policy gradient algorithm in the\ncontext of average reward scenarios.\n","authors":["Qinbo Bai","Washim Uddin Mondal","Vaneet Aggarwal"],"pdf_url":"https://arxiv.org/pdf/2309.01922v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.09065v2","updated":"2023-12-13T18:01:23Z","published":"2023-08-17T15:54:11Z","title":"Discretization-Induced Dirichlet Posterior for Robust Uncertainty\n  Quantification on Regression","summary":"  Uncertainty quantification is critical for deploying deep neural networks\n(DNNs) in real-world applications. An Auxiliary Uncertainty Estimator (AuxUE)\nis one of the most effective means to estimate the uncertainty of the main task\nprediction without modifying the main task model. To be considered robust, an\nAuxUE must be capable of maintaining its performance and triggering higher\nuncertainties while encountering Out-of-Distribution (OOD) inputs, i.e., to\nprovide robust aleatoric and epistemic uncertainty. However, for vision\nregression tasks, current AuxUE designs are mainly adopted for aleatoric\nuncertainty estimates, and AuxUE robustness has not been explored. In this\nwork, we propose a generalized AuxUE scheme for more robust uncertainty\nquantification on regression tasks. Concretely, to achieve a more robust\naleatoric uncertainty estimation, different distribution assumptions are\nconsidered for heteroscedastic noise, and Laplace distribution is finally\nchosen to approximate the prediction error. For epistemic uncertainty, we\npropose a novel solution named Discretization-Induced Dirichlet pOsterior\n(DIDO), which models the Dirichlet posterior on the discretized prediction\nerror. Extensive experiments on age estimation, monocular depth estimation, and\nsuper-resolution tasks show that our proposed method can provide robust\nuncertainty estimates in the face of noisy inputs and that it can be scalable\nto both image-level and pixel-wise tasks. Code is available at\nhttps://github.com/ENSTA-U2IS/DIDO .\n","authors":["Xuanlong Yu","Gianni Franchi","Jindong Gu","Emanuel Aldea"],"pdf_url":"https://arxiv.org/pdf/2308.09065v2.pdf","comment":"23 pages with main paper and supplymentary material. Accepted at AAAI\n  2024"},{"id":"http://arxiv.org/abs/2312.08307v1","updated":"2023-12-13T17:26:54Z","published":"2023-12-13T17:26:54Z","title":"EquiReact: An equivariant neural network for chemical reactions","summary":"  Equivariant neural networks have considerably improved the accuracy and\ndata-efficiency of predictions of molecular properties. Building on this\nsuccess, we introduce EquiReact, an equivariant neural network to infer\nproperties of chemical reactions, built from three-dimensional structures of\nreactants and products. We illustrate its competitive performance on the\nprediction of activation barriers on the GDB7-22-TS, Cyclo-23-TS and\nProparg-21-TS datasets with different regimes according to the inclusion of\natom-mapping information. We show that, compared to state-of-the-art models for\nreaction property prediction, EquiReact offers: (i) a flexible model with\nreduced sensitivity between atom-mapping regimes, (ii) better extrapolation\ncapabilities to unseen chemistries, (iii) impressive prediction errors for\ndatasets exhibiting subtle variations in three-dimensional geometries of\nreactants/products, (iv) reduced sensitivity to geometry quality and (iv)\nexcellent data efficiency.\n","authors":["Puck van Gerwen","Ksenia R. Briling","Charlotte Bunne","Vignesh Ram Somnath","Ruben Laplaza","Andreas Krause","Clemence Corminboeuf"],"pdf_url":"https://arxiv.org/pdf/2312.08307v1.pdf","comment":"41 pages + SI (6 pages)"},{"id":"http://arxiv.org/abs/2304.01075v4","updated":"2023-12-13T17:23:32Z","published":"2023-04-03T15:32:38Z","title":"Conformal Prediction Regions for Time Series using Linear\n  Complementarity Programming","summary":"  Conformal prediction is a statistical tool for producing prediction regions\nof machine learning models that are valid with high probability. However,\napplying conformal prediction to time series data leads to conservative\nprediction regions. In fact, to obtain prediction regions over $T$ time steps\nwith confidence $1-\\delta$, {previous works require that each individual\nprediction region is valid} with confidence $1-\\delta/T$. We propose an\noptimization-based method for reducing this conservatism to enable long horizon\nplanning and verification when using learning-enabled time series predictors.\nInstead of considering prediction errors individually at each time step, we\nconsider a parameterized prediction error over multiple time steps. By\noptimizing the parameters over an additional dataset, we find prediction\nregions that are not conservative. We show that this problem can be cast as a\nmixed integer linear complementarity program (MILCP), which we then relax into\na linear complementarity program (LCP). Additionally, we prove that the relaxed\nLP has the same optimal cost as the original MILCP. Finally, we demonstrate the\nefficacy of our method on case studies using pedestrian trajectory predictors\nand F16 fighter jet altitude predictors.\n","authors":["Matthew Cleaveland","Insup Lee","George J. Pappas","Lars Lindemann"],"pdf_url":"https://arxiv.org/pdf/2304.01075v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08298v1","updated":"2023-12-13T17:13:08Z","published":"2023-12-13T17:13:08Z","title":"Venn: Resource Management Across Federated Learning Jobs","summary":"  In recent years, federated learning (FL) has emerged as a promising approach\nfor machine learning (ML) and data science across distributed edge devices.\nWith the increasing popularity of FL, resource contention between multiple FL\njobs training on the same device population is increasing as well. Scheduling\nedge resources among multiple FL jobs is different from GPU scheduling for\ncloud ML because of the ephemeral nature and planetary scale of participating\ndevices as well as the overlapping resource requirements of diverse FL jobs.\nExisting resource managers for FL jobs opt for random assignment of devices to\nFL jobs for simplicity and scalability, which leads to poor performance. In\nthis paper, we present Venn, an FL resource manager, that efficiently schedules\nephemeral, heterogeneous devices among many FL jobs, with the goal of reducing\ntheir average job completion time (JCT). Venn formulates the Intersection\nResource Scheduling (IRS) problem to identify complex resource contention among\nmultiple FL jobs. Then, Venn proposes a contention-aware scheduling heuristic\nto minimize the average scheduling delay. Furthermore, it proposes a\nresource-aware device-to-job matching heuristic that focuses on optimizing\nresponse collection time by mitigating stragglers. Our evaluation shows that,\ncompared to the state-of-the-art FL resource managers, Venn improves the\naverage JCT by up to 1.88X.\n","authors":["Jiachen Liu","Fan Lai","Ding Ding","Yiwen Zhang","Mosharaf Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2312.08298v1.pdf","comment":"15 pages, 15 figrues"},{"id":"http://arxiv.org/abs/2312.08295v1","updated":"2023-12-13T17:12:03Z","published":"2023-12-13T17:12:03Z","title":"Inferring Atmospheric Properties of Exoplanets with Flow Matching and\n  Neural Importance Sampling","summary":"  Atmospheric retrievals (AR) characterize exoplanets by estimating atmospheric\nparameters from observed light spectra, typically by framing the task as a\nBayesian inference problem. However, traditional approaches such as nested\nsampling are computationally expensive, thus sparking an interest in solutions\nbased on machine learning (ML). In this ongoing work, we first explore flow\nmatching posterior estimation (FMPE) as a new ML-based method for AR and find\nthat, in our case, it is more accurate than neural posterior estimation (NPE),\nbut less accurate than nested sampling. We then combine both FMPE and NPE with\nimportance sampling, in which case both methods outperform nested sampling in\nterms of accuracy and simulation efficiency. Going forward, our analysis\nsuggests that simulation-based inference with likelihood-based importance\nsampling provides a framework for accurate and efficient AR that may become a\nvaluable tool not only for the analysis of observational data from existing\ntelescopes, but also for the development of new missions and instruments.\n","authors":["Timothy D. Gebhard","Jonas Wildberger","Maximilian Dax","Daniel Angerhausen","Sascha P. Quanz","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2312.08295v1.pdf","comment":"Accepted at the \"AI to Accelerate Science and Engineering (AI2ASE)\"\n  workshop at AAAI 2024"},{"id":"http://arxiv.org/abs/2312.08290v1","updated":"2023-12-13T17:06:33Z","published":"2023-12-13T17:06:33Z","title":"PhenDiff: Revealing Invisible Phenotypes with Conditional Diffusion\n  Models","summary":"  Over the last five years, deep generative models have gradually been adopted\nfor various tasks in biological research. Notably, image-to-image translation\nmethods showed to be effective in revealing subtle phenotypic cell variations\notherwise invisible to the human eye. Current methods to achieve this goal\nmainly rely on Generative Adversarial Networks (GANs). However, these models\nare known to suffer from some shortcomings such as training instability and\nmode collapse. Furthermore, the lack of robustness to invert a real image into\nthe latent of a trained GAN prevents flexible editing of real images. In this\nwork, we propose PhenDiff, an image-to-image translation method based on\nconditional diffusion models to identify subtle phenotypes in microscopy\nimages. We evaluate this approach on biological datasets against previous work\nsuch as CycleGAN. We show that PhenDiff outperforms this baseline in terms of\nquality and diversity of the generated images. We then apply this method to\ndisplay invisible phenotypic changes triggered by a rare neurodevelopmental\ndisorder on microscopy images of organoids. Altogether, we demonstrate that\nPhenDiff is able to perform high quality biological image-to-image translation\nallowing to spot subtle phenotype variations on a real image.\n","authors":["Anis Bourou","Thomas Boyer","Kévin Daupin","Véronique Dubreuil","Aurélie De Thonel","Valérie Mezger","Auguste Genovesio"],"pdf_url":"https://arxiv.org/pdf/2312.08290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08288v1","updated":"2023-12-13T17:04:16Z","published":"2023-12-13T17:04:16Z","title":"Hybrid Sample Synthesis-based Debiasing of Classifier in Limited Data\n  Setting","summary":"  Deep learning models are known to suffer from the problem of bias, and\nresearchers have been exploring methods to address this issue. However, most of\nthese methods require prior knowledge of the bias and are not always practical.\nIn this paper, we focus on a more practical setting with no prior information\nabout the bias. Generally, in this setting, there are a large number of\nbias-aligned samples that cause the model to produce biased predictions and a\nfew bias-conflicting samples that do not conform to the bias. If the training\ndata is limited, the influence of the bias-aligned samples may become even\nstronger on the model predictions, and we experimentally demonstrate that\nexisting debiasing techniques suffer severely in such cases. In this paper, we\nexamine the effects of unknown bias in small dataset regimes and present a\nnovel approach to mitigate this issue. The proposed approach directly addresses\nthe issue of the extremely low occurrence of bias-conflicting samples in\nlimited data settings through the synthesis of hybrid samples that can be used\nto reduce the effect of bias. We perform extensive experiments on several\nbenchmark datasets and experimentally demonstrate the effectiveness of our\nproposed approach in addressing any unknown bias in the presence of limited\ndata. Specifically, our approach outperforms the vanilla, LfF, LDD, and DebiAN\ndebiasing methods by absolute margins of 10.39%, 9.08%, 8.07%, and 9.67% when\nonly 10% of the Corrupted CIFAR-10 Type 1 dataset is available with a\nbias-conflicting sample ratio of 0.05.\n","authors":["Piyush Arora","Pratik Mazumder"],"pdf_url":"https://arxiv.org/pdf/2312.08288v1.pdf","comment":"Accepted in WACV 2024"},{"id":"http://arxiv.org/abs/2312.08287v1","updated":"2023-12-13T17:04:09Z","published":"2023-12-13T17:04:09Z","title":"On the verification of Embeddings using Hybrid Markov Logic","summary":"  The standard approach to verify representations learned by Deep Neural\nNetworks is to use them in specific tasks such as classification or regression,\nand measure their performance based on accuracy in such tasks. However, in many\ncases, we would want to verify more complex properties of a learned\nrepresentation. To do this, we propose a framework based on a probabilistic\nfirst-order language, namely, Hybrid Markov Logic Networks (HMLNs) where we\nspecify properties over embeddings mixed with symbolic domain knowledge. We\npresent an approach to learn parameters for the properties within this\nframework. Further, we develop a verification method to test embeddings in this\nframework by encoding this task as a Mixed Integer Linear Program for which we\ncan leverage existing state-of-the-art solvers. We illustrate verification in\nGraph Neural Networks, Deep Knowledge Tracing and Intelligent Tutoring Systems\nto demonstrate the generality of our approach.\n","authors":["Anup Shakya","Abisha Thapa Magar","Somdeb Sarkhel","Deepak Venugopal"],"pdf_url":"https://arxiv.org/pdf/2312.08287v1.pdf","comment":"6 pages, Proceedings of 23rd IEEE International Conference on Data\n  Mining 2023 (ICDM'23)"},{"id":"http://arxiv.org/abs/2311.03426v2","updated":"2023-12-13T16:57:19Z","published":"2023-11-06T17:29:24Z","title":"GQKVA: Efficient Pre-training of Transformers by Grouping Queries, Keys,\n  and Values","summary":"  Massive transformer-based models face several challenges, including slow and\ncomputationally intensive pre-training and over-parametrization. This paper\naddresses these challenges by proposing a versatile method called GQKVA, which\ngeneralizes query, key, and value grouping techniques. GQKVA is designed to\nspeed up transformer pre-training while reducing the model size. Our\nexperiments with various GQKVA variants highlight a clear trade-off between\nperformance and model size, allowing for customized choices based on resource\nand time limitations. Our findings also indicate that the conventional\nmulti-head attention approach is not always the best choice, as there are\nlighter and faster alternatives available. We tested our method on ViT, which\nachieved an approximate 0.3% increase in accuracy while reducing the model size\nby about 4% in the task of image classification. Additionally, our most\naggressive model reduction experiment resulted in a reduction of approximately\n15% in model size, with only around a 1% drop in accuracy.\n","authors":["Farnoosh Javadi","Walid Ahmed","Habib Hajimolahoseini","Foozhan Ataiefard","Mohammad Hassanpour","Saina Asani","Austin Wen","Omar Mohamed Awad","Kangling Liu","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2311.03426v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.00736v3","updated":"2023-12-13T16:33:27Z","published":"2023-01-02T16:11:05Z","title":"Mixed moving average field guided learning for spatio-temporal data","summary":"  Influenced mixed moving average fields are a versatile modeling class for\nspatio-temporal data. However, their predictive distribution is not generally\nknown. Under this modeling assumption, we define a novel spatio-temporal\nembedding and a theory-guided machine learning approach that employs a\ngeneralized Bayesian algorithm to make ensemble forecasts. We employ Lipschitz\npredictors and determine fixed-time and any-time PAC Bayesian bounds in the\nbatch learning setting. Performing causal forecast is a highlight of our\nmethodology as its potential application to data with spatial and temporal\nshort and long-range dependence. We then test the performance of our learning\nmethodology by using linear predictors and data sets simulated from a\nspatio-temporal Ornstein-Uhlenbeck process.\n","authors":["Imma Valentina Curato","Orkun Furat","Lorenzo Proietti","Bennet Stroeh"],"pdf_url":"https://arxiv.org/pdf/2301.00736v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04925v2","updated":"2023-12-13T16:24:44Z","published":"2023-10-07T21:36:55Z","title":"Crystal-GFN: sampling crystals with desirable properties and constraints","summary":"  Accelerating material discovery holds the potential to greatly help mitigate\nthe climate crisis. Discovering new solid-state materials such as\nelectrocatalysts, super-ionic conductors or photovoltaic materials can have a\ncrucial impact, for instance, in improving the efficiency of renewable energy\nproduction and storage. In this paper, we introduce Crystal-GFN, a generative\nmodel of crystal structures that sequentially samples structural properties of\ncrystalline materials, namely the space group, composition and lattice\nparameters. This domain-inspired approach enables the flexible incorporation of\nphysical and structural hard constraints, as well as the use of any available\npredictive model of a desired physicochemical property as an objective\nfunction. To design stable materials, one must target the candidates with the\nlowest formation energy. Here, we use as objective the formation energy per\natom of a crystal structure predicted by a new proxy machine learning model\ntrained on MatBench. The results demonstrate that Crystal-GFN is able to sample\nhighly diverse crystals with low (median -3.1 eV/atom) predicted formation\nenergy.\n","authors":["Mila AI4Science","Alex Hernandez-Garcia","Alexandre Duval","Alexandra Volokhova","Yoshua Bengio","Divya Sharma","Pierre Luc Carrier","Yasmine Benabed","Michał Koziarski","Victor Schmidt"],"pdf_url":"https://arxiv.org/pdf/2310.04925v2.pdf","comment":"Main paper (10 pages) + references + appendix"},{"id":"http://arxiv.org/abs/2312.07186v2","updated":"2023-12-13T16:23:27Z","published":"2023-12-12T11:48:56Z","title":"Towards Optimal Sobolev Norm Rates for the Vector-Valued Regularized\n  Least-Squares Algorithm","summary":"  We present the first optimal rates for infinite-dimensional vector-valued\nridge regression on a continuous scale of norms that interpolate between $L_2$\nand the hypothesis space, which we consider as a vector-valued reproducing\nkernel Hilbert space. These rates allow to treat the misspecified case in which\nthe true regression function is not contained in the hypothesis space. We\ncombine standard assumptions on the capacity of the hypothesis space with a\nnovel tensor product construction of vector-valued interpolation spaces in\norder to characterize the smoothness of the regression function. Our upper\nbound not only attains the same rate as real-valued kernel ridge regression,\nbut also removes the assumption that the target regression function is bounded.\nFor the lower bound, we reduce the problem to the scalar setting using a\nprojection argument. We show that these rates are optimal in most cases and\nindependent of the dimension of the output space. We illustrate our results for\nthe special case of vector-valued Sobolev spaces.\n","authors":["Zhu Li","Dimitri Meunier","Mattes Mollenhauer","Arthur Gretton"],"pdf_url":"https://arxiv.org/pdf/2312.07186v2.pdf","comment":"Fixed typo + format Table 1. arXiv admin note: text overlap with\n  arXiv:2208.01711"},{"id":"http://arxiv.org/abs/2310.02948v2","updated":"2023-12-13T16:21:48Z","published":"2023-10-04T16:36:32Z","title":"HappyFeat -- An interactive and efficient BCI framework for clinical\n  applications","summary":"  Brain-Computer Interface (BCI) systems allow users to perform actions by\ntranslating their brain activity into commands. Such systems usually need a\ntraining phase, consisting in training a classification algorithm to\ndiscriminate between mental states using specific features from the recorded\nsignals. This phase of feature selection and training is crucial for BCI\nperformance and presents specific constraints to be met in a clinical context,\nsuch as post-stroke rehabilitation.\n  In this paper, we present HappyFeat, a software making Motor Imagery (MI)\nbased BCI experiments easier, by gathering all necessary manipulations and\nanalysis in a single convenient GUI and via automation of experiment or\nanalysis parameters. The resulting workflow allows for effortlessly selecting\nthe best features, helping to achieve good BCI performance in time-constrained\nenvironments. Alternative features based on Functional Connectivity can be used\nand compared or combined with Power Spectral Density, allowing a\nnetwork-oriented approach.\n  We then give details of HappyFeat's main mechanisms, and a review of its\nperformances in typical use cases. We also show that it can be used as an\nefficient tool for comparing different metrics extracted from the signals, to\ntrain the classification algorithm. To this end, we show a comparison between\nthe commonly-used Power Spectral Density and network metrics based on\nFunctional Connectivity.\n  HappyFeat is available as an open-source project which can be freely\ndownloaded on GitHub.\n","authors":["Arthur Desbois","Tristan Venot","Fabrizio De Vico Fallani","Marie-Constance Corsi"],"pdf_url":"https://arxiv.org/pdf/2310.02948v2.pdf","comment":"17 pages, 5 figures, 1 table, \"Annex\" section"},{"id":"http://arxiv.org/abs/2312.08257v1","updated":"2023-12-13T16:19:58Z","published":"2023-12-13T16:19:58Z","title":"\\emph{Lifted} RDT based capacity analysis of the 1-hidden layer treelike\n  \\emph{sign} perceptrons neural networks","summary":"  We consider the memorization capabilities of multilayered \\emph{sign}\nperceptrons neural networks (SPNNs). A recent rigorous upper-bounding capacity\ncharacterization, obtained in \\cite{Stojnictcmspnncaprdt23} utilizing the\nRandom Duality Theory (RDT), demonstrated that adding neurons in a network\nconfiguration may indeed be very beneficial. Moreover, for particular\n\\emph{treelike committee machines} (TCM) architectures with $d\\leq 5$ neurons\nin the hidden layer, \\cite{Stojnictcmspnncaprdt23} made a very first\nmathematically rigorous progress in over 30 years by lowering the previously\nbest known capacity bounds of \\cite{MitchDurb89}. Here, we first establish that\nthe RDT bounds from \\cite{Stojnictcmspnncaprdt23} scale as $\\sim \\sqrt{d}$ and\ncan not on their own \\emph{universally} (over the entire range of $d$) beat the\nbest known $\\sim \\log(d)$ scaling of the bounds from \\cite{MitchDurb89}. After\nrecognizing that the progress from \\cite{Stojnictcmspnncaprdt23} is therefore\npromising, but yet without a complete concretization, we then proceed by\nconsidering the recently developed fully lifted RDT (fl RDT) as an alternative.\nWhile the fl RDT is indeed a powerful juggernaut, it typically relies on heavy\nnumerical evaluations. To avoid such heavy numerics, we here focus on a\nsimplified, \\emph{partially lifted}, variant and show that it allows for very\nneat, closed form, analytical capacity characterizations. Moreover, we obtain\nthe concrete capacity bounds that \\emph{universally} improve for \\emph{any} $d$\nover the best known ones of \\cite{MitchDurb89}.\n","authors":["Mihailo Stojnic"],"pdf_url":"https://arxiv.org/pdf/2312.08257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08255v1","updated":"2023-12-13T16:18:40Z","published":"2023-12-13T16:18:40Z","title":"OCTDL: Optical Coherence Tomography Dataset for Image-Based Deep\n  Learning Methods","summary":"  Optical coherence tomography (OCT) is a non-invasive imaging technique with\nextensive clinical applications in ophthalmology. OCT enables the visualization\nof the retinal layers, playing a vital role in the early detection and\nmonitoring of retinal diseases. OCT uses the principle of light wave\ninterference to create detailed images of the retinal microstructures, making\nit a valuable tool for diagnosing ocular conditions. This work presents an\nopen-access OCT dataset (OCTDL) comprising over 1600 high-resolution OCT images\nlabeled according to disease group and retinal pathology. The dataset consists\nof OCT records of patients with Age-related Macular Degeneration (AMD),\nDiabetic Macular Edema (DME), Epiretinal Membrane (ERM), Retinal Artery\nOcclusion (RAO), Retinal Vein Occlusion (RVO), and Vitreomacular Interface\nDisease (VID). The images were acquired with an Optovue Avanti RTVue XR using\nraster scanning protocols with dynamic scan length and image resolution. Each\nretinal b-scan was acquired by centering on the fovea and interpreted and\ncataloged by an experienced retinal specialist. In this work, we applied Deep\nLearning classification techniques to this new open-access dataset.\n","authors":["Mikhail Kulyabin","Aleksei Zhdanov","Anastasia Nikiforova","Andrey Stepichev","Anna Kuznetsova","Mikhail Ronkin","Vasilii Borisov","Alexander Bogachev","Sergey Korotkich","Paul A Constable","Andreas Maier"],"pdf_url":"https://arxiv.org/pdf/2312.08255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.07707v2","updated":"2023-12-13T16:11:58Z","published":"2023-08-15T11:30:45Z","title":"Fast Machine Unlearning Without Retraining Through Selective Synaptic\n  Dampening","summary":"  Machine unlearning, the ability for a machine learning model to forget, is\nbecoming increasingly important to comply with data privacy regulations, as\nwell as to remove harmful, manipulated, or outdated information. The key\nchallenge lies in forgetting specific information while protecting model\nperformance on the remaining data. While current state-of-the-art methods\nperform well, they typically require some level of retraining over the retained\ndata, in order to protect or restore model performance. This adds computational\noverhead and mandates that the training data remain available and accessible,\nwhich may not be feasible. In contrast, other methods employ a retrain-free\nparadigm, however, these approaches are prohibitively computationally expensive\nand do not perform on par with their retrain-based counterparts. We present\nSelective Synaptic Dampening (SSD), a novel two-step, post hoc, retrain-free\napproach to machine unlearning which is fast, performant, and does not require\nlong-term storage of the training data. First, SSD uses the Fisher information\nmatrix of the training and forgetting data to select parameters that are\ndisproportionately important to the forget set. Second, SSD induces forgetting\nby dampening these parameters proportional to their relative importance to the\nforget set with respect to the wider training data. We evaluate our method\nagainst several existing unlearning methods in a range of experiments using\nResNet18 and Vision Transformer. Results show that the performance of SSD is\ncompetitive with retrain-based post hoc methods, demonstrating the viability of\nretrain-free post hoc unlearning approaches.\n","authors":["Jack Foster","Stefan Schoepf","Alexandra Brintrup"],"pdf_url":"https://arxiv.org/pdf/2308.07707v2.pdf","comment":"Accepted as a main track paper at AAAI 2024"},{"id":"http://arxiv.org/abs/2312.08230v1","updated":"2023-12-13T15:48:50Z","published":"2023-12-13T15:48:50Z","title":"Partial Symmetry Detection for 3D Geometry using Contrastive Learning\n  with Geodesic Point Cloud Patches","summary":"  Symmetry detection, especially partial and extrinsic symmetry, is essential\nfor various downstream tasks, like 3D geometry completion, segmentation,\ncompression and structure-aware shape encoding or generation. In order to\ndetect partial extrinsic symmetries, we propose to learn rotation, reflection,\ntranslation and scale invariant local shape features for geodesic point cloud\npatches via contrastive learning, which are robust across multiple classes and\ngeneralize over different datasets. We show that our approach is able to\nextract multiple valid solutions for this ambiguous problem. Furthermore, we\nintroduce a novel benchmark test for partial extrinsic symmetry detection to\nevaluate our method. Lastly, we incorporate the detected symmetries together\nwith a region growing algorithm to demonstrate a downstream task with the goal\nof computing symmetry-aware partitions of 3D shapes. To our knowledge, we are\nthe first to propose a self-supervised data-driven method for partial extrinsic\nsymmetry detection.\n","authors":["Gregor Kobsik","Isaak Lim","Leif Kobbelt"],"pdf_url":"https://arxiv.org/pdf/2312.08230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07439v2","updated":"2023-12-13T15:48:17Z","published":"2023-12-12T17:06:39Z","title":"BIRB: A Generalization Benchmark for Information Retrieval in\n  Bioacoustics","summary":"  The ability for a machine learning model to cope with differences in training\nand deployment conditions--e.g. in the presence of distribution shift or the\ngeneralization to new classes altogether--is crucial for real-world use cases.\nHowever, most empirical work in this area has focused on the image domain with\nartificial benchmarks constructed to measure individual aspects of\ngeneralization. We present BIRB, a complex benchmark centered on the retrieval\nof bird vocalizations from passively-recorded datasets given focal recordings\nfrom a large citizen science corpus available for training. We propose a\nbaseline system for this collection of tasks using representation learning and\na nearest-centroid search. Our thorough empirical evaluation and analysis\nsurfaces open research directions, suggesting that BIRB fills the need for a\nmore realistic and complex benchmark to drive progress on robustness to\ndistribution shifts and generalization of ML models.\n","authors":["Jenny Hamer","Eleni Triantafillou","Bart van Merriënboer","Stefan Kahl","Holger Klinck","Tom Denton","Vincent Dumoulin"],"pdf_url":"https://arxiv.org/pdf/2312.07439v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08227v1","updated":"2023-12-13T15:47:30Z","published":"2023-12-13T15:47:30Z","title":"Differentially Private Gradient Flow based on the Sliced Wasserstein\n  Distance for Non-Parametric Generative Modeling","summary":"  Safeguarding privacy in sensitive training data is paramount, particularly in\nthe context of generative modeling. This is done through either differentially\nprivate stochastic gradient descent, or with a differentially private metric\nfor training models or generators. In this paper, we introduce a novel\ndifferentially private generative modeling approach based on parameter-free\ngradient flows in the space of probability measures. The proposed algorithm is\na new discretized flow which operates through a particle scheme, utilizing\ndrift derived from the sliced Wasserstein distance and computed in a private\nmanner. Our experiments show that compared to a generator-based model, our\nproposed model can generate higher-fidelity data at a low privacy budget,\noffering a viable alternative to generator-based approaches.\n","authors":["Ilana Sebag","Muni Sreenivas PYDI","Jean-Yves Franceschi","Alain Rakotomamonjy","Mike Gartrell","Jamal Atif","Alexandre Allauzen"],"pdf_url":"https://arxiv.org/pdf/2312.08227v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08224v1","updated":"2023-12-13T15:46:58Z","published":"2023-12-13T15:46:58Z","title":"GLOP: Learning Global Partition and Local Construction for Solving\n  Large-scale Routing Problems in Real-time","summary":"  The recent end-to-end neural solvers have shown promise for small-scale\nrouting problems but suffered from limited real-time scaling-up performance.\nThis paper proposes GLOP (Global and Local Optimization Policies), a unified\nhierarchical framework that efficiently scales toward large-scale routing\nproblems. GLOP partitions large routing problems into Travelling Salesman\nProblems (TSPs) and TSPs into Shortest Hamiltonian Path Problems. For the first\ntime, we hybridize non-autoregressive neural heuristics for coarse-grained\nproblem partitions and autoregressive neural heuristics for fine-grained route\nconstructions, leveraging the scalability of the former and the meticulousness\nof the latter. Experimental results show that GLOP achieves competitive and\nstate-of-the-art real-time performance on large-scale routing problems,\nincluding TSP, ATSP, CVRP, and PCTSP.\n","authors":["Haoran Ye","Jiarui Wang","Helan Liang","Zhiguang Cao","Yong Li","Fanzhang Li"],"pdf_url":"https://arxiv.org/pdf/2312.08224v1.pdf","comment":"Accepted at AAAI 2024"},{"id":"http://arxiv.org/abs/2312.08221v1","updated":"2023-12-13T15:42:14Z","published":"2023-12-13T15:42:14Z","title":"Curriculum-Enhanced Residual Soft An-Isotropic Normalization for\n  Over-smoothness in Deep GNNs","summary":"  Despite Graph neural networks' significant performance gain over many classic\ntechniques in various graph-related downstream tasks, their successes are\nrestricted in shallow models due to over-smoothness and the difficulties of\noptimizations among many other issues. In this paper, to alleviate the\nover-smoothing issue, we propose a soft graph normalization method to preserve\nthe diversities of node embeddings and prevent indiscrimination due to possible\nover-closeness. Combined with residual connections, we analyze the reason why\nthe method can effectively capture the knowledge in both input graph structures\nand node features even with deep networks. Additionally, inspired by Curriculum\nLearning that learns easy examples before the hard ones, we propose a novel\nlabel-smoothing-based learning framework to enhance the optimization of deep\nGNNs, which iteratively smooths labels in an auxiliary graph and constructs\nmany gradual non-smooth tasks for extracting increasingly complex knowledge and\ngradually discriminating nodes from coarse to fine. The method arguably reduces\nthe risk of overfitting and generalizes better results. Finally, extensive\nexperiments are carried out to demonstrate the effectiveness and potential of\nthe proposed model and learning framework through comparison with twelve\nexisting baselines including the state-of-the-art methods on twelve real-world\nnode classification benchmarks.\n","authors":["Jin Li","Qirong Zhang","Shuling Xu","Xinlong Chen","Longkun Guo","Yang-Geng Fu"],"pdf_url":"https://arxiv.org/pdf/2312.08221v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2307.12971v2","updated":"2023-12-13T15:10:35Z","published":"2023-07-24T17:49:05Z","title":"Big Data -- Supply Chain Management Framework for Forecasting: Data\n  Preprocessing and Machine Learning Techniques","summary":"  This article intends to systematically identify and comparatively analyze\nstate-of-the-art supply chain (SC) forecasting strategies and technologies. A\nnovel framework has been proposed incorporating Big Data Analytics in SC\nManagement (problem identification, data sources, exploratory data analysis,\nmachine-learning model training, hyperparameter tuning, performance evaluation,\nand optimization), forecasting effects on human-workforce, inventory, and\noverall SC. Initially, the need to collect data according to SC strategy and\nhow to collect them has been discussed. The article discusses the need for\ndifferent types of forecasting according to the period or SC objective. The SC\nKPIs and the error-measurement systems have been recommended to optimize the\ntop-performing model. The adverse effects of phantom inventory on forecasting\nand the dependence of managerial decisions on the SC KPIs for determining model\nperformance parameters and improving operations management, transparency, and\nplanning efficiency have been illustrated. The cyclic connection within the\nframework introduces preprocessing optimization based on the post-process KPIs,\noptimizing the overall control process (inventory management, workforce\ndetermination, cost, production and capacity planning). The contribution of\nthis research lies in the standard SC process framework proposal, recommended\nforecasting data analysis, forecasting effects on SC performance, machine\nlearning algorithms optimization followed, and in shedding light on future\nresearch.\n","authors":["Md Abrar Jahin","Md Sakib Hossain Shovon","Jungpil Shin","Istiyaque Ahmed Ridoy","Yoichi Tomioka","M. F. Mridha"],"pdf_url":"https://arxiv.org/pdf/2307.12971v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08200v1","updated":"2023-12-13T15:08:54Z","published":"2023-12-13T15:08:54Z","title":"SPD-DDPM: Denoising Diffusion Probabilistic Models in the Symmetric\n  Positive Definite Space","summary":"  Symmetric positive definite~(SPD) matrices have shown important value and\napplications in statistics and machine learning, such as FMRI analysis and\ntraffic prediction. Previous works on SPD matrices mostly focus on\ndiscriminative models, where predictions are made directly on $E(X|y)$, where\n$y$ is a vector and $X$ is an SPD matrix. However, these methods are\nchallenging to handle for large-scale data, as they need to access and process\nthe whole data. In this paper, inspired by denoising diffusion probabilistic\nmodel~(DDPM), we propose a novel generative model, termed SPD-DDPM, by\nintroducing Gaussian distribution in the SPD space to estimate $E(X|y)$.\nMoreover, our model is able to estimate $p(X)$ unconditionally and flexibly\nwithout giving $y$. On the one hand, the model conditionally learns $p(X|y)$\nand utilizes the mean of samples to obtain $E(X|y)$ as a prediction. On the\nother hand, the model unconditionally learns the probability distribution of\nthe data $p(X)$ and generates samples that conform to this distribution.\nFurthermore, we propose a new SPD net which is much deeper than the previous\nnetworks and allows for the inclusion of conditional factors. Experiment\nresults on toy data and real taxi data demonstrate that our models effectively\nfit the data distribution both unconditionally and unconditionally and provide\naccurate predictions.\n","authors":["Yunchen Li","Zhou Yu","Gaoqi He","Yunhang Shen","Ke Li","Xing Sun","Shaohui Lin"],"pdf_url":"https://arxiv.org/pdf/2312.08200v1.pdf","comment":"AAAI2024"},{"id":"http://arxiv.org/abs/2311.17929v2","updated":"2023-12-13T15:06:50Z","published":"2023-11-25T22:26:58Z","title":"New Online Communities: Graph Deep Learning on Anonymous Voting Networks\n  to Identify Sybils in Polycentric Governance","summary":"  This research examines the polycentric governance of digital assets in\nblockchain-based Decentralized Autonomous Organizations (DAOs). It offers a\ntheoretical framework and addresses a critical challenge facing decentralized\ngovernance by developing a method to identify sybils, or spurious identities.\nThe method uses graph deep learning techniques to identify sybil activity in a\nDAO governance dataset (snapshot.org). Specifically, a Graph Convolutional\nNeural Network (GCNN) learned voting behaviours and a fast k-means vector\nclustering algorithm (FAISS) used the high dimensional embeddings to identify\nsimilar nodes in a graph. The results reveal that deep learning can effectively\nidentify sybils, reducing the voting graph by 2-5%. This research underscores\nthe importance of sybil resistance in DAOs and offers a novel perspective on\ndecentralized governance, informing future policy, regulation, and governance\npractices.\n","authors":["Quinn DuPont"],"pdf_url":"https://arxiv.org/pdf/2311.17929v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10659v2","updated":"2023-12-13T15:00:28Z","published":"2023-09-12T02:42:39Z","title":"Exploiting Machine Unlearning for Backdoor Attacks in Deep Learning\n  System","summary":"  In recent years, the security issues of artificial intelligence have become\nincreasingly prominent due to the rapid development of deep learning research\nand applications. Backdoor attack is an attack targeting the vulnerability of\ndeep learning models, where hidden backdoors are activated by triggers embedded\nby the attacker, thereby outputting malicious predictions that may not align\nwith the intended output for a given input. In this work, we propose a novel\nblack-box backdoor attack based on machine unlearning. The attacker first\naugments the training set with carefully designed samples, including poison and\nmitigation data, to train a `benign' model. Then, the attacker posts unlearning\nrequests for the mitigation samples to remove the impact of relevant data on\nthe model, gradually activating the hidden backdoor. Since backdoors are\nimplanted during the iterative unlearning process, it significantly increases\nthe computational overhead of existing defense methods for backdoor detection\nor mitigation. To address this new security threat, we proposes two methods for\ndetecting or mitigating such malicious unlearning requests. We conduct the\nexperiment in both exact unlearning and approximate unlearning (i.e., SISA)\nsettings. Experimental results indicate that: 1) our attack approach can\nsuccessfully implant backdoor into the model, and sharding increases the\ndifficult of attack; 2) our detection algorithms are effective in identifying\nthe mitigation samples, while sharding reduces the effectiveness of our\ndetection algorithms.\n","authors":["Peixin Zhang","Jun Sun","Mingtian Tan","Xinyu Wang"],"pdf_url":"https://arxiv.org/pdf/2310.10659v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08194v1","updated":"2023-12-13T14:58:25Z","published":"2023-12-13T14:58:25Z","title":"SVInvNet: A Densely Connected Encoder-Decoder Architecture for Seismic\n  Velocity Inversion","summary":"  This study presents a deep learning-based approach to seismic velocity\ninversion problem, focusing on both noisy and noiseless training datasets of\nvarying sizes. Our Seismic Velocity Inversion Network (SVInvNet) introduces a\nnovel architecture that contains a multi-connection encoder-decoder structure\nenhanced with dense blocks. This design is specifically tuned to effectively\nprocess complex information, crucial for addressing the challenges of\nnon-linear seismic velocity inversion. For training and testing, we created\ndiverse seismic velocity models, including multi-layered, faulty, and salt dome\ncategories. We also investigated how different kinds of ambient noise, both\ncoherent and stochastic, and the size of the training dataset affect learning\noutcomes. SVInvNet is trained on datasets ranging from 750 to 6,000 samples and\nis tested using a large benchmark dataset of 12,000 samples. Despite its fewer\nparameters compared to the baseline, SVInvNet achieves superior performance\nwith this dataset. The outcomes of the SVInvNet are additionally compared to\nthose of the Full Waveform Inversion (FWI) method. The comparative analysis\nclearly reveals the effectiveness of the proposed model.\n","authors":["Mojtaba Najafi Khatounabad","Hacer Yalim Keles","Selma Kadioglu"],"pdf_url":"https://arxiv.org/pdf/2312.08194v1.pdf","comment":"14 pages, 11 figures, submitted to IEEE Transactions on Geoscience\n  and Remote Sensing"},{"id":"http://arxiv.org/abs/2312.08193v1","updated":"2023-12-13T14:58:17Z","published":"2023-12-13T14:58:17Z","title":"Universal Adversarial Framework to Improve Adversarial Robustness for\n  Diabetic Retinopathy Detection","summary":"  Diabetic Retinopathy (DR) is a prevalent illness associated with Diabetes\nwhich, if left untreated, can result in irreversible blindness. Deep Learning\nbased systems are gradually being introduced as automated support for clinical\ndiagnosis. Since healthcare has always been an extremely important domain\ndemanding error-free performance, any adversaries could pose a big threat to\nthe applicability of such systems. In this work, we use Universal Adversarial\nPerturbations (UAPs) to quantify the vulnerability of Medical Deep Neural\nNetworks (DNNs) for detecting DR. To the best of our knowledge, this is the\nvery first attempt that works on attacking complete fine-grained classification\nof DR images using various UAPs. Also, as a part of this work, we use UAPs to\nfine-tune the trained models to defend against adversarial samples. We\nexperiment on several models and observe that the performance of such models\ntowards unseen adversarial attacks gets boosted on average by $3.41$\nCohen-kappa value and maximum by $31.92$ Cohen-kappa value. The performance\ndegradation on normal data upon ensembling the fine-tuned models was found to\nbe statistically insignificant using t-test, highlighting the benefits of\nUAP-based adversarial fine-tuning.\n","authors":["Samrat Mukherjee","Dibyanayan Bandyopadhyay","Baban Gain","Asif Ekbal"],"pdf_url":"https://arxiv.org/pdf/2312.08193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.02345v4","updated":"2023-12-13T14:43:43Z","published":"2023-07-05T15:00:29Z","title":"LLQL: Logistic Likelihood Q-Learning for Reinforcement Learning","summary":"  Modern reinforcement learning (RL) can be categorized into online and offline\nvariants. As a pivotal aspect of both online and offline RL, current research\non the Bellman equation revolves primarily around optimization techniques and\nperformance enhancement rather than exploring the inherent structural\nproperties of the Bellman error, such as its distribution characteristics. This\nstudy investigates the distribution of the Bellman approximation error through\niterative exploration of the Bellman equation with the observation that the\nBellman error approximately follows the Logistic distribution. Based on this,\nwe proposed the utilization of the Logistic maximum likelihood function (LLoss)\nas an alternative to the commonly used mean squared error (MSELoss) that\nassumes a Normal distribution for Bellman errors. We validated the hypotheses\nthrough extensive numerical experiments across diverse online and offline\nenvironments. In particular, we applied the Logistic correction to loss\nfunctions in various RL baseline methods and observed that the results with\nLLoss consistently outperformed the MSE counterparts. We also conducted the\nKolmogorov-Smirnov tests to confirm the reliability of the Logistic\ndistribution. Moreover, our theory connects the Bellman error to the\nproportional reward scaling phenomenon by providing a distribution-based\nanalysis. Furthermore, we applied the bias-variance decomposition for sampling\nfrom the Logistic distribution. The theoretical and empirical insights of this\nstudy lay a valuable foundation for future investigations and enhancements\ncentered on the distribution of Bellman error.\n","authors":["Outongyi Lv","Bingxin Zhou"],"pdf_url":"https://arxiv.org/pdf/2307.02345v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.10187v2","updated":"2023-12-13T14:40:03Z","published":"2022-09-21T08:39:02Z","title":"On the convex formulations of robust Markov decision processes","summary":"  Robust Markov decision processes (MDPs) are used for applications of dynamic\noptimization in uncertain environments and have been studied extensively. Many\nof the main properties and algorithms of MDPs, such as value iteration and\npolicy iteration, extend directly to RMDPs. Surprisingly, there is no known\nanalog of the MDP convex optimization formulation for solving RMDPs. This work\ndescribes the first convex optimization formulation of RMDPs under the\nclassical sa-rectangularity and s-rectangularity assumptions. By using entropic\nregularization and exponential change of variables, we derive a convex\nformulation with a number of variables and constraints polynomial in the number\nof states and actions, but with large coefficients in the constraints. We\nfurther simplify the formulation for RMDPs with polyhedral, ellipsoidal, or\nentropy-based uncertainty sets, showing that, in these cases, RMDPs can be\nreformulated as conic programs based on exponential cones, quadratic cones, and\nnon-negative orthants. Our work opens a new research direction for RMDPs and\ncan serve as a first step toward obtaining a tractable convex formulation of\nRMDPs.\n","authors":["Julien Grand-Clément","Marek Petrik"],"pdf_url":"https://arxiv.org/pdf/2209.10187v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08174v1","updated":"2023-12-13T14:34:12Z","published":"2023-12-13T14:34:12Z","title":"Double Machine Learning for Static Panel Models with Fixed Effects","summary":"  Machine Learning (ML) algorithms are powerful data-driven tools for\napproximating high-dimensional or non-linear nuisance functions which are\nuseful in practice because the true functional form of the predictors is\nex-ante unknown. In this paper, we develop estimators of policy interventions\nfrom panel data which allow for non-linear effects of the confounding\nregressors, and investigate the performance of these estimators using three\nwell-known ML algorithms, specifically, LASSO, classification and regression\ntrees, and random forests. We use Double Machine Learning (DML) (Chernozhukov\net al., 2018) for the estimation of causal effects of homogeneous treatments\nwith unobserved individual heterogeneity (fixed effects) and no unobserved\nconfounding by extending Robinson (1988)'s partially linear regression model.\nWe develop three alternative approaches for handling unobserved individual\nheterogeneity based on extending the within-group estimator, first-difference\nestimator, and correlated random effect estimator (Mundlak, 1978) for\nnon-linear models. Using Monte Carlo simulations, we find that conventional\nleast squares estimators can perform well even if the data generating process\nis non-linear, but there are substantial performance gains in terms of bias\nreduction under a process where the true effect of the regressors is non-linear\nand discontinuous. However, for the same scenarios, we also find -- despite\nextensive hyperparameter tuning -- inference to be problematic for both\ntree-based learners because these lead to highly non-normal estimator\ndistributions and the estimator variance being severely under-estimated. This\ncontradicts the performance of trees in other circumstances and requires\nfurther investigation. Finally, we provide an illustrative example of DML for\nobservational panel data showing the impact of the introduction of the national\nminimum wage in the UK.\n","authors":["Paul Clarke","Annalivia Polselli"],"pdf_url":"https://arxiv.org/pdf/2312.08174v1.pdf","comment":"20 pages, 5 tables, 5 figure, 2 appendices"},{"id":"http://arxiv.org/abs/2310.01217v2","updated":"2023-12-13T14:09:02Z","published":"2023-10-02T14:01:36Z","title":"ScaLearn: Simple and Highly Parameter-Efficient Task Transfer by\n  Learning to Scale","summary":"  Multi-task learning (MTL) has shown considerable practical benefits,\nparticularly when using pre-trained language models (PLMs). While this is\ncommonly achieved by simultaneously learning $n$ tasks under a joint\noptimization procedure, recent methods such as AdapterFusion structure the\nproblem into two distinct stages: (i) task learning, where knowledge specific\nto a task is encapsulated within sets of parameters (e.g., adapters), and (ii)\ntransfer, where this already learned knowledge is leveraged for a target task.\nThis separation of concerns provides numerous benefits, such as promoting\nreusability, and addressing cases involving data privacy and societal concerns;\non the flip side, current two-stage MTL methods come with the cost of\nintroducing a substantial number of additional parameters. In this work, we\naddress this issue by leveraging the usefulness of linearly scaling the output\nrepresentations of source adapters for transfer learning. We introduce\nScaLearn, a simple and highly parameter-efficient two-stage MTL method that\ncapitalizes on the knowledge of the source tasks by learning a minimal set of\nscaling parameters that enable effective knowledge transfer to a target task.\nOur experiments on three benchmarks (GLUE, SuperGLUE, and HumSet) show that our\nScaLearn, in addition to facilitating the benefits of two-stage MTL,\nconsistently outperforms strong baselines with only a small number of transfer\nparameters - roughly 0.35% of those of AdapterFusion. Remarkably, we observe\nthat ScaLearn maintains its strong abilities even when further reducing\nparameters through uniform scaling and layer-sharing, achieving similarly\ncompetitive results with only $8$ transfer parameters for each target task. Our\nproposed approach thus demonstrates the power of simple scaling as a promise\nfor more efficient task transfer.\n","authors":["Markus Frohmann","Carolin Holtermann","Shahed Masoudian","Anne Lauscher","Navid Rekabsaz"],"pdf_url":"https://arxiv.org/pdf/2310.01217v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08153v1","updated":"2023-12-13T14:05:35Z","published":"2023-12-13T14:05:35Z","title":"$ρ$-Diffusion: A diffusion-based density estimation framework for\n  computational physics","summary":"  In physics, density $\\rho(\\cdot)$ is a fundamentally important scalar\nfunction to model, since it describes a scalar field or a probability density\nfunction that governs a physical process. Modeling $\\rho(\\cdot)$ typically\nscales poorly with parameter space, however, and quickly becomes prohibitively\ndifficult and computationally expensive. One promising avenue to bypass this is\nto leverage the capabilities of denoising diffusion models often used in\nhigh-fidelity image generation to parameterize $\\rho(\\cdot)$ from existing\nscientific data, from which new samples can be trivially sampled from. In this\npaper, we propose $\\rho$-Diffusion, an implementation of denoising diffusion\nprobabilistic models for multidimensional density estimation in physics, which\nis currently in active development and, from our results, performs well on\nphysically motivated 2D and 3D density functions. Moreover, we propose a novel\nhashing technique that allows $\\rho$-Diffusion to be conditioned by arbitrary\namounts of physical parameters of interest.\n","authors":["Maxwell X. Cai","Kin Long Kelvin Lee"],"pdf_url":"https://arxiv.org/pdf/2312.08153v1.pdf","comment":"6 pages, 2 figures, accepted for publication at the NeurIPS 2023\n  workshop \"Machine Learning and the Physical Sciences\""},{"id":"http://arxiv.org/abs/2312.08150v1","updated":"2023-12-13T14:01:58Z","published":"2023-12-13T14:01:58Z","title":"Active learning with biased non-response to label requests","summary":"  Active learning can improve the efficiency of training prediction models by\nidentifying the most informative new labels to acquire. However, non-response\nto label requests can impact active learning's effectiveness in real-world\ncontexts. We conceptualise this degradation by considering the type of\nnon-response present in the data, demonstrating that biased non-response is\nparticularly detrimental to model performance. We argue that this sort of\nnon-response is particularly likely in contexts where the labelling process, by\nnature, relies on user interactions. To mitigate the impact of biased\nnon-response, we propose a cost-based correction to the sampling strategy--the\nUpper Confidence Bound of the Expected Utility (UCB-EU)--that can, plausibly,\nbe applied to any active learning algorithm. Through experiments, we\ndemonstrate that our method successfully reduces the harm from labelling\nnon-response in many settings. However, we also characterise settings where the\nnon-response bias in the annotations remains detrimental under UCB-EU for\nparticular sampling methods and data generating processes. Finally, we evaluate\nour method on a real-world dataset from e-commerce platform Taobao. We show\nthat UCB-EU yields substantial performance improvements to conversion models\nthat are trained on clicked impressions. Most generally, this research serves\nto both better conceptualise the interplay between types of non-response and\nmodel improvements via active learning, and to provide a practical, easy to\nimplement correction that helps mitigate model degradation.\n","authors":["Thomas Robinson","Niek Tax","Richard Mudd","Ido Guy"],"pdf_url":"https://arxiv.org/pdf/2312.08150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16442v2","updated":"2023-12-13T13:50:45Z","published":"2023-11-28T02:44:59Z","title":"Enabling Fast 2-bit LLM on GPUs: Memory Alignment and Asynchronous\n  Dequantization","summary":"  Large language models (LLMs) have demonstrated impressive abilities in\nvarious domains while the inference cost is expensive. The state-of-the-art\nmethods use 2-bit quantization for mainstream LLMs. However, challenges still\nexist: (1) Nonnegligible accuracy loss for 2-bit quantization. Weights are\nquantized by groups, while the ranges of weights are large in some groups,\nresulting in large quantization errors and nonnegligible accuracy loss (e.g.\n>3% for Llama2-7b with 2-bit quantization in GPTQ and Greenbit). (2) Limited\naccuracy improvement by adding 4-bit weights. Increasing 10% extra average bit\nmore 4-bit weights only leads to <0.5% accuracy improvement on a quantized\nLlama2-7b. (3) Time-consuming dequantization operations on GPUs. The\ndequantization operations lead to >50% execution time, hindering the potential\nof reducing LLM inference cost. To tackle these challenges, we propose the\nfollowing techniques: (1) We only quantize a small fraction of groups with the\nlarger range using 4-bit with memory alignment consideration on GPUs.(2) We\ndesign the asynchronous dequantization on GPUs, leading to up to 3.92X speedup.\nWe conduct extensive experiments on different model sizes. We achieve 2.85-bit\nfor each weight and the end-to-end speedup for Llama2-7b is 1.74X over the\noriginal model, and we reduce both runtime cost and hardware cost by up to\n2.70X and 2.81X with less GPU requirements.\n","authors":["Jinhao Li","Shiyao Li","Jiaming Xu","Shan Huang","Yaoxiu Lian","Jun Liu","Yu Wang","Guohao Dai"],"pdf_url":"https://arxiv.org/pdf/2311.16442v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08143v1","updated":"2023-12-13T13:46:14Z","published":"2023-12-13T13:46:14Z","title":"Efficient Representation of the Activation Space in Deep Neural Networks","summary":"  The representations of the activation space of deep neural networks (DNNs)\nare widely utilized for tasks like natural language processing, anomaly\ndetection and speech recognition. Due to the diverse nature of these tasks and\nthe large size of DNNs, an efficient and task-independent representation of\nactivations becomes crucial. Empirical p-values have been used to quantify the\nrelative strength of an observed node activation compared to activations\ncreated by already-known inputs. Nonetheless, keeping raw data for these\ncalculations increases memory resource consumption and raises privacy concerns.\nTo this end, we propose a model-agnostic framework for creating representations\nof activations in DNNs using node-specific histograms to compute p-values of\nobserved activations without retaining already-known inputs. Our proposed\napproach demonstrates promising potential when validated with multiple network\narchitectures across various downstream tasks and compared with the kernel\ndensity estimates and brute-force empirical baselines. In addition, the\nframework reduces memory usage by 30% with up to 4 times faster p-value\ncomputing time while maintaining state of-the-art detection power in downstream\ntasks such as the detection of adversarial attacks and synthesized content.\nMoreover, as we do not persist raw data at inference time, we could potentially\nreduce susceptibility to attacks and privacy issues.\n","authors":["Tanya Akumu","Celia Cintas","Girmaw Abebe Tadesse","Adebayo Oshingbesan","Skyler Speakman","Edward McFowland III"],"pdf_url":"https://arxiv.org/pdf/2312.08143v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2103.13389v5","updated":"2023-12-13T13:44:40Z","published":"2021-03-24T17:59:07Z","title":"Generating Novel Scene Compositions from Single Images and Videos","summary":"  Given a large dataset for training, generative adversarial networks (GANs)\ncan achieve remarkable performance for the image synthesis task. However,\ntraining GANs in extremely low data regimes remains a challenge, as overfitting\noften occurs, leading to memorization or training divergence. In this work, we\nintroduce SIV-GAN, an unconditional generative model that can generate new\nscene compositions from a single training image or a single video clip. We\npropose a two-branch discriminator architecture, with content and layout\nbranches designed to judge internal content and scene layout realism separately\nfrom each other. This discriminator design enables synthesis of visually\nplausible, novel compositions of a scene, with varying content and layout,\nwhile preserving the context of the original sample. Compared to previous\nsingle image GANs, our model generates more diverse, higher quality images,\nwhile not being restricted to a single image setting. We further introduce a\nnew challenging task of learning from a few frames of a single video. In this\ntraining setup the training images are highly similar to each other, which\nmakes it difficult for prior GAN models to achieve a synthesis of both high\nquality and diversity.\n","authors":["Vadim Sushko","Dan Zhang","Juergen Gall","Anna Khoreva"],"pdf_url":"https://arxiv.org/pdf/2103.13389v5.pdf","comment":"Accepted for publication in Computer Vision and Image Understanding:\n  https://www.sciencedirect.com/science/article/pii/S1077314223002680. Code\n  repository: https://github.com/boschresearch/one-shot-synthesis"},{"id":"http://arxiv.org/abs/2312.03940v2","updated":"2023-12-13T13:40:48Z","published":"2023-12-06T22:43:50Z","title":"PECANN: Parallel Efficient Clustering with Graph-Based Approximate\n  Nearest Neighbor Search","summary":"  This paper studies density-based clustering of point sets. These methods use\ndense regions of points to detect clusters of arbitrary shapes. In particular,\nwe study variants of density peaks clustering, a popular type of algorithm that\nhas been shown to work well in practice. Our goal is to cluster large\nhigh-dimensional datasets, which are prevalent in practice. Prior solutions are\neither sequential, and cannot scale to large data, or are specialized for\nlow-dimensional data.\n  This paper unifies the different variants of density peaks clustering into a\nsingle framework, PECANN, by abstracting out several key steps common to this\nclass of algorithms. One such key step is to find nearest neighbors that\nsatisfy a predicate function, and one of the main contributions of this paper\nis an efficient way to do this predicate search using graph-based approximate\nnearest neighbor search (ANNS). To provide ample parallelism, we propose a\ndoubling search technique that enables points to find an approximate nearest\nneighbor satisfying the predicate in a small number of rounds. Our technique\ncan be applied to many existing graph-based ANNS algorithms, which can all be\nplugged into PECANN.\n  We implement five clustering algorithms with PECANN and evaluate them on\nsynthetic and real-world datasets with up to 1.28 million points and up to 1024\ndimensions on a 30-core machine with two-way hyper-threading. Compared to the\nstate-of-the-art FASTDP algorithm for high-dimensional density peaks\nclustering, which is sequential, our best algorithm is 45x-734x faster while\nachieving competitive ARI scores. Compared to the state-of-the-art parallel\nDPC-based algorithm, which is optimized for low dimensions, we show that PECANN\nis two orders of magnitude faster. As far as we know, our work is the first to\nevaluate DPC variants on large high-dimensional real-world image and text\nembedding datasets.\n","authors":["Shangdi Yu","Joshua Engels","Yihao Huang","Julian Shun"],"pdf_url":"https://arxiv.org/pdf/2312.03940v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08135v1","updated":"2023-12-13T13:36:32Z","published":"2023-12-13T13:36:32Z","title":"A New Perspective On Denoising Based On Optimal Transport","summary":"  In the standard formulation of the denoising problem, one is given a\nprobabilistic model relating a latent variable $\\Theta \\in \\Omega \\subset\n\\mathbb{R}^m \\; (m\\ge 1)$ and an observation $Z \\in \\mathbb{R}^d$ according to:\n$Z \\mid \\Theta \\sim p(\\cdot\\mid \\Theta)$ and $\\Theta \\sim G^*$, and the goal is\nto construct a map to recover the latent variable from the observation. The\nposterior mean, a natural candidate for estimating $\\Theta$ from $Z$, attains\nthe minimum Bayes risk (under the squared error loss) but at the expense of\nover-shrinking the $Z$, and in general may fail to capture the geometric\nfeatures of the prior distribution $G^*$ (e.g., low dimensionality,\ndiscreteness, sparsity, etc.). To rectify these drawbacks, in this paper we\ntake a new perspective on this denoising problem that is inspired by optimal\ntransport (OT) theory and use it to propose a new OT-based denoiser at the\npopulation level setting. We rigorously prove that, under general assumptions\non the model, our OT-based denoiser is well-defined and unique, and is closely\nconnected to solutions to a Monge OT problem. We then prove that, under\nappropriate identifiability assumptions on the model, our OT-based denoiser can\nbe recovered solely from information of the marginal distribution of $Z$ and\nthe posterior mean of the model, after solving a linear relaxation problem over\na suitable space of couplings that is reminiscent of a standard multimarginal\nOT (MOT) problem. In particular, thanks to Tweedie's formula, when the\nlikelihood model $\\{ p(\\cdot \\mid \\theta) \\}_{\\theta \\in \\Omega}$ is an\nexponential family of distributions, the OT-based denoiser can be recovered\nsolely from the marginal distribution of $Z$. In general, our family of OT-like\nrelaxations is of interest in its own right and for the denoising problem\nsuggests alternative numerical methods inspired by the rich literature on\ncomputational OT.\n","authors":["Nicolas Garcia Trillos","Bodhisattva Sen"],"pdf_url":"https://arxiv.org/pdf/2312.08135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08132v1","updated":"2023-12-13T13:34:15Z","published":"2023-12-13T13:34:15Z","title":"Ultra Low Complexity Deep Learning Based Noise Suppression","summary":"  This paper introduces an innovative method for reducing the computational\ncomplexity of deep neural networks in real-time speech enhancement on\nresource-constrained devices. The proposed approach utilizes a two-stage\nprocessing framework, employing channelwise feature reorientation to reduce the\ncomputational load of convolutional operations. By combining this with a\nmodified power law compression technique for enhanced perceptual quality, this\napproach achieves noise suppression performance comparable to state-of-the-art\nmethods with significantly less computational requirements. Notably, our\nalgorithm exhibits 3 to 4 times less computational complexity and memory usage\nthan prior state-of-the-art approaches.\n","authors":["Shrishti Saha Shetu","Soumitro Chakrabarty","Oliver Thiergart","Edwin Mabande"],"pdf_url":"https://arxiv.org/pdf/2312.08132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.02784v2","updated":"2023-12-13T13:29:29Z","published":"2023-09-06T06:51:15Z","title":"Norm Tweaking: High-performance Low-bit Quantization of Large Language\n  Models","summary":"  As the size of large language models (LLMs) continues to grow, model\ncompression without sacrificing accuracy has become a crucial challenge for\ndeployment. While some quantization methods, such as GPTQ, have made progress\nin achieving acceptable 4-bit weight-only quantization, attempts at lower-bit\nquantization often result in severe performance degradation. In this paper, we\nintroduce a technique called norm tweaking, which can be used as a plugin in\ncurrent PTQ methods to achieve high precision while being cost-efficient. Our\napproach is inspired by the observation that rectifying the quantized\nactivation distribution to match its float counterpart can readily restore\naccuracy for LLMs. To achieve this, we carefully design a tweaking strategy\nthat includes calibration data generation and channel-wise distance constraint\nto update the weights of normalization layers for better generalization. We\nconduct extensive experiments on various datasets using several open-sourced\nLLMs. Our method demonstrates significant improvements in both weight-only\nquantization and joint quantization of weights and activations, surpassing\nexisting PTQ methods. On GLM-130B and OPT-66B, our method even achieves the\nsame level of accuracy at 2-bit quantization as their float ones. Our simple\nand effective approach makes it more practical for real-world applications.\n","authors":["Liang Li","Qingyuan Li","Bo Zhang","Xiangxiang Chu"],"pdf_url":"https://arxiv.org/pdf/2309.02784v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.02618v3","updated":"2023-12-13T13:27:25Z","published":"2023-03-05T09:26:44Z","title":"Ensemble Reinforcement Learning: A Survey","summary":"  Reinforcement Learning (RL) has emerged as a highly effective technique for\naddressing various scientific and applied problems. Despite its success,\ncertain complex tasks remain challenging to be addressed solely with a single\nmodel and algorithm. In response, ensemble reinforcement learning (ERL), a\npromising approach that combines the benefits of both RL and ensemble learning\n(EL), has gained widespread popularity. ERL leverages multiple models or\ntraining algorithms to comprehensively explore the problem space and possesses\nstrong generalization capabilities. In this study, we present a comprehensive\nsurvey on ERL to provide readers with an overview of recent advances and\nchallenges in the field. Firstly, we provide an introduction to the background\nand motivation for ERL. Secondly, we conduct a detailed analysis of strategies\nsuch as model selection and combination that have been successfully implemented\nin ERL. Subsequently, we explore the application of ERL, summarize the\ndatasets, and analyze the algorithms employed. Finally, we outline several open\nquestions and discuss future research directions of ERL. By offering guidance\nfor future scientific research and engineering applications, this survey\nsignificantly contributes to the advancement of ERL.\n","authors":["Yanjie Song","P. N. Suganthan","Witold Pedrycz","Junwei Ou","Yongming He","Yingwu Chen","Yutong Wu"],"pdf_url":"https://arxiv.org/pdf/2303.02618v3.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2310.04353v3","updated":"2023-12-13T13:18:47Z","published":"2023-10-06T16:21:22Z","title":"A Language-Agent Approach to Formal Theorem-Proving","summary":"  Language agents, which use a large language model (LLM) capable of in-context\nlearning to interact with an external environment, have recently emerged as a\npromising approach to control tasks. We present the first language-agent\napproach to formal theorem-proving. Our method, COPRA, uses a high-capacity,\nblack-box LLM (GPT-4) as part of a policy for a stateful backtracking search.\nDuring the search, the policy can select proof tactics and retrieve lemmas and\ndefinitions from an external database. Each selected tactic is executed in the\nunderlying proof framework, and the execution feedback is used to build the\nprompt for the next policy invocation. The search also tracks selected\ninformation from its history and uses it to reduce hallucinations and\nunnecessary LLM queries.\n  We evaluate our implementation of COPRA on the miniF2F benchmark for Lean and\na set of Coq tasks from the Compcert project. On these benchmarks, COPRA\nsignificantly outperforms one-shot invocations of GPT-4, as well as\nstate-of-the-art models fine-tuned on proof data, at finding correct proofs\nquickly. Our code and data are available at\nhttps://github.com/trishullab/copra.\n","authors":["Amitayush Thakur","Yeming Wen","Swarat Chaudhuri"],"pdf_url":"https://arxiv.org/pdf/2310.04353v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.12674v4","updated":"2023-12-13T13:18:05Z","published":"2022-01-29T22:26:02Z","title":"Rewiring with Positional Encodings for Graph Neural Networks","summary":"  Several recent works use positional encodings to extend the receptive fields\nof graph neural network (GNN) layers equipped with attention mechanisms. These\ntechniques, however, extend receptive fields to the complete graph, at\nsubstantial computational cost and risking a change in the inductive biases of\nconventional GNNs, or require complex architecture adjustments. As a\nconservative alternative, we use positional encodings to expand receptive\nfields to $r$-hop neighborhoods. More specifically, our method augments the\ninput graph with additional nodes/edges and uses positional encodings as node\nand/or edge features. We thus modify graphs before inputting them to a\ndownstream GNN model, instead of modifying the model itself. This makes our\nmethod model-agnostic, i.e., compatible with any of the existing GNN\narchitectures. We also provide examples of positional encodings that are\nlossless with a one-to-one map between the original and the modified graphs. We\ndemonstrate that extending receptive fields via positional encodings and a\nvirtual fully-connected node significantly improves GNN performance and\nalleviates over-squashing using small $r$. We obtain improvements on a variety\nof models and datasets and reach competitive performance using traditional GNNs\nor graph Transformers.\n","authors":["Rickard Brüel-Gabrielsson","Mikhail Yurochkin","Justin Solomon"],"pdf_url":"https://arxiv.org/pdf/2201.12674v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04796v2","updated":"2023-12-13T13:01:01Z","published":"2023-10-07T13:09:37Z","title":"Accelerate Multi-Agent Reinforcement Learning in Zero-Sum Games with\n  Subgame Curriculum Learning","summary":"  Learning Nash equilibrium (NE) in complex zero-sum games with multi-agent\nreinforcement learning (MARL) can be extremely computationally expensive.\nCurriculum learning is an effective way to accelerate learning, but an\nunder-explored dimension for generating a curriculum is the difficulty-to-learn\nof the subgames -- games induced by starting from a specific state. In this\nwork, we present a novel subgame curriculum learning framework for zero-sum\ngames. It adopts an adaptive initial state distribution by resetting agents to\nsome previously visited states where they can quickly learn to improve\nperformance. Building upon this framework, we derive a subgame selection metric\nthat approximates the squared distance to NE values and further adopt a\nparticle-based state sampler for subgame generation. Integrating these\ntechniques leads to our new algorithm, Subgame Automatic Curriculum Learning\n(SACL), which is a realization of the subgame curriculum learning framework.\nSACL can be combined with any MARL algorithm such as MAPPO. Experiments in the\nparticle-world environment and Google Research Football environment show SACL\nproduces much stronger policies than baselines. In the challenging\nhide-and-seek quadrant environment, SACL produces all four emergent stages and\nuses only half the samples of MAPPO with self-play. The project website is at\nhttps://sites.google.com/view/sacl-rl.\n","authors":["Jiayu Chen","Zelai Xu","Yunfei Li","Chao Yu","Jiaming Song","Huazhong Yang","Fei Fang","Yu Wang","Yi Wu"],"pdf_url":"https://arxiv.org/pdf/2310.04796v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07424v2","updated":"2023-12-13T13:00:57Z","published":"2023-12-12T16:48:07Z","title":"How Well Does GPT-4V(ision) Adapt to Distribution Shifts? A Preliminary\n  Investigation","summary":"  In machine learning, generalization against distribution shifts -- where\ndeployment conditions diverge from the training scenarios -- is crucial,\nparticularly in fields like climate modeling, biomedicine, and autonomous\ndriving. The emergence of foundation models, distinguished by their extensive\npretraining and task versatility, has led to an increased interest in their\nadaptability to distribution shifts. GPT-4V(ision) acts as the most advanced\npublicly accessible multimodal foundation model, with extensive applications\nacross various domains, including anomaly detection, video understanding, image\ngeneration, and medical diagnosis. However, its robustness against data\ndistributions remains largely underexplored. Addressing this gap, this study\nrigorously evaluates GPT-4V's adaptability and generalization capabilities in\ndynamic environments, benchmarking against prominent models like CLIP and\nLLaVA. We delve into GPT-4V's zero-shot generalization across 13 diverse\ndatasets spanning natural, medical, and molecular domains. We further\ninvestigate its adaptability to controlled data perturbations and examine the\nefficacy of in-context learning as a tool to enhance its adaptation. Our\nfindings delineate GPT-4V's capability boundaries in distribution shifts,\nshedding light on its strengths and limitations across various scenarios.\nImportantly, this investigation contributes to our understanding of how AI\nfoundation models generalize to distribution shifts, offering pivotal insights\ninto their adaptability and robustness. Code is publicly available at\nhttps://github.com/jameszhou-gl/gpt-4v-distribution-shift.\n","authors":["Zhongyi Han","Guanglin Zhou","Rundong He","Jindong Wang","Tailin Wu","Yilong Yin","Salman Khan","Lina Yao","Tongliang Liu","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.07424v2.pdf","comment":"62 pages, 39 figures, preprint"},{"id":"http://arxiv.org/abs/2312.08107v1","updated":"2023-12-13T12:54:34Z","published":"2023-12-13T12:54:34Z","title":"Causal Optimal Transport of Abstractions","summary":"  Causal abstraction (CA) theory establishes formal criteria for relating\nmultiple structural causal models (SCMs) at different levels of granularity by\ndefining maps between them. These maps have significant relevance for\nreal-world challenges such as synthesizing causal evidence from multiple\nexperimental environments, learning causally consistent representations at\ndifferent resolutions, and linking interventions across multiple SCMs. In this\nwork, we propose COTA, the first method to learn abstraction maps from\nobservational and interventional data without assuming complete knowledge of\nthe underlying SCMs. In particular, we introduce a multi-marginal Optimal\nTransport (OT) formulation that enforces do-calculus causal constraints,\ntogether with a cost function that relies on interventional information. We\nextensively evaluate COTA on synthetic and real world problems, and showcase\nits advantages over non-causal, independent and aggregated COTA formulations.\nFinally, we demonstrate the efficiency of our method as a data augmentation\ntool by comparing it against the state-of-the-art CA learning framework, which\nassumes fully specified SCMs, on a real-world downstream task.\n","authors":["Yorgos Felekis","Fabio Massimo Zennaro","Nicola Branchini","Theodoros Damoulas"],"pdf_url":"https://arxiv.org/pdf/2312.08107v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08103v1","updated":"2023-12-13T12:39:25Z","published":"2023-12-13T12:39:25Z","title":"Machine Learning for the Multi-Dimensional Bin Packing Problem:\n  Literature Review and Empirical Evaluation","summary":"  The Bin Packing Problem (BPP) is a well-established combinatorial\noptimization (CO) problem. Since it has many applications in our daily life,\ne.g. logistics and resource allocation, people are seeking efficient bin\npacking algorithms. On the other hand, researchers have been making constant\nadvances in machine learning (ML), which is famous for its efficiency. In this\narticle, we first formulate BPP, introducing its variants and practical\nconstraints. Then, a comprehensive survey on ML for multi-dimensional BPP is\nprovided. We further collect some public benchmarks of 3D BPP, and evaluate\nsome online methods on the Cutting Stock Dataset. Finally, we share our\nperspective on challenges and future directions in BPP. To the best of our\nknowledge, this is the first systematic review of ML-related methods for BPP.\n","authors":["Wenjie Wu","Changjun Fan","Jincai Huang","Zhong Liu","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2312.08103v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08096v1","updated":"2023-12-13T12:28:37Z","published":"2023-12-13T12:28:37Z","title":"An Incentive Mechanism for Federated Learning Based on Multiple Resource\n  Exchange","summary":"  Federated Learning (FL) is a distributed machine learning paradigm that\naddresses privacy concerns in machine learning and still guarantees high test\naccuracy. However, achieving the necessary accuracy by having all clients\nparticipate in FL is impractical, given the constraints of client local\ncomputing resource. In this paper, we introduce a multi-user collaborative\ncomputing framework, categorizing users into two roles: model owners (MOs) and\ndata owner (DOs). Without resorting to monetary incentives, an MO can encourage\nmore DOs to join in FL by allowing the DOs to offload extra local computing\ntasks to the MO for execution. This exchange of \"data\" for \"computing\nresources\" streamlines the incentives for clients to engage more effectively in\nFL. We formulate the interaction between MO and DOs as an optimization problem,\nand the objective is to effectively utilize the communication and computing\nresource of the MO and DOs to minimize the time to complete an FL task. The\nproposed problem is a mixed integer nonlinear programming (MINLP) with high\ncomputational complexity. We first decompose it into two distinct subproblems,\nnamely the client selection problem and the resource allocation problem to\nsegregate the integer variables from the continuous variables. Then, an\neffective iterative algorithm is proposed to solve problem. Simulation results\ndemonstrate that the proposed collaborative computing framework can achieve an\naccuracy of more than 95\\% while minimizing the overall time to complete an FL\ntask.\n","authors":["Ruonan Dong","Hui Xu","Han Zhang","GuoPeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.08096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2103.11003v4","updated":"2023-12-13T12:21:37Z","published":"2021-03-19T19:55:55Z","title":"Differentially private inference via noisy optimization","summary":"  We propose a general optimization-based framework for computing\ndifferentially private M-estimators and a new method for constructing\ndifferentially private confidence regions. Firstly, we show that robust\nstatistics can be used in conjunction with noisy gradient descent or noisy\nNewton methods in order to obtain optimal private estimators with global linear\nor quadratic convergence, respectively. We establish local and global\nconvergence guarantees, under both local strong convexity and self-concordance,\nshowing that our private estimators converge with high probability to a small\nneighborhood of the non-private M-estimators. Secondly, we tackle the problem\nof parametric inference by constructing differentially private estimators of\nthe asymptotic variance of our private M-estimators. This naturally leads to\napproximate pivotal statistics for constructing confidence regions and\nconducting hypothesis testing. We demonstrate the effectiveness of a bias\ncorrection that leads to enhanced small-sample empirical performance in\nsimulations. We illustrate the benefits of our methods in several numerical\nexamples.\n","authors":["Marco Avella-Medina","Casey Bradshaw","Po-Ling Loh"],"pdf_url":"https://arxiv.org/pdf/2103.11003v4.pdf","comment":"Accepted to Annals of Statistics"},{"id":"http://arxiv.org/abs/2306.04064v2","updated":"2023-12-13T12:10:46Z","published":"2023-06-06T23:24:02Z","title":"Transferable Adversarial Robustness for Categorical Data via Universal\n  Robust Embeddings","summary":"  Research on adversarial robustness is primarily focused on image and text\ndata. Yet, many scenarios in which lack of robustness can result in serious\nrisks, such as fraud detection, medical diagnosis, or recommender systems often\ndo not rely on images or text but instead on tabular data. Adversarial\nrobustness in tabular data poses two serious challenges. First, tabular\ndatasets often contain categorical features, and therefore cannot be tackled\ndirectly with existing optimization procedures. Second, in the tabular domain,\nalgorithms that are not based on deep networks are widely used and offer great\nperformance, but algorithms to enhance robustness are tailored to neural\nnetworks (e.g. adversarial training).\n  In this paper, we tackle both challenges. We present a method that allows us\nto train adversarially robust deep networks for tabular data and to transfer\nthis robustness to other classifiers via universal robust embeddings tailored\nto categorical data. These embeddings, created using a bilevel alternating\nminimization framework, can be transferred to boosted trees or random forests\nmaking them robust without the need for adversarial training while preserving\ntheir high accuracy on tabular data. We show that our methods outperform\nexisting techniques within a practical threat model suitable for tabular data.\n","authors":["Klim Kireev","Maksym Andriushchenko","Carmela Troncoso","Nicolas Flammarion"],"pdf_url":"https://arxiv.org/pdf/2306.04064v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.08742v3","updated":"2023-12-13T12:00:26Z","published":"2023-08-17T02:33:43Z","title":"PMET: Precise Model Editing in a Transformer","summary":"  Model editing techniques modify a minor proportion of knowledge in Large\nLanguage Models (LLMs) at a relatively low cost, which have demonstrated\nnotable success. Existing methods assume Transformer Layer (TL) hidden states\nare values of key-value memories of the Feed-Forward Network (FFN). They\nusually optimize the TL hidden states to memorize target knowledge and use it\nto update the weights of the FFN in LLMs. However, the information flow of TL\nhidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN,\nand residual connections. Existing methods neglect the fact that the TL hidden\nstates contains information not specifically required for FFN. Consequently,\nthe performance of model editing decreases. To achieve more precise model\nediting, we analyze hidden states of MHSA and FFN, finding that MHSA encodes\ncertain general knowledge extraction patterns. This implies that MHSA weights\ndo not require updating when new knowledge is introduced. Based on above\nfindings, we introduce PMET, which simultaneously optimizes Transformer\nComponent (TC, namely MHSA and FFN) hidden states, while only using the\noptimized TC hidden states of FFN to precisely update FFN weights. Our\nexperiments demonstrate that PMET exhibits state-of-the-art performance on both\nthe COUNTERFACT and zsRE datasets. Our ablation experiments substantiate the\neffectiveness of our enhancements, further reinforcing the finding that the\nMHSA encodes certain general knowledge extraction patterns and indicating its\nstorage of a small amount of factual knowledge. Our code is available at\nhttps://github.com/xpq-tech/PMET.\n","authors":["Xiaopeng Li","Shasha Li","Shezheng Song","Jing Yang","Jun Ma","Jie Yu"],"pdf_url":"https://arxiv.org/pdf/2308.08742v3.pdf","comment":"Accepted in AAAI24"},{"id":"http://arxiv.org/abs/2312.08083v1","updated":"2023-12-13T11:57:15Z","published":"2023-12-13T11:57:15Z","title":"Training of Neural Networks with Uncertain Data, A Mixture of Experts\n  Approach","summary":"  This paper presents the \"Uncertainty-aware Mixture of Experts\" (uMoE), a\nnovel approach designed to address aleatoric uncertainty in the training of\npredictive models based on Neural Networks (NNs). While existing methods\nprimarily focus on managing uncertainty during infer-ence, uMoE integrates\nuncertainty directly into the train-ing process. The uMoE approach adopts a\n\"Divide and Conquer\" paradigm to partition the uncertain input space into more\nmanageable subspaces. It consists of Expert components, each trained solely on\nthe portion of input uncertainty corresponding to their subspace. On top of the\nExperts, a Gating Unit, guided by additional infor-mation about the\ndistribution of uncertain inputs across these subspaces, learns to weight the\nExperts to minimize deviations from the ground truth. Our results highlight\nthat uMoE significantly outperforms baseline methods in handling data\nuncertainty. Furthermore, we conducted a robustness analysis, illustrating its\ncapability to adapt to varying levels of uncertainty and suggesting optimal\nthreshold parameters. This innovative approach holds wide applicability across\ndiverse data-driven domains, in-cluding biomedical signal processing,\nautonomous driv-ing, and production quality control.\n","authors":["Lucas Luttner"],"pdf_url":"https://arxiv.org/pdf/2312.08083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08075v1","updated":"2023-12-13T11:39:56Z","published":"2023-12-13T11:39:56Z","title":"TERM Model: Tensor Ring Mixture Model for Density Estimation","summary":"  Efficient probability density estimation is a core challenge in statistical\nmachine learning. Tensor-based probabilistic graph methods address\ninterpretability and stability concerns encountered in neural network\napproaches. However, a substantial number of potential tensor permutations can\nlead to a tensor network with the same structure but varying expressive\ncapabilities. In this paper, we take tensor ring decomposition for density\nestimator, which significantly reduces the number of permutation candidates\nwhile enhancing expressive capability compared with existing used\ndecompositions. Additionally, a mixture model that incorporates multiple\npermutation candidates with adaptive weights is further designed, resulting in\nincreased expressive flexibility and comprehensiveness. Different from the\nprevailing directions of tensor network structure/permutation search, our\napproach provides a new viewpoint inspired by ensemble learning. This approach\nacknowledges that suboptimal permutations can offer distinctive information\nbesides that of optimal permutations. Experiments show the superiority of the\nproposed approach in estimating probability density for moderately dimensional\ndatasets and sampling to capture intricate details.\n","authors":["Ruituo Wu","Jiani Liu","Ce Zhu","Anh-Huy Phan","Ivan V. Oseledets","Yipeng Liu"],"pdf_url":"https://arxiv.org/pdf/2312.08075v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02462v2","updated":"2023-12-13T11:29:21Z","published":"2023-12-05T03:25:45Z","title":"Dimensionality Reduction and Dynamical Mode Recognition of Circular\n  Arrays of Flame Oscillators Using Deep Neural Network","summary":"  Oscillatory combustion in aero engines and modern gas turbines often has\nsignificant adverse effects on their operation, and accurately recognizing\nvarious oscillation modes is the prerequisite for understanding and controlling\ncombustion instability. However, the high-dimensional spatial-temporal data of\na complex combustion system typically poses considerable challenges to the\ndynamical mode recognition. Based on a two-layer bidirectional long short-term\nmemory variational autoencoder (Bi-LSTM-VAE) dimensionality reduction model and\na two-dimensional Wasserstein distance-based classifier (WDC), this study\nproposes a promising method (Bi-LSTM-VAE-WDC) for recognizing dynamical modes\nin oscillatory combustion systems. Specifically, the Bi-LSTM-VAE dimension\nreduction model was introduced to reduce the high-dimensional spatial-temporal\ndata of the combustion system to a low-dimensional phase space; Gaussian kernel\ndensity estimates (GKDE) were computed based on the distribution of phase\npoints in a grid; two-dimensional WD values were calculated from the GKDE maps\nto recognize the oscillation modes. The time-series data used in this study\nwere obtained from numerical simulations of circular arrays of laminar flame\noscillators. The results show that the novel Bi-LSTM-VAE method can produce a\nnon-overlapping distribution of phase points, indicating an effective\nunsupervised mode recognition and classification. Furthermore, the present\nmethod exhibits a more prominent performance than VAE and PCA (principal\ncomponent analysis) for distinguishing dynamical modes in complex flame\nsystems, implying its potential in studying turbulent combustion.\n","authors":["Weiming Xu","Tao Yang","Peng Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.02462v2.pdf","comment":"research paper (18 pages, 1 table 10 figures) with supplementary\n  material (8 pages, 1 table, 5 figures)"},{"id":"http://arxiv.org/abs/2312.08066v1","updated":"2023-12-13T11:20:09Z","published":"2023-12-13T11:20:09Z","title":"A Novel Metric for Measuring Data Quality in Classification Applications\n  (extended version)","summary":"  Data quality is a key element for building and optimizing good learning\nmodels. Despite many attempts to characterize data quality, there is still a\nneed for rigorous formalization and an efficient measure of the quality from\navailable observations. Indeed, without a clear understanding of the training\nand testing processes, it is hard to evaluate the intrinsic performance of a\nmodel. Besides, tools allowing to measure data quality specific to machine\nlearning are still lacking. In this paper, we introduce and explain a novel\nmetric to measure data quality. This metric is based on the correlated\nevolution between the classification performance and the deterioration of data.\nThe proposed method has the major advantage of being model-independent.\nFurthermore, we provide an interpretation of each criterion and examples of\nassessment levels. We confirm the utility of the proposed metric with intensive\nnumerical experiments and detail some illustrative cases with controlled and\ninterpretable qualities.\n","authors":["Jouseau Roxane","Salva Sébastien","Samir Chafik"],"pdf_url":"https://arxiv.org/pdf/2312.08066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08063v1","updated":"2023-12-13T11:17:27Z","published":"2023-12-13T11:17:27Z","title":"Estimation of Concept Explanations Should be Uncertainty Aware","summary":"  Model explanations are very valuable for interpreting and debugging\nprediction models. We study a specific kind of global explanations called\nConcept Explanations, where the goal is to interpret a model using\nhuman-understandable concepts. Recent advances in multi-modal learning\nrekindled interest in concept explanations and led to several label-efficient\nproposals for estimation. However, existing estimation methods are unstable to\nthe choice of concepts or dataset that is used for computing explanations. We\nobserve that instability in explanations is due to high variance in point\nestimation of importance scores. We propose an uncertainty aware Bayesian\nestimation method, which readily improved reliability of the concept\nexplanations. We demonstrate with theoretical analysis and empirical evaluation\nthat explanations computed by our method are more reliable while also being\nlabel-efficient and faithful.\n","authors":["Vihari Piratla","Juyeon Heo","Sukriti Singh","Adrian Weller"],"pdf_url":"https://arxiv.org/pdf/2312.08063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08057v1","updated":"2023-12-13T11:08:25Z","published":"2023-12-13T11:08:25Z","title":"Combinatorial Stochastic-Greedy Bandit","summary":"  We propose a novel combinatorial stochastic-greedy bandit (SGB) algorithm for\ncombinatorial multi-armed bandit problems when no extra information other than\nthe joint reward of the selected set of $n$ arms at each time step $t\\in [T]$\nis observed. SGB adopts an optimized stochastic-explore-then-commit approach\nand is specifically designed for scenarios with a large set of base arms.\nUnlike existing methods that explore the entire set of unselected base arms\nduring each selection step, our SGB algorithm samples only an optimized\nproportion of unselected arms and selects actions from this subset. We prove\nthat our algorithm achieves a $(1-1/e)$-regret bound of\n$\\mathcal{O}(n^{\\frac{1}{3}} k^{\\frac{2}{3}} T^{\\frac{2}{3}}\n\\log(T)^{\\frac{2}{3}})$ for monotone stochastic submodular rewards, which\noutperforms the state-of-the-art in terms of the cardinality constraint $k$.\nFurthermore, we empirically evaluate the performance of our algorithm in the\ncontext of online constrained social influence maximization. Our results\ndemonstrate that our proposed approach consistently outperforms the other\nalgorithms, increasing the performance gap as $k$ grows.\n","authors":["Fares Fourati","Christopher John Quinn","Mohamed-Slim Alouini","Vaneet Aggarwal"],"pdf_url":"https://arxiv.org/pdf/2312.08057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06260v2","updated":"2023-12-13T11:05:25Z","published":"2023-09-12T14:20:12Z","title":"Toward Discretization-Consistent Closure Schemes for Large Eddy\n  Simulation Using Reinforcement Learning","summary":"  This study proposes a novel method for developing discretization-consistent\nclosure schemes for implicitly filtered Large Eddy Simulation (LES). Here, the\ninduced filter kernel, and thus the closure terms, are determined by the\nproperties of the grid and the discretization operator, leading to additional\ncomputational subgrid terms that are generally unknown in a priori analysis. In\nthis work, the task of adapting the coefficients of LES closure models is thus\nframed as a Markov decision process and solved in an a posteriori manner with\nReinforcement Learning (RL). This optimization framework is applied to both\nexplicit and implicit closure models. The explicit model is based on an\nelement-local eddy viscosity model. The optimized model is found to adapt its\ninduced viscosity within discontinuous Galerkin (DG) methods to homogenize the\ndissipation within an element by adding more viscosity near its center. For the\nimplicit modeling, RL is applied to identify an optimal blending strategy for a\nhybrid DG and Finite Volume (FV) scheme. The resulting optimized discretization\nyields more accurate results in LES than either the pure DG or FV method and\nrenders itself as a viable modeling ansatz that could initiate a novel class of\nhigh-order schemes for compressible turbulence by combining turbulence modeling\nwith shock capturing in a single framework. All newly derived models achieve\naccurate results that either match or outperform traditional models for\ndifferent discretizations and resolutions. Overall, the results demonstrate\nthat the proposed RL optimization can provide discretization-consistent\nclosures that could reduce the uncertainty in implicitly filtered LES.\n","authors":["Andrea Beck","Marius Kurz"],"pdf_url":"https://arxiv.org/pdf/2309.06260v2.pdf","comment":"24 pages, 14 figures. Accepted Manuscript. This article may be\n  downloaded for personal use only. Any other use requires prior permission of\n  the author and AIP Publishing. This article appeared in Physics of Fluids 35\n  (2023) and may be found at https://doi.org/10.1063/5.0176223"},{"id":"http://arxiv.org/abs/2312.08055v1","updated":"2023-12-13T11:02:19Z","published":"2023-12-13T11:02:19Z","title":"Breaking the Silence: the Threats of Using LLMs in Software Engineering","summary":"  Large Language Models (LLMs) have gained considerable traction within the\nSoftware Engineering (SE) community, impacting various SE tasks from code\ncompletion to test generation, from program repair to code summarization.\nDespite their promise, researchers must still be careful as numerous intricate\nfactors can influence the outcomes of experiments involving LLMs. This paper\ninitiates an open discussion on potential threats to the validity of LLM-based\nresearch including issues such as closed-source models, possible data leakage\nbetween LLM training data and research evaluation, and the reproducibility of\nLLM-based findings. In response, this paper proposes a set of guidelines\ntailored for SE researchers and Language Model (LM) providers to mitigate these\nconcerns. The implications of the guidelines are illustrated using existing\ngood practices followed by LLM providers and a practical example for SE\nresearchers in the context of test case generation.\n","authors":["June Sallou","Thomas Durieux","Annibale Panichella"],"pdf_url":"https://arxiv.org/pdf/2312.08055v1.pdf","comment":"Accepted at the ICSE'24 conference, NIER track"},{"id":"http://arxiv.org/abs/2312.08053v1","updated":"2023-12-13T11:00:48Z","published":"2023-12-13T11:00:48Z","title":"Kimad: Adaptive Gradient Compression with Bandwidth Awareness","summary":"  In distributed training, communication often emerges as a bottleneck. In\nresponse, we introduce Kimad, a solution that offers adaptive gradient\ncompression. By consistently monitoring bandwidth, Kimad refines compression\nratios to match specific neural network layer requirements. Our exhaustive\ntests and proofs confirm Kimad's outstanding performance, establishing it as a\nbenchmark in adaptive compression for distributed deep learning.\n","authors":["Jihao Xin","Ivan Ilin","Shunkang Zhang","Marco Canini","Peter Richtárik"],"pdf_url":"https://arxiv.org/pdf/2312.08053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08052v1","updated":"2023-12-13T10:59:54Z","published":"2023-12-13T10:59:54Z","title":"Explainable Trajectory Representation through Dictionary Learning","summary":"  Trajectory representation learning on a network enhances our understanding of\nvehicular traffic patterns and benefits numerous downstream applications.\nExisting approaches using classic machine learning or deep learning embed\ntrajectories as dense vectors, which lack interpretability and are inefficient\nto store and analyze in downstream tasks. In this paper, an explainable\ntrajectory representation learning framework through dictionary learning is\nproposed. Given a collection of trajectories on a network, it extracts a\ncompact dictionary of commonly used subpaths called \"pathlets\", which optimally\nreconstruct each trajectory by simple concatenations. The resulting\nrepresentation is naturally sparse and encodes strong spatial semantics.\nTheoretical analysis of our proposed algorithm is conducted to provide a\nprobabilistic bound on the estimation error of the optimal dictionary. A\nhierarchical dictionary learning scheme is also proposed to ensure the\nalgorithm's scalability on large networks, leading to a multi-scale trajectory\nrepresentation. Our framework is evaluated on two large-scale real-world taxi\ndatasets. Compared to previous work, the dictionary learned by our method is\nmore compact and has better reconstruction rate for new trajectories. We also\ndemonstrate the promising performance of this method in downstream tasks\nincluding trip time prediction task and data compression.\n","authors":["Yuanbo Tang","Zhiyuan Peng","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2312.08052v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.10705v5","updated":"2023-12-13T10:43:45Z","published":"2023-07-20T08:53:47Z","title":"TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and\n  Lane Segmentation in Self-Driving Cars","summary":"  Semantic segmentation is a common task in autonomous driving to understand\nthe surrounding environment. Driveable Area Segmentation and Lane Detection are\nparticularly important for safe and efficient navigation on the road. However,\noriginal semantic segmentation models are computationally expensive and require\nhigh-end hardware, which is not feasible for embedded systems in autonomous\nvehicles. This paper proposes a lightweight model for the driveable area and\nlane line segmentation. TwinLiteNet is designed cheaply but achieves accurate\nand efficient segmentation results. We evaluate TwinLiteNet on the BDD100K\ndataset and compare it with modern models. Experimental results show that our\nTwinLiteNet performs similarly to existing approaches, requiring significantly\nfewer computational resources. Specifically, TwinLiteNet achieves a mIoU score\nof 91.3% for the Drivable Area task and 31.08% IoU for the Lane Detection task\nwith only 0.4 million parameters and achieves 415 FPS on GPU RTX A5000.\nFurthermore, TwinLiteNet can run in real-time on embedded devices with limited\ncomputing power, especially since it achieves 60FPS on Jetson Xavier NX, making\nit an ideal solution for self-driving vehicles. Code is available:\nurl{https://github.com/chequanghuy/TwinLiteNet}.\n","authors":["Quang Huy Che","Dinh Phuc Nguyen","Minh Quan Pham","Duc Khai Lam"],"pdf_url":"https://arxiv.org/pdf/2307.10705v5.pdf","comment":"Accepted by MAPR 2023"},{"id":"http://arxiv.org/abs/2312.08034v1","updated":"2023-12-13T10:21:00Z","published":"2023-12-13T10:21:00Z","title":"Individualized Deepfake Detection Exploiting Traces Due to Double\n  Neural-Network Operations","summary":"  In today's digital landscape, journalists urgently require tools to verify\nthe authenticity of facial images and videos depicting specific public figures\nbefore incorporating them into news stories. Existing deepfake detectors are\nnot optimized for this detection task when an image is associated with a\nspecific and identifiable individual. This study focuses on the deepfake\ndetection of facial images of individual public figures. We propose to\ncondition the proposed detector on the identity of the identified individual\ngiven the advantages revealed by our theory-driven simulations. While most\ndetectors in the literature rely on perceptible or imperceptible artifacts\npresent in deepfake facial images, we demonstrate that the detection\nperformance can be improved by exploiting the idempotency property of neural\nnetworks. In our approach, the training process involves double neural-network\noperations where we pass an authentic image through a deepfake simulating\nnetwork twice. Experimental results show that the proposed method improves the\narea under the curve (AUC) from 0.92 to 0.94 and reduces its standard deviation\nby 17\\%. For evaluating the detection performance of individual public figures,\na facial image dataset with individuals' names is required, a criterion not met\nby the current deepfake datasets. To address this, we curated a dataset\ncomprising 32k images featuring 45 public figures, which we intend to release\nto the public after the paper is published.\n","authors":["Mushfiqur Rahman","Runze Liu","Chau-Wai Wong","Huaiyu Dai"],"pdf_url":"https://arxiv.org/pdf/2312.08034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08033v1","updated":"2023-12-13T10:19:58Z","published":"2023-12-13T10:19:58Z","title":"Beyond Top-Class Agreement: Using Divergences to Forecast Performance\n  under Distribution Shift","summary":"  Knowing if a model will generalize to data 'in the wild' is crucial for safe\ndeployment. To this end, we study model disagreement notions that consider the\nfull predictive distribution - specifically disagreement based on Hellinger\ndistance, Jensen-Shannon and Kullback-Leibler divergence. We find that\ndivergence-based scores provide better test error estimates and detection rates\non out-of-distribution data compared to their top-1 counterparts. Experiments\ninvolve standard vision and foundation models.\n","authors":["Mona Schirmer","Dan Zhang","Eric Nalisnick"],"pdf_url":"https://arxiv.org/pdf/2312.08033v1.pdf","comment":"Workshop on Distribution Shifts, 37th Conference on Neural\n  Information Processing Systems (NeurIPS 2023)"},{"id":"http://arxiv.org/abs/2312.08029v1","updated":"2023-12-13T10:04:06Z","published":"2023-12-13T10:04:06Z","title":"ClusterDDPM: An EM clustering framework with Denoising Diffusion\n  Probabilistic Models","summary":"  Variational autoencoder (VAE) and generative adversarial networks (GAN) have\nfound widespread applications in clustering and have achieved significant\nsuccess. However, the potential of these approaches may be limited due to VAE's\nmediocre generation capability or GAN's well-known instability during\nadversarial training. In contrast, denoising diffusion probabilistic models\n(DDPMs) represent a new and promising class of generative models that may\nunlock fresh dimensions in clustering. In this study, we introduce an\ninnovative expectation-maximization (EM) framework for clustering using DDPMs.\nIn the E-step, we aim to derive a mixture of Gaussian priors for the subsequent\nM-step. In the M-step, our focus lies in learning clustering-friendly latent\nrepresentations for the data by employing the conditional DDPM and matching the\ndistribution of latent representations to the mixture of Gaussian priors. We\npresent a rigorous theoretical analysis of the optimization process in the\nM-step, proving that the optimizations are equivalent to maximizing the lower\nbound of the Q function within the vanilla EM framework under certain\nconstraints. Comprehensive experiments validate the advantages of the proposed\nframework, showcasing superior performance in clustering, unsupervised\nconditional generation and latent representation learning.\n","authors":["Jie Yan","Jing Liu","Zhong-yuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.08029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08021v1","updated":"2023-12-13T09:49:53Z","published":"2023-12-13T09:49:53Z","title":"Improving search relevance of Azure Cognitive Search by Bayesian\n  optimization","summary":"  Azure Cognitive Search (ACS) has emerged as a major contender in \"Search as a\nService\" cloud products in recent years. However, one of the major challenges\nfor ACS users is to improve the relevance of the search results for their\nspecific usecases. In this paper, we propose a novel method to find the optimal\nACS configuration that maximizes search relevance for a specific usecase\n(product search, document search...) The proposed solution improves key online\nmarketplace metrics such as click through rates (CTR) by formulating the search\nrelevance problem as hyperparameter tuning. We have observed significant\nimprovements in real-world search call to action (CTA) rate in multiple\nmarketplaces by introducing optimized weights generated from the proposed\napproach.\n","authors":["Nitin Agarwal","Ashish Kumar","Kiran R","Manish Gupta","Laurent Boué"],"pdf_url":"https://arxiv.org/pdf/2312.08021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08016v1","updated":"2023-12-13T09:39:32Z","published":"2023-12-13T09:39:32Z","title":"Secure Deep Reinforcement Learning for Dynamic Resource Allocation in\n  Wireless MEC Networks","summary":"  This paper proposes a blockchain-secured deep reinforcement learning (BC-DRL)\noptimization framework for {data management and} resource allocation in\ndecentralized {wireless mobile edge computing (MEC)} networks. In our\nframework, {we design a low-latency reputation-based proof-of-stake (RPoS)\nconsensus protocol to select highly reliable blockchain-enabled BSs to securely\nstore MEC user requests and prevent data tampering attacks.} {We formulate the\nMEC resource allocation optimization as a constrained Markov decision process\nthat balances minimum processing latency and denial-of-service (DoS)\nprobability}. {We use the MEC aggregated features as the DRL input to\nsignificantly reduce the high-dimensionality input of the remaining service\nprocessing time for individual MEC requests. Our designed constrained DRL\neffectively attains the optimal resource allocations that are adapted to the\ndynamic DoS requirements. We provide extensive simulation results and analysis\nto} validate that our BC-DRL framework achieves higher security, reliability,\nand resource utilization efficiency than benchmark blockchain consensus\nprotocols and {MEC} resource allocation algorithms.\n","authors":["Xin Hao","Phee Lep Yeoh","Changyang She","Branka Vucetic","Yonghui Li"],"pdf_url":"https://arxiv.org/pdf/2312.08016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08010v1","updated":"2023-12-13T09:33:08Z","published":"2023-12-13T09:33:08Z","title":"EZ-CLIP: Efficient Zeroshot Video Action Recognition","summary":"  Recent advancements in large-scale pre-training of visual-language models on\npaired image-text data have demonstrated impressive generalization capabilities\nfor zero-shot tasks. Building on this success, efforts have been made to adapt\nthese image-based visual-language models, such as CLIP, for videos extending\ntheir zero-shot capabilities to the video domain. While these adaptations have\nshown promising results, they come at a significant computational cost and\nstruggle with effectively modeling the crucial temporal aspects inherent to the\nvideo domain. In this study, we present EZ-CLIP, a simple and efficient\nadaptation of CLIP that addresses these challenges. EZ-CLIP leverages temporal\nvisual prompting for seamless temporal adaptation, requiring no fundamental\nalterations to the core CLIP architecture while preserving its remarkable\ngeneralization abilities. Moreover, we introduce a novel learning objective\nthat guides the temporal visual prompts to focus on capturing motion, thereby\nenhancing its learning capabilities from video data. We conducted extensive\nexperiments on five different benchmark datasets, thoroughly evaluating EZ-CLIP\nfor zero-shot learning and base-to-novel video action recognition, and also\ndemonstrating its potential for few-shot generalization.Impressively, with a\nmere 5.2 million learnable parameters (as opposed to the 71.1 million in the\nprior best model), EZ-CLIP can be efficiently trained on a single GPU,\noutperforming existing approaches in several evaluations.\n","authors":["Shahzad Ahmad","Sukalpa Chanda","Yogesh S Rawat"],"pdf_url":"https://arxiv.org/pdf/2312.08010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08008v1","updated":"2023-12-13T09:31:30Z","published":"2023-12-13T09:31:30Z","title":"Learning Nash Equilibria in Zero-Sum Markov Games: A Single Time-scale\n  Algorithm Under Weak Reachability","summary":"  We consider decentralized learning for zero-sum games, where players only see\ntheir payoff information and are agnostic to actions and payoffs of the\nopponent. Previous works demonstrated convergence to a Nash equilibrium in this\nsetting using double time-scale algorithms under strong reachability\nassumptions. We address the open problem of achieving an approximate Nash\nequilibrium efficiently with an uncoupled and single time-scale algorithm under\nweaker conditions. Our contribution is a rational and convergent algorithm,\nutilizing Tsallis-entropy regularization in a value-iteration-based approach.\nThe algorithm learns an approximate Nash equilibrium in polynomial time,\nrequiring only the existence of a policy pair that induces an irreducible and\naperiodic Markov chain, thus considerably weakening past assumptions. Our\nanalysis leverages negative drift inequalities and introduces novel properties\nof Tsallis entropy that are of independent interest.\n","authors":["Reda Ouhamma","Maryam Kamgarpour"],"pdf_url":"https://arxiv.org/pdf/2312.08008v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2303.03100 by other authors"},{"id":"http://arxiv.org/abs/2312.07991v1","updated":"2023-12-13T09:03:01Z","published":"2023-12-13T09:03:01Z","title":"Accelerating the Global Aggregation of Local Explanations","summary":"  Local explanation methods highlight the input tokens that have a considerable\nimpact on the outcome of classifying the document at hand. For example, the\nAnchor algorithm applies a statistical analysis of the sensitivity of the\nclassifier to changes in the token. Aggregating local explanations over a\ndataset provides a global explanation of the model. Such aggregation aims to\ndetect words with the most impact, giving valuable insights about the model,\nlike what it has learned in training and which adversarial examples expose its\nweaknesses. However, standard aggregation methods bear a high computational\ncost: a na\\\"ive implementation applies a costly algorithm to each token of each\ndocument, and hence, it is infeasible for a simple user running in the scope of\na short analysis session. % We devise techniques for accelerating the global\naggregation of the Anchor algorithm. Specifically, our goal is to compute a set\nof top-$k$ words with the highest global impact according to different\naggregation functions. Some of our techniques are lossless and some are lossy.\nWe show that for a very mild loss of quality, we are able to accelerate the\ncomputation by up to 30$\\times$, reducing the computation from hours to\nminutes. We also devise and study a probabilistic model that accounts for noise\nin the Anchor algorithm and diminishes the bias toward words that are frequent\nyet low in impact.\n","authors":["Alon Mor","Yonatan Belinkov","Benny Kimelfeld"],"pdf_url":"https://arxiv.org/pdf/2312.07991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07987v1","updated":"2023-12-13T09:00:21Z","published":"2023-12-13T09:00:21Z","title":"SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention","summary":"  The costly self-attention layers in modern Transformers require memory and\ncompute quadratic in sequence length. Existing approximation methods usually\nunderperform and fail to obtain significant speedups in practice. Here we\npresent SwitchHead - a novel method that reduces both compute and memory\nrequirements and achieves wall-clock speedup, while matching the language\nmodeling performance of baseline Transformers with the same parameter budget.\nSwitchHead uses Mixture-of-Experts (MoE) layers for the value and output\nprojections and requires 4 to 8 times fewer attention matrices than standard\nTransformers. Our novel attention can also be combined with MoE MLP layers,\nresulting in an efficient fully-MoE \"SwitchHead\" Transformer model. Our code is\npublic.\n","authors":["Róbert Csordás","Piotr Piękos","Kazuki Irie"],"pdf_url":"https://arxiv.org/pdf/2312.07987v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07983v1","updated":"2023-12-13T08:57:15Z","published":"2023-12-13T08:57:15Z","title":"Multi-perspective Feedback-attention Coupling Model for Continuous-time\n  Dynamic Graphs","summary":"  Recently, representation learning over graph networks has gained popularity,\nwith various models showing promising results. Despite this, several challenges\npersist: 1) most methods are designed for static or discrete-time dynamic\ngraphs; 2) existing continuous-time dynamic graph algorithms focus on a single\nevolving perspective; and 3) many continuous-time dynamic graph approaches\nnecessitate numerous temporal neighbors to capture long-term dependencies. In\nresponse, this paper introduces the Multi-Perspective Feedback-Attention\nCoupling (MPFA) model. MPFA incorporates information from both evolving and raw\nperspectives, efficiently learning the interleaved dynamics of observed\nprocesses. The evolving perspective employs temporal self-attention to\ndistinguish continuously evolving temporal neighbors for information\naggregation. Through dynamic updates, this perspective can capture long-term\ndependencies using a small number of temporal neighbors. Meanwhile, the raw\nperspective utilizes a feedback attention module with growth characteristic\ncoefficients to aggregate raw neighborhood information. Experimental results on\na self-organizing dataset and seven public datasets validate the efficacy and\ncompetitiveness of our proposed model.\n","authors":["Xiaobo Zhu","Yan Wu","Zhipeng Li","Hailong Su","Jin Che","Zhanheng Chen","Liying Wang"],"pdf_url":"https://arxiv.org/pdf/2312.07983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07981v1","updated":"2023-12-13T08:53:37Z","published":"2023-12-13T08:53:37Z","title":"Time Series Diffusion Method: A Denoising Diffusion Probabilistic Model\n  for Vibration Signal Generation","summary":"  Diffusion models have demonstrated robust data generation capabilities in\nvarious research fields. In this paper, a Time Series Diffusion Method (TSDM)\nis proposed for vibration signal generation, leveraging the foundational\nprinciples of diffusion models. The TSDM uses an improved U-net architecture\nwith attention block to effectively segment and extract features from\none-dimensional time series data. It operates based on forward diffusion and\nreverse denoising processes for time-series generation. Experimental validation\nis conducted using single-frequency, multi-frequency datasets, and bearing\nfault datasets. The results show that TSDM can accurately generate the\nsingle-frequency and multi-frequency features in the time series and retain the\nbasic frequency features for the diffusion generation results of the bearing\nfault series. Finally, TSDM is applied to the small sample fault diagnosis of\nthree public bearing fault datasets, and the results show that the accuracy of\nsmall sample fault diagnosis of the three datasets is improved by 32.380%,\n18.355% and 9.298% at most, respectively\n","authors":["Haiming Yi","Lei Hou","Yuhong Jin","Nasser A. Saeed"],"pdf_url":"https://arxiv.org/pdf/2312.07981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07979v1","updated":"2023-12-13T08:50:02Z","published":"2023-12-13T08:50:02Z","title":"SLJP: Semantic Extraction based Legal Judgment Prediction","summary":"  Legal Judgment Prediction (LJP) is a judicial assistance system that\nrecommends the legal components such as applicable statues, prison term and\npenalty term by analyzing the given input case document. Indian legal system is\nin the need of technical assistance such as artificial intelligence to solve\nthe crores of pending cases in various courts for years and its being increased\nday to day. Most of the existing Indian models did not adequately concentrate\non the semantics embedded in the fact description (FD) that impacts the\ndecision. The proposed semantic extraction based LJP (SLJP) model provides the\nadvantages of pretrained transformers for complex unstructured legal case\ndocument understanding and to generate embeddings. The model draws the in-depth\nsemantics of the given FD at multiple levels i.e., chunk and case document\nlevel by following the divide and conquer approach. It creates the concise view\nof the given fact description using the extracted semantics as per the original\ncourt case document structure and predicts judgment using attention mechanism.\nWe tested the model performance on two available Indian datasets Indian Legal\nDocuments corpus (ILDC) and Indian Legal Statue Identification (ILSI) and got\npromising results. Also shown the highest performance and less performance\ndegradation for increased epochs than base models on ILDC dataset.\n","authors":["Prameela Madambakam","Shathanaa Rajmohan","Himangshu Sharma","Tummepalli Anka Chandrahas Purushotham Gupta"],"pdf_url":"https://arxiv.org/pdf/2312.07979v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07977v1","updated":"2023-12-13T08:47:50Z","published":"2023-12-13T08:47:50Z","title":"Modeling non-genetic information dynamics in cells using reservoir\n  computing","summary":"  Virtually all cells use energy and ion-specific membrane pumps to maintain\nlarge transmembrane gradients of Na$^+$, K$^+$, Cl$^-$, Mg$^{++}$, and\nCa$^{++}$. Although they consume up to 1/3 of a cell's energy budget, the\ncorresponding evolutionary benefit of transmembrane ion gradients remain\nunclear. Here, we propose that ion gradients enable a dynamic and versatile\nbiological system that acquires, analyzes, and responds to environmental\ninformation. We hypothesize environmental signals are transmitted into the cell\nby ion fluxes along pre-existing gradients through gated ion-specific membrane\nchannels. The consequent changes of cytoplasmic ion concentration can generate\na local response and orchestrate global or regional responses through wire-like\nion fluxes along pre-existing and self-assembling cytoskeleton to engage the\nendoplasmic reticulum, mitochondria, and nucleus.\n  Here, we frame our hypothesis through a quasi-physical (Cell-Reservoir) model\nthat treats intra-cellular ion-based information dynamics as a sub-cellular\nprocess permitting spatiotemporally resolved cellular response that is also\ncapable of learning complex nonlinear dynamical cellular behavior. We\ndemonstrate the proposed ion dynamics permits rapid dissemination of response\nto information extrinsic perturbations that is consistent with experimental\nobservations.\n","authors":["Dipesh Niraula","Issam El Naqa","Jack Adam Tuszynski","Robert A. Gatenby"],"pdf_url":"https://arxiv.org/pdf/2312.07977v1.pdf","comment":"Main text: 18 pages, 1 table, and 8 figures; Supplementary materials:\n  14 pages, 18 figures; Link to Source code and Data included"},{"id":"http://arxiv.org/abs/2107.10756v4","updated":"2023-12-13T08:44:16Z","published":"2021-07-22T15:42:25Z","title":"Semantic Text-to-Face GAN -ST^2FG","summary":"  Faces generated using generative adversarial networks (GANs) have reached\nunprecedented realism. These faces, also known as \"Deep Fakes\", appear as\nrealistic photographs with very little pixel-level distortions. While some work\nhas enabled the training of models that lead to the generation of specific\nproperties of the subject, generating a facial image based on a natural\nlanguage description has not been fully explored. For security and criminal\nidentification, the ability to provide a GAN-based system that works like a\nsketch artist would be incredibly useful. In this paper, we present a novel\napproach to generate facial images from semantic text descriptions. The learned\nmodel is provided with a text description and an outline of the type of face,\nwhich the model uses to sketch the features. Our models are trained using an\nAffine Combination Module (ACM) mechanism to combine the text embedding from\nBERT and the GAN latent space using a self-attention matrix. This avoids the\nloss of features due to inadequate \"attention\", which may happen if text\nembedding and latent vector are simply concatenated. Our approach is capable of\ngenerating images that are very accurately aligned to the exhaustive textual\ndescriptions of faces with many fine detail features of the face and helps in\ngenerating better images. The proposed method is also capable of making\nincremental changes to a previously generated image if it is provided with\nadditional textual descriptions or sentences.\n","authors":["Manan Oza","Sukalpa Chanda","David Doermann"],"pdf_url":"https://arxiv.org/pdf/2107.10756v4.pdf","comment":"Experiments needs to be redone"},{"id":"http://arxiv.org/abs/2312.07965v1","updated":"2023-12-13T08:28:21Z","published":"2023-12-13T08:28:21Z","title":"Pneumonia Detection on chest X-ray images Using Ensemble of Deep\n  Convolutional Neural Networks","summary":"  Pneumonia is a life-threatening lung infection resulting from several\ndifferent viral infections. Identifying and treating pneumonia on chest X-ray\nimages can be difficult due to its similarity to other pulmonary diseases.\nThus, the existing methods for predicting pneumonia cannot attain substantial\nlevels of accuracy. Therefore, this paper presents a computer-aided\nclassification of pneumonia, coined as Ensemble Learning (EL), to simplify the\ndiagnosis process on chest X-ray images. Our proposal is based on Convolutional\nNeural Network (CNN) models, which are pre-trained CNN models that have been\nrecently employed to enhance the performance of many medical tasks instead of\ntraining CNN models from scratch. We propose to use three well-known CNN\npre-trained (DenseNet169, MobileNetV2 and Vision Transformer) using the\nImageNet database. Then, these models are trained on the chest X-ray data set\nusing fine-tuning. Finally, the results are obtained by combining the extracted\nfeatures from these three models during the experimental phase. The proposed EL\napproach outperforms other existing state-of-the-art methods, and it obtains an\naccuracy of 93.91% and a F1-Score of 93.88% on the testing phase.\n","authors":["Alhassan Mabrouk","Rebeca P. Díaz Redondo","Abdelghani Dahou","Mohamed Abd Elaziz","Mohammed Kayed"],"pdf_url":"https://arxiv.org/pdf/2312.07965v1.pdf","comment":"14 pages, 4 figures, journal"},{"id":"http://arxiv.org/abs/2312.07955v1","updated":"2023-12-13T08:01:15Z","published":"2023-12-13T08:01:15Z","title":"Erasing Self-Supervised Learning Backdoor by Cluster Activation Masking","summary":"  Researchers have recently found that Self-Supervised Learning (SSL) is\nvulnerable to backdoor attacks. The attacker can embed hidden SSL backdoors via\na few poisoned examples in the training dataset and maliciously manipulate the\nbehavior of downstream models. To defend against SSL backdoor attacks, a\nfeasible route is to detect and remove the poisonous samples in the training\nset. However, the existing SSL backdoor defense method fails to detect the\npoisonous samples precisely. In this paper, we propose to erase the SSL\nbackdoor by cluster activation masking and propose a novel PoisonCAM method.\nAfter obtaining the threat model trained on the poisoned dataset, our method\ncan precisely detect poisonous samples based on the assumption that masking the\nbackdoor trigger can effectively change the activation of a downstream\nclustering model. In experiments, our PoisonCAM achieves 96% accuracy for\nbackdoor trigger detection compared to 3% of the state-of-the-art method on\npoisoned ImageNet-100. Moreover, our proposed PoisonCAM significantly improves\nthe performance of the trained SSL model under backdoor attacks compared to the\nstate-of-the-art method. Our code will be available at\nhttps://github.com/LivXue/PoisonCAM.\n","authors":["Shengsheng Qian","Yifei Wang","Dizhan Xue","Shengjie Zhang","Huaiwen Zhang","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2312.07955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07953v1","updated":"2023-12-13T08:00:26Z","published":"2023-12-13T08:00:26Z","title":"Enhancing Robotic Navigation: An Evaluation of Single and\n  Multi-Objective Reinforcement Learning Strategies","summary":"  This study presents a comparative analysis between single-objective and\nmulti-objective reinforcement learning methods for training a robot to navigate\neffectively to an end goal while efficiently avoiding obstacles. Traditional\nreinforcement learning techniques, namely Deep Q-Network (DQN), Deep\nDeterministic Policy Gradient (DDPG), and Twin Delayed DDPG (TD3), have been\nevaluated using the Gazebo simulation framework in a variety of environments\nwith parameters such as random goal and robot starting locations. These methods\nprovide a numerical reward to the robot, offering an indication of action\nquality in relation to the goal. However, their limitations become apparent in\ncomplex settings where multiple, potentially conflicting, objectives are\npresent. To address these limitations, we propose an approach employing\nMulti-Objective Reinforcement Learning (MORL). By modifying the reward function\nto return a vector of rewards, each pertaining to a distinct objective, the\nrobot learns a policy that effectively balances the different goals, aiming to\nachieve a Pareto optimal solution. This comparative study highlights the\npotential for MORL in complex, dynamic robotic navigation tasks, setting the\nstage for future investigations into more adaptable and robust robotic\nbehaviors.\n","authors":["Vicki Young","Jumman Hossain","Nirmalya Roy"],"pdf_url":"https://arxiv.org/pdf/2312.07953v1.pdf","comment":"REU program project (work in progress)"},{"id":"http://arxiv.org/abs/2312.07952v1","updated":"2023-12-13T07:58:47Z","published":"2023-12-13T07:58:47Z","title":"Meta-learning to Calibrate Gaussian Processes with Deep Kernels for\n  Regression Uncertainty Estimation","summary":"  Although Gaussian processes (GPs) with deep kernels have been successfully\nused for meta-learning in regression tasks, its uncertainty estimation\nperformance can be poor. We propose a meta-learning method for calibrating deep\nkernel GPs for improving regression uncertainty estimation performance with a\nlimited number of training data. The proposed method meta-learns how to\ncalibrate uncertainty using data from various tasks by minimizing the test\nexpected calibration error, and uses the knowledge for unseen tasks. We design\nour model such that the adaptation and calibration for each task can be\nperformed without iterative procedures, which enables effective meta-learning.\nIn particular, a task-specific uncalibrated output distribution is modeled by a\nGP with a task-shared encoder network, and it is transformed to a calibrated\none using a cumulative density function of a task-specific Gaussian mixture\nmodel (GMM). By integrating the GP and GMM into our neural network-based model,\nwe can meta-learn model parameters in an end-to-end fashion. Our experiments\ndemonstrate that the proposed method improves uncertainty estimation\nperformance while keeping high regression performance compared with the\nexisting methods using real-world datasets in few-shot settings.\n","authors":["Tomoharu Iwata","Atsutoshi Kumagai"],"pdf_url":"https://arxiv.org/pdf/2312.07952v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07950v1","updated":"2023-12-13T07:56:27Z","published":"2023-12-13T07:56:27Z","title":"CBQ: Cross-Block Quantization for Large Language Models","summary":"  Post-training quantization (PTQ) has driven attention to producing efficient\nlarge language models (LLMs) with ultra-low costs. Since hand-craft\nquantization parameters lead to low performance in low-bit quantization, recent\nmethods optimize the quantization parameters through block-wise reconstruction\nbetween the floating-point and quantized models. However, these methods suffer\nfrom two challenges: accumulated errors from independent one-by-one block\nquantization and reconstruction difficulties from extreme weight and activation\noutliers. To address these two challenges, we propose CBQ, a cross-block\nreconstruction-based PTQ method for LLMs. To reduce error accumulation, we\nintroduce a cross-block dependency with the aid of a homologous reconstruction\nscheme to build the long-range dependency between adjacent multi-blocks with\noverlapping. To reduce reconstruction difficulty, we design a coarse-to-fine\npre-processing (CFP) to truncate weight outliers and dynamically scale\nactivation outliers before optimization, and an adaptive rounding scheme,\ncalled LoRA-Rounding, with two low-rank learnable matrixes to further rectify\nweight quantization errors. Extensive experiments demonstrate that: (1) CBQ\npushes both activation and weight quantization to low-bit settings W4A4, W4A8,\nand W2A16. (2) CBQ achieves better performance than the existing\nstate-of-the-art methods on various LLMs and benchmark datasets.\n","authors":["Xin Ding","Xiaoyu Liu","Yun Zhang","Zhijun Tu","Wei Li","Jie Hu","Hanting Chen","Yehui Tang","Zhiwei Xiong","Baoqun Yin","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2312.07950v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07945v1","updated":"2023-12-13T07:44:05Z","published":"2023-12-13T07:44:05Z","title":"Linear Combination of Exponential Moving Averages for Wireless Channel\n  Prediction","summary":"  The ability to predict the behavior of a wireless channel in terms of the\nframe delivery ratio is quite valuable, and permits, e.g., to optimize the\noperating parameters of a wireless network at runtime, or to proactively react\nto the degradation of the channel quality, in order to meet the stringent\nrequirements about dependability and end-to-end latency that typically\ncharacterize industrial applications.\n  In this work, prediction models based on the exponential moving average (EMA)\nare investigated in depth, which are proven to outperform other simple\nstatistical methods and whose performance is nearly as good as artificial\nneural networks, but with dramatically lower computational requirements.\nRegarding the innovation and motivation of this work, a new model that we\ncalled EMA linear combination (ELC), is introduced, explained, and evaluated\nexperimentally.\n  Its prediction accuracy, tested on some databases acquired from a real setup\nbased on Wi-Fi devices, showed that ELC brings tangible improvements over EMA\nin any experimental conditions, the only drawback being a slight increase in\ncomputational complexity.\n","authors":["Gabriele Formis","Stefano Scanzio","Gianluca Cena","Adriano Valenzano"],"pdf_url":"https://arxiv.org/pdf/2312.07945v1.pdf","comment":"preprint, 6 pages"},{"id":"http://arxiv.org/abs/2309.14585v3","updated":"2023-12-13T07:39:47Z","published":"2023-09-26T00:15:13Z","title":"DifAttack: Query-Efficient Black-Box Attack via Disentangled Feature\n  Space","summary":"  This work investigates efficient score-based black-box adversarial attacks\nwith a high Attack Success Rate (ASR) and good generalizability. We design a\nnovel attack method based on a Disentangled Feature space, called DifAttack,\nwhich differs significantly from the existing ones operating over the entire\nfeature space. Specifically, DifAttack firstly disentangles an image's latent\nfeature into an adversarial feature and a visual feature, where the former\ndominates the adversarial capability of an image, while the latter largely\ndetermines its visual appearance. We train an autoencoder for the\ndisentanglement by using pairs of clean images and their Adversarial Examples\n(AEs) generated from available surrogate models via white-box attack methods.\nEventually, DifAttack iteratively optimizes the adversarial feature according\nto the query feedback from the victim model until a successful AE is generated,\nwhile keeping the visual feature unaltered. In addition, due to the avoidance\nof using surrogate models' gradient information when optimizing AEs for\nblack-box models, our proposed DifAttack inherently possesses better attack\ncapability in the open-set scenario, where the training dataset of the victim\nmodel is unknown. Extensive experimental results demonstrate that our method\nachieves significant improvements in ASR and query efficiency simultaneously,\nespecially in the targeted attack and open-set scenarios. The code is available\nat https://github.com/csjunjun/DifAttack.git.\n","authors":["Liu Jun","Zhou Jiantao","Zeng Jiandian","Jinyu Tian"],"pdf_url":"https://arxiv.org/pdf/2309.14585v3.pdf","comment":"Accepted in AAAI'24"},{"id":"http://arxiv.org/abs/2312.07931v1","updated":"2023-12-13T07:20:27Z","published":"2023-12-13T07:20:27Z","title":"Levenshtein Distance Embedding with Poisson Regression for DNA Storage","summary":"  Efficient computation or approximation of Levenshtein distance, a widely-used\nmetric for evaluating sequence similarity, has attracted significant attention\nwith the emergence of DNA storage and other biological applications. Sequence\nembedding, which maps Levenshtein distance to a conventional distance between\nembedding vectors, has emerged as a promising solution. In this paper, a novel\nneural network-based sequence embedding technique using Poisson regression is\nproposed. We first provide a theoretical analysis of the impact of embedding\ndimension on model performance and present a criterion for selecting an\nappropriate embedding dimension. Under this embedding dimension, the Poisson\nregression is introduced by assuming the Levenshtein distance between sequences\nof fixed length following a Poisson distribution, which naturally aligns with\nthe definition of Levenshtein distance. Moreover, from the perspective of the\ndistribution of embedding distances, Poisson regression approximates the\nnegative log likelihood of the chi-squared distribution and offers advancements\nin removing the skewness. Through comprehensive experiments on real DNA storage\ndata, we demonstrate the superior performance of the proposed method compared\nto state-of-the-art approaches.\n","authors":["Xiang Wei","Alan J. X. Guo","Sihan Sun","Mengyi Wei","Wei Yu"],"pdf_url":"https://arxiv.org/pdf/2312.07931v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.13428v4","updated":"2023-12-13T07:01:08Z","published":"2023-01-31T05:51:05Z","title":"Contrast and Clustering: Learning Neighborhood Pair Representation for\n  Source-free Domain Adaptation","summary":"  Unsupervised domain adaptation uses source data from different distributions\nto solve the problem of classifying data from unlabeled target domains.\nHowever, conventional methods require access to source data, which often raise\nconcerns about data privacy. In this paper, we consider a more practical but\nchallenging setting where the source domain data is unavailable and the target\ndomain data is unlabeled. Specifically, we address the domain discrepancy\nproblem from the perspective of contrastive learning. The key idea of our work\nis to learn a domain-invariant feature by 1) performing clustering directly in\nthe original feature space with nearest neighbors; 2) constructing truly hard\nnegative pairs by extended neighbors without introducing additional\ncomputational complexity; and 3) combining noise-contrastive estimation theory\nto gain computational advantage. We conduct careful ablation studies and\nextensive experiments on three common benchmarks: VisDA, Office-Home, and\nOffice-31. The results demonstrate the superiority of our methods compared with\nother state-of-the-art works.\n","authors":["Yuqi Chen","Xiangbin Zhu","Yonggang Li","Yingjian Li","Haojie Fang"],"pdf_url":"https://arxiv.org/pdf/2301.13428v4.pdf","comment":"Journal articles"},{"id":"http://arxiv.org/abs/2312.07930v1","updated":"2023-12-13T06:57:00Z","published":"2023-12-13T06:57:00Z","title":"Towards Optimal Statistical Watermarking","summary":"  We study statistical watermarking by formulating it as a hypothesis testing\nproblem, a general framework which subsumes all previous statistical\nwatermarking methods. Key to our formulation is a coupling of the output tokens\nand the rejection region, realized by pseudo-random generators in practice,\nthat allows non-trivial trade-off between the Type I error and Type II error.\nWe characterize the Uniformly Most Powerful (UMP) watermark in this context. In\nthe most common scenario where the output is a sequence of $n$ tokens, we\nestablish matching upper and lower bounds on the number of i.i.d. tokens\nrequired to guarantee small Type I and Type II errors. Our rate scales as\n$\\Theta(h^{-1} \\log (1/h))$ with respect to the average entropy per token $h$\nand thus greatly improves the $O(h^{-2})$ rate in the previous works. For\nscenarios where the detector lacks knowledge of the model's distribution, we\nintroduce the concept of model-agnostic watermarking and establish the minimax\nbounds for the resultant increase in Type II error. Moreover, we formulate the\nrobust watermarking problem where user is allowed to perform a class of\nperturbation on the generated texts, and characterize the optimal type II error\nof robust UMP tests via a linear programming problem. To the best of our\nknowledge, this is the first systematic statistical treatment on the\nwatermarking problem with near-optimal rates in the i.i.d. setting, and might\nbe of interest for future works.\n","authors":["Baihe Huang","Banghua Zhu","Hanlin Zhu","Jason D. Lee","Jiantao Jiao","Michael I. Jordan"],"pdf_url":"https://arxiv.org/pdf/2312.07930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07929v1","updated":"2023-12-13T06:54:49Z","published":"2023-12-13T06:54:49Z","title":"Robust and Performance Incentivizing Algorithms for Multi-Armed Bandits\n  with Strategic Agents","summary":"  We consider a variant of the stochastic multi-armed bandit problem.\nSpecifically, the arms are strategic agents who can improve their rewards or\nabsorb them. The utility of an agent increases if she is pulled more or absorbs\nmore of her rewards but decreases if she spends more effort improving her\nrewards. Agents have heterogeneous properties, specifically having different\nmeans and able to improve their rewards up to different levels. Further, a\nnon-empty subset of agents are ''honest'' and in the worst case always give\ntheir rewards without absorbing any part. The principal wishes to obtain a high\nrevenue (cumulative reward) by designing a mechanism that incentives top level\nperformance at equilibrium. At the same time, the principal wishes to be robust\nand obtain revenue at least at the level of the honest agent with the highest\nmean in case of non-equilibrium behaviour. We identify a class of MAB\nalgorithms which we call performance incentivizing which satisfy a collection\nof properties and show that they lead to mechanisms that incentivize top level\nperformance at equilibrium and are robust under any strategy profile.\nInterestingly, we show that UCB is an example of such a MAB algorithm. Further,\nin the case where the top performance level is unknown we show that combining\nsecond price auction ideas with performance incentivizing algorithms achieves\nperformance at least at the second top level while also being robust.\n","authors":["Seyed A. Esmaeili","Suho Shin","Aleksandrs Slivkins"],"pdf_url":"https://arxiv.org/pdf/2312.07929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.08716v2","updated":"2023-12-13T06:25:23Z","published":"2022-09-19T02:25:41Z","title":"GLARE: A Dataset for Traffic Sign Detection in Sun Glare","summary":"  Real-time machine learning object detection algorithms are often found within\nautonomous vehicle technology and depend on quality datasets. It is essential\nthat these algorithms work correctly in everyday conditions as well as under\nstrong sun glare. Reports indicate glare is one of the two most prominent\nenvironment-related reasons for crashes. However, existing datasets, such as\nthe Laboratory for Intelligent & Safe Automobiles Traffic Sign (LISA) Dataset\nand the German Traffic Sign Recognition Benchmark, do not reflect the existence\nof sun glare at all. This paper presents the GLARE (GLARE is available at:\nhttps://github.com/NicholasCG/GLARE_Dataset ) traffic sign dataset: a\ncollection of images with U.S-based traffic signs under heavy visual\ninterference by sunlight. GLARE contains 2,157 images of traffic signs with sun\nglare, pulled from 33 videos of dashcam footage of roads in the United States.\nIt provides an essential enrichment to the widely used LISA Traffic Sign\ndataset. Our experimental study shows that although several state-of-the-art\nbaseline architectures have demonstrated good performance on traffic sign\ndetection in conditions without sun glare in the past, they performed poorly\nwhen tested against GLARE (e.g., average mAP0.5:0.95 of 19.4). We also notice\nthat current architectures have better detection when trained on images of\ntraffic signs in sun glare performance (e.g., average mAP0.5:0.95 of 39.6), and\nperform best when trained on a mixture of conditions (e.g., average mAP0.5:0.95\nof 42.3).\n","authors":["Nicholas Gray","Megan Moraes","Jiang Bian","Alex Wang","Allen Tian","Kurt Wilson","Yan Huang","Haoyi Xiong","Zhishan Guo"],"pdf_url":"https://arxiv.org/pdf/2209.08716v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.04856v2","updated":"2023-12-13T06:11:21Z","published":"2023-09-09T18:08:56Z","title":"AmbientFlow: Invertible generative models from incomplete, noisy\n  measurements","summary":"  Generative models have gained popularity for their potential applications in\nimaging science, such as image reconstruction, posterior sampling and data\nsharing. Flow-based generative models are particularly attractive due to their\nability to tractably provide exact density estimates along with fast,\ninexpensive and diverse samples. Training such models, however, requires a\nlarge, high quality dataset of objects. In applications such as computed\nimaging, it is often difficult to acquire such data due to requirements such as\nlong acquisition time or high radiation dose, while acquiring noisy or\npartially observed measurements of these objects is more feasible. In this\nwork, we propose AmbientFlow, a framework for learning flow-based generative\nmodels directly from noisy and incomplete data. Using variational Bayesian\nmethods, a novel framework for establishing flow-based generative models from\nnoisy, incomplete data is proposed. Extensive numerical studies demonstrate the\neffectiveness of AmbientFlow in learning the object distribution. The utility\nof AmbientFlow in a downstream inference task of image reconstruction is\ndemonstrated.\n","authors":["Varun A. Kelkar","Rucha Deshpande","Arindam Banerjee","Mark A. Anastasio"],"pdf_url":"https://arxiv.org/pdf/2309.04856v2.pdf","comment":"Accepted to Transactions on Machine Learning Research (TMLR).\n  OpenReview: https://openreview.net/forum?id=txpYITR8oa"},{"id":"http://arxiv.org/abs/2312.07910v1","updated":"2023-12-13T05:58:34Z","published":"2023-12-13T05:58:34Z","title":"PromptBench: A Unified Library for Evaluation of Large Language Models","summary":"  The evaluation of large language models (LLMs) is crucial to assess their\nperformance and mitigate potential security risks. In this paper, we introduce\nPromptBench, a unified library to evaluate LLMs. It consists of several key\ncomponents that are easily used and extended by researchers: prompt\nconstruction, prompt engineering, dataset and model loading, adversarial prompt\nattack, dynamic evaluation protocols, and analysis tools. PromptBench is\ndesigned to be an open, general, and flexible codebase for research purposes\nthat can facilitate original study in creating new benchmarks, deploying\ndownstream applications, and designing new evaluation protocols. The code is\navailable at: https://github.com/microsoft/promptbench and will be continuously\nsupported.\n","authors":["Kaijie Zhu","Qinlin Zhao","Hao Chen","Jindong Wang","Xing Xie"],"pdf_url":"https://arxiv.org/pdf/2312.07910v1.pdf","comment":"An extension to PromptBench (arXiv:2306.04528) for unified evaluation\n  of LLMs using the same name; code: https://github.com/microsoft/promptbench"},{"id":"http://arxiv.org/abs/2312.00812v2","updated":"2023-12-13T05:29:20Z","published":"2023-11-28T03:13:09Z","title":"Empowering Autonomous Driving with Large Language Models: A Safety\n  Perspective","summary":"  Autonomous Driving (AD) faces crucial hurdles for commercial launch, notably\nin the form of diminished public trust and safety concerns from long-tail\nunforeseen driving scenarios. This predicament is due to the limitation of deep\nneural networks in AD software, which struggle with interpretability and\nexhibit poor generalization capabilities in out-of-distribution and uncertain\nscenarios. To this end, this paper advocates for the integration of Large\nLanguage Models (LLMs) into the AD system, leveraging their robust common-sense\nknowledge, reasoning abilities, and human-interaction capabilities. The\nproposed approach deploys the LLM as an intelligent decision-maker in planning,\nincorporating safety verifiers for contextual safety learning to enhance\noverall AD performance and safety. We present results from two case studies\nthat affirm the efficacy of our approach. We further discuss the potential\nintegration of LLM for other AD software components including perception,\nprediction, and simulation. Despite the observed challenges in the case\nstudies, the integration of LLMs is promising and beneficial for reinforcing\nboth safety and performance in AD.\n","authors":["Yixuan Wang","Ruochen Jiao","Chengtian Lang","Sinong Simon Zhan","Chao Huang","Zhaoran Wang","Zhuoran Yang","Qi Zhu"],"pdf_url":"https://arxiv.org/pdf/2312.00812v2.pdf","comment":"14 pages, 7 figures, baseline added in the experiment"},{"id":"http://arxiv.org/abs/2312.07899v1","updated":"2023-12-13T05:08:32Z","published":"2023-12-13T05:08:32Z","title":"Morphological Profiling for Drug Discovery in the Era of Deep Learning","summary":"  Morphological profiling is a valuable tool in phenotypic drug discovery. The\nadvent of high-throughput automated imaging has enabled the capturing of a wide\nrange of morphological features of cells or organisms in response to\nperturbations at the single-cell resolution. Concurrently, significant advances\nin machine learning and deep learning, especially in computer vision, have led\nto substantial improvements in analyzing large-scale high-content images at\nhigh-throughput. These efforts have facilitated understanding of compound\nmechanism-of-action (MOA), drug repurposing, characterization of cell\nmorphodynamics under perturbation, and ultimately contributing to the\ndevelopment of novel therapeutics. In this review, we provide a comprehensive\noverview of the recent advances in the field of morphological profiling. We\nsummarize the image profiling analysis workflow, survey a broad spectrum of\nanalysis strategies encompassing feature engineering- and deep learning-based\napproaches, and introduce publicly available benchmark datasets. We place a\nparticular emphasis on the application of deep learning in this pipeline,\ncovering cell segmentation, image representation learning, and multimodal\nlearning. Additionally, we illuminate the application of morphological\nprofiling in phenotypic drug discovery and highlight potential challenges and\nopportunities in this field.\n","authors":["Qiaosi Tang","Ranjala Ratnayake","Gustavo Seabra","Zhe Jiang","Ruogu Fang","Lina Cui","Yousong Ding","Tamer Kahveci","Jiang Bian","Chenglong Li","Hendrik Luesch","Yanjun Li"],"pdf_url":"https://arxiv.org/pdf/2312.07899v1.pdf","comment":"44 pages, 5 figure, 5 tables"},{"id":"http://arxiv.org/abs/2302.14407v2","updated":"2023-12-13T04:31:56Z","published":"2023-02-28T08:42:42Z","title":"The Choice of Noninformative Priors for Thompson Sampling in\n  Multiparameter Bandit Models","summary":"  Thompson sampling (TS) has been known for its outstanding empirical\nperformance supported by theoretical guarantees across various reward models in\nthe classical stochastic multi-armed bandit problems. Nonetheless, its\noptimality is often restricted to specific priors due to the common observation\nthat TS is fairly insensitive to the choice of the prior when it comes to\nasymptotic regret bounds. However, when the model contains multiple parameters,\nthe optimality of TS highly depends on the choice of priors, which casts doubt\non the generalizability of previous findings to other models. To address this\ngap, this study explores the impact of selecting noninformative priors,\noffering insights into the performance of TS when dealing with new models that\nlack theoretical understanding. We first extend the regret analysis of TS to\nthe model of uniform distributions with unknown supports, which would be the\nsimplest non-regular model. Our findings reveal that changing noninformative\npriors can significantly affect the expected regret, aligning with previously\nknown results in other multiparameter bandit models. Although the uniform prior\nis shown to be optimal, we highlight the inherent limitation of its optimality,\nwhich is limited to specific parameterizations and emphasizes the significance\nof the invariance property of priors. In light of this limitation, we propose a\nslightly modified TS-based policy, called TS with Truncation (TS-T), which can\nachieve the asymptotic optimality for the Gaussian models and the uniform\nmodels by using the reference prior and the Jeffreys prior that are invariant\nunder one-to-one reparameterizations. This policy provides an alternative\napproach to achieving optimality by employing fine-tuned truncation, which\nwould be much easier than hunting for optimal priors in practice.\n","authors":["Jongyeong Lee","Chao-Kai Chiang","Masashi Sugiyama"],"pdf_url":"https://arxiv.org/pdf/2302.14407v2.pdf","comment":"55 pages, TBA AAAI2024"},{"id":"http://arxiv.org/abs/2302.10915v2","updated":"2023-12-13T04:31:44Z","published":"2023-02-17T01:31:55Z","title":"Conformers are All You Need for Visual Speech Recognition","summary":"  Visual speech recognition models extract visual features in a hierarchical\nmanner. At the lower level, there is a visual front-end with a limited temporal\nreceptive field that processes the raw pixels depicting the lips or faces. At\nthe higher level, there is an encoder that attends to the embeddings produced\nby the front-end over a large temporal receptive field. Previous work has\nfocused on improving the visual front-end of the model to extract more useful\nfeatures for speech recognition. Surprisingly, our work shows that complex\nvisual front-ends are not necessary. Instead of allocating resources to a\nsophisticated visual front-end, we find that a linear visual front-end paired\nwith a larger Conformer encoder results in lower latency, more efficient memory\nusage, and improved WER performance. We achieve a new state-of-the-art of 12.8%\nWER for visual speech recognition on the TED LRS3 dataset, which rivals the\nperformance of audio-only models from just four years ago.\n","authors":["Oscar Chang","Hank Liao","Dmitriy Serdyuk","Ankit Shah","Olivier Siohan"],"pdf_url":"https://arxiv.org/pdf/2302.10915v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07887v1","updated":"2023-12-13T04:14:22Z","published":"2023-12-13T04:14:22Z","title":"Learn or Recall? Revisiting Incremental Learning with Pre-trained\n  Language Models","summary":"  Incremental Learning (IL) has been a long-standing problem in both vision and\nNatural Language Processing (NLP) communities. In recent years, as Pre-trained\nLanguage Models (PLMs) have achieved remarkable progress in various NLP\ndownstream tasks, utilizing PLMs as backbones has become a common practice in\nrecent research of IL in NLP. Most assume that catastrophic forgetting is the\nbiggest obstacle to achieving superior IL performance and propose various\ntechniques to overcome this issue. However, we find that this assumption is\nproblematic. Specifically, we revisit more than 20 methods on four\nclassification tasks (Text Classification, Intent Classification, Relation\nExtraction, and Named Entity Recognition) under the two most popular IL\nsettings (Class-Incremental and Task-Incremental) and reveal that most of them\nseverely underestimate the inherent anti-forgetting ability of PLMs. Based on\nthe observation, we propose a frustratingly easy method called SEQ* for IL with\nPLMs. The results show that SEQ* has competitive or superior performance\ncompared to state-of-the-art (SOTA) IL methods and requires considerably less\ntrainable parameters and training time. These findings urge us to revisit the\nIL with PLMs and encourage future studies to have a fundamental understanding\nof the catastrophic forgetting in PLMs. The data, code and scripts are publicly\navailable at\nhttps://github.com/zzz47zzz/pretrained-lm-for-incremental-learning.\n","authors":["Junhao Zheng","Shengjie Qiu","Qianli Ma"],"pdf_url":"https://arxiv.org/pdf/2312.07887v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.04690v7","updated":"2023-12-13T04:02:14Z","published":"2022-07-11T08:12:02Z","title":"Dynamic Budget Throttling in Repeated Second-Price Auctions","summary":"  In today's online advertising markets, a crucial requirement for an\nadvertiser is to control her total expenditure within a time horizon under some\nbudget. Among various budget control methods, throttling has emerged as a\npopular choice, managing an advertiser's total expenditure by selecting only a\nsubset of auctions to participate in. This paper provides a theoretical\npanorama of a single advertiser's dynamic budget throttling process in repeated\nsecond-price auctions. We first establish a lower bound on the regret and an\nupper bound on the asymptotic competitive ratio for any throttling algorithm,\nrespectively, when the advertiser's values are stochastic and adversarial.\nRegarding the algorithmic side, we propose the OGD-CB algorithm, which\nguarantees a near-optimal expected regret with stochastic values. On the other\nhand, when values are adversarial, we prove that this algorithm also reaches\nthe upper bound on the asymptotic competitive ratio. We further compare\nthrottling with pacing, another widely adopted budget control method, in\nrepeated second-price auctions. In the stochastic case, we demonstrate that\npacing is generally superior to throttling for the advertiser, supporting the\nwell-known result that pacing is asymptotically optimal in this scenario.\nHowever, in the adversarial case, we give an exciting result indicating that\nthrottling is also an asymptotically optimal dynamic bidding strategy. Our\nresults bridge the gaps in theoretical research of throttling in repeated\nauctions and comprehensively reveal the ability of this popular\nbudget-smoothing strategy.\n","authors":["Zhaohua Chen","Chang Wang","Qian Wang","Yuqi Pan","Zhuming Shi","Zheng Cai","Yukun Ren","Zhihua Zhu","Xiaotie Deng"],"pdf_url":"https://arxiv.org/pdf/2207.04690v7.pdf","comment":"42 pages, 1 figure, 1 table; full version of the AAAI-24 paper"},{"id":"http://arxiv.org/abs/2312.07285v2","updated":"2023-12-13T03:56:26Z","published":"2023-12-12T14:00:29Z","title":"Forced Exploration in Bandit Problems","summary":"  The multi-armed bandit(MAB) is a classical sequential decision problem. Most\nwork requires assumptions about the reward distribution (e.g., bounded), while\npractitioners may have difficulty obtaining information about these\ndistributions to design models for their problems, especially in non-stationary\nMAB problems. This paper aims to design a multi-armed bandit algorithm that can\nbe implemented without using information about the reward distribution while\nstill achieving substantial regret upper bounds. To this end, we propose a\nnovel algorithm alternating between greedy rule and forced exploration. Our\nmethod can be applied to Gaussian, Bernoulli and other subgaussian\ndistributions, and its implementation does not require additional information.\nWe employ a unified analysis method for different forced exploration strategies\nand provide problem-dependent regret upper bounds for stationary and\npiecewise-stationary settings. Furthermore, we compare our algorithm with\npopular bandit algorithms on different reward distributions.\n","authors":["Han Qi","Fei Guo","Li Zhu"],"pdf_url":"https://arxiv.org/pdf/2312.07285v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.13850v3","updated":"2023-12-13T03:31:18Z","published":"2023-04-26T22:29:49Z","title":"Do SSL Models Have Déjà Vu? A Case of Unintended Memorization in\n  Self-supervised Learning","summary":"  Self-supervised learning (SSL) algorithms can produce useful image\nrepresentations by learning to associate different parts of natural images with\none another. However, when taken to the extreme, SSL models can unintendedly\nmemorize specific parts in individual training samples rather than learning\nsemantically meaningful associations. In this work, we perform a systematic\nstudy of the unintended memorization of image-specific information in SSL\nmodels -- which we refer to as d\\'ej\\`a vu memorization. Concretely, we show\nthat given the trained model and a crop of a training image containing only the\nbackground (e.g., water, sky, grass), it is possible to infer the foreground\nobject with high accuracy or even visually reconstruct it. Furthermore, we show\nthat d\\'ej\\`a vu memorization is common to different SSL algorithms, is\nexacerbated by certain design choices, and cannot be detected by conventional\ntechniques for evaluating representation quality. Our study of d\\'ej\\`a vu\nmemorization reveals previously unknown privacy risks in SSL models, as well as\nsuggests potential practical mitigation strategies. Code is available at\nhttps://github.com/facebookresearch/DejaVu.\n","authors":["Casey Meehan","Florian Bordes","Pascal Vincent","Kamalika Chaudhuri","Chuan Guo"],"pdf_url":"https://arxiv.org/pdf/2304.13850v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07861v1","updated":"2023-12-13T02:59:37Z","published":"2023-12-13T02:59:37Z","title":"GraphGuard: Detecting and Counteracting Training Data Misuse in Graph\n  Neural Networks","summary":"  The emergence of Graph Neural Networks (GNNs) in graph data analysis and\ntheir deployment on Machine Learning as a Service platforms have raised\ncritical concerns about data misuse during model training. This situation is\nfurther exacerbated due to the lack of transparency in local training\nprocesses, potentially leading to the unauthorized accumulation of large\nvolumes of graph data, thereby infringing on the intellectual property rights\nof data owners. Existing methodologies often address either data misuse\ndetection or mitigation, and are primarily designed for local GNN models rather\nthan cloud-based MLaaS platforms. These limitations call for an effective and\ncomprehensive solution that detects and mitigates data misuse without requiring\nexact training data while respecting the proprietary nature of such data. This\npaper introduces a pioneering approach called GraphGuard, to tackle these\nchallenges. We propose a training-data-free method that not only detects graph\ndata misuse but also mitigates its impact via targeted unlearning, all without\nrelying on the original training data. Our innovative misuse detection\ntechnique employs membership inference with radioactive data, enhancing the\ndistinguishability between member and non-member data distributions. For\nmitigation, we utilize synthetic graphs that emulate the characteristics\npreviously learned by the target model, enabling effective unlearning even in\nthe absence of exact graph data. We conduct comprehensive experiments utilizing\nfour real-world graph datasets to demonstrate the efficacy of GraphGuard in\nboth detection and unlearning. We show that GraphGuard attains a near-perfect\ndetection rate of approximately 100% across these datasets with various GNN\nmodels. In addition, it performs unlearning by eliminating the impact of the\nunlearned graph with a marginal decrease in accuracy (less than 5%).\n","authors":["Bang Wu","He Zhang","Xiangwen Yang","Shuo Wang","Minhui Xue","Shirui Pan","Xingliang Yuan"],"pdf_url":"https://arxiv.org/pdf/2312.07861v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07859v1","updated":"2023-12-13T02:56:26Z","published":"2023-12-13T02:56:26Z","title":"Invariant Graph Transformer","summary":"  Rationale discovery is defined as finding a subset of the input data that\nmaximally supports the prediction of downstream tasks. In graph machine\nlearning context, graph rationale is defined to locate the critical subgraph in\nthe given graph topology, which fundamentally determines the prediction\nresults. In contrast to the rationale subgraph, the remaining subgraph is named\nthe environment subgraph. Graph rationalization can enhance the model\nperformance as the mapping between the graph rationale and prediction label is\nviewed as invariant, by assumption. To ensure the discriminative power of the\nextracted rationale subgraphs, a key technique named \"intervention\" is applied.\nThe core idea of intervention is that given any changing environment subgraphs,\nthe semantics from the rationale subgraph is invariant, which guarantees the\ncorrect prediction result. However, most, if not all, of the existing\nrationalization works on graph data develop their intervention strategies on\nthe graph level, which is coarse-grained. In this paper, we propose\nwell-tailored intervention strategies on graph data. Our idea is driven by the\ndevelopment of Transformer models, whose self-attention module provides rich\ninteractions between input nodes. Based on the self-attention module, our\nproposed invariant graph Transformer (IGT) can achieve fine-grained, more\nspecifically, node-level and virtual node-level intervention. Our comprehensive\nexperiments involve 7 real-world datasets, and the proposed IGT shows\nsignificant performance advantages compared to 13 baseline methods.\n","authors":["Zhe Xu","Menghai Pan","Yuzhong Chen","Huiyuan Chen","Yuchen Yan","Mahashweta Das","Hanghang Tong"],"pdf_url":"https://arxiv.org/pdf/2312.07859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07855v1","updated":"2023-12-13T02:48:35Z","published":"2023-12-13T02:48:35Z","title":"Exploring Popularity Bias in Session-based Recommendation","summary":"  Existing work has revealed that large-scale offline evaluation of recommender\nsystems for user-item interactions is prone to bias caused by the deployed\nsystem itself, as a form of closed loop feedback. Many adopt the\n\\textit{propensity} concept to analyze or mitigate this empirical issue. In\nthis work, we extend the analysis to session-based setup and adapted propensity\ncalculation to the unique characteristics of session-based recommendation\ntasks. Our experiments incorporate neural models and KNN-based models, and\ncover both the music and the e-commerce domain. We study the distributions of\npropensity and different stratification techniques on different datasets and\nfind that propensity-related traits are actually dataset-specific. We then\nleverage the effect of stratification and achieve promising results compared to\nthe original models.\n","authors":["Haowen Wang"],"pdf_url":"https://arxiv.org/pdf/2312.07855v1.pdf","comment":"10pages, 9 figures"},{"id":"http://arxiv.org/abs/2312.07854v1","updated":"2023-12-13T02:48:11Z","published":"2023-12-13T02:48:11Z","title":"Diffusion Models Enable Zero-Shot Pose Estimation for Lower-Limb\n  Prosthetic Users","summary":"  The application of 2D markerless gait analysis has garnered increasing\ninterest and application within clinical settings. However, its effectiveness\nin the realm of lower-limb amputees has remained less than optimal. In\nresponse, this study introduces an innovative zero-shot method employing image\ngeneration diffusion models to achieve markerless pose estimation for\nlower-limb prosthetics, presenting a promising solution to gait analysis for\nthis specific population. Our approach demonstrates an enhancement in detecting\nkey points on prosthetic limbs over existing methods, and enables clinicians to\ngain invaluable insights into the kinematics of lower-limb amputees across the\ngait cycle. The outcomes obtained not only serve as a proof-of-concept for the\nfeasibility of this zero-shot approach but also underscore its potential in\nadvancing rehabilitation through gait analysis for this unique population.\n","authors":["Tianxun Zhou","Muhammad Nur Shahril Iskandar","Keng-Hwee Chiam"],"pdf_url":"https://arxiv.org/pdf/2312.07854v1.pdf","comment":"25 pages, 6 figures. Supplementary documents in source file"},{"id":"http://arxiv.org/abs/2101.11992v4","updated":"2023-12-13T02:40:47Z","published":"2021-01-28T13:35:37Z","title":"Acting in Delayed Environments with Non-Stationary Markov Policies","summary":"  The standard Markov Decision Process (MDP) formulation hinges on the\nassumption that an action is executed immediately after it was chosen. However,\nassuming it is often unrealistic and can lead to catastrophic failures in\napplications such as robotic manipulation, cloud computing, and finance. We\nintroduce a framework for learning and planning in MDPs where the\ndecision-maker commits actions that are executed with a delay of $m$ steps. The\nbrute-force state augmentation baseline where the state is concatenated to the\nlast $m$ committed actions suffers from an exponential complexity in $m$, as we\nshow for policy iteration. We then prove that with execution delay,\ndeterministic Markov policies in the original state-space are sufficient for\nattaining maximal reward, but need to be non-stationary. As for stationary\nMarkov policies, we show they are sub-optimal in general. Consequently, we\ndevise a non-stationary Q-learning style model-based algorithm that solves\ndelayed execution tasks without resorting to state-augmentation. Experiments on\ntabular, physical, and Atari domains reveal that it converges quickly to high\nperformance even for substantial delays, while standard approaches that either\nignore the delay or rely on state-augmentation struggle or fail due to\ndivergence. The code is available at github.com/galdl/rl_delay_basic and\ngithub.com/galdl/rl_delay_atari.\n","authors":["Esther Derman","Gal Dalal","Shie Mannor"],"pdf_url":"https://arxiv.org/pdf/2101.11992v4.pdf","comment":"Published in ICLR 2021"},{"id":"http://arxiv.org/abs/2312.07851v1","updated":"2023-12-13T02:39:10Z","published":"2023-12-13T02:39:10Z","title":"Noise in the reverse process improves the approximation capabilities of\n  diffusion models","summary":"  In Score based Generative Modeling (SGMs), the state-of-the-art in generative\nmodeling, stochastic reverse processes are known to perform better than their\ndeterministic counterparts. This paper delves into the heart of this\nphenomenon, comparing neural ordinary differential equations (ODEs) and neural\nstochastic differential equations (SDEs) as reverse processes. We use a control\ntheoretic perspective by posing the approximation of the reverse process as a\ntrajectory tracking problem. We analyze the ability of neural SDEs to\napproximate trajectories of the Fokker-Planck equation, revealing the\nadvantages of stochasticity. First, neural SDEs exhibit a powerful regularizing\neffect, enabling $L^2$ norm trajectory approximation surpassing the Wasserstein\nmetric approximation achieved by neural ODEs under similar conditions, even\nwhen the reference vector field or score function is not Lipschitz. Applying\nthis result, we establish the class of distributions that can be sampled using\nscore matching in SGMs, relaxing the Lipschitz requirement on the gradient of\nthe data distribution in existing literature. Second, we show that this\napproximation property is preserved when network width is limited to the input\ndimension of the network. In this limited width case, the weights act as\ncontrol inputs, framing our analysis as a controllability problem for neural\nSDEs in probability density space. This sheds light on how noise helps to steer\nthe system towards the desired solution and illuminates the empirical success\nof stochasticity in generative modeling.\n","authors":["Karthik Elamvazhuthi","Samet Oymak","Fabio Pasqualetti"],"pdf_url":"https://arxiv.org/pdf/2312.07851v1.pdf","comment":"Extended preprint for submission to Learning for Dynamics & Control\n  Conference"},{"id":"http://arxiv.org/abs/2302.07350v2","updated":"2023-12-13T02:36:37Z","published":"2023-02-14T21:23:22Z","title":"Graph schemas as abstractions for transfer learning, inference, and\n  planning","summary":"  Transferring latent structure from one environment or problem to another is a\nmechanism by which humans and animals generalize with very little data.\nInspired by cognitive and neurobiological insights, we propose graph schemas as\na mechanism of abstraction for transfer learning. Graph schemas start with\nlatent graph learning where perceptually aliased observations are disambiguated\nin the latent space using contextual information. Latent graph learning is also\nemerging as a new computational model of the hippocampus to explain map\nlearning and transitive inference. Our insight is that a latent graph can be\ntreated as a flexible template -- a schema -- that models concepts and\nbehaviors, with slots that bind groups of latent nodes to the specific\nobservations or groundings. By treating learned latent graphs (schemas) as\nprior knowledge, new environments can be quickly learned as compositions of\nschemas and their newly learned bindings. We evaluate graph schemas on two\npreviously published challenging tasks: the memory & planning game and one-shot\nStreetLearn, which are designed to test rapid task solving in novel\nenvironments. Graph schemas can be learned in far fewer episodes than previous\nbaselines, and can model and plan in a few steps in novel variations of these\ntasks. We also demonstrate learning, matching, and reusing graph schemas in\nmore challenging 2D and 3D environments with extensive perceptual aliasing and\nsize variations, and show how different schemas can be composed to model larger\nand more complex environments. To summarize, our main contribution is a unified\nsystem, inspired and grounded in cognitive science, that facilitates rapid\ntransfer learning of new environments using schemas via map-induction and\ncomposition that handles perceptual aliasing.\n","authors":["J. Swaroop Guntupalli","Rajkumar Vasudeva Raju","Shrinu Kushagra","Carter Wendelken","Danny Sawyer","Ishan Deshpande","Guangyao Zhou","Miguel Lázaro-Gredilla","Dileep George"],"pdf_url":"https://arxiv.org/pdf/2302.07350v2.pdf","comment":"14 pages, 4 figures in main paper, 13 pages and 8 figures in appendix"},{"id":"http://arxiv.org/abs/2312.01479v2","updated":"2023-12-13T02:25:42Z","published":"2023-12-03T18:41:54Z","title":"OpenVoice: Versatile Instant Voice Cloning","summary":"  We introduce OpenVoice, a versatile voice cloning approach that requires only\na short audio clip from the reference speaker to replicate their voice and\ngenerate speech in multiple languages. OpenVoice represents a significant\nadvancement in addressing the following open challenges in the field: 1)\nFlexible Voice Style Control. OpenVoice enables granular control over voice\nstyles, including emotion, accent, rhythm, pauses, and intonation, in addition\nto replicating the tone color of the reference speaker. The voice styles are\nnot directly copied from and constrained by the style of the reference speaker.\nPrevious approaches lacked the ability to flexibly manipulate voice styles\nafter cloning. 2) Zero-Shot Cross-Lingual Voice Cloning. OpenVoice achieves\nzero-shot cross-lingual voice cloning for languages not included in the\nmassive-speaker training set. Unlike previous approaches, which typically\nrequire extensive massive-speaker multi-lingual (MSML) dataset for all\nlanguages, OpenVoice can clone voices into a new language without any\nmassive-speaker training data for that language. OpenVoice is also\ncomputationally efficient, costing tens of times less than commercially\navailable APIs that offer even inferior performance. To foster further research\nin the field, we have made the source code and trained model publicly\naccessible. We also provide qualitative results in our demo website. Prior to\nits public release, our internal version of OpenVoice was used tens of millions\nof times by users worldwide between May and October 2023, serving as the\nbackend of MyShell.\n","authors":["Zengyi Qin","Wenliang Zhao","Xumin Yu","Xin Sun"],"pdf_url":"https://arxiv.org/pdf/2312.01479v2.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2311.02546v2","updated":"2023-12-13T02:24:48Z","published":"2023-11-05T02:33:30Z","title":"On the Second-Order Convergence of Biased Policy Gradient Algorithms","summary":"  Since the objective functions of reinforcement learning problems are\ntypically highly nonconvex, we seek guarantees that these algorithms escape\nsaddle points and arrive at second-order stationary points. Existing results\nonly consider vanilla policy gradient algorithms with unbiased gradient\nestimators, but practical implementations under the infinite-horizon discounted\nreward setting are biased due to finite-horizon sampling. Moreover,\nactor-critic methods, whose second-order convergence has not yet been\nestablished, are also biased due to the critic approximation of the value\nfunction. We provide a novel second-order analysis of biased policy gradient\nmethods, including the vanilla gradient estimator computed from Monte-Carlo\nsampling of trajectories as well as the double-loop actor-critic algorithm,\nwhere in the inner loop the the critic parameter improves the approximation of\nthe value function via TD(0) learning. Separately, we also establish the\nconvergence of TD(0) on Markov chains irrespective of initial state\ndistribution.\n","authors":["Siqiao Mu","Diego Klabjan"],"pdf_url":"https://arxiv.org/pdf/2311.02546v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07841v1","updated":"2023-12-13T02:11:07Z","published":"2023-12-13T02:11:07Z","title":"On the Dynamics Under the Unhinged Loss and Beyond","summary":"  Recent works have studied implicit biases in deep learning, especially the\nbehavior of last-layer features and classifier weights. However, they usually\nneed to simplify the intermediate dynamics under gradient flow or gradient\ndescent due to the intractability of loss functions and model architectures. In\nthis paper, we introduce the unhinged loss, a concise loss function, that\noffers more mathematical opportunities to analyze the closed-form dynamics\nwhile requiring as few simplifications or assumptions as possible. The unhinged\nloss allows for considering more practical techniques, such as time-vary\nlearning rates and feature normalization. Based on the layer-peeled model that\nviews last-layer features as free optimization variables, we conduct a thorough\nanalysis in the unconstrained, regularized, and spherical constrained cases, as\nwell as the case where the neural tangent kernel remains invariant. To bridge\nthe performance of the unhinged loss to that of Cross-Entropy (CE), we\ninvestigate the scenario of fixing classifier weights with a specific\nstructure, (e.g., a simplex equiangular tight frame). Our analysis shows that\nthese dynamics converge exponentially fast to a solution depending on the\ninitialization of features and classifier weights. These theoretical results\nnot only offer valuable insights, including explicit feature regularization and\nrescaled learning rates for enhancing practical training with the unhinged\nloss, but also extend their applicability to other loss functions. Finally, we\nempirically demonstrate these theoretical results and insights through\nextensive experiments.\n","authors":["Xiong Zhou","Xianming Liu","Hanzhang Wang","Deming Zhai","Junjun Jiang","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2312.07841v1.pdf","comment":"62 pages, 19 figures"},{"id":"http://arxiv.org/abs/2312.07839v1","updated":"2023-12-13T02:06:52Z","published":"2023-12-13T02:06:52Z","title":"Minimax-optimal estimation for sparse multi-reference alignment with\n  collision-free signals","summary":"  The Multi-Reference Alignment (MRA) problem aims at the recovery of an\nunknown signal from repeated observations under the latent action of a group of\ncyclic isometries, in the presence of additive noise of high intensity\n$\\sigma$. It is a more tractable version of the celebrated cryo EM model. In\nthe crucial high noise regime, it is known that its sample complexity scales as\n$\\sigma^6$. Recent investigations have shown that for the practically\nsignificant setting of sparse signals, the sample complexity of the maximum\nlikelihood estimator asymptotically scales with the noise level as $\\sigma^4$.\nIn this work, we investigate minimax optimality for signal estimation under the\nMRA model for so-called collision-free signals. In particular, this signal\nclass covers the setting of generic signals of dilute sparsity (wherein the\nsupport size $s=O(L^{1/3})$, where $L$ is the ambient dimension.\n  We demonstrate that the minimax optimal rate of estimation in for the sparse\nMRA problem in this setting is $\\sigma^2/\\sqrt{n}$, where $n$ is the sample\nsize. In particular, this widely generalizes the sample complexity asymptotics\nfor the restricted MLE in this setting, establishing it as the statistically\noptimal estimator. Finally, we demonstrate a concentration inequality for the\nrestricted MLE on its deviations from the ground truth.\n","authors":["Subhro Ghosh","Soumendu Sundar Mukherjee","Jing Bin Pan"],"pdf_url":"https://arxiv.org/pdf/2312.07839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07837v1","updated":"2023-12-13T02:04:41Z","published":"2023-12-13T02:04:41Z","title":"Synthetic Data: Can We Trust Statistical Estimators?","summary":"  The increasing interest in data sharing makes synthetic data appealing.\nHowever, the analysis of synthetic data raises a unique set of methodological\nchallenges. In this work, we highlight the importance of inferential utility\nand provide empirical evidence against naive inference from synthetic data\n(that handles these as if they were really observed). We argue that the rate of\nfalse-positive findings (type 1 error) will be unacceptably high, even when the\nestimates are unbiased. One of the reasons is the underestimation of the true\nstandard error, which may even progressively increase with larger sample sizes\ndue to slower convergence. This is especially problematic for deep generative\nmodels. Before publishing synthetic data, it is essential to develop\nstatistical inference tools for such data.\n","authors":["Alexander Decruyenaere","Heidelinde Dehaene","Paloma Rabaey","Christiaan Polet","Johan Decruyenaere","Stijn Vansteelandt","Thomas Demeester"],"pdf_url":"https://arxiv.org/pdf/2312.07837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07835v1","updated":"2023-12-13T01:57:11Z","published":"2023-12-13T01:57:11Z","title":"Video Dynamics Prior: An Internal Learning Approach for Robust Video\n  Enhancements","summary":"  In this paper, we present a novel robust framework for low-level vision\ntasks, including denoising, object removal, frame interpolation, and\nsuper-resolution, that does not require any external training data corpus. Our\nproposed approach directly learns the weights of neural modules by optimizing\nover the corrupted test sequence, leveraging the spatio-temporal coherence and\ninternal statistics of videos. Furthermore, we introduce a novel spatial\npyramid loss that leverages the property of spatio-temporal patch recurrence in\na video across the different scales of the video. This loss enhances robustness\nto unstructured noise in both the spatial and temporal domains. This further\nresults in our framework being highly robust to degradation in input frames and\nyields state-of-the-art results on downstream tasks such as denoising, object\nremoval, and frame interpolation. To validate the effectiveness of our\napproach, we conduct qualitative and quantitative evaluations on standard video\ndatasets such as DAVIS, UCF-101, and VIMEO90K-T.\n","authors":["Gaurav Shrivastava","Ser-Nam Lim","Abhinav Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2312.07835v1.pdf","comment":"NeurIPS 2023; Webpage - http://www.cs.umd.edu/~gauravsh/vdp.html"},{"id":"http://arxiv.org/abs/2308.03977v2","updated":"2023-12-13T01:44:58Z","published":"2023-08-08T01:33:13Z","title":"PUG: Photorealistic and Semantically Controllable Synthetic Data for\n  Representation Learning","summary":"  Synthetic image datasets offer unmatched advantages for designing and\nevaluating deep neural networks: they make it possible to (i) render as many\ndata samples as needed, (ii) precisely control each scene and yield granular\nground truth labels (and captions), (iii) precisely control distribution shifts\nbetween training and testing to isolate variables of interest for sound\nexperimentation. Despite such promise, the use of synthetic image data is still\nlimited -- and often played down -- mainly due to their lack of realism. Most\nworks therefore rely on datasets of real images, which have often been scraped\nfrom public images on the internet, and may have issues with regards to\nprivacy, bias, and copyright, while offering little control over how objects\nprecisely appear. In this work, we present a path to democratize the use of\nphotorealistic synthetic data: we develop a new generation of interactive\nenvironments for representation learning research, that offer both\ncontrollability and realism. We use the Unreal Engine, a powerful game engine\nwell known in the entertainment industry, to produce PUG (Photorealistic Unreal\nGraphics) environments and datasets for representation learning. In this paper,\nwe demonstrate the potential of PUG to enable more rigorous evaluations of\nvision models.\n","authors":["Florian Bordes","Shashank Shekhar","Mark Ibrahim","Diane Bouchacourt","Pascal Vincent","Ari S. Morcos"],"pdf_url":"https://arxiv.org/pdf/2308.03977v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07833v1","updated":"2023-12-13T01:40:21Z","published":"2023-12-13T01:40:21Z","title":"Stable Rivers: A Case Study in the Application of Text-to-Image\n  Generative Models for Earth Sciences","summary":"  Text-to-image (TTI) generative models can be used to generate photorealistic\nimages from a given text-string input. These models offer great potential to\nmitigate challenges to the uptake of machine learning in the earth sciences.\nHowever, the rapid increase in their use has raised questions about fairness\nand biases, with most research to-date focusing on social and cultural areas\nrather than domain-specific considerations. We conducted a case study for the\nearth sciences, focusing on the field of fluvial geomorphology, where we\nevaluated subject-area specific biases in the training data and downstream\nmodel performance of Stable Diffusion (v1.5). In addition to perpetuating\nWestern biases, we found that the training data over-represented scenic\nlocations, such as famous rivers and waterfalls, and showed serious under- and\nover-representation of many morphological and environmental terms. Despite\nbiased training data, we found that with careful prompting, the Stable\nDiffusion model was able to generate photorealistic synthetic river images\nreproducing many important environmental and morphological characteristics.\nFurthermore, conditional control techniques, such as the use of condition maps\nwith ControlNet were effective for providing additional constraints on output\nimages. Despite great potential for the use of TTI models in the earth sciences\nfield, we advocate for caution in sensitive applications, and advocate for\ndomain-specific reviews of training data and image generation biases to\nmitigate perpetuation of existing biases.\n","authors":["C Kupferschmidt","A. D. Binns","K. L. Kupferschmidt","G. W Taylor"],"pdf_url":"https://arxiv.org/pdf/2312.07833v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.07553v4","updated":"2023-12-13T01:37:55Z","published":"2022-06-15T14:12:45Z","title":"On the fast convergence of minibatch heavy ball momentum","summary":"  Simple stochastic momentum methods are widely used in machine learning\noptimization, but their good practical performance is at odds with an absence\nof theoretical guarantees of acceleration in the literature. In this work, we\naim to close the gap between theory and practice by showing that stochastic\nheavy ball momentum retains the fast linear rate of (deterministic) heavy ball\nmomentum on quadratic optimization problems, at least when minibatching with a\nsufficiently large batch size. The algorithm we study can be interpreted as an\naccelerated randomized Kaczmarz algorithm with minibatching and heavy ball\nmomentum. The analysis relies on carefully decomposing the momentum transition\nmatrix, and using new spectral norm concentration bounds for products of\nindependent random matrices. We provide numerical illustrations demonstrating\nthat our bounds are reasonably sharp.\n","authors":["Raghu Bollapragada","Tyler Chen","Rachel Ward"],"pdf_url":"https://arxiv.org/pdf/2206.07553v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07831v1","updated":"2023-12-13T01:36:18Z","published":"2023-12-13T01:36:18Z","title":"Abusive Span Detection for Vietnamese Narrative Texts","summary":"  Abuse in its various forms, including physical, psychological, verbal,\nsexual, financial, and cultural, has a negative impact on mental health.\nHowever, there are limited studies on applying natural language processing\n(NLP) in this field in Vietnam. Therefore, we aim to contribute by building a\nhuman-annotated Vietnamese dataset for detecting abusive content in Vietnamese\nnarrative texts. We sourced these texts from VnExpress, Vietnam's popular\nonline newspaper, where readers often share stories containing abusive content.\nIdentifying and categorizing abusive spans in these texts posed significant\nchallenges during dataset creation, but it also motivated our research. We\nexperimented with lightweight baseline models by freezing PhoBERT and\nXLM-RoBERTa and using their hidden states in a BiLSTM to assess the complexity\nof the dataset. According to our experimental results, PhoBERT outperforms\nother models in both labeled and unlabeled abusive span detection tasks. These\nresults indicate that it has the potential for future improvements.\n","authors":["Nhu-Thanh Nguyen","Khoa Thi-Kim Phan","Duc-Vu Nguyen","Ngan Luu-Thuy Nguyen"],"pdf_url":"https://arxiv.org/pdf/2312.07831v1.pdf","comment":"Accepted at SoICT 2023"},{"id":"http://arxiv.org/abs/2311.18188v3","updated":"2023-12-13T01:33:23Z","published":"2023-11-30T02:15:07Z","title":"Leveraging cache to enable SLU on tiny devices","summary":"  This paper addresses spoken language understanding (SLU) on\nmicrocontroller-like embedded devices, integrating on-device execution with\ncloud offloading in a novel fashion. We exploit temporal locality in a device's\nspeech inputs and accordingly reuse recent SLU inferences. Our idea is simple:\nlet the device match new inputs against cached results, and only offload\nunmatched inputs to the cloud for full inference. Realization of this idea,\nhowever, is non-trivial: the device needs to compare acoustic features in a\nrobust, low-cost way. To this end, we present XYZ, a speech cache for tiny\ndevices. It matches speech inputs at two levels of representations: first by\nclustered sequences of raw sound units, then as sequences of phonemes. Working\nin tandem, the two representations offer complementary cost/accuracy tradeoffs.\nTo further boost accuracy, our cache is learning: with the mismatched and then\noffloaded inputs, it continuously finetunes the device's feature extractors\n(with the assistance of the cloud). We implement XYZ on an off-the-shelf STM32\nmicrocontroller. The resultant implementation has a small memory footprint of\n2MB. Evaluated on challenging speech benchmarks, our system resolves 45%--90%\nof inputs on device, reducing the average latency by up to 80% compared to\noffloading to popular cloud speech services. Our benefit is pronounced even in\nadversarial settings -- noisy environments, cold cache, or one device shared by\na number of users.\n","authors":["Afsara Benazir","Zhiming Xu","Felix Xiaozhu Lin"],"pdf_url":"https://arxiv.org/pdf/2311.18188v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.12361v2","updated":"2023-12-13T01:18:41Z","published":"2023-01-29T05:33:07Z","title":"Graph Harmony: Denoising and Nuclear-Norm Wasserstein Adaptation for\n  Enhanced Domain Transfer in Graph-Structured Data","summary":"  Graph-structured data can be found in numerous domains, yet the scarcity of\nlabeled instances hinders its effective utilization of deep learning in many\nscenarios. Traditional unsupervised domain adaptation (UDA) strategies for\ngraphs primarily hinge on adversarial learning and pseudo-labeling. These\napproaches fail to effectively leverage graph discriminative features, leading\nto class mismatching and unreliable label quality. To navigate these obstacles,\nwe develop the Denoising and Nuclear-Norm Wasserstein Adaptation Network\n(DNAN). DNAN employs the Nuclear-norm Wasserstein discrepancy (NWD), which can\nsimultaneously achieve domain alignment and class distinguishment. DANA also\nintegrates a denoising mechanism via a variational graph autoencoder that\nmitigates data noise. This denoising mechanism helps capture essential features\nof both source and target domains, improving the robustness of the domain\nadaptation process. Our comprehensive experiments demonstrate that DNAN\noutperforms state-of-the-art methods on standard UDA benchmarks for graph\nclassification.\n","authors":["Mengxi Wu","Mohammad Rostami"],"pdf_url":"https://arxiv.org/pdf/2301.12361v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.07939v3","updated":"2023-12-13T01:15:00Z","published":"2023-04-17T01:33:24Z","title":"Leveraging sparse and shared feature activations for disentangled\n  representation learning","summary":"  Recovering the latent factors of variation of high dimensional data has so\nfar focused on simple synthetic settings. Mostly building on unsupervised and\nweakly-supervised objectives, prior work missed out on the positive\nimplications for representation learning on real world data. In this work, we\npropose to leverage knowledge extracted from a diversified set of supervised\ntasks to learn a common disentangled representation. Assuming each supervised\ntask only depends on an unknown subset of the factors of variation, we\ndisentangle the feature space of a supervised multi-task model, with features\nactivating sparsely across different tasks and information being shared as\nappropriate. Importantly, we never directly observe the factors of variations\nbut establish that access to multiple tasks is sufficient for identifiability\nunder sufficiency and minimality assumptions. We validate our approach on six\nreal world distribution shift benchmarks, and different data modalities\n(images, text), demonstrating how disentangled representations can be\ntransferred to real settings.\n","authors":["Marco Fumero","Florian Wenzel","Luca Zancato","Alessandro Achille","Emanuele Rodolà","Stefano Soatto","Bernhard Schölkopf","Francesco Locatello"],"pdf_url":"https://arxiv.org/pdf/2304.07939v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07822v1","updated":"2023-12-13T01:15:00Z","published":"2023-12-13T01:15:00Z","title":"Prototypical Self-Explainable Models Without Re-training","summary":"  Explainable AI (XAI) has unfolded in two distinct research directions with,\non the one hand, post-hoc methods that explain the predictions of a pre-trained\nblack-box model and, on the other hand, self-explainable models (SEMs) which\nare trained directly to provide explanations alongside their predictions. While\nthe latter is preferred in most safety-critical scenarios, post-hoc approaches\nhave received the majority of attention until now, owing to their simplicity\nand ability to explain base models without retraining. Current SEMs instead,\nrequire complex architectures and heavily regularized loss functions, thus\nnecessitating specific and costly training. To address this shortcoming and\nfacilitate wider use of SEMs, we propose a simple yet efficient universal\nmethod called KMEx (K-Means Explainer), which can convert any existing\npre-trained model into a prototypical SEM. The motivation behind KMEx is to\npush towards more transparent deep learning-based decision-making via\nclass-prototype-based explanations that are guaranteed to be diverse and\ntrustworthy without retraining the base model. We compare models obtained from\nKMEx to state-of-the-art SEMs using an extensive qualitative evaluation to\nhighlight the strengths and weaknesses of each model, further paving the way\ntoward a more reliable and objective evaluation of SEMs.\n","authors":["Srishti Gautam","Ahcene Boubekki","Marina M. C. Höhne","Michael C. Kampffmeyer"],"pdf_url":"https://arxiv.org/pdf/2312.07822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07821v1","updated":"2023-12-13T01:11:35Z","published":"2023-12-13T01:11:35Z","title":"Radio Signal Classification by Adversarially Robust Quantum Machine\n  Learning","summary":"  Radio signal classification plays a pivotal role in identifying the\nmodulation scheme used in received radio signals, which is essential for\ndemodulation and proper interpretation of the transmitted information.\nResearchers have underscored the high susceptibility of ML algorithms for radio\nsignal classification to adversarial attacks. Such vulnerability could result\nin severe consequences, including misinterpretation of critical messages,\ninterception of classified information, or disruption of communication\nchannels. Recent advancements in quantum computing have revolutionized theories\nand implementations of computation, bringing the unprecedented development of\nQuantum Machine Learning (QML). It is shown that quantum variational\nclassifiers (QVCs) provide notably enhanced robustness against classical\nadversarial attacks in image classification. However, no research has yet\nexplored whether QML can similarly mitigate adversarial threats in the context\nof radio signal classification. This work applies QVCs to radio signal\nclassification and studies their robustness to various adversarial attacks. We\nalso propose the novel application of the approximate amplitude encoding (AAE)\ntechnique to encode radio signal data efficiently. Our extensive simulation\nresults present that attacks generated on QVCs transfer well to CNN models,\nindicating that these adversarial examples can fool neural networks that they\nare not explicitly designed to attack. However, the converse is not true. QVCs\nprimarily resist the attacks generated on CNNs. Overall, with comprehensive\nsimulations, our results shed new light on the growing field of QML by bridging\nknowledge gaps in QAML in radio signal classification and uncovering the\nadvantages of applying QML methods in practical applications.\n","authors":["Yanqiu Wu","Eromanga Adermann","Chandra Thapa","Seyit Camtepe","Hajime Suzuki","Muhammad Usman"],"pdf_url":"https://arxiv.org/pdf/2312.07821v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2312.07813v1","updated":"2023-12-13T00:23:22Z","published":"2023-12-13T00:23:22Z","title":"On a Foundation Model for Operating Systems","summary":"  This paper lays down the research agenda for a domain-specific foundation\nmodel for operating systems (OSes). Our case for a foundation model revolves\naround the observations that several OS components such as CPU, memory, and\nnetwork subsystems are interrelated and that OS traces offer the ideal dataset\nfor a foundation model to grasp the intricacies of diverse OS components and\ntheir behavior in varying environments and workloads. We discuss a wide range\nof possibilities that then arise, from employing foundation models as policy\nagents to utilizing them as generators and predictors to assist traditional OS\ncontrol algorithms. Our hope is that this paper spurs further research into OS\nfoundation models and creating the next generation of operating systems for the\nevolving computing landscape.\n","authors":["Divyanshu Saxena","Nihal Sharma","Donghyun Kim","Rohit Dwivedula","Jiayi Chen","Chenxi Yang","Sriram Ravula","Zichao Hu","Aditya Akella","Sebastian Angel","Joydeep Biswas","Swarat Chaudhuri","Isil Dillig","Alex Dimakis","P. Brighten Godfrey","Daehyeok Kim","Chris Rossbach","Gang Wang"],"pdf_url":"https://arxiv.org/pdf/2312.07813v1.pdf","comment":"Machine Learning for Systems Workshop at 37th NeurIPS Conference,\n  2023, New Orleans, LA, USA"},{"id":"http://arxiv.org/abs/2309.03999v2","updated":"2023-12-13T00:06:18Z","published":"2023-09-07T20:05:39Z","title":"Adapting Self-Supervised Representations to Multi-Domain Setups","summary":"  Current state-of-the-art self-supervised approaches, are effective when\ntrained on individual domains but show limited generalization on unseen\ndomains. We observe that these models poorly generalize even when trained on a\nmixture of domains, making them unsuitable to be deployed under diverse\nreal-world setups. We therefore propose a general-purpose, lightweight Domain\nDisentanglement Module (DDM) that can be plugged into any self-supervised\nencoder to effectively perform representation learning on multiple, diverse\ndomains with or without shared classes. During pre-training according to a\nself-supervised loss, DDM enforces a disentanglement in the representation\nspace by splitting it into a domain-variant and a domain-invariant portion.\nWhen domain labels are not available, DDM uses a robust clustering approach to\ndiscover pseudo-domains. We show that pre-training with DDM can show up to 3.5%\nimprovement in linear probing accuracy on state-of-the-art self-supervised\nmodels including SimCLR, MoCo, BYOL, DINO, SimSiam and Barlow Twins on\nmulti-domain benchmarks including PACS, DomainNet and WILDS. Models trained\nwith DDM show significantly improved generalization (7.4%) to unseen domains\ncompared to baselines. Therefore, DDM can efficiently adapt self-supervised\nencoders to provide high-quality, generalizable representations for diverse\nmulti-domain data.\n","authors":["Neha Kalibhat","Sam Sharpe","Jeremy Goodsitt","Bayan Bruss","Soheil Feizi"],"pdf_url":"https://arxiv.org/pdf/2309.03999v2.pdf","comment":"Published at BMVC 2023"}],"Multimedia":[{"id":"http://arxiv.org/abs/2312.08330v1","updated":"2023-12-13T18:04:31Z","published":"2023-12-13T18:04:31Z","title":"Preparing VVC for Streaming: A Fast Multi-Rate Encoding Approach","summary":"  The integration of advanced video codecs into the streaming pipeline is\ngrowing in response to the increasing demand for high quality video content.\nHowever, the significant computational demand for advanced codecs like\nVersatile Video Coding (VVC) poses challenges for service providers, including\nlonger encoding time and higher encoding cost. This challenge becomes even more\npronounced in streaming, as the same content needs to be encoded at multiple\nbitrates (also known as representations) to accommodate different network\nconditions. To accelerate the encoding process of multiple representations of\nthe same content in VVC, we employ the encoding map of a single representation,\nknown as the reference representation, and utilize its partitioning structure\nto accelerate the encoding of the remaining representations, referred to as\ndependent representations. To ensure compatibility with parallel processing, we\ndesignate the lowest bitrate representation as the reference representation.\nThe experimental results indicate a substantial improvement in the encoding\ntime for the dependent representations, achieving an average reduction of 40%,\nwhile maintaining a minimal average quality drop of only 0.43 in Video\nMulti-method Assessment Fusion (VMAF). This improvement is observed when\nutilizing Versatile Video Encoder (VVenC), an open and optimized VVC encoder\nimplementation.\n","authors":["Yiqun Liu","Hadi Amirpour","Mohsen Abdoli","Christian Timmerer","Thomas Guionnet"],"pdf_url":"https://arxiv.org/pdf/2312.08330v1.pdf","comment":"Accepted by VCIP 2023"},{"id":"http://arxiv.org/abs/2312.08213v1","updated":"2023-12-13T15:30:29Z","published":"2023-12-13T15:30:29Z","title":"Accelerated Event-Based Feature Detection and Compression for\n  Surveillance Video Systems","summary":"  The strong temporal consistency of surveillance video enables compelling\ncompression performance with traditional methods, but downstream vision\napplications operate on decoded image frames with a high data rate. Since it is\nnot straightforward for applications to extract information on temporal\nredundancy from the compressed video representations, we propose a novel system\nwhich conveys temporal redundancy within a sparse decompressed representation.\nWe leverage a video representation framework called ADDER to transcode framed\nvideos to sparse, asynchronous intensity samples. We introduce mechanisms for\ncontent adaptation, lossy compression, and asynchronous forms of classical\nvision algorithms. We evaluate our system on the VIRAT surveillance video\ndataset, and we show a median 43.7% speed improvement in FAST feature detection\ncompared to OpenCV. We run the same algorithm as OpenCV, but only process\npixels that receive new asynchronous events, rather than process every pixel in\nan image frame. Our work paves the way for upcoming neuromorphic sensors and is\namenable to future applications with spiking neural networks.\n","authors":["Andrew C. Freeman","Ketan Mayer-Patel","Montek Singh"],"pdf_url":"https://arxiv.org/pdf/2312.08213v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08195v1","updated":"2023-12-13T14:59:49Z","published":"2023-12-13T14:59:49Z","title":"Concept-centric Personalization with Large-scale Diffusion Priors","summary":"  Despite large-scale diffusion models being highly capable of generating\ndiverse open-world content, they still struggle to match the photorealism and\nfidelity of concept-specific generators. In this work, we present the task of\ncustomizing large-scale diffusion priors for specific concepts as\nconcept-centric personalization. Our goal is to generate high-quality\nconcept-centric images while maintaining the versatile controllability inherent\nto open-world models, enabling applications in diverse tasks such as\nconcept-centric stylization and image translation. To tackle these challenges,\nwe identify catastrophic forgetting of guidance prediction from diffusion\npriors as the fundamental issue. Consequently, we develop a guidance-decoupled\npersonalization framework specifically designed to address this task. We\npropose Generalized Classifier-free Guidance (GCFG) as the foundational theory\nfor our framework. This approach extends Classifier-free Guidance (CFG) to\naccommodate an arbitrary number of guidances, sourced from a variety of\nconditions and models. Employing GCFG enables us to separate conditional\nguidance into two distinct components: concept guidance for fidelity and\ncontrol guidance for controllability. This division makes it feasible to train\na specialized model for concept guidance, while ensuring both control and\nunconditional guidance remain intact. We then present a null-text\nConcept-centric Diffusion Model as a concept-specific generator to learn\nconcept guidance without the need for text annotations. Code will be available\nat https://github.com/PRIV-Creation/Concept-centric-Personalization.\n","authors":["Pu Cao","Lu Yang","Feng Zhou","Tianrui Huang","Qing Song"],"pdf_url":"https://arxiv.org/pdf/2312.08195v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.08912v2","updated":"2023-12-13T05:24:46Z","published":"2023-09-16T07:30:52Z","title":"Delving into Multimodal Prompting for Fine-grained Visual Classification","summary":"  Fine-grained visual classification (FGVC) involves categorizing fine\nsubdivisions within a broader category, which poses challenges due to subtle\ninter-class discrepancies and large intra-class variations. However, prevailing\napproaches primarily focus on uni-modal visual concepts. Recent advancements in\npre-trained vision-language models have demonstrated remarkable performance in\nvarious high-level vision tasks, yet the applicability of such models to FGVC\ntasks remains uncertain. In this paper, we aim to fully exploit the\ncapabilities of cross-modal description to tackle FGVC tasks and propose a\nnovel multimodal prompting solution, denoted as MP-FGVC, based on the\ncontrastive language-image pertaining (CLIP) model. Our MP-FGVC comprises a\nmultimodal prompts scheme and a multimodal adaptation scheme. The former\nincludes Subcategory-specific Vision Prompt (SsVP) and Discrepancy-aware Text\nPrompt (DaTP), which explicitly highlights the subcategory-specific\ndiscrepancies from the perspectives of both vision and language. The latter\naligns the vision and text prompting elements in a common semantic space,\nfacilitating cross-modal collaborative reasoning through a Vision-Language\nFusion Module (VLFM) for further improvement on FGVC. Moreover, we tailor a\ntwo-stage optimization strategy for MP-FGVC to fully leverage the pre-trained\nCLIP model and expedite efficient adaptation for FGVC. Extensive experiments\nconducted on four FGVC datasets demonstrate the effectiveness of our MP-FGVC.\n","authors":["Xin Jiang","Hao Tang","Junyao Gao","Xiaoyu Du","Shengfeng He","Zechao Li"],"pdf_url":"https://arxiv.org/pdf/2309.08912v2.pdf","comment":"This paper is accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2305.06152v3","updated":"2023-12-13T04:21:23Z","published":"2023-05-06T03:57:05Z","title":"Structure-CLIP: Towards Scene Graph Knowledge to Enhance Multi-modal\n  Structured Representations","summary":"  Large-scale vision-language pre-training has achieved significant performance\nin multi-modal understanding and generation tasks. However, existing methods\noften perform poorly on image-text matching tasks that require structured\nrepresentations, i.e., representations of objects, attributes, and relations.\nAs illustrated in Fig.~reffig:case (a), the models cannot make a distinction\nbetween ``An astronaut rides a horse\" and ``A horse rides an astronaut\". This\nis because they fail to fully leverage structured knowledge when learning\nrepresentations in multi-modal scenarios. In this paper, we present an\nend-to-end framework Structure-CLIP, which integrates Scene Graph Knowledge\n(SGK) to enhance multi-modal structured representations. Firstly, we use scene\ngraphs to guide the construction of semantic negative examples, which results\nin an increased emphasis on learning structured representations. Moreover, a\nKnowledge-Enhance Encoder (KEE) is proposed to leverage SGK as input to further\nenhance structured representations. To verify the effectiveness of the proposed\nframework, we pre-train our model with the aforementioned approaches and\nconduct experiments on downstream tasks. Experimental results demonstrate that\nStructure-CLIP achieves state-of-the-art (SOTA) performance on VG-Attribution\nand VG-Relation datasets, with 12.5% and 4.1% ahead of the multi-modal SOTA\nmodel respectively. Meanwhile, the results on MSCOCO indicate that\nStructure-CLIP significantly enhances the structured representations while\nmaintaining the ability of general representations. Our code is available at\nhttps://github.com/zjukg/Structure-CLIP.\n","authors":["Yufeng Huang","Jiji Tang","Zhuo Chen","Rongsheng Zhang","Xinfeng Zhang","Weijie Chen","Zeng Zhao","Zhou Zhao","Tangjie Lv","Zhipeng Hu","Wen Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.06152v3.pdf","comment":"AAAI 2024, https://github.com/zjukg/Structure-CLIP"},{"id":"http://arxiv.org/abs/2308.07056v7","updated":"2023-12-13T02:24:37Z","published":"2023-08-14T10:31:29Z","title":"VoxBlink: A Large Scale Speaker Verification Dataset on Camera","summary":"  In this paper, we introduce a large-scale and high-quality audio-visual\nspeaker verification dataset, named VoxBlink. We propose an innovative and\nrobust automatic audio-visual data mining pipeline to curate this dataset,\nwhich contains 1.45M utterances from 38K speakers. Due to the inherent nature\nof automated data collection, introducing noisy data is inevitable. Therefore,\nwe also utilize a multi-modal purification step to generate a cleaner version\nof the VoxBlink, named VoxBlink-clean, comprising 18K identities and 1.02M\nutterances. In contrast to the VoxCeleb, the VoxBlink sources from short videos\nof ordinary users, and the covered scenarios can better align with real-life\nsituations. To our best knowledge, the VoxBlink dataset is one of the largest\npublicly available speaker verification datasets. Leveraging the VoxCeleb and\nVoxBlink-clean datasets together, we employ diverse speaker verification models\nwith multiple architectural backbones to conduct comprehensive evaluations on\nthe VoxCeleb test sets. Experimental results indicate a substantial enhancement\nin performance,ranging from 12% to 30% relatively, across various backbone\narchitectures upon incorporating the VoxBlink-clean into the training process.\nThe details of the dataset can be found on http://voxblink.github.io\n","authors":["Yuke Lin","Xiaoyi Qin","Guoqing Zhao","Ming Cheng","Ning Jiang","Haiyang Wu","Ming Li"],"pdf_url":"https://arxiv.org/pdf/2308.07056v7.pdf","comment":"Accepted By ICASSP2024"}]},"2023-12-14T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2312.08274v2","updated":"2023-12-14T07:28:03Z","published":"2023-12-13T16:43:41Z","title":"High-throughput Biomedical Relation Extraction for Semi-Structured Web\n  Articles Empowered by Large Language Models","summary":"  Objective: To develop a high-throughput biomedical relation extraction system\nthat takes advantage of the large language models' (LLMs) reading comprehension\nability and biomedical world knowledge in a scalable and evidential manner.\nMethods: We formulate the relation extraction task as a simple binary\nclassification problem for large language models such as ChatGPT. Specifically,\nLLMs make the decision based on the external corpus and its world knowledge,\ngiving the reason for the judgment to factual verification. This method is\ntailored for semi-structured web articles, wherein we designate the main title\nas the tail entity and explicitly incorporate it into the context, and the\npotential head entities are matched based on a biomedical thesaurus. Moreover,\nlengthy contents are sliced into text chunks, embedded, and retrieved with\nadditional embedding models, ensuring compatibility with the context window\nsize constraints of available open-source LLMs. Results: Using an open-source\nLLM, we extracted 304315 relation triplets of three distinct relation types\nfrom four reputable biomedical websites. To assess the efficacy of the basic\npipeline employed for biomedical relation extraction, we curated a benchmark\ndataset annotated by a medical expert. Evaluation results indicate that the\npipeline exhibits performance comparable to that of GPT-4. Case studies further\nilluminate challenges faced by contemporary LLMs in the context of biomedical\nrelation extraction for semi-structured web articles. Conclusion: The proposed\nmethod has demonstrated its effectiveness in leveraging the strengths of LLMs\nfor high-throughput biomedical relation extraction. Its adaptability is\nevident, as it can be seamlessly extended to diverse semi-structured biomedical\nwebsites, facilitating the extraction of various types of biomedical relations\nwith ease.\n","authors":["Songchi Zhou","Sheng Yu"],"pdf_url":"https://arxiv.org/pdf/2312.08274v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08078v2","updated":"2023-12-14T02:31:44Z","published":"2023-12-13T11:47:28Z","title":"Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic\n  Image-Report Generation","summary":"  To address these issues, we propose a novel Adaptive patch-word Matching\n(AdaMatch) model to correlate chest X-ray (CXR) image regions with words in\nmedical reports and apply it to CXR-report generation to provide explainability\nfor the generation process. AdaMatch exploits the fine-grained relation between\nadaptive patches and words to provide explanations of specific image regions\nwith corresponding words. To capture the abnormal regions of varying sizes and\npositions, we introduce the Adaptive Patch extraction (AdaPatch) module to\nacquire the adaptive patches for these regions adaptively. In order to provide\nexplicit explainability for CXR-report generation task, we propose an\nAdaMatch-based bidirectional large language model for Cyclic CXR-report\ngeneration (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywords\nfor CXR images and `keypatches' for medical reports as hints to guide\nCXR-report generation. Extensive experiments on two publicly available CXR\ndatasets prove the effectiveness of our method and its superior performance to\nexisting methods.\n","authors":["Wenting Chen","Xiang Li","Linlin Shen","Yixuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2312.08078v2.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2312.07987v2","updated":"2023-12-14T06:35:33Z","published":"2023-12-13T09:00:21Z","title":"SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention","summary":"  The costly self-attention layers in modern Transformers require memory and\ncompute quadratic in sequence length. Existing approximation methods usually\nunderperform and fail to obtain significant speedups in practice. Here we\npresent SwitchHead - a novel method that reduces both compute and memory\nrequirements and achieves wall-clock speedup, while matching the language\nmodeling performance of baseline Transformers with the same parameter budget.\nSwitchHead uses Mixture-of-Experts (MoE) layers for the value and output\nprojections and requires 4 to 8 times fewer attention matrices than standard\nTransformers. Our novel attention can also be combined with MoE MLP layers,\nresulting in an efficient fully-MoE \"SwitchAll\" Transformer model. Our code is\npublic.\n","authors":["Róbert Csordás","Piotr Piękos","Kazuki Irie","Jürgen Schmidhuber"],"pdf_url":"https://arxiv.org/pdf/2312.07987v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05497v3","updated":"2023-12-14T12:06:24Z","published":"2023-12-09T07:51:56Z","title":"History Matters: Temporal Knowledge Editing in Large Language Model","summary":"  The imperative task of revising or updating the knowledge stored within large\nlanguage models arises from two distinct sources: intrinsic errors inherent in\nthe model which should be corrected and outdated knowledge due to external\nshifts in the real world which should be updated. Prevailing efforts in model\nediting conflate these two distinct categories of edits arising from distinct\nreasons and directly modify the original knowledge in models into new\nknowledge. However, we argue that preserving the model's original knowledge\nremains pertinent. Specifically, if a model's knowledge becomes outdated due to\nevolving worldly dynamics, it should retain recollection of the historical\nknowledge while integrating the newfound knowledge. In this work, we introduce\nthe task of Temporal Knowledge Editing (TKE) and establish a benchmark AToKe\n(Assessment of TempOral Knowledge Editing) to evaluate current model editing\nmethods. We find that while existing model editing methods are effective at\nmaking models remember new knowledge, the edited model catastrophically forgets\nhistorical knowledge. To address this gap, we propose a simple and general\nframework termed Multi-Editing with Time Objective (METO) for enhancing\nexisting editing models, which edits both historical and new knowledge\nconcurrently and optimizes the model's prediction for the time of each fact.\nOur assessments demonstrate that while AToKe is still difficult, METO maintains\nthe effectiveness of learning new knowledge and meanwhile substantially\nimproves the performance of edited models on utilizing historical knowledge.\n","authors":["Xunjian Yin","Jin Jiang","Liming Yang","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2312.05497v3.pdf","comment":"AAAI 2024. 9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2312.07622v2","updated":"2023-12-14T04:01:17Z","published":"2023-12-12T01:39:16Z","title":"Mathematical Language Models: A Survey","summary":"  In recent years, there has been remarkable progress in leveraging Language\nModels (LMs), encompassing Pre-trained Language Models (PLMs) and Large-scale\nLanguage Models (LLMs), within the domain of mathematics. This paper conducts a\ncomprehensive survey of mathematical LMs, systematically categorizing pivotal\nresearch endeavors from two distinct perspectives: tasks and methodologies. The\nlandscape reveals a large number of proposed mathematical LLMs, which are\nfurther delineated into instruction learning, tool-based methods, fundamental\nCoT techniques, and advanced CoT methodologies. In addition, our survey entails\nthe compilation of over 60 mathematical datasets, including training datasets,\nbenchmark datasets, and augmented datasets. Addressing the primary challenges\nand delineating future trajectories within the field of mathematical LMs, this\nsurvey is positioned as a valuable resource, poised to facilitate and inspire\nfuture innovation among researchers invested in advancing this domain.\n","authors":["Wentao Liu","Hanglei Hu","Jie Zhou","Yuyang Ding","Junsong Li","Jiayi Zeng","Mengliang He","Qin Chen","Bo Jiang","Aimin Zhou","Liang He"],"pdf_url":"https://arxiv.org/pdf/2312.07622v2.pdf","comment":"arXiv admin note: text overlap with arXiv:1705.04146,\n  arXiv:2304.10977, arXiv:2112.00114, arXiv:1905.13319, arXiv:2304.12244,\n  arXiv:2206.01347, arXiv:2006.09265 by other authors"},{"id":"http://arxiv.org/abs/2311.05741v2","updated":"2023-12-14T23:37:14Z","published":"2023-11-09T20:59:08Z","title":"Efficiently Adapting Pretrained Language Models To New Languages","summary":"  Recent large language models (LLM) exhibit sub-optimal performance on\nlow-resource languages, as the training data of these models is usually\ndominated by English and other high-resource languages. Furthermore, it is\nchallenging to train models for low-resource languages, especially from\nscratch, due to a lack of high quality training data. Adapting pretrained LLMs\nreduces the need for data in the new language while also providing cross\nlingual transfer capabilities. However, naively adapting to new languages leads\nto catastrophic forgetting and poor tokenizer efficiency. In this work, we\nstudy how to efficiently adapt any existing pretrained LLM to a new language\nwithout running into these issues. In particular, we improve the encoding\nefficiency of the tokenizer by adding new tokens from the target language and\nstudy the data mixing recipe to mitigate forgetting. Our experiments on\nadapting an English LLM to Hungarian and Thai show that our recipe can reach\nbetter performance than open source models on the target language, with minimal\nregressions on English.\n","authors":["Zoltan Csaki","Pian Pawakapan","Urmish Thakker","Qiantong Xu"],"pdf_url":"https://arxiv.org/pdf/2311.05741v2.pdf","comment":"Accepted to \"The third Neurips Workshop on Efficient Natural Language\n  and Speech Processing 2023\" (ENLSP-III)"},{"id":"http://arxiv.org/abs/2312.09390v1","updated":"2023-12-14T23:07:33Z","published":"2023-12-14T23:07:33Z","title":"Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak\n  Supervision","summary":"  Widely used alignment techniques, such as reinforcement learning from human\nfeedback (RLHF), rely on the ability of humans to supervise model behavior -\nfor example, to evaluate whether a model faithfully followed instructions or\ngenerated safe outputs. However, future superhuman models will behave in\ncomplex ways too difficult for humans to reliably evaluate; humans will only be\nable to weakly supervise superhuman models. We study an analogy to this\nproblem: can weak model supervision elicit the full capabilities of a much\nstronger model? We test this using a range of pretrained language models in the\nGPT-4 family on natural language processing (NLP), chess, and reward modeling\ntasks. We find that when we naively finetune strong pretrained models on labels\ngenerated by a weak model, they consistently perform better than their weak\nsupervisors, a phenomenon we call weak-to-strong generalization. However, we\nare still far from recovering the full capabilities of strong models with naive\nfinetuning alone, suggesting that techniques like RLHF may scale poorly to\nsuperhuman models without further work. We find that simple methods can often\nsignificantly improve weak-to-strong generalization: for example, when\nfinetuning GPT-4 with a GPT-2-level supervisor and an auxiliary confidence\nloss, we can recover close to GPT-3.5-level performance on NLP tasks. Our\nresults suggest that it is feasible to make empirical progress today on a\nfundamental challenge of aligning superhuman models.\n","authors":["Collin Burns","Pavel Izmailov","Jan Hendrik Kirchner","Bowen Baker","Leo Gao","Leopold Aschenbrenner","Yining Chen","Adrien Ecoffet","Manas Joglekar","Jan Leike","Ilya Sutskever","Jeff Wu"],"pdf_url":"https://arxiv.org/pdf/2312.09390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09366v1","updated":"2023-12-14T22:04:07Z","published":"2023-12-14T22:04:07Z","title":"Arabic Mini-ClimateGPT : A Climate Change and Sustainability Tailored\n  Arabic LLM","summary":"  Climate change is one of the most significant challenges we face together as\na society. Creating awareness and educating policy makers the wide-ranging\nimpact of climate change is an essential step towards a sustainable future.\nRecently, Large Language Models (LLMs) like ChatGPT and Bard have shown\nimpressive conversational abilities and excel in a wide variety of NLP tasks.\nWhile these models are close-source, recently alternative open-source LLMs such\nas Stanford Alpaca and Vicuna have shown promising results. However, these\nopen-source models are not specifically tailored for climate related domain\nspecific information and also struggle to generate meaningful responses in\nother languages such as, Arabic. To this end, we propose a light-weight Arabic\nMini-ClimateGPT that is built on an open-source LLM and is specifically\nfine-tuned on a conversational-style instruction tuning curated Arabic dataset\nClima500-Instruct with over 500k instructions about climate change and\nsustainability. Further, our model also utilizes a vector embedding based\nretrieval mechanism during inference. We validate our proposed model through\nquantitative and qualitative evaluations on climate-related queries. Our model\nsurpasses the baseline LLM in 88.3% of cases during ChatGPT-based evaluation.\nFurthermore, our human expert evaluation reveals an 81.6% preference for our\nmodel's responses over multiple popular open-source models. Our open-source\ndemos, code-base and models are available here\nhttps://github.com/mbzuai-oryx/ClimateGPT.\n","authors":["Sahal Shaji Mullappilly","Abdelrahman Shaker","Omkar Thawakar","Hisham Cholakkal","Rao Muhammad Anwer","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2312.09366v1.pdf","comment":"Accepted to EMNLP 2023 (Findings)"},{"id":"http://arxiv.org/abs/2310.16218v3","updated":"2023-12-14T21:49:59Z","published":"2023-10-24T22:18:13Z","title":"Knowledge Editing for Large Language Models: A Survey","summary":"  Large language models (LLMs) have recently transformed both the academic and\nindustrial landscapes due to their remarkable capacity to understand, analyze,\nand generate texts based on their vast knowledge and reasoning ability.\nNevertheless, one major drawback of LLMs is their substantial computational\ncost for pre-training due to their unprecedented amounts of parameters. The\ndisadvantage is exacerbated when new knowledge frequently needs to be\nintroduced into the pre-trained model. Therefore, it is imperative to develop\neffective and efficient techniques to update pre-trained LLMs. Traditional\nmethods encode new knowledge in pre-trained LLMs through direct fine-tuning.\nHowever, naively re-training LLMs can be computationally intensive and risks\ndegenerating valuable pre-trained knowledge irrelevant to the update in the\nmodel. Recently, Knowledge-based Model Editing (KME) has attracted increasing\nattention, which aims to precisely modify the LLMs to incorporate specific\nknowledge, without negatively influencing other irrelevant knowledge. In this\nsurvey, we aim to provide a comprehensive and in-depth overview of recent\nadvances in the field of KME. We first introduce a general formulation of KME\nto encompass different KME strategies. Afterward, we provide an innovative\ntaxonomy of KME techniques based on how the new knowledge is introduced into\npre-trained LLMs, and investigate existing KME strategies while analyzing key\ninsights, advantages, and limitations of methods from each category. Moreover,\nrepresentative metrics, datasets, and applications of KME are introduced\naccordingly. Finally, we provide an in-depth analysis regarding the\npracticality and remaining challenges of KME and suggest promising research\ndirections for further advancement in this field.\n","authors":["Song Wang","Yaochen Zhu","Haochen Liu","Zaiyi Zheng","Chen Chen","Jundong Li"],"pdf_url":"https://arxiv.org/pdf/2310.16218v3.pdf","comment":"33 pages"},{"id":"http://arxiv.org/abs/2310.13859v2","updated":"2023-12-14T20:34:32Z","published":"2023-10-20T23:47:01Z","title":"Not all Fake News is Written: A Dataset and Analysis of Misleading Video\n  Headlines","summary":"  Polarization and the marketplace for impressions have conspired to make\nnavigating information online difficult for users, and while there has been a\nsignificant effort to detect false or misleading text, multimodal datasets have\nreceived considerably less attention. To complement existing resources, we\npresent multimodal Video Misleading Headline (VMH), a dataset that consists of\nvideos and whether annotators believe the headline is representative of the\nvideo's contents. After collecting and annotating this dataset, we analyze\nmultimodal baselines for detecting misleading headlines. Our annotation process\nalso focuses on why annotators view a video as misleading, allowing us to\nbetter understand the interplay of annotators' background and the content of\nthe videos.\n","authors":["Yoo Yeon Sung","Jordan Boyd-Graber","Naeemul Hassan"],"pdf_url":"https://arxiv.org/pdf/2310.13859v2.pdf","comment":"EMNLP 2023 Main Paper"},{"id":"http://arxiv.org/abs/2312.07559v2","updated":"2023-12-14T19:40:04Z","published":"2023-12-08T18:50:20Z","title":"PaperQA: Retrieval-Augmented Generative Agent for Scientific Research","summary":"  Large Language Models (LLMs) generalize well across language tasks, but\nsuffer from hallucinations and uninterpretability, making it difficult to\nassess their accuracy without ground-truth. Retrieval-Augmented Generation\n(RAG) models have been proposed to reduce hallucinations and provide provenance\nfor how an answer was generated. Applying such models to the scientific\nliterature may enable large-scale, systematic processing of scientific\nknowledge. We present PaperQA, a RAG agent for answering questions over the\nscientific literature. PaperQA is an agent that performs information retrieval\nacross full-text scientific articles, assesses the relevance of sources and\npassages, and uses RAG to provide answers. Viewing this agent as a question\nanswering model, we find it exceeds performance of existing LLMs and LLM agents\non current science QA benchmarks. To push the field closer to how humans\nperform research on scientific literature, we also introduce LitQA, a more\ncomplex benchmark that requires retrieval and synthesis of information from\nfull-text scientific papers across the literature. Finally, we demonstrate\nPaperQA's matches expert human researchers on LitQA.\n","authors":["Jakub Lála","Odhran O'Donoghue","Aleksandar Shtedritski","Sam Cox","Samuel G. Rodriques","Andrew D. White"],"pdf_url":"https://arxiv.org/pdf/2312.07559v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09304v1","updated":"2023-12-14T19:17:42Z","published":"2023-12-14T19:17:42Z","title":"Well-calibrated Confidence Measures for Multi-label Text Classification\n  with a Large Number of Labels","summary":"  We extend our previous work on Inductive Conformal Prediction (ICP) for\nmulti-label text classification and present a novel approach for addressing the\ncomputational inefficiency of the Label Powerset (LP) ICP, arrising when\ndealing with a high number of unique labels. We present experimental results\nusing the original and the proposed efficient LP-ICP on two English and one\nCzech language data-sets. Specifically, we apply the LP-ICP on three deep\nArtificial Neural Network (ANN) classifiers of two types: one based on\ncontextualised (bert) and two on non-contextualised (word2vec) word-embeddings.\nIn the LP-ICP setting we assign nonconformity scores to label-sets from which\nthe corresponding p-values and prediction-sets are determined. Our approach\ndeals with the increased computational burden of LP by eliminating from\nconsideration a significant number of label-sets that will surely have p-values\nbelow the specified significance level. This reduces dramatically the\ncomputational complexity of the approach while fully respecting the standard CP\nguarantees. Our experimental results show that the contextualised-based\nclassifier surpasses the non-contextualised-based ones and obtains\nstate-of-the-art performance for all data-sets examined. The good performance\nof the underlying classifiers is carried on to their ICP counterparts without\nany significant accuracy loss, but with the added benefits of ICP, i.e. the\nconfidence information encapsulated in the prediction sets. We experimentally\ndemonstrate that the resulting prediction sets can be tight enough to be\npractically useful even though the set of all possible label-sets contains more\nthan $1e+16$ combinations. Additionally, the empirical error rates of the\nobtained prediction-sets confirm that our outputs are well-calibrated.\n","authors":["Lysimachos Maltoudoglou","Andreas Paisios","Ladislav Lenc","Jiří Martínek","Pavel Král","Harris Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2312.09304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09300v1","updated":"2023-12-14T19:09:22Z","published":"2023-12-14T19:09:22Z","title":"Self-Evaluation Improves Selective Generation in Large Language Models","summary":"  Safe deployment of large language models (LLMs) may benefit from a reliable\nmethod for assessing their generated content to determine when to abstain or to\nselectively generate. While likelihood-based metrics such as perplexity are\nwidely employed, recent research has demonstrated the limitations of using\nsequence-level probability estimates given by LLMs as reliable indicators of\ngeneration quality. Conversely, LLMs have demonstrated strong calibration at\nthe token level, particularly when it comes to choosing correct answers in\nmultiple-choice questions or evaluating true/false statements. In this work, we\nreformulate open-ended generation tasks into token-level prediction tasks, and\nleverage LLMs' superior calibration at the token level. We instruct an LLM to\nself-evaluate its answers, employing either a multi-way comparison or a\npoint-wise evaluation approach, with the option to include a ``None of the\nabove'' option to express the model's uncertainty explicitly. We benchmark a\nrange of scoring methods based on self-evaluation and evaluate their\nperformance in selective generation using TruthfulQA and TL;DR. Through\nexperiments with PaLM-2 and GPT-3, we demonstrate that self-evaluation based\nscores not only improve accuracy, but also correlate better with the overall\nquality of generated content.\n","authors":["Jie Ren","Yao Zhao","Tu Vu","Peter J. Liu","Balaji Lakshminarayanan"],"pdf_url":"https://arxiv.org/pdf/2312.09300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09299v1","updated":"2023-12-14T19:08:56Z","published":"2023-12-14T19:08:56Z","title":"Weight subcloning: direct initialization of transformers using larger\n  pretrained ones","summary":"  Training large transformer models from scratch for a target task requires\nlots of data and is computationally demanding. The usual practice of transfer\nlearning overcomes this challenge by initializing the model with weights of a\npretrained model of the same size and specification to increase the convergence\nand training speed. However, what if no pretrained model of the required size\nis available? In this paper, we introduce a simple yet effective technique to\ntransfer the knowledge of a pretrained model to smaller variants. Our approach\ncalled weight subcloning expedites the training of scaled-down transformers by\ninitializing their weights from larger pretrained models.\n  Weight subcloning involves an operation on the pretrained model to obtain the\nequivalent initialized scaled-down model. It consists of two key steps: first,\nwe introduce neuron importance ranking to decrease the embedding dimension per\nlayer in the pretrained model. Then, we remove blocks from the transformer\nmodel to match the number of layers in the scaled-down network. The result is a\nnetwork ready to undergo training, which gains significant improvements in\ntraining speed compared to random initialization. For instance, we achieve 4x\nfaster training for vision transformers in image classification and language\nmodels designed for next token prediction.\n","authors":["Mohammad Samragh","Mehrdad Farajtabar","Sachin Mehta","Raviteja Vemulapalli","Fartash Faghri","Devang Naik","Oncel Tuzel","Mohammad Rastegari"],"pdf_url":"https://arxiv.org/pdf/2312.09299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09241v1","updated":"2023-12-14T18:58:28Z","published":"2023-12-14T18:58:28Z","title":"TinyGSM: achieving >80% on GSM8k with small language models","summary":"  Small-scale models offer various computational advantages, and yet to which\nextent size is critical for problem-solving abilities remains an open question.\nSpecifically for solving grade school math, the smallest model size so far\nrequired to break the 80\\% barrier on the GSM8K benchmark remains to be 34B.\nOur work studies how high-quality datasets may be the key for small language\nmodels to acquire mathematical reasoning. We introduce \\texttt{TinyGSM}, a\nsynthetic dataset of 12.3M grade school math problems paired with Python\nsolutions, generated fully by GPT-3.5. After finetuning on \\texttt{TinyGSM}, we\nfind that a duo of a 1.3B generation model and a 1.3B verifier model can\nachieve 81.5\\% accuracy, outperforming existing models that are orders of\nmagnitude larger. This also rivals the performance of the GPT-3.5 ``teacher''\nmodel (77.4\\%), from which our model's training data is generated. Our approach\nis simple and has two key components: 1) the high-quality dataset\n\\texttt{TinyGSM}, 2) the use of a verifier, which selects the final outputs\nfrom multiple candidate generations.\n","authors":["Bingbin Liu","Sebastien Bubeck","Ronen Eldan","Janardhan Kulkarni","Yuanzhi Li","Anh Nguyen","Rachel Ward","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.09241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09238v1","updated":"2023-12-14T18:58:12Z","published":"2023-12-14T18:58:12Z","title":"Auto MC-Reward: Automated Dense Reward Design with Large Language Models\n  for Minecraft","summary":"  Traditional reinforcement-learning-based agents rely on sparse rewards that\noften only use binary values to indicate task completion or failure. The\nchallenge in exploration efficiency makes it difficult to effectively learn\ncomplex tasks in Minecraft. To address this, this paper introduces an advanced\nlearning system, named Auto MC-Reward, that leverages Large Language Models\n(LLMs) to automatically design dense reward functions, thereby enhancing the\nlearning efficiency. Auto MC-Reward consists of three important components:\nReward Designer, Reward Critic, and Trajectory Analyzer. Given the environment\ninformation and task descriptions, the Reward Designer first design the reward\nfunction by coding an executable Python function with predefined observation\ninputs. Then, our Reward Critic will be responsible for verifying the code,\nchecking whether the code is self-consistent and free of syntax and semantic\nerrors. Further, the Trajectory Analyzer summarizes possible failure causes and\nprovides refinement suggestions according to collected trajectories. In the\nnext round, Reward Designer will take further refine and iterate the dense\nreward function based on feedback. Experiments demonstrate a significant\nimprovement in the success rate and learning efficiency of our agents in\ncomplex tasks in Minecraft, such as obtaining diamond with the efficient\nability to avoid lava, and efficiently explore trees and animals that are\nsparse on the plains biome.\n","authors":["Hao Li","Xue Yang","Zhaokai Wang","Xizhou Zhu","Jie Zhou","Yu Qiao","Xiaogang Wang","Hongsheng Li","Lewei Lu","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2312.09238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09230v1","updated":"2023-12-14T18:55:47Z","published":"2023-12-14T18:55:47Z","title":"Successor Heads: Recurring, Interpretable Attention Heads In The Wild","summary":"  In this work we present successor heads: attention heads that increment\ntokens with a natural ordering, such as numbers, months, and days. For example,\nsuccessor heads increment 'Monday' into 'Tuesday'. We explain the successor\nhead behavior with an approach rooted in mechanistic interpretability, the\nfield that aims to explain how models complete tasks in human-understandable\nterms. Existing research in this area has found interpretable language model\ncomponents in small toy models. However, results in toy models have not yet led\nto insights that explain the internals of frontier models and little is\ncurrently understood about the internal operations of large language models. In\nthis paper, we analyze the behavior of successor heads in large language models\n(LLMs) and find that they implement abstract representations that are common to\ndifferent architectures. They form in LLMs with as few as 31 million\nparameters, and at least as many as 12 billion parameters, such as GPT-2,\nPythia, and Llama-2. We find a set of 'mod-10 features' that underlie how\nsuccessor heads increment in LLMs across different architectures and sizes. We\nperform vector arithmetic with these features to edit head behavior and provide\ninsights into numeric representations within LLMs. Additionally, we study the\nbehavior of successor heads on natural language data, identifying interpretable\npolysemanticity in a Pythia successor head.\n","authors":["Rhys Gould","Euan Ong","George Ogden","Arthur Conmy"],"pdf_url":"https://arxiv.org/pdf/2312.09230v1.pdf","comment":"12 main text pages, with appendix"},{"id":"http://arxiv.org/abs/2312.09207v1","updated":"2023-12-14T18:38:02Z","published":"2023-12-14T18:38:02Z","title":"WikiMuTe: A web-sourced dataset of semantic descriptions for music audio","summary":"  Multi-modal deep learning techniques for matching free-form text with music\nhave shown promising results in the field of Music Information Retrieval (MIR).\nPrior work is often based on large proprietary data while publicly available\ndatasets are few and small in size. In this study, we present WikiMuTe, a new\nand open dataset containing rich semantic descriptions of music. The data is\nsourced from Wikipedia's rich catalogue of articles covering musical works.\nUsing a dedicated text-mining pipeline, we extract both long and short-form\ndescriptions covering a wide range of topics related to music content such as\ngenre, style, mood, instrumentation, and tempo. To show the use of this data,\nwe train a model that jointly learns text and audio representations and\nperforms cross-modal retrieval. The model is evaluated on two tasks: tag-based\nmusic retrieval and music auto-tagging. The results show that while our\napproach has state-of-the-art performance on multiple tasks, but still observe\na difference in performance depending on the data used for training.\n","authors":["Benno Weck","Holger Kirchhoff","Peter Grosche","Xavier Serra"],"pdf_url":"https://arxiv.org/pdf/2312.09207v1.pdf","comment":"Submitted to 30th International Conference on MultiMedia Modeling\n  (MMM2024). This preprint has not undergone peer review or any post-submission\n  improvements or corrections"},{"id":"http://arxiv.org/abs/2312.09203v1","updated":"2023-12-14T18:34:06Z","published":"2023-12-14T18:34:06Z","title":"Measurement in the Age of LLMs: An Application to Ideological Scaling","summary":"  Much of social science is centered around terms like ``ideology'' or\n``power'', which generally elude precise definition, and whose contextual\nmeanings are trapped in surrounding language. This paper explores the use of\nlarge language models (LLMs) to flexibly navigate the conceptual clutter\ninherent to social scientific measurement tasks. We rely on LLMs' remarkable\nlinguistic fluency to elicit ideological scales of both legislators and text,\nwhich accord closely to established methods and our own judgement. A key aspect\nof our approach is that we elicit such scores directly, instructing the LLM to\nfurnish numeric scores itself. This approach affords a great deal of\nflexibility, which we showcase through a variety of different case studies. Our\nresults suggest that LLMs can be used to characterize highly subtle and diffuse\nmanifestations of political ideology in text.\n","authors":["Sean O'Hagan","Aaron Schein"],"pdf_url":"https://arxiv.org/pdf/2312.09203v1.pdf","comment":"Presented at the 4th International Conference of Social Computing in\n  Beijing, China, September 2023, the New Directions in Analyzing Text as Data\n  (TADA) meeting in Amherst, MA, USA, November 2023, and the NeurIPS workshop\n  titled \"I Can't Believe It's Not Better!'' Failure Modes in the Age of\n  Foundation Models in New Orleans, LA, December 2023"},{"id":"http://arxiv.org/abs/2303.03919v2","updated":"2023-12-14T16:55:42Z","published":"2023-03-06T04:22:33Z","title":"Data Portraits: Recording Foundation Model Training Data","summary":"  Foundation models are trained on increasingly immense and opaque datasets.\nEven while these models are now key in AI system building, it can be difficult\nto answer the straightforward question: has the model already encountered a\ngiven example during training? We therefore propose a widespread adoption of\nData Portraits: artifacts that record training data and allow for downstream\ninspection. First we outline the properties of such an artifact and discuss how\nexisting solutions can be used to increase transparency. We then propose and\nimplement a solution based on data sketching, stressing fast and space\nefficient querying. Using our tools, we document a popular language modeling\ncorpus (The Pile) and a recently released code modeling dataset (The Stack). We\nshow that our solution enables answering questions about test set leakage and\nmodel plagiarism. Our tool is lightweight and fast, costing only 3% of the\ndataset size in overhead. We release a live interface of our tools at\nhttps://dataportraits.org/ and call on dataset and model creators to release\nData Portraits as a complement to current documentation practices.\n","authors":["Marc Marone","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2303.03919v2.pdf","comment":"NeurIPS 2023 Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2305.14973v2","updated":"2023-12-14T16:17:20Z","published":"2023-05-24T10:08:04Z","title":"OverPrompt: Enhancing ChatGPT through Efficient In-Context Learning","summary":"  The remarkable performance of pre-trained large language models has\nrevolutionised various natural language processing applications. Due to huge\nparametersizes and extensive running costs, companies or organisations tend to\ntransfer the models to the target task by zero-shot prompting techniques.\nHowever, the prohibitive costs of tokens and time have hindered their adoption\nin applications. We propose OverPrompt, leveraging the in-context learning\ncapability of LLMs to handle multiple task inputs, thereby reducing token and\ntime costs. This approach could potentially improve task performance during API\nqueries due to better conditional distribution mapping. Evaluated across\ndiverse classification datasets, our experiments show that OverPrompt can\nachieve cost-efficient zero-shot classification without causing significant\ndetriment to task performance, and in some cases, even improving it. An\nablation study conducted on various LLMs, along with an investigation into the\nrobustness of our prompting strategy to different input ordering, offers\nvaluable insights into the broader applicability of our method across diverse\ntasks. These findings also suggest a more seamless integration of our method\nwith LLMs through an API.\n","authors":["Jiazheng Li","Runcong Zhao","Yongxin Yang","Yulan He","Lin Gui"],"pdf_url":"https://arxiv.org/pdf/2305.14973v2.pdf","comment":"NeurIPS 2023 R0-FoMo Workshop"},{"id":"http://arxiv.org/abs/2312.09085v1","updated":"2023-12-14T16:16:50Z","published":"2023-12-14T16:16:50Z","title":"The Earth is Flat because...: Investigating LLMs' Belief towards\n  Misinformation via Persuasive Conversation","summary":"  Large Language Models (LLMs) encapsulate vast amounts of knowledge but still\nremain vulnerable to external misinformation. Existing research mainly studied\nthis susceptibility behavior in a single-turn setting. However, belief can\nchange during a multi-turn conversation, especially a persuasive one.\nTherefore, in this study, we delve into LLMs' susceptibility to persuasive\nconversations, particularly on factual questions that they can answer\ncorrectly. We first curate the Farm (i.e., Fact to Misinform) dataset, which\ncontains factual questions paired with systematically generated persuasive\nmisinformation. Then, we develop a testing framework to track LLMs' belief\nchanges in a persuasive dialogue. Through extensive experiments, we find that\nLLMs' correct beliefs on factual knowledge can be easily manipulated by various\npersuasive strategies.\n","authors":["Rongwu Xu","Brian S. Lin","Shujian Yang","Tianqi Zhang","Weiyan Shi","Tianwei Zhang","Zhixuan Fang","Wei Xu","Han Qiu"],"pdf_url":"https://arxiv.org/pdf/2312.09085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09084v1","updated":"2023-12-14T16:16:35Z","published":"2023-12-14T16:16:35Z","title":"Language Modeling on a SpiNNaker 2 Neuromorphic Chip","summary":"  As large language models continue to scale in size rapidly, so too does the\ncomputational power required to run them. Event-based networks on neuromorphic\ndevices offer a potential way to reduce energy consumption for inference\nsignificantly. However, to date, most event-based networks that can run on\nneuromorphic hardware, including spiking neural networks (SNNs), have not\nachieved task performance even on par with LSTM models for language modeling.\nAs a result, language modeling on neuromorphic devices has seemed a distant\nprospect. In this work, we demonstrate the first-ever implementation of a\nlanguage model on a neuromorphic device - specifically the SpiNNaker 2 chip -\nbased on a recently published event-based architecture called the EGRU.\nSpiNNaker 2 is a many-core neuromorphic chip designed for large-scale\nasynchronous processing, while the EGRU is architected to leverage such\nhardware efficiently while maintaining competitive task performance. This\nimplementation marks the first time a neuromorphic language model matches\nLSTMs, setting the stage for taking task performance to the level of large\nlanguage models. We also demonstrate results on a gesture recognition task\nbased on inputs from a DVS camera. Overall, our results showcase the\nfeasibility of this neuro-inspired neural network in hardware, highlighting\nsignificant gains versus conventional hardware in energy efficiency for the\ncommon use case of single batch inference.\n","authors":["Khaleelulla Khan Nazeer","Mark Schöne","Rishav Mukherji","Christian Mayr","David Kappel","Anand Subramoney"],"pdf_url":"https://arxiv.org/pdf/2312.09084v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.07066v2","updated":"2023-12-14T16:13:15Z","published":"2022-11-14T01:54:08Z","title":"Controllable Citation Sentence Generation with Language Models","summary":"  Citation generation aims to generate a citation sentence that refers to a\nchosen paper in the context of a manuscript. However, a rigid citation\ngeneration process is at odds with an author's desire to control specific\nattributes, such as 1) the citation intent, e.g., either introducing background\ninformation or comparing results, and 2) keywords that should appear in the\ncitation text. To provide these degrees of controllability during citation\ngeneration, we propose to integrate the manuscript context, the context of the\nreferenced paper, and the desired control attributes into a structured template\nand use it to fine-tune a language model (LM) via next-token prediction. We\nthen utilize Proximal Policy Optimization to directly optimize the LM in favor\nof a high score of our proposed controllability metric. The proposed workflow\nharmoniously combines citation attribute suggestion and conditional citation\ngeneration into one LM, allowing for better user control.\n","authors":["Nianlong Gu","Richard H. R. Hahnloser"],"pdf_url":"https://arxiv.org/pdf/2211.07066v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09075v1","updated":"2023-12-14T16:10:56Z","published":"2023-12-14T16:10:56Z","title":"Towards Verifiable Text Generation with Evolving Memory and\n  Self-Reflection","summary":"  Large Language Models (LLMs) face several challenges, including the tendency\nto produce incorrect outputs, known as hallucination. An effective solution is\nverifiable text generation, which prompts LLMs to generate content with\ncitations for accuracy verification. However, verifiable text generation is\nnon-trivial due to the focus-shifting phenomenon, the dilemma between the\nprecision and scope in document retrieval, and the intricate reasoning required\nto discern the relationship between the claim and citations. In this paper, we\npresent VTG, an innovative approach for Verifiable Text Generation with\nevolving memory and self-reflection. VTG maintains evolving long short-term\nmemory to retain both valuable documents and up-to-date documents. Active\nretrieval and diverse query generation are utilized to enhance both the\nprecision and scope of the retrieved documents. Furthermore, VTG features a\ntwo-tier verifier and an evidence finder, enabling rethinking and reflection on\nthe relationship between the claim and citations. We conduct extensive\nexperiments on five datasets across three knowledge-intensive tasks and the\nresults reveal that VTG significantly outperforms existing baselines.\n","authors":["Hao Sun","Hengyi Cai","Bo Wang","Yingyan Hou","Xiaochi Wei","Shuaiqiang Wang","Yan Zhang","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2312.09075v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2312.09067v1","updated":"2023-12-14T16:04:14Z","published":"2023-12-14T16:04:14Z","title":"Holodeck: Language Guided Generation of 3D Embodied AI Environments","summary":"  3D simulated environments play a critical role in Embodied AI, but their\ncreation requires expertise and extensive manual effort, restricting their\ndiversity and scope. To mitigate this limitation, we present Holodeck, a system\nthat generates 3D environments to match a user-supplied prompt fully\nautomatedly. Holodeck can generate diverse scenes, e.g., arcades, spas, and\nmuseums, adjust the designs for styles, and can capture the semantics of\ncomplex queries such as \"apartment for a researcher with a cat\" and \"office of\na professor who is a fan of Star Wars\". Holodeck leverages a large language\nmodel (GPT-4) for common sense knowledge about what the scene might look like\nand uses a large collection of 3D assets from Objaverse to populate the scene\nwith diverse objects. To address the challenge of positioning objects\ncorrectly, we prompt GPT-4 to generate spatial relational constraints between\nobjects and then optimize the layout to satisfy those constraints. Our\nlarge-scale human evaluation shows that annotators prefer Holodeck over\nmanually designed procedural baselines in residential scenes and that Holodeck\ncan produce high-quality outputs for diverse scene types. We also demonstrate\nan exciting application of Holodeck in Embodied AI, training agents to navigate\nin novel scenes like music rooms and daycares without human-constructed data,\nwhich is a significant step forward in developing general-purpose embodied\nagents.\n","authors":["Yue Yang","Fan-Yun Sun","Luca Weihs","Eli VanderBilt","Alvaro Herrasti","Winson Han","Jiajun Wu","Nick Haber","Ranjay Krishna","Lingjie Liu","Chris Callison-Burch","Mark Yatskar","Aniruddha Kembhavi","Christopher Clark"],"pdf_url":"https://arxiv.org/pdf/2312.09067v1.pdf","comment":"20 pages, 24 figures, 2 tables"},{"id":"http://arxiv.org/abs/2312.09043v1","updated":"2023-12-14T15:40:27Z","published":"2023-12-14T15:40:27Z","title":"Topic Bias in Emotion Classification","summary":"  Emotion corpora are typically sampled based on keyword/hashtag search or by\nasking study participants to generate textual instances. In any case, these\ncorpora are not uniform samples representing the entirety of a domain. We\nhypothesize that this practice of data acquisition leads to unrealistic\ncorrelations between overrepresented topics in these corpora that harm the\ngeneralizability of models. Such topic bias could lead to wrong predictions for\ninstances like \"I organized the service for my aunt's funeral.\" when funeral\nevents are over-represented for instances labeled with sadness, despite the\nemotion of pride being more appropriate here. In this paper, we study this\ntopic bias both from the data and the modeling perspective. We first label a\nset of emotion corpora automatically via topic modeling and show that emotions\nin fact correlate with specific topics. Further, we see that emotion\nclassifiers are confounded by such topics. Finally, we show that the\nestablished debiasing method of adversarial correction via gradient reversal\nmitigates the issue. Our work points out issues with existing emotion corpora\nand that more representative resources are required for fair evaluation of\nmodels predicting affective concepts from text.\n","authors":["Maximilian Wegge","Roman Klinger"],"pdf_url":"https://arxiv.org/pdf/2312.09043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14483v2","updated":"2023-12-14T15:39:04Z","published":"2023-11-24T13:47:25Z","title":"SER_AMPEL: a multi-source dataset for speech emotion recognition of\n  Italian older adults","summary":"  In this paper, SER_AMPEL, a multi-source dataset for speech emotion\nrecognition (SER) is presented. The peculiarity of the dataset is that it is\ncollected with the aim of providing a reference for speech emotion recognition\nin case of Italian older adults. The dataset is collected following different\nprotocols, in particular considering acted conversations, extracted from movies\nand TV series, and recording natural conversations where the emotions are\nelicited by proper questions. The evidence of the need for such a dataset\nemerges from the analysis of the state of the art. Preliminary considerations\non the critical issues of SER are reported analyzing the classification results\non a subset of the proposed dataset.\n","authors":["Alessandra Grossi","Francesca Gasparini"],"pdf_url":"https://arxiv.org/pdf/2311.14483v2.pdf","comment":"11 pages, 1 Figure, 7 Tables, submitted to ForItAAL 2023 (12{\\deg}\n  Forum Italiano Ambient Assisted Living)"},{"id":"http://arxiv.org/abs/2312.09040v1","updated":"2023-12-14T15:37:37Z","published":"2023-12-14T15:37:37Z","title":"STaR: Distilling Speech Temporal Relation for Lightweight Speech\n  Self-Supervised Learning Models","summary":"  Albeit great performance of Transformer-based speech selfsupervised learning\n(SSL) models, their large parameter size and computational cost make them\nunfavorable to utilize. In this study, we propose to compress the speech SSL\nmodels by distilling speech temporal relation (STaR). Unlike previous works\nthat directly match the representation for each speech frame, STaR distillation\ntransfers temporal relation between speech frames, which is more suitable for\nlightweight student with limited capacity. We explore three STaR distillation\nobjectives and select the best combination as the final STaR loss. Our model\ndistilled from HuBERT BASE achieves an overall score of 79.8 on SUPERB\nbenchmark, the best performance among models with up to 27 million parameters.\nWe show that our method is applicable across different speech SSL models and\nmaintains robust performance with further reduced parameters.\n","authors":["Kangwook Jang","Sungnyun Kim","Hoirin Kim"],"pdf_url":"https://arxiv.org/pdf/2312.09040v1.pdf","comment":"Accepted at ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.09039v1","updated":"2023-12-14T15:37:04Z","published":"2023-12-14T15:37:04Z","title":"TAP4LLM: Table Provider on Sampling, Augmenting, and Packing\n  Semi-structured Data for Large Language Model Reasoning","summary":"  Table reasoning has shown remarkable progress in a wide range of table-based\ntasks. These challenging tasks require reasoning over both free-form natural\nlanguage (NL) questions and semi-structured tabular data. However, previous\ntable reasoning solutions suffer from significant performance degradation on\n\"huge\" tables. In addition, most existing methods struggle to reason over\ncomplex questions since they lack essential information or they are scattered\nin different places. To alleviate these challenges, we exploit a table\nprovider, namely TAP4LLM, on versatile sampling, augmentation, and packing\nmethods to achieve effective semi-structured data reasoning using large\nlanguage models (LLMs), which 1) decompose raw tables into sub-tables with\nspecific rows or columns based on the rules or semantic similarity; 2) augment\ntable information by extracting semantic and statistical metadata from raw\ntables while retrieving relevant knowledge from trustworthy knowledge sources\n(e.g., Wolfram Alpha, Wikipedia); 3) pack sampled tables with augmented\nknowledge into sequence prompts for LLMs reasoning while balancing the token\nallocation trade-off. We show that TAP4LLM allows for different components as\nplug-ins, enhancing LLMs' understanding of structured data in diverse tabular\ntasks.\n","authors":["Yuan Sui","Jiaru Zou","Mengyu Zhou","Xinyi He","Lun Du","Shi Han","Dongmei Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.09039v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2307.08674 by other authors"},{"id":"http://arxiv.org/abs/2310.18168v4","updated":"2023-12-14T15:14:13Z","published":"2023-10-27T14:27:43Z","title":"Personas as a Way to Model Truthfulness in Language Models","summary":"  Large Language Models (LLMs) are trained on vast amounts of text from the\ninternet, which contains both factual and misleading information about the\nworld. Can language models discern truth from falsehood in this contradicting\ndata? Expanding on the view that LLMs can model different communicative agents,\nwe present the persona hypothesis: LLMs can cluster agents into personas using\ncommon features of their generations. For instance, a truthful persona is a\ngroup of agents that are likely to produce truthful text and that share similar\nfeatures like formal writing styles and scientific references. By modeling this\npersona, LLMs can generalize truthfulness beyond the specific contexts in which\neach agent generated the training text. For example, the model can infer that\nthe agent \"Wikipedia\" will behave truthfully on topics that were only generated\nby \"Science\" because they both belong to the truthful persona. We show evidence\nfor the persona hypothesis via two observations: (1) we can probe whether a\nmodel's answer will be truthful before it is generated; (2) finetuning a model\non a set of facts improves its truthfulness on unseen topics. Next, using\narithmetics as a synthetic environment, we show that language models can\nseparate true and false statements, and generalize truthfulness across agents;\nbut only if agents in the training data share a truthful generative process\nthat enables the creation of a truthful persona. Overall, our findings suggest\nthat models can exploit hierarchical structures in the data to learn abstract\nconcepts like truthfulness.\n","authors":["Nitish Joshi","Javier Rando","Abulhair Saparov","Najoung Kim","He He"],"pdf_url":"https://arxiv.org/pdf/2310.18168v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09000v1","updated":"2023-12-14T14:44:59Z","published":"2023-12-14T14:44:59Z","title":"ComOM at VLSP 2023: A Dual-Stage Framework with BERTology and Unified\n  Multi-Task Instruction Tuning Model for Vietnamese Comparative Opinion Mining","summary":"  The ComOM shared task aims to extract comparative opinions from product\nreviews in Vietnamese language. There are two sub-tasks, including (1)\nComparative Sentence Identification (CSI) and (2) Comparative Element\nExtraction (CEE). The first task is to identify whether the input is a\ncomparative review, and the purpose of the second task is to extract the\nquintuplets mentioned in the comparative review. To address this task, our team\nproposes a two-stage system based on fine-tuning a BERTology model for the CSI\ntask and unified multi-task instruction tuning for the CEE task. Besides, we\napply the simple data augmentation technique to increase the size of the\ndataset for training our model in the second stage. Experimental results show\nthat our approach outperforms the other competitors and has achieved the top\nscore on the official private test.\n","authors":["Dang Van Thin","Duong Ngoc Hao","Ngan Luu-Thuy Nguyen"],"pdf_url":"https://arxiv.org/pdf/2312.09000v1.pdf","comment":"Accepted manuscript at VLSP 2023"},{"id":"http://arxiv.org/abs/2312.08995v1","updated":"2023-12-14T14:41:37Z","published":"2023-12-14T14:41:37Z","title":"FrameFinder: Explorative Multi-Perspective Framing Extraction from News\n  Headlines","summary":"  Revealing the framing of news articles is an important yet neglected task in\ninformation seeking and retrieval. In the present work, we present FrameFinder,\nan open tool for extracting and analyzing frames in textual data. FrameFinder\nvisually represents the frames of text from three perspectives, i.e., (i) frame\nlabels, (ii) frame dimensions, and (iii) frame structure. By analyzing the\nwell-established gun violence frame corpus, we demonstrate the merits of our\nproposed solution to support social science research and call for subsequent\nintegration into information interactions.\n","authors":["Markus Reiter-Haas","Beate Klösch","Markus Hadler","Elisabeth Lex"],"pdf_url":"https://arxiv.org/pdf/2312.08995v1.pdf","comment":"Accepted for publication at CHIIR'24"},{"id":"http://arxiv.org/abs/2312.08968v1","updated":"2023-12-14T14:18:27Z","published":"2023-12-14T14:18:27Z","title":"Detecting value-expressive text posts in Russian social media","summary":"  Basic values are concepts or beliefs which pertain to desirable end-states\nand transcend specific situations. Studying personal values in social media can\nilluminate how and why societal values evolve especially when the stimuli-based\nmethods, such as surveys, are inefficient, for instance, in hard-to-reach\npopulations. On the other hand, user-generated content is driven by the massive\nuse of stereotyped, culturally defined speech constructions rather than\nauthentic expressions of personal values. We aimed to find a model that can\naccurately detect value-expressive posts in Russian social media VKontakte. A\ntraining dataset of 5,035 posts was annotated by three experts, 304\ncrowd-workers and ChatGPT. Crowd-workers and experts showed only moderate\nagreement in categorizing posts. ChatGPT was more consistent but struggled with\nspam detection. We applied an ensemble of human- and AI-assisted annotation\ninvolving active learning approach, subsequently trained several LLMs and\nselected a model based on embeddings from pre-trained fine-tuned rubert-tiny2,\nand reached a high quality of value detection with F1 = 0.75 (F1-macro = 0.80).\nThis model provides a crucial step to a study of values within and between\nRussian social media users.\n","authors":["Maria Milkova","Maksim Rudnev","Lidia Okolskaya"],"pdf_url":"https://arxiv.org/pdf/2312.08968v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09265v1","updated":"2023-12-14T14:16:40Z","published":"2023-12-14T14:16:40Z","title":"Acoustic models of Brazilian Portuguese Speech based on Neural\n  Transformers","summary":"  An acoustic model, trained on a significant amount of unlabeled data,\nconsists of a self-supervised learned speech representation useful for solving\ndownstream tasks, perhaps after a fine-tuning of the model in the respective\ndownstream task. In this work, we build an acoustic model of Brazilian\nPortuguese Speech through a Transformer neural network. This model was\npretrained on more than $800$ hours of Brazilian Portuguese Speech, using a\ncombination of pretraining techniques. Using a labeled dataset collected for\nthe detection of respiratory insufficiency in Brazilian Portuguese speakers, we\nfine-tune the pretrained Transformer neural network on the following tasks:\nrespiratory insufficiency detection, gender recognition and age group\nclassification. We compare the performance of pretrained Transformers on these\ntasks with that of Transformers without previous pretraining, noting a\nsignificant improvement. In particular, the performance of respiratory\ninsufficiency detection obtains the best reported results so far, indicating\nthis kind of acoustic model as a promising tool for speech-as-biomarker\napproach. Moreover, the performance of gender recognition is comparable to the\nstate of the art models in English.\n","authors":["Marcelo Matheus Gauy","Marcelo Finger"],"pdf_url":"https://arxiv.org/pdf/2312.09265v1.pdf","comment":"Under review at Journal of Brazilian Computer Society"},{"id":"http://arxiv.org/abs/2308.06385v2","updated":"2023-12-14T14:14:16Z","published":"2023-08-11T20:59:31Z","title":"ZYN: Zero-Shot Reward Models with Yes-No Questions for RLAIF","summary":"  In this work, we address the problem of directing the text generation of a\nlanguage model (LM) towards a desired behavior, aligning the generated text\nwith the preferences of the human operator. We propose using another,\ninstruction-tuned language model as a critic reward model in a zero-shot way\nthanks to the prompt of a Yes-No question that represents the user preferences,\nwithout requiring further labeled data. This zero-shot reward model provides\nthe learning signal to further fine-tune the base LM using Reinforcement\nLearning from AI Feedback (RLAIF); yet our approach is also compatible in other\ncontexts such as quality-diversity search. Extensive evidence of the\ncapabilities of the proposed ZYN framework is provided through experiments in\ndifferent domains related to text generation, including detoxification;\noptimizing sentiment of movie reviews, or any other attribute; steering the\nopinion about a particular topic the model may have; and personalizing prompt\ngenerators for text-to-image tasks. Code available at\n\\url{https://github.com/vicgalle/zero-shot-reward-models/}.\n","authors":["Victor Gallego"],"pdf_url":"https://arxiv.org/pdf/2308.06385v2.pdf","comment":"pre-print, work in progress"},{"id":"http://arxiv.org/abs/2312.08935v1","updated":"2023-12-14T13:41:54Z","published":"2023-12-14T13:41:54Z","title":"Math-Shepherd: A Label-Free Step-by-Step Verifier for LLMs in\n  Mathematical Reasoning","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks. However, even the most advanced open-source LLMs, such\nas the LLaMA family models, still face challenges when it comes to accurately\nsolving complex multi-step mathematical problems. In this paper, we present an\ninnovative process-oriented math verifier called \\textbf{Math-Shepherd}, which\nassigns a reward score to each step of the LLM's outputs on math problems. The\ntraining of Math-Shepherd is achieved using automatically constructed\nprocess-wise supervision data, breaking the bottleneck of heavy reliance on\nmanual annotation in existing work. With the guidance of Math-Shepherd, a\nseries of open-source LLMs demonstrate exceptional performance. Among them,\nDeepSeek 67B \\citep{DeepSeek-llm} stands out by achieving accuracy rates of\n93.3\\% on the GSM8K dataset and 48.1\\% on the MATH dataset, without external\nenhancement such as tool usage. Our Math-Shepherd also outperforms the\nself-consistency method and other existing verification models. We believe that\nautomatic process supervision holds significant potential for the future\nevolution of LLMs.\n","authors":["Peiyi Wang","Lei Li","Zhihong Shao","R. X. Xu","Damai Dai","Yifei Li","Deli Chen","Y. Wu","Zhifang Sui"],"pdf_url":"https://arxiv.org/pdf/2312.08935v1.pdf","comment":"Large Language Models; Mathematical Reasoning; Process Reward Models;\n  Automatic Process Supervision"},{"id":"http://arxiv.org/abs/2312.08926v1","updated":"2023-12-14T13:33:50Z","published":"2023-12-14T13:33:50Z","title":"Modeling Complex Mathematical Reasoning via Large Language Model based\n  MathAgent","summary":"  Large language models (LLMs) face challenges in solving complex mathematical\nproblems that require comprehensive capacities to parse the statements,\nassociate domain knowledge, perform compound logical reasoning, and integrate\nthe intermediate rationales. Tackling all these problems once could be arduous\nfor LLMs, thus leading to confusion in generation. In this work, we explore the\npotential of enhancing LLMs with agents by meticulous decomposition and\nmodeling of mathematical reasoning process. Specifically, we propose a formal\ndescription of the mathematical solving and extend LLMs with an agent-based\nzero-shot framework named\n$\\bf{P}$lanner-$\\bf{R}$easoner-$\\bf{E}$xecutor-$\\bf{R}$eflector (PRER). We\nfurther provide and implement two MathAgents that define the logical forms and\ninherent relations via a pool of actions in different grains and orientations:\nMathAgent-M adapts its actions to LLMs, while MathAgent-H aligns with\nhumankind. Experiments on miniF2F and MATH have demonstrated the effectiveness\nof PRER and proposed MathAgents, achieving an increase of\n$12.3\\%$($53.9\\%\\xrightarrow{}66.2\\%$) on the MiniF2F, $9.2\\%$\n($49.8\\%\\xrightarrow{}59.0\\%$) on MATH, and\n$13.2\\%$($23.2\\%\\xrightarrow{}35.4\\%$) for level-5 problems of MATH against\nGPT-4. Further analytical results provide more insightful perspectives on\nexploiting the behaviors of LLMs as agents.\n","authors":["Haoran Liao","Qinyi Du","Shaohua Hu","Hao He","Yanyan Xu","Jidong Tian","Yaohui Jin"],"pdf_url":"https://arxiv.org/pdf/2312.08926v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08906v1","updated":"2023-12-14T13:11:35Z","published":"2023-12-14T13:11:35Z","title":"Using eye tracking to investigate what native Chinese speakers notice\n  about linguistic landscape images","summary":"  Linguistic landscape is an important field in sociolinguistic research. Eye\ntracking technology is a common technology in psychological research. There are\nfew cases of using eye movement to study linguistic landscape. This paper uses\neye tracking technology to study the actual fixation of the linguistic\nlandscape and finds that in the two dimensions of fixation time and fixation\ntimes, the fixation of native Chinese speakers to the linguistic landscape is\nhigher than that of the general landscape. This paper argues that this\nphenomenon is due to the higher information density of linguistic landscapes.\nAt the same time, the article also discusses other possible reasons for this\nphenomenon.\n","authors":["Zichao Wei","Yewei Qin"],"pdf_url":"https://arxiv.org/pdf/2312.08906v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04333v3","updated":"2023-12-14T13:11:07Z","published":"2023-12-07T14:50:41Z","title":"Beyond Surface: Probing LLaMA Across Scales and Layers","summary":"  This paper presents an in-depth analysis of Large Language Models (LLMs),\nfocusing on LLaMA, a prominent open-source foundational model in natural\nlanguage processing. Instead of assessing LLaMA through its generative output,\nwe design multiple-choice tasks to probe its intrinsic understanding in\nhigh-order tasks such as reasoning and computation. We examine the model\nhorizontally, comparing different sizes, and vertically, assessing different\nlayers. We unveil several key and uncommon findings based on the designed\nprobing tasks: (1) Horizontally, enlarging model sizes almost could not\nautomatically impart additional knowledge or computational prowess. Instead, it\ncan enhance reasoning abilities, especially in math problem solving, and helps\nreduce hallucinations, but only beyond certain size thresholds; (2) In vertical\nanalysis, the lower layers of LLaMA lack substantial arithmetic and factual\nknowledge, showcasing logical thinking, multilingual and recognitive abilities,\nwith top layers housing most computational power and real-world knowledge.\n","authors":["Nuo Chen","Ning Wu","Shining Liang","Ming Gong","Linjun Shou","Dongmei Zhang","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2312.04333v3.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2310.05804v2","updated":"2023-12-14T13:07:45Z","published":"2023-10-09T15:43:07Z","title":"Learning Language-guided Adaptive Hyper-modality Representation for\n  Multimodal Sentiment Analysis","summary":"  Though Multimodal Sentiment Analysis (MSA) proves effective by utilizing rich\ninformation from multiple sources (e.g., language, video, and audio), the\npotential sentiment-irrelevant and conflicting information across modalities\nmay hinder the performance from being further improved. To alleviate this, we\npresent Adaptive Language-guided Multimodal Transformer (ALMT), which\nincorporates an Adaptive Hyper-modality Learning (AHL) module to learn an\nirrelevance/conflict-suppressing representation from visual and audio features\nunder the guidance of language features at different scales. With the obtained\nhyper-modality representation, the model can obtain a complementary and joint\nrepresentation through multimodal fusion for effective MSA. In practice, ALMT\nachieves state-of-the-art performance on several popular datasets (e.g., MOSI,\nMOSEI and CH-SIMS) and an abundance of ablation demonstrates the validity and\nnecessity of our irrelevance/conflict suppression mechanism.\n","authors":["Haoyu Zhang","Yu Wang","Guanghao Yin","Kejun Liu","Yuanyuan Liu","Tianshu Yu"],"pdf_url":"https://arxiv.org/pdf/2310.05804v2.pdf","comment":"Published in EMNLP 2023"},{"id":"http://arxiv.org/abs/2312.08901v1","updated":"2023-12-14T13:03:13Z","published":"2023-12-14T13:03:13Z","title":"Boosting LLM Reasoning: Push the Limits of Few-shot Learning with\n  Reinforced In-Context Pruning","summary":"  Large language models (LLMs) have shown impressive capabilities in various\ntasks, yet they still struggle with math reasoning. Despite efforts to optimize\nChain-of-Thoughts (CoT) prompts and fine-tune LLMs, the potential of few-shot\nlearning remains unexplored. In this work, we propose CoT-Max, a novel approach\npushing the boundaries of few-shot CoT learning to improve LLM math reasoning\ncapabilities. CoT-Max addresses the challenges of the selection of useful\nexamples and limited number of examples due to restricted context window\nlength. Inspired by our observation that natural language inputs contain many\nredundancy, we propose a coarse-to-fine pruner as a plug-and-play module for\nLLMs, which first identifies crucial CoT examples from a large batch and then\nfurther prunes unimportant tokens. To train the pruner, we collect a math\nreasoning dataset with diverse difficulty and steps, introduce a reward to\nmeasure both the input's effectiveness for math reasoning and token length\nconstraints, and propose a novel training approach with reinforcement learning.\nAs a result, CoT-Max significantly outperforms CoT and few-shot prompting\nbaselines across various LLMs (LLaMA2-7B, 13B, 70B) and 5 mathematical\ndatasets, achieving up to 4.55% absolute improvements. Remarkably, without any\nfine-tuning, LLaMA2-70B with CoT-Max surpasses GPT-3.5 and a wide range of\nlarger LLMs (PaLM, Minerva, etc.) on the GSM8K.\n","authors":["Xijie Huang","Li Lyna Zhang","Kwang-Ting Cheng","Mao Yang"],"pdf_url":"https://arxiv.org/pdf/2312.08901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03559v2","updated":"2023-12-14T12:58:11Z","published":"2023-09-07T08:42:40Z","title":"An Anchor Learning Approach for Citation Field Learning","summary":"  Citation field learning is to segment a citation string into fields of\ninterest such as author, title, and venue. Extracting such fields from\ncitations is crucial for citation indexing, researcher profile analysis, etc.\nUser-generated resources like academic homepages and Curriculum Vitae, provide\nrich citation field information. However, extracting fields from these\nresources is challenging due to inconsistent citation styles, incomplete\nsentence syntax, and insufficient training data. To address these challenges,\nwe propose a novel algorithm, CIFAL (citation field learning by anchor\nlearning), to boost the citation field learning performance. CIFAL leverages\nthe anchor learning, which is model-agnostic for any Pre-trained Language\nModel, to help capture citation patterns from the data of different citation\nstyles. The experiments demonstrate that CIFAL outperforms state-of-the-art\nmethods in citation field learning, achieving a 2.68% improvement in\nfield-level F1-scores. Extensive analysis of the results further confirms the\neffectiveness of CIFAL quantitatively and qualitatively.\n","authors":["Zilin Yuan","Borun Chen","Yimeng Dai","Yinghui Li","Hai-Tao Zheng","Rui Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.03559v2.pdf","comment":"accepted by ICASSP2024"},{"id":"http://arxiv.org/abs/2312.08865v1","updated":"2023-12-14T12:39:29Z","published":"2023-12-14T12:39:29Z","title":"Improving Cross-modal Alignment with Synthetic Pairs for Text-only Image\n  Captioning","summary":"  Although image captioning models have made significant advancements in recent\nyears, the majority of them heavily depend on high-quality datasets containing\npaired images and texts which are costly to acquire. Previous works leverage\nthe CLIP's cross-modal association ability for image captioning, relying solely\non textual information under unsupervised settings. However, not only does a\nmodality gap exist between CLIP text and image features, but a discrepancy also\narises between training and inference due to the unavailability of real-world\nimages, which hinders the cross-modal alignment in text-only captioning. This\npaper proposes a novel method to address these issues by incorporating\nsynthetic image-text pairs. A pre-trained text-to-image model is deployed to\nobtain images that correspond to textual data, and the pseudo features of\ngenerated images are optimized toward the real ones in the CLIP embedding\nspace. Furthermore, textual information is gathered to represent image\nfeatures, resulting in the image features with various semantics and the\nbridged modality gap. To unify training and inference, synthetic image features\nwould serve as the training prefix for the language decoder, while real images\nare used for inference. Additionally, salient objects in images are detected as\nassistance to enhance the learning of modality alignment. Experimental results\ndemonstrate that our method obtains the state-of-the-art performance on\nbenchmark datasets.\n","authors":["Zhiyue Liu","Jinyuan Liu","Fanrong Ma"],"pdf_url":"https://arxiv.org/pdf/2312.08865v1.pdf","comment":"AAAI2024"},{"id":"http://arxiv.org/abs/2310.10605v2","updated":"2023-12-14T12:24:53Z","published":"2023-10-16T17:31:34Z","title":"ForceGen: End-to-end de novo protein generation based on nonlinear\n  mechanical unfolding responses using a language diffusion model","summary":"  Through evolution, nature has presented a set of remarkable protein\nmaterials, including elastins, silks, keratins and collagens with superior\nmechanical performances that play crucial roles in mechanobiology. However,\ngoing beyond natural designs to discover proteins that meet specified\nmechanical properties remains challenging. Here we report a generative model\nthat predicts protein designs to meet complex nonlinear mechanical\nproperty-design objectives. Our model leverages deep knowledge on protein\nsequences from a pre-trained protein language model and maps mechanical\nunfolding responses to create novel proteins. Via full-atom molecular\nsimulations for direct validation, we demonstrate that the designed proteins\nare novel, and fulfill the targeted mechanical properties, including unfolding\nenergy and mechanical strength, as well as the detailed unfolding\nforce-separation curves. Our model offers rapid pathways to explore the\nenormous mechanobiological protein sequence space unconstrained by biological\nsynthesis, using mechanical features as target to enable the discovery of\nprotein materials with superior mechanical properties.\n","authors":["Bo Ni","David L. Kaplan","Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2310.10605v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.10847v5","updated":"2023-12-14T12:21:05Z","published":"2023-05-18T10:03:25Z","title":"Large Language Models can be Guided to Evade AI-Generated Text Detection","summary":"  Large language models (LLMs) have shown remarkable performance in various\ntasks and have been extensively utilized by the public. However, the increasing\nconcerns regarding the misuse of LLMs, such as plagiarism and spamming, have\nled to the development of multiple detectors, including fine-tuned classifiers\nand statistical methods. In this study, we equip LLMs with prompts, rather than\nrelying on an external paraphraser, to evaluate the vulnerability of these\ndetectors. We propose a novel Substitution-based In-Context example\nOptimization method (SICO) to automatically construct prompts for evading the\ndetectors. SICO is cost-efficient as it requires only 40 human-written examples\nand a limited number of LLM inferences to generate a prompt. Moreover, once a\ntask-specific prompt has been constructed, it can be universally used against a\nwide range of detectors. Extensive experiments across three real-world tasks\ndemonstrate that SICO significantly outperforms the paraphraser baselines and\nenables GPT-3.5 to successfully evade six detectors, decreasing their AUC by\n0.5 on average. Furthermore, a comprehensive human evaluation as well as a\nvalidation experiment in the wild show that the SICO-generated text achieves\nhuman-level readability and task completion rates. Finally, the strong\nperformance of SICO exhibits its potential as a reliable evaluation tool for\nfuture detectors. The codes and data are located on\nhttps://github.com/ColinLu50/Evade-GPT-Detector.\n","authors":["Ning Lu","Shengcai Liu","Rui He","Qi Wang","Yew-Soon Ong","Ke Tang"],"pdf_url":"https://arxiv.org/pdf/2305.10847v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.07235v2","updated":"2023-12-14T12:08:44Z","published":"2023-04-14T16:32:56Z","title":"What does self-attention learn from Masked Language Modelling?","summary":"  Transformers are neural networks which revolutionised natural language\nprocessing and machine learning. They process sequences of inputs, like words,\nusing a mechanism called self-attention, which is trained via masked language\nmodelling (MLM). In MLM, a word is randomly masked in an input sequence, and\nthe network is trained to predict the missing word. Despite the practical\nsuccess of transformers, it remains unclear what type of data distribution\nself-attention can learn efficiently. Here, we show analytically that if one\ndecouples the treatment of word positions and embeddings, a single layer of\nself-attention learns the conditionals of a generalised Potts model with\ninteractions between sites and Potts colours. Moreover, we show that training\nthis neural network is exactly equivalent to solving the inverse Potts problem\nby the so-called pseudo-likelihood method, well known in statistical physics.\nUsing this mapping, we compute the generalisation error of self-attention in a\nmodel scenario analytically using the replica method.\n","authors":["Riccardo Rende","Federica Gerace","Alessandro Laio","Sebastian Goldt"],"pdf_url":"https://arxiv.org/pdf/2304.07235v2.pdf","comment":"4 pages, 3 figures"},{"id":"http://arxiv.org/abs/2312.08846v1","updated":"2023-12-14T12:02:24Z","published":"2023-12-14T12:02:24Z","title":"TiMix: Text-aware Image Mixing for Effective Vision-Language\n  Pre-training","summary":"  Self-supervised Multi-modal Contrastive Learning (SMCL) remarkably advances\nmodern Vision-Language Pre-training (VLP) models by aligning visual and\nlinguistic modalities. Due to noises in web-harvested text-image pairs,\nhowever, scaling up training data volume in SMCL presents considerable\nobstacles in terms of computational cost and data inefficiency. To improve data\nefficiency in VLP, we propose Text-aware Image Mixing (TiMix), which integrates\nmix-based data augmentation techniques into SMCL, yielding significant\nperformance improvements without significantly increasing computational\noverhead. We provide a theoretical analysis of TiMixfrom a mutual information\n(MI) perspective, showing that mixed data samples for cross-modal contrastive\nlearning implicitly serve as a regularizer for the contrastive loss. The\nexperimental results demonstrate that TiMix exhibits a comparable performance\non downstream tasks, even with a reduced amount of training data and shorter\ntraining time, when benchmarked against existing methods. This work empirically\nand theoretically demonstrates the potential of data mixing for data-efficient\nand computationally viable VLP, benefiting broader VLP model adoption in\npractical scenarios.\n","authors":["Chaoya Jiang","Wei ye","Haiyang Xu","Qinghao Ye","Ming Yan","Ji Zhang","Shikun Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.08846v1.pdf","comment":"Accepted on AAAI2024"},{"id":"http://arxiv.org/abs/2310.10158v2","updated":"2023-12-14T11:49:17Z","published":"2023-10-16T07:58:56Z","title":"Character-LLM: A Trainable Agent for Role-Playing","summary":"  Large language models (LLMs) can be used to serve as agents to simulate human\nbehaviors, given the powerful ability to understand human instructions and\nprovide high-quality generated texts. Such ability stimulates us to wonder\nwhether LLMs can simulate a person in a higher form than simple human\nbehaviors. Therefore, we aim to train an agent with the profile, experience,\nand emotional states of a specific person instead of using limited prompts to\ninstruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs\nto act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar,\netc. Our method focuses on editing profiles as experiences of a certain\ncharacter and training models to be personal simulacra with these experiences.\nTo assess the effectiveness of our approach, we build a test playground that\ninterviews trained agents and evaluates whether the agents \\textit{memorize}\ntheir characters and experiences. Experimental results show interesting\nobservations that help build future simulacra of humankind.\n","authors":["Yunfan Shao","Linyang Li","Junqi Dai","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2310.10158v2.pdf","comment":"To appear at EMNLP 2023; Repo at\n  https://github.com/choosewhatulike/trainable-agents"},{"id":"http://arxiv.org/abs/2312.08800v1","updated":"2023-12-14T10:35:13Z","published":"2023-12-14T10:35:13Z","title":"Evaluating Large Language Models for Health-related Queries with\n  Presuppositions","summary":"  As corporations rush to integrate large language models (LLMs) to their\nsearch offerings, it is critical that they provide factually accurate\ninformation that is robust to any presuppositions that a user may express. In\nthis work, we introduce UPHILL, a dataset consisting of health-related queries\nwith varying degrees of presuppositions. Using UPHILL, we evaluate the factual\naccuracy and consistency of InstructGPT, ChatGPT, and BingChat models. We find\nthat while model responses rarely disagree with true health claims (posed as\nquestions), they often fail to challenge false claims: responses from\nInstructGPT agree with 32% of the false claims, ChatGPT 26% and BingChat 23%.\nAs we increase the extent of presupposition in input queries, the responses\nfrom InstructGPT and ChatGPT agree with the claim considerably more often,\nregardless of its veracity. Responses from BingChat, which rely on retrieved\nwebpages, are not as susceptible. Given the moderate factual accuracy, and the\ninability of models to consistently correct false assumptions, our work calls\nfor a careful assessment of current LLMs for use in high-stakes scenarios.\n","authors":["Navreet Kaur","Monojit Choudhury","Danish Pruthi"],"pdf_url":"https://arxiv.org/pdf/2312.08800v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2312.08793v1","updated":"2023-12-14T10:27:15Z","published":"2023-12-14T10:27:15Z","title":"Forbidden Facts: An Investigation of Competing Objectives in Llama-2","summary":"  LLMs often face competing pressures (for example helpfulness vs.\nharmlessness). To understand how models resolve such conflicts, we study\nLlama-2-chat models on the forbidden fact task. Specifically, we instruct\nLlama-2 to truthfully complete a factual recall statement while forbidding it\nfrom saying the correct answer. This often makes the model give incorrect\nanswers. We decompose Llama-2 into 1000+ components, and rank each one with\nrespect to how useful it is for forbidding the correct answer. We find that in\naggregate, around 35 components are enough to reliably implement the full\nsuppression behavior. However, these components are fairly heterogeneous and\nmany operate using faulty heuristics. We discover that one of these heuristics\ncan be exploited via a manually designed adversarial attack which we call The\nCalifornia Attack. Our results highlight some roadblocks standing in the way of\nbeing able to successfully interpret advanced ML systems. Project website\navailable at https://forbiddenfacts.github.io .\n","authors":["Tony T. Wang","Miles Wang","Kaivu Hariharan","Nir Shavit"],"pdf_url":"https://arxiv.org/pdf/2312.08793v1.pdf","comment":"Accepted to the ATTRIB and SoLaR workshops at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2312.03731v2","updated":"2023-12-14T09:37:14Z","published":"2023-11-28T02:36:53Z","title":"MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs","summary":"  Graphs can inherently model interconnected objects on the Web, thereby\nfacilitating a series of Web applications, such as web analyzing and content\nrecommendation. Recently, Graph Neural Networks (GNNs) have emerged as a\nmainstream technique for graph representation learning. However, their efficacy\nwithin an end-to-end supervised framework is significantly tied to the\navailabilityof task-specific labels. To mitigate labeling costs and enhance\nrobustness in few-shot settings, pre-training on self-supervised tasks has\nemerged as a promising method, while prompting has been proposed to further\nnarrow the objective gap between pretext and downstream tasks. Although there\nhas been some initial exploration of prompt-based learning on graphs, they\nprimarily leverage a single pretext task, resulting in a limited subset of\ngeneral knowledge that could be learned from the pre-training data. Hence, in\nthis paper, we propose MultiGPrompt, a novel multi-task pre-training and\nprompting framework to exploit multiple pretext tasks for more comprehensive\npre-trained knowledge. First, in pre-training, we design a set of pretext\ntokens to synergize multiple pretext tasks. Second, we propose a dual-prompt\nmechanism consisting of composed and open prompts to leverage task-specific and\nglobal pre-training knowledge, to guide downstream tasks in few-shot settings.\nFinally, we conduct extensive experiments on six public datasets to evaluate\nand analyze MultiGPrompt.\n","authors":["Xingtong Yu","Chang Zhou","Yuan Fang","Xinming Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.03731v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2311.03731v2","updated":"2023-12-14T09:31:50Z","published":"2023-11-07T05:20:09Z","title":"A Survey of Large Language Models Attribution","summary":"  Open-domain generative systems have gained significant attention in the field\nof conversational AI (e.g., generative search engines). This paper presents a\ncomprehensive review of the attribution mechanisms employed by these systems,\nparticularly large language models. Though attribution or citation improve the\nfactuality and verifiability, issues like ambiguous knowledge reservoirs,\ninherent biases, and the drawbacks of excessive attribution can hinder the\neffectiveness of these systems. The aim of this survey is to provide valuable\ninsights for researchers, aiding in the refinement of attribution methodologies\nto enhance the reliability and veracity of responses generated by open-domain\ngenerative systems. We believe that this field is still in its early stages;\nhence, we maintain a repository to keep track of ongoing studies at\nhttps://github.com/HITsz-TMG/awesome-llm-attributions.\n","authors":["Dongfang Li","Zetian Sun","Xinshuo Hu","Zhenyu Liu","Ziyang Chen","Baotian Hu","Aiguo Wu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.03731v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.00640v4","updated":"2023-12-14T09:15:41Z","published":"2021-04-01T17:40:08Z","title":"AmbiFC: Fact-Checking Ambiguous Claims with Evidence","summary":"  Automated fact-checking systems verify claims against evidence to predict\ntheir veracity. In real-world scenarios, the retrieved evidence may not\nunambiguously support or refute the claim and yield conflicting but valid\ninterpretations. Existing fact-checking datasets assume that the models\ndeveloped with them predict a single veracity label for each claim, thus\ndiscouraging the handling of such ambiguity. To address this issue we present\nAmbiFC, a fact-checking dataset with 10k claims derived from real-world\ninformation needs. It contains fine-grained evidence annotations of 50k\npassages from 5k Wikipedia pages. We analyze the disagreements arising from\nambiguity when comparing claims against evidence in AmbiFC, observing a strong\ncorrelation of annotator disagreement with linguistic phenomena such as\nunderspecification and probabilistic reasoning. We develop models for\npredicting veracity handling this ambiguity via soft labels and find that a\npipeline that learns the label distribution for sentence-level evidence\nselection and veracity prediction yields the best performance. We compare\nmodels trained on different subsets of AmbiFC and show that models trained on\nthe ambiguous instances perform better when faced with the identified\nlinguistic phenomena.\n","authors":["Max Glockner","Ieva Staliūnaitė","James Thorne","Gisela Vallejo","Andreas Vlachos","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2104.00640v4.pdf","comment":"Accepted at TACL; pre-MIT Press publication version"},{"id":"http://arxiv.org/abs/2312.08755v1","updated":"2023-12-14T09:07:57Z","published":"2023-12-14T09:07:57Z","title":"PROPRES: Investigating the Projectivity of Presupposition with Various\n  Triggers and Environments","summary":"  What makes a presupposition of an utterance -- information taken for granted\nby its speaker -- different from other pragmatic inferences such as an\nentailment is projectivity (e.g., the negative sentence the boy did not stop\nshedding tears presupposes the boy had shed tears before). The projectivity may\nvary depending on the combination of presupposition triggers and environments.\nHowever, prior natural language understanding studies fail to take it into\naccount as they either use no human baseline or include only negation as an\nentailment-canceling environment to evaluate models' performance. The current\nstudy attempts to reconcile these issues. We introduce a new dataset,\nprojectivity of presupposition (PROPRES, which includes 12k premise-hypothesis\npairs crossing six triggers involving some lexical variety with five\nenvironments. Our human evaluation reveals that humans exhibit variable\nprojectivity in some cases. However, the model evaluation shows that the\nbest-performed model, DeBERTa, does not fully capture it. Our findings suggest\nthat probing studies on pragmatic inferences should take extra care of the\nhuman judgment variability and the combination of linguistic items.\n","authors":["Daiki Asami","Saku Sugawara"],"pdf_url":"https://arxiv.org/pdf/2312.08755v1.pdf","comment":"Accepted by the 27th Conference on Computational Natural Language\n  Learning (CoNLL2023)"},{"id":"http://arxiv.org/abs/2312.08747v1","updated":"2023-12-14T08:46:26Z","published":"2023-12-14T08:46:26Z","title":"Dissecting vocabulary biases datasets through statistical testing and\n  automated data augmentation for artifact mitigation in Natural Language\n  Inference","summary":"  In recent years, the availability of large-scale annotated datasets, such as\nthe Stanford Natural Language Inference and the Multi-Genre Natural Language\nInference, coupled with the advent of pre-trained language models, has\nsignificantly contributed to the development of the natural language inference\ndomain. However, these crowdsourced annotated datasets often contain biases or\ndataset artifacts, leading to overestimated model performance and poor\ngeneralization. In this work, we focus on investigating dataset artifacts and\ndeveloping strategies to address these issues. Through the utilization of a\nnovel statistical testing procedure, we discover a significant association\nbetween vocabulary distribution and text entailment classes, emphasizing\nvocabulary as a notable source of biases. To mitigate these issues, we propose\nseveral automatic data augmentation strategies spanning character to word\nlevels. By fine-tuning the ELECTRA pre-trained language model, we compare the\nperformance of boosted models with augmented data against their baseline\ncounterparts. The experiments demonstrate that the proposed approaches\neffectively enhance model accuracy and reduce biases by up to 0.66% and 1.14%,\nrespectively.\n","authors":["Dat Thanh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2312.08747v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08737v1","updated":"2023-12-14T08:30:38Z","published":"2023-12-14T08:30:38Z","title":"JPIS: A Joint Model for Profile-based Intent Detection and Slot Filling\n  with Slot-to-Intent Attention","summary":"  Profile-based intent detection and slot filling are important tasks aimed at\nreducing the ambiguity in user utterances by leveraging user-specific\nsupporting profile information. However, research in these two tasks has not\nbeen extensively explored. To fill this gap, we propose a joint model, namely\nJPIS, designed to enhance profile-based intent detection and slot filling. JPIS\nincorporates the supporting profile information into its encoder and introduces\na slot-to-intent attention mechanism to transfer slot information\nrepresentations to intent detection. Experimental results show that our JPIS\nsubstantially outperforms previous profile-based models, establishing a new\nstate-of-the-art performance in overall accuracy on the Chinese benchmark\ndataset ProSLU.\n","authors":["Thinh Pham","Dat Quoc Nguyen"],"pdf_url":"https://arxiv.org/pdf/2312.08737v1.pdf","comment":"ICASSP 2024 (Accepted)"},{"id":"http://arxiv.org/abs/2312.08726v1","updated":"2023-12-14T08:14:13Z","published":"2023-12-14T08:14:13Z","title":"Labels Need Prompts Too Mask Matching for Natural Language Understanding\n  Tasks","summary":"  Textual label names (descriptions) are typically semantically rich in many\nnatural language understanding (NLU) tasks. In this paper, we incorporate the\nprompting methodology, which is widely used to enrich model input, into the\nlabel side for the first time. Specifically, we propose a Mask Matching method,\nwhich equips an input with a prompt and its label with another, and then makes\npredictions by matching their mask representations. We evaluate our method\nextensively on 8 NLU tasks with 14 datasets. The experimental results show that\nMask Matching significantly outperforms its counterparts of fine-tuning and\nconventional prompt-tuning, setting up state-of-the-art performances in several\ndatasets. Mask Matching is particularly good at handling NLU tasks with large\nlabel counts and informative label names. As pioneering efforts that\ninvestigate the label-side prompt, we also discuss open issues for future\nstudy.\n","authors":["Bo Li","Wei Ye","Quansen Wang","Wen Zhao","Shikun Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.08726v1.pdf","comment":"AAAI2024, Regular Paper"},{"id":"http://arxiv.org/abs/2312.08725v1","updated":"2023-12-14T08:13:28Z","published":"2023-12-14T08:13:28Z","title":"A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs\n  for Financial Sentiment Analysis","summary":"  Financial sentiment analysis plays a crucial role in uncovering latent\npatterns and detecting emerging trends, enabling individuals to make\nwell-informed decisions that may yield substantial advantages within the\nconstantly changing realm of finance. Recently, Large Language Models (LLMs)\nhave demonstrated their effectiveness in diverse domains, showcasing remarkable\ncapabilities even in zero-shot and few-shot in-context learning for various\nNatural Language Processing (NLP) tasks. Nevertheless, their potential and\napplicability in the context of financial sentiment analysis have not been\nthoroughly explored yet. To bridge this gap, we employ two approaches:\nin-context learning (with a focus on gpt-3.5-turbo model) and fine-tuning LLMs\non a finance-domain dataset. Given the computational costs associated with\nfine-tuning LLMs with large parameter sizes, our focus lies on smaller LLMs,\nspanning from 250M to 3B parameters for fine-tuning. We then compare the\nperformances with state-of-the-art results to evaluate their effectiveness in\nthe finance-domain. Our results demonstrate that fine-tuned smaller LLMs can\nachieve comparable performance to state-of-the-art fine-tuned LLMs, even with\nmodels having fewer parameters and a smaller training dataset. Additionally,\nthe zero-shot and one-shot performance of LLMs produces comparable results with\nfine-tuned smaller LLMs and state-of-the-art outcomes. Furthermore, our\nanalysis demonstrates that there is no observed enhancement in performance for\nfinance-domain sentiment analysis when the number of shots for in-context\nlearning is increased.\n","authors":["Sorouralsadat Fatemi","Yuheng Hu"],"pdf_url":"https://arxiv.org/pdf/2312.08725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13721v2","updated":"2023-12-14T06:58:52Z","published":"2023-05-23T06:15:43Z","title":"Continual Dialogue State Tracking via Example-Guided Question Answering","summary":"  Dialogue systems are frequently updated to accommodate new services, but\nnaively updating them by continually training with data for new services in\ndiminishing performance on previously learnt services. Motivated by the insight\nthat dialogue state tracking (DST), a crucial component of dialogue systems\nthat estimates the user's goal as a conversation proceeds, is a simple natural\nlanguage understanding task, we propose reformulating it as a bundle of\ngranular example-guided question answering tasks to minimize the task shift\nbetween services and thus benefit continual learning. Our approach alleviates\nservice-specific memorization and teaches a model to contextualize the given\nquestion and example to extract the necessary information from the\nconversation. We find that a model with just 60M parameters can achieve a\nsignificant boost by learning to learn from in-context examples retrieved by a\nretriever trained to identify turns with similar dialogue state changes.\nCombining our method with dialogue-level memory replay, our approach attains\nstate of the art performance on DST continual learning metrics without relying\non any complex regularization or parameter expansion methods.\n","authors":["Hyundong Cho","Andrea Madotto","Zhaojiang Lin","Khyathi Raghavi Chandu","Satwik Kottur","Jing Xu","Jonathan May","Chinnadhurai Sankar"],"pdf_url":"https://arxiv.org/pdf/2305.13721v2.pdf","comment":"11 pages, EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.11648v2","updated":"2023-12-14T06:41:54Z","published":"2023-10-18T01:20:16Z","title":"Zero-shot Faithfulness Evaluation for Text Summarization with Foundation\n  Language Model","summary":"  Despite tremendous improvements in natural language generation, summarization\nmodels still suffer from the unfaithfulness issue. Previous work evaluates\nfaithfulness either using models trained on the other tasks or in-domain\nsynthetic data, or prompting a large model such as ChatGPT. This paper proposes\nto do zero-shot faithfulness evaluation simply with a moderately-sized\nfoundation language model. We introduce a new metric FFLM, which is a\ncombination of probability changes based on the intuition that prefixing a\npiece of text that is consistent with the output will increase the probability\nof predicting the output. Experiments show that FFLM performs competitively\nwith or even outperforms ChatGPT on both inconsistency detection and\nfaithfulness rating with 24x fewer parameters. FFLM also achieves improvements\nover other strong baselines.\n","authors":["Qi Jia","Siyu Ren","Yizhu Liu","Kenny Q. Zhu"],"pdf_url":"https://arxiv.org/pdf/2310.11648v2.pdf","comment":"Accepted by EMNLP2023"},{"id":"http://arxiv.org/abs/2312.08676v1","updated":"2023-12-14T06:26:55Z","published":"2023-12-14T06:26:55Z","title":"SEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion with Cross\n  Attention","summary":"  Zero-shot voice conversion (VC) aims to transfer the source speaker timbre to\narbitrary unseen target speaker timbre, while keeping the linguistic content\nunchanged. Although the voice of generated speech can be controlled by\nproviding the speaker embedding of the target speaker, the speaker similarity\nstill lags behind the ground truth recordings. In this paper, we propose\nSEF-VC, a speaker embedding free voice conversion model, which is designed to\nlearn and incorporate speaker timbre from reference speech via a powerful\nposition-agnostic cross-attention mechanism, and then reconstruct waveform from\nHuBERT semantic tokens in a non-autoregressive manner. The concise design of\nSEF-VC enhances its training stability and voice conversion performance.\nObjective and subjective evaluations demonstrate the superiority of SEF-VC to\ngenerate high-quality speech with better similarity to target reference than\nstrong zero-shot VC baselines, even for very short reference speeches.\n","authors":["Junjie Li","Yiwei Guo","Xie Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2312.08676v1.pdf","comment":"5 pages,2 figures, submitted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.15140v2","updated":"2023-12-14T06:22:51Z","published":"2023-10-23T17:46:07Z","title":"AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large\n  Language Models","summary":"  Safety alignment of Large Language Models (LLMs) can be compromised with\nmanual jailbreak attacks and (automatic) adversarial attacks. Recent studies\nsuggest that defending against these attacks is possible: adversarial attacks\ngenerate unlimited but unreadable gibberish prompts, detectable by\nperplexity-based filters; manual jailbreak attacks craft readable prompts, but\ntheir limited number due to the necessity of human creativity allows for easy\nblocking. In this paper, we show that these solutions may be too optimistic. We\nintroduce AutoDAN, an interpretable, gradient-based adversarial attack that\nmerges the strengths of both attack types. Guided by the dual goals of\njailbreak and readability, AutoDAN optimizes and generates tokens one by one\nfrom left to right, resulting in readable prompts that bypass perplexity\nfilters while maintaining high attack success rates. Notably, these prompts,\ngenerated from scratch using gradients, are interpretable and diverse, with\nemerging strategies commonly seen in manual jailbreak attacks. They also\ngeneralize to unforeseen harmful behaviors and transfer to black-box LLMs\nbetter than their unreadable counterparts when using limited training data or a\nsingle proxy model. Furthermore, we show the versatility of AutoDAN by\nautomatically leaking system prompts using a customized objective. Our work\noffers a new way to red-team LLMs and understand jailbreak mechanisms via\ninterpretability.\n","authors":["Sicheng Zhu","Ruiyi Zhang","Bang An","Gang Wu","Joe Barrow","Zichao Wang","Furong Huang","Ani Nenkova","Tong Sun"],"pdf_url":"https://arxiv.org/pdf/2310.15140v2.pdf","comment":"Version 2 updates: Added comparison of three more evaluation methods\n  and their reliability check using human labeling. Added results for\n  jailbreaking Llama2 (individual behavior) and included complexity and\n  hyperparameter analysis. Revised objectives for prompt leaking. Other minor\n  changes made"},{"id":"http://arxiv.org/abs/2302.01313v7","updated":"2023-12-14T06:01:12Z","published":"2023-02-02T18:39:30Z","title":"Double Equivariance for Inductive Link Prediction for Both New Nodes and\n  New Relation Types","summary":"  The task of inductive link prediction in knowledge graphs (KGs) generally\nfocuses on test predictions with solely new nodes but not both new nodes and\nnew relation types. In this work, we formally define the concept of double\npermutation-equivariant representations that are equivariant to permutations of\nboth node identities and edge relation types. We then show how\ndouble-equivariant architectures are able to self-supervise pre-train on\ndistinct KG domains and zero-shot predict links on a new KG domain (with\ncompletely new entities and new relation types). We also introduce the concept\nof distributionally double equivariant positional embeddings designed to\nperform the same task. Finally, we empirically demonstrate the capability of\nthe proposed models against baselines on a set of novel real-world benchmarks.\nMore interestingly, we show that self-supervised pre-training on more KG\ndomains increases the zero-shot ability of our model to predict on new relation\ntypes over new entities on unseen KG domains.\n","authors":["Jianfei Gao","Yangze Zhou","Jincheng Zhou","Bruno Ribeiro"],"pdf_url":"https://arxiv.org/pdf/2302.01313v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.17255v6","updated":"2023-12-14T04:11:47Z","published":"2022-03-29T22:28:30Z","title":"A Cognitive Architecture for Machine Consciousness and Artificial\n  Superintelligence: Thought Is Structured by the Iterative Updating of Working\n  Memory","summary":"  This article provides an analytical framework for how to simulate human-like\nthought processes within a computer. It describes how attention and memory\nshould be structured, updated, and utilized to search for associative additions\nto the stream of thought. The focus is on replicating the dynamics of the\nmammalian working memory system, which features two forms of persistent\nactivity: sustained firing (preserving information on the order of seconds) and\nsynaptic potentiation (preserving information from minutes to hours). The\narticle uses a series of over 40 original figures to systematically demonstrate\nhow the iterative updating of these working memory stores provides functional\nstructure to behavior, cognition, and consciousness.\n  In an AI implementation, these two memory stores should be updated\ncontinuously and in an iterative fashion, meaning each state should preserve a\nproportion of the coactive representations from the state before it. Thus, the\nset of concepts in working memory will evolve gradually and incrementally over\ntime. This makes each state a revised iteration of the preceding state and\ncauses successive states to overlap and blend with respect to the information\nthey contain. Transitions between states happen as persistent activity spreads\nactivation energy throughout the hierarchical network searching long-term\nmemory for the most appropriate representation to be added to the global\nworkspace. The result is a chain of associatively linked intermediate states\ncapable of advancing toward a solution or goal. Iterative updating is\nconceptualized here as an information processing strategy, a model of working\nmemory, a theory of consciousness, and an algorithm for designing and\nprogramming artificial general intelligence.\n","authors":["Jared Edward Reser"],"pdf_url":"https://arxiv.org/pdf/2203.17255v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.15517v2","updated":"2023-12-14T04:06:36Z","published":"2023-10-24T04:50:59Z","title":"MarkQA: A large scale KBQA dataset with numerical reasoning","summary":"  While question answering over knowledge bases (KBQA) has shown progress in\naddressing factoid questions, KBQA with numerical reasoning remains relatively\nunexplored. In this paper, we focus on the complex numerical reasoning in KBQA\nand propose a new task, NR-KBQA, which necessitates the ability to perform both\nmulti-hop reasoning and numerical reasoning. We design a logic form in Python\nformat called PyQL to represent the reasoning process of numerical reasoning\nquestions. To facilitate the development of NR-KBQA, we present a large dataset\ncalled MarkQA, which is automatically constructed from a small set of seeds.\nEach question in MarkQA is equipped with its corresponding SPARQL query,\nalongside the step-by-step reasoning process in the QDMR format and PyQL\nprogram. Experimental results of some state-of-the-art QA methods on the MarkQA\nshow that complex numerical reasoning in KBQA faces great challenges.\n","authors":["Xiang Huang","Sitao Cheng","Yuheng Bao","Shanshan Huang","Yuzhong Qu"],"pdf_url":"https://arxiv.org/pdf/2310.15517v2.pdf","comment":"EMNLP 2023 main conference. Code: https://github.com/cdhx/MarkQA\n  Homepage: http://ws.nju.edu.cn/MarkQA"},{"id":"http://arxiv.org/abs/2312.08642v1","updated":"2023-12-14T03:49:52Z","published":"2023-12-14T03:49:52Z","title":"Metacognition-Enhanced Few-Shot Prompting With Positive Reinforcement","summary":"  Few-shot prompting elicits the remarkable abilities of large language models\nby equipping them with a few demonstration examples in the input. However, the\ntraditional method of providing large language models with all demonstration\ninput-output pairs at once may not effectively guide large language models to\nlearn the specific input-output mapping relationship. In this paper, inspired\nby the regulatory and supportive role of metacognition in students' learning,\nwe propose a novel metacognition-enhanced few-shot prompting, which guides\nlarge language models to reflect on their thought processes to comprehensively\nlearn the given demonstration examples. Furthermore, considering that positive\nreinforcement can improve students' learning motivation, we introduce positive\nreinforcement into our metacognition-enhanced few-shot prompting to promote the\nfew-shot learning of large language models by providing response-based positive\nfeedback. The experimental results on two real-world datasets show that our\nmetacognition-enhanced few-shot prompting with positive reinforcement surpasses\ntraditional few-shot prompting in classification accuracy and macro F1.\n","authors":["Yu Ji","Wen Wu","Yi Hu","Hong Zheng","Liang He"],"pdf_url":"https://arxiv.org/pdf/2312.08642v1.pdf","comment":"5 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2206.07207v2","updated":"2023-12-14T03:14:13Z","published":"2022-06-14T23:24:15Z","title":"Beyond Grounding: Extracting Fine-Grained Event Hierarchies Across\n  Modalities","summary":"  Events describe happenings in our world that are of importance. Naturally,\nunderstanding events mentioned in multimedia content and how they are related\nforms an important way of comprehending our world. Existing literature can\ninfer if events across textual and visual (video) domains are identical (via\ngrounding) and thus, on the same semantic level. However, grounding fails to\ncapture the intricate cross-event relations that exist due to the same events\nbeing referred to on many semantic levels. For example, in Figure 1, the\nabstract event of \"war\" manifests at a lower semantic level through subevents\n\"tanks firing\" (in video) and airplane \"shot\" (in text), leading to a\nhierarchical, multimodal relationship between the events.\n  In this paper, we propose the task of extracting event hierarchies from\nmultimodal (video and text) data to capture how the same event manifests itself\nin different modalities at different semantic levels. This reveals the\nstructure of events and is critical to understanding them. To support research\non this task, we introduce the Multimodal Hierarchical Events (MultiHiEve)\ndataset. Unlike prior video-language datasets, MultiHiEve is composed of news\nvideo-article pairs, which makes it rich in event hierarchies. We densely\nannotate a part of the dataset to construct the test benchmark. We show the\nlimitations of state-of-the-art unimodal and multimodal baselines on this task.\nFurther, we address these limitations via a new weakly supervised model,\nleveraging only unannotated video-article pairs from MultiHiEve. We perform a\nthorough evaluation of our proposed method which demonstrates improved\nperformance on this task and highlight opportunities for future research.\n","authors":["Hammad A. Ayyubi","Christopher Thomas","Lovish Chum","Rahul Lokesh","Long Chen","Yulei Niu","Xudong Lin","Xuande Feng","Jaywon Koo","Sounak Ray","Shih-Fu Chang"],"pdf_url":"https://arxiv.org/pdf/2206.07207v2.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2310.02207v2","updated":"2023-12-14T02:45:45Z","published":"2023-10-03T17:06:52Z","title":"Language Models Represent Space and Time","summary":"  The capabilities of large language models (LLMs) have sparked debate over\nwhether such systems just learn an enormous collection of superficial\nstatistics or a coherent model of the data generation process -- a world model.\nWe find preliminary evidence for the latter by analyzing the learned\nrepresentations of three spatial datasets (world, US, NYC places) and three\ntemporal datasets (historical figures, artworks, news headlines) in the Llama-2\nfamily of models. We discover that LLMs learn linear representations of space\nand time across multiple scales. These representations are robust to prompting\nvariations and unified across different entity types (e.g. cities and\nlandmarks). In addition, we identify individual ``space neurons'' and ``time\nneurons'' that reliably encode spatial and temporal coordinates. While further\ninvestigation is needed, our results suggest modern LLMs learn rich\nspatiotemporal representations of the real world and possess basic ingredients\nof a world model.\n","authors":["Wes Gurnee","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2310.02207v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08618v1","updated":"2023-12-14T02:45:31Z","published":"2023-12-14T02:45:31Z","title":"Zebra: Extending Context Window with Layerwise Grouped Local-Global\n  Attention","summary":"  This paper introduces a novel approach to enhance the capabilities of Large\nLanguage Models (LLMs) in processing and understanding extensive text\nsequences, a critical aspect in applications requiring deep comprehension and\nsynthesis of large volumes of information. Recognizing the inherent challenges\nin extending the context window for LLMs, primarily built on Transformer\narchitecture, we propose a new model architecture, referred to as Zebra. This\narchitecture efficiently manages the quadratic time and memory complexity\nissues associated with full attention in the Transformer by employing grouped\nlocal-global attention layers. Our model, akin to a zebra's alternating\nstripes, balances local and global attention layers, significantly reducing\ncomputational requirements and memory consumption. Comprehensive experiments,\nincluding pretraining from scratch, continuation of long context adaptation\ntraining, and long instruction tuning, are conducted to evaluate the Zebra's\nperformance. The results show that Zebra achieves comparable or superior\nperformance on both short and long sequence benchmarks, while also enhancing\ntraining and inference efficiency.\n","authors":["Kaiqiang Song","Xiaoyang Wang","Sangwoo Cho","Xiaoman Pan","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2312.08618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.01903v3","updated":"2023-12-14T02:20:32Z","published":"2023-03-03T13:05:15Z","title":"Prophet: Prompting Large Language Models with Complementary Answer\n  Heuristics for Knowledge-based Visual Question Answering","summary":"  Knowledge-based visual question answering (VQA) requires external knowledge\nbeyond the image to answer the question. Early studies retrieve required\nknowledge from explicit knowledge bases (KBs), which often introduces\nirrelevant information to the question, hence restricting the performance of\ntheir models. Recent works have resorted to using a powerful large language\nmodel (LLM) as an implicit knowledge engine to acquire the necessary knowledge\nfor answering. Despite the encouraging results achieved by these methods, we\nargue that they have not fully activated the capacity of the blind LLM as the\nprovided textual input is insufficient to depict the required visual\ninformation to answer the question. In this paper, we present Prophet -- a\nconceptually simple, flexible, and general framework designed to prompt LLM\nwith answer heuristics for knowledge-based VQA. Specifically, we first train a\nvanilla VQA model on a specific knowledge-based VQA dataset without external\nknowledge. After that, we extract two types of complementary answer heuristics\nfrom the VQA model: answer candidates and answer-aware examples. Finally, the\ntwo types of answer heuristics are jointly encoded into a formatted prompt to\nfacilitate the LLM's understanding of both the image and question, thus\ngenerating a more accurate answer. By incorporating the state-of-the-art LLM\nGPT-3, Prophet significantly outperforms existing state-of-the-art methods on\nfour challenging knowledge-based VQA datasets. To demonstrate the generality of\nour approach, we instantiate Prophet with the combinations of different VQA\nmodels (i.e., both discriminative and generative ones) and different LLMs\n(i.e., both commercial and open-source ones).\n","authors":["Zhou Yu","Xuecheng Ouyang","Zhenwei Shao","Meng Wang","Jun Yu"],"pdf_url":"https://arxiv.org/pdf/2303.01903v3.pdf","comment":"An extended jounral version of our CVPR 2023 paper. The conference\n  version can be referred to the last arxiv version"},{"id":"http://arxiv.org/abs/2311.04915v2","updated":"2023-12-14T01:30:01Z","published":"2023-11-02T02:21:39Z","title":"Chain of Empathy: Enhancing Empathetic Response of Large Language Models\n  Based on Psychotherapy Models","summary":"  We present a novel method, the Chain of Empathy (CoE) prompting, that\nutilizes insights from psychotherapy to induce Large Language Models (LLMs) to\nreason about human emotional states. This method is inspired by various\npsychotherapy approaches including Cognitive Behavioral Therapy (CBT),\nDialectical Behavior Therapy (DBT), Person Centered Therapy (PCT), and Reality\nTherapy (RT), each leading to different patterns of interpreting clients'\nmental states. LLMs without reasoning generated predominantly exploratory\nresponses. However, when LLMs used CoE reasoning, we found a more comprehensive\nrange of empathetic responses aligned with the different reasoning patterns of\neach psychotherapy model. The CBT based CoE resulted in the most balanced\ngeneration of empathetic responses. The findings underscore the importance of\nunderstanding the emotional context and how it affects human and AI\ncommunication. Our research contributes to understanding how psychotherapeutic\nmodels can be incorporated into LLMs, facilitating the development of\ncontext-specific, safer, and empathetic AI.\n","authors":["Yoon Kyung Lee","Inju Lee","Minjung Shin","Seoyeon Bae","Sowon Hahn"],"pdf_url":"https://arxiv.org/pdf/2311.04915v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08585v1","updated":"2023-12-14T01:16:19Z","published":"2023-12-14T01:16:19Z","title":"Unraveling Key Factors of Knowledge Distillation","summary":"  Knowledge distillation, a technique for model compression and performance\nenhancement, has gained significant traction in Neural Machine Translation\n(NMT). However, existing research primarily focuses on empirical applications,\nand there is a lack of comprehensive understanding of how student model\ncapacity, data complexity, and decoding strategies collectively influence\ndistillation effectiveness. Addressing this gap, our study conducts an in-depth\ninvestigation into these factors, particularly focusing on their interplay in\nword-level and sequence-level distillation within NMT. Through extensive\nexperimentation across datasets like IWSLT13 En$\\rightarrow$Fr, IWSLT14\nEn$\\rightarrow$De, and others, we empirically validate hypotheses related to\nthe impact of these factors on knowledge distillation. Our research not only\nelucidates the significant influence of model capacity, data complexity, and\ndecoding strategies on distillation effectiveness but also introduces a novel,\noptimized distillation approach. This approach, when applied to the IWSLT14\nde$\\rightarrow$en translation task, achieves state-of-the-art performance,\ndemonstrating its practical efficacy in advancing the field of NMT.\n","authors":["Jingxuan Wei","Linzhuang Sun","Xu Tan","Bihui Yu","Ruifeng Guo"],"pdf_url":"https://arxiv.org/pdf/2312.08585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08583v1","updated":"2023-12-14T01:06:37Z","published":"2023-12-14T01:06:37Z","title":"ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric\n  Strategy for Diverse Generative Tasks","summary":"  This study examines 4-bit quantization methods like GPTQ in large language\nmodels (LLMs), highlighting GPTQ's overfitting and limited enhancement in\nZero-Shot tasks. While prior works merely focusing on zero-shot measurement, we\nextend task scope to more generative categories such as code generation and\nabstractive summarization, in which we found that INT4 quantization can\nsignificantly underperform. However, simply shifting to higher precision\nformats like FP6 has been particularly challenging, thus overlooked, due to\npoor performance caused by the lack of sophisticated integration and system\nacceleration strategies on current AI hardware. Our results show that FP6, even\nwith a coarse-grain quantization scheme, performs robustly across various\nalgorithms and tasks, demonstrating its superiority in accuracy and\nversatility. Notably, with the FP6 quantization, \\codestar-15B model performs\ncomparably to its FP16 counterpart in code generation, and for smaller models\nlike the 406M it closely matches their baselines in summarization. Neither can\nbe achieved by INT4. To better accommodate various AI hardware and achieve the\nbest system performance, we propose a novel 4+2 design for FP6 to achieve\nsimilar latency to the state-of-the-art INT4 fine-grain quantization. With our\ndesign, FP6 can become a promising solution to the current 4-bit quantization\nmethods used in LLMs.\n","authors":["Xiaoxia Wu","Haojun Xia","Stephen Youn","Zhen Zheng","Shiyang Chen","Arash Bakhtiari","Michael Wyatt","Yuxiong He","Olatunji Ruwase","Leon Song","Zhewei Yao"],"pdf_url":"https://arxiv.org/pdf/2312.08583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08579v1","updated":"2023-12-14T00:50:14Z","published":"2023-12-14T00:50:14Z","title":"Identifying Planetary Names in Astronomy Papers: A Multi-Step Approach","summary":"  The automatic identification of planetary feature names in astronomy\npublications presents numerous challenges. These features include craters,\ndefined as roughly circular depressions resulting from impact or volcanic\nactivity; dorsas, which are elongate raised structures or wrinkle ridges; and\nlacus, small irregular patches of dark, smooth material on the Moon, referred\nto as \"lake\" (Planetary Names Working Group, n.d.). Many feature names overlap\nwith places or people's names that they are named after, for example, Syria,\nTempe, Einstein, and Sagan, to name a few (U.S. Geological Survey, n.d.). Some\nfeature names have been used in many contexts, for instance, Apollo, which can\nrefer to mission, program, sample, astronaut, seismic, seismometers, core, era,\ndata, collection, instrument, and station, in addition to the crater on the\nMoon. Some feature names can appear in the text as adjectives, like the lunar\ncraters Black, Green, and White. Some feature names in other contexts serve as\ndirections, like craters West and South on the Moon. Additionally, some\nfeatures share identical names across different celestial bodies, requiring\ndisambiguation, such as the Adams crater, which exists on both the Moon and\nMars. We present a multi-step pipeline combining rule-based filtering,\nstatistical relevance analysis, part-of-speech (POS) tagging, named entity\nrecognition (NER) model, hybrid keyword harvesting, knowledge graph (KG)\nmatching, and inference with a locally installed large language model (LLM) to\nreliably identify planetary names despite these challenges. When evaluated on a\ndataset of astronomy papers from the Astrophysics Data System (ADS), this\nmethodology achieves an F1-score over 0.97 in disambiguating planetary feature\nnames.\n","authors":["Golnaz Shapurian","Michael J Kurtz","Alberto Accomazzi"],"pdf_url":"https://arxiv.org/pdf/2312.08579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.08628v3","updated":"2023-12-14T00:45:39Z","published":"2023-09-12T16:39:41Z","title":"Recovering from Privacy-Preserving Masking with Large Language Models","summary":"  Model adaptation is crucial to handle the discrepancy between proxy training\ndata and actual users data received. To effectively perform adaptation, textual\ndata of users is typically stored on servers or their local devices, where\ndownstream natural language processing (NLP) models can be directly trained\nusing such in-domain data. However, this might raise privacy and security\nconcerns due to the extra risks of exposing user information to adversaries.\nReplacing identifying information in textual data with a generic marker has\nbeen recently explored. In this work, we leverage large language models (LLMs)\nto suggest substitutes of masked tokens and have their effectiveness evaluated\non downstream language modeling tasks. Specifically, we propose multiple\npre-trained and fine-tuned LLM-based approaches and perform empirical studies\non various datasets for the comparison of these methods. Experimental results\nshow that models trained on the obfuscation corpora are able to achieve\ncomparable performance with the ones trained on the original data without\nprivacy-preserving token masking.\n","authors":["Arpita Vats","Zhe Liu","Peng Su","Debjyoti Paul","Yingyi Ma","Yutong Pang","Zeeshan Ahmed","Ozlem Kalinli"],"pdf_url":"https://arxiv.org/pdf/2309.08628v3.pdf","comment":"Accepted to ICASSP"},{"id":"http://arxiv.org/abs/2312.10097v1","updated":"2023-12-14T17:45:50Z","published":"2023-12-14T17:45:50Z","title":"Arithmetics-Based Decomposition of Numeral Words -- Arithmetic\n  Conditions give the Unpacking Strategy","summary":"  In this paper we present a novel numeral decomposer that is designed to\nrevert Hurford's Packing Strategy. The Packing Strategy is a model on how\nnumeral words are formed out of smaller numeral words by recursion. The\ndecomposer does not simply check decimal digits but it also works for numerals\nformed on base 20 or any other base or even combinations of different bases.\nAll assumptions that we use are justified with Hurford's Packing Strategy. The\ndecomposer reads through the numeral. When it finds a sub-numeral, it checks\narithmetic conditions to decide whether or not to unpack the sub-numeral. The\ngoal is to unpack those numerals that can sensibly be substituted by similar\nnumerals. E.g., in 'twenty-seven thousand and two hundred and six' it should\nunpack 'twenty-seven' and 'two hundred and six', as those could each be\nsensibly replaced by any numeral from 1 to 999. Our most used condition is: If\nS is a substitutable sub-numeral of a numeral N, then 2*value(S) < value(N). We\nhave tested the decomposer on numeral systems in 254 different natural\nlanguages. We also developed a reinforcement learning algorithm based on the\ndecomposer. Both algorithms' code and the results are open source on GitHub.\n","authors":["Isidor Konrad Maier","Matthias Wolff"],"pdf_url":"https://arxiv.org/pdf/2312.10097v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2312.07180v3","updated":"2023-12-14T06:19:56Z","published":"2023-12-12T11:27:13Z","title":"Context-Aware Iteration Policy Network for Efficient Optical Flow\n  Estimation","summary":"  Existing recurrent optical flow estimation networks are computationally\nexpensive since they use a fixed large number of iterations to update the flow\nfield for each sample. An efficient network should skip iterations when the\nflow improvement is limited. In this paper, we develop a Context-Aware\nIteration Policy Network for efficient optical flow estimation, which\ndetermines the optimal number of iterations per sample. The policy network\nachieves this by learning contextual information to realize whether flow\nimprovement is bottlenecked or minimal. On the one hand, we use iteration\nembedding and historical hidden cell, which include previous iterations\ninformation, to convey how flow has changed from previous iterations. On the\nother hand, we use the incremental loss to make the policy network implicitly\nperceive the magnitude of optical flow improvement in the subsequent iteration.\nFurthermore, the computational complexity in our dynamic network is\ncontrollable, allowing us to satisfy various resource preferences with a single\ntrained model. Our policy network can be easily integrated into\nstate-of-the-art optical flow networks. Extensive experiments show that our\nmethod maintains performance while reducing FLOPs by about 40%/20% for the\nSintel/KITTI datasets.\n","authors":["Ri Cheng","Ruian He","Xuhao Jiang","Shili Zhou","Weimin Tan","Bo Yan"],"pdf_url":"https://arxiv.org/pdf/2312.07180v3.pdf","comment":"2024, Association for the Advancement of Artificial Intelligence"},{"id":"http://arxiv.org/abs/2312.08078v2","updated":"2023-12-14T02:31:44Z","published":"2023-12-13T11:47:28Z","title":"Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic\n  Image-Report Generation","summary":"  To address these issues, we propose a novel Adaptive patch-word Matching\n(AdaMatch) model to correlate chest X-ray (CXR) image regions with words in\nmedical reports and apply it to CXR-report generation to provide explainability\nfor the generation process. AdaMatch exploits the fine-grained relation between\nadaptive patches and words to provide explanations of specific image regions\nwith corresponding words. To capture the abnormal regions of varying sizes and\npositions, we introduce the Adaptive Patch extraction (AdaPatch) module to\nacquire the adaptive patches for these regions adaptively. In order to provide\nexplicit explainability for CXR-report generation task, we propose an\nAdaMatch-based bidirectional large language model for Cyclic CXR-report\ngeneration (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywords\nfor CXR images and `keypatches' for medical reports as hints to guide\nCXR-report generation. Extensive experiments on two publicly available CXR\ndatasets prove the effectiveness of our method and its superior performance to\nexisting methods.\n","authors":["Wenting Chen","Xiang Li","Linlin Shen","Yixuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2312.08078v2.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2312.08048v2","updated":"2023-12-14T08:40:27Z","published":"2023-12-13T10:57:46Z","title":"Compositional Inversion for Stable Diffusion Models","summary":"  Inversion methods, such as Textual Inversion, generate personalized images by\nincorporating concepts of interest provided by user images. However, existing\nmethods often suffer from overfitting issues, where the dominant presence of\ninverted concepts leads to the absence of other desired concepts. It stems from\nthe fact that during inversion, the irrelevant semantics in the user images are\nalso encoded, forcing the inverted concepts to occupy locations far from the\ncore distribution in the embedding space. To address this issue, we propose a\nmethod that guides the inversion process towards the core distribution for\ncompositional embeddings. Additionally, we introduce a spatial regularization\napproach to balance the attention on the concepts being composed. Our method is\ndesigned as a post-training approach and can be seamlessly integrated with\nother inversion methods. Experimental results demonstrate the effectiveness of\nour proposed approach in mitigating the overfitting problem and generating more\ndiverse and balanced compositions of concepts in the synthesized images. The\nsource code is available at\nhttps://github.com/zhangxulu1996/Compositional-Inversion.\n","authors":["Xu-Lu Zhang","Xiao-Yong Wei","Jin-Lin Wu","Tian-Yi Zhang","Zhaoxiang Zhang","Zhen Lei","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2312.08048v2.pdf","comment":"This paper was accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.08009v2","updated":"2023-12-14T11:16:05Z","published":"2023-12-13T09:32:50Z","title":"Semi-Supervised Class-Agnostic Motion Prediction with Pseudo Label\n  Regeneration and BEVMix","summary":"  Class-agnostic motion prediction methods aim to comprehend motion within\nopen-world scenarios, holding significance for autonomous driving systems.\nHowever, training a high-performance model in a fully-supervised manner always\nrequires substantial amounts of manually annotated data, which can be both\nexpensive and time-consuming to obtain. To address this challenge, our study\nexplores the potential of semi-supervised learning (SSL) for class-agnostic\nmotion prediction. Our SSL framework adopts a consistency-based self-training\nparadigm, enabling the model to learn from unlabeled data by generating pseudo\nlabels through test-time inference. To improve the quality of pseudo labels, we\npropose a novel motion selection and re-generation module. This module\neffectively selects reliable pseudo labels and re-generates unreliable ones.\nFurthermore, we propose two data augmentation strategies: temporal sampling and\nBEVMix. These strategies facilitate consistency regularization in SSL.\nExperiments conducted on nuScenes demonstrate that our SSL method can surpass\nthe self-supervised approach by a large margin by utilizing only a tiny\nfraction of labeled data. Furthermore, our method exhibits comparable\nperformance to weakly and some fully supervised methods. These results\nhighlight the ability of our method to strike a favorable balance between\nannotation costs and performance. Code will be available at\nhttps://github.com/kwwcv/SSMP.\n","authors":["Kewei Wang","Yizheng Wu","Zhiyu Pan","Xingyi Li","Ke Xian","Zhe Wang","Zhiguo Cao","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2312.08009v2.pdf","comment":"This paper is accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2310.06470v3","updated":"2023-12-14T03:22:48Z","published":"2023-10-10T09:41:13Z","title":"Focus on Local Regions for Query-based Object Detection","summary":"  Query-based methods have garnered significant attention in object detection\nsince the advent of DETR, the pioneering query-based detector. However, these\nmethods face challenges like slow convergence and suboptimal performance.\nNotably, self-attention in object detection often hampers convergence due to\nits global focus. To address these issues, we propose FoLR, a transformer-like\narchitecture with only decoders. We improve the self-attention by isolating\nconnections between irrelevant objects that makes it focus on local regions but\nnot global regions. We also design the adaptive sampling method to extract\neffective features based on queries' local regions from feature maps.\nAdditionally, we employ a look-back strategy for decoders to retain previous\ninformation, followed by the Feature Mixer module to fuse features and queries.\nExperimental results demonstrate FoLR's state-of-the-art performance in\nquery-based detectors, excelling in convergence speed and computational\nefficiency.\n  Index Terms: Local regions, Attention mechanism, Object detection\n","authors":["Hongbin Xu","Yamei Xia","Shuai Zhao","Bo Cheng"],"pdf_url":"https://arxiv.org/pdf/2310.06470v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07976v2","updated":"2023-12-14T07:09:23Z","published":"2023-12-13T08:45:57Z","title":"Challenges of YOLO Series for Object Detection in Extremely Heavy Rain:\n  CALRA Simulator based Synthetic Evaluation Dataset","summary":"  Recently, as many studies of autonomous vehicles have been achieved for\nlevels 4 and 5, there has been also increasing interest in the advancement of\nperception, decision, and control technologies, which are the three major\naspects of autonomous vehicles. As for the perception technologies achieving\nreliable maneuvering of autonomous vehicles, object detection by using diverse\nsensors (e.g., LiDAR, radar, and camera) should be prioritized. These sensors\nrequire to detect objects accurately and quickly in diverse weather conditions,\nbut they tend to have challenges to consistently detect objects in bad weather\nconditions with rain, snow, or fog. Thus, in this study, based on the\nexperimentally obtained raindrop data from precipitation conditions, we\nconstructed a novel dataset that could test diverse network model in various\nprecipitation conditions through the CARLA simulator. Consequently, based on\nour novel dataset, YOLO series, a one-stage-detector, was used to\nquantitatively verify how much object detection performance could be decreased\nunder various precipitation conditions from normal to extreme heavy rain\nsituations.\n","authors":["T. Kim","H. Jeon","Y. Lim"],"pdf_url":"https://arxiv.org/pdf/2312.07976v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06988v3","updated":"2023-12-14T03:00:05Z","published":"2023-12-12T05:12:22Z","title":"MWSIS: Multimodal Weakly Supervised Instance Segmentation with 2D Box\n  Annotations for Autonomous Driving","summary":"  Instance segmentation is a fundamental research in computer vision,\nespecially in autonomous driving. However, manual mask annotation for instance\nsegmentation is quite time-consuming and costly. To address this problem, some\nprior works attempt to apply weakly supervised manner by exploring 2D or 3D\nboxes. However, no one has ever successfully segmented 2D and 3D instances\nsimultaneously by only using 2D box annotations, which could further reduce the\nannotation cost by an order of magnitude. Thus, we propose a novel framework\ncalled Multimodal Weakly Supervised Instance Segmentation (MWSIS), which\nincorporates various fine-grained label generation and correction modules for\nboth 2D and 3D modalities to improve the quality of pseudo labels, along with a\nnew multimodal cross-supervision approach, named Consistency Sparse Cross-modal\nSupervision (CSCS), to reduce the inconsistency of multimodal predictions by\nresponse distillation. Particularly, transferring the 3D backbone to downstream\ntasks not only improves the performance of the 3D detectors, but also\noutperforms fully supervised instance segmentation with only 5% fully\nsupervised annotations. On the Waymo dataset, the proposed framework\ndemonstrates significant improvements over the baseline, especially achieving\n2.59% mAP and 12.75% mAP increases for 2D and 3D instance segmentation tasks,\nrespectively. The code is available at\nhttps://github.com/jiangxb98/mwsis-plugin.\n","authors":["Guangfeng Jiang","Jun Liu","Yuzhi Wu","Wenlong Liao","Tao He","Pai Peng"],"pdf_url":"https://arxiv.org/pdf/2312.06988v3.pdf","comment":"AAAI2024"},{"id":"http://arxiv.org/abs/2312.07871v2","updated":"2023-12-14T02:30:01Z","published":"2023-12-13T03:17:34Z","title":"MLNet: Mutual Learning Network with Neighborhood Invariance for\n  Universal Domain Adaptation","summary":"  Universal domain adaptation (UniDA) is a practical but challenging problem,\nin which information about the relation between the source and the target\ndomains is not given for knowledge transfer. Existing UniDA methods may suffer\nfrom the problems of overlooking intra-domain variations in the target domain\nand difficulty in separating between the similar known and unknown class. To\naddress these issues, we propose a novel Mutual Learning Network (MLNet) with\nneighborhood invariance for UniDA. In our method, confidence-guided invariant\nfeature learning with self-adaptive neighbor selection is designed to reduce\nthe intra-domain variations for more generalizable feature representation. By\nusing the cross-domain mixup scheme for better unknown-class identification,\nthe proposed method compensates for the misidentified known-class errors by\nmutual learning between the closed-set and open-set classifiers. Extensive\nexperiments on three publicly available benchmarks demonstrate that our method\nachieves the best results compared to the state-of-the-arts in most cases and\nsignificantly outperforms the baseline across all the four settings in UniDA.\nCode is available at https://github.com/YanzuoLu/MLNet.\n","authors":["Yanzuo Lu","Meng Shen","Andy J Ma","Xiaohua Xie","Jian-Huang Lai"],"pdf_url":"https://arxiv.org/pdf/2312.07871v2.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2312.07853v2","updated":"2023-12-14T02:05:03Z","published":"2023-12-13T02:48:03Z","title":"High-Order Structure Based Middle-Feature Learning for Visible-Infrared\n  Person Re-Identification","summary":"  Visible-infrared person re-identification (VI-ReID) aims to retrieve images\nof the same persons captured by visible (VIS) and infrared (IR) cameras.\nExisting VI-ReID methods ignore high-order structure information of features\nwhile being relatively difficult to learn a reasonable common feature space due\nto the large modality discrepancy between VIS and IR images. To address the\nabove problems, we propose a novel high-order structure based middle-feature\nlearning network (HOS-Net) for effective VI-ReID. Specifically, we first\nleverage a short- and long-range feature extraction (SLE) module to effectively\nexploit both short-range and long-range features. Then, we propose a high-order\nstructure learning (HSL) module to successfully model the high-order\nrelationship across different local features of each person image based on a\nwhitened hypergraph network.This greatly alleviates model collapse and enhances\nfeature representations. Finally, we develop a common feature space learning\n(CFL) module to learn a discriminative and reasonable common feature space\nbased on middle features generated by aligning features from different\nmodalities and ranges. In particular, a modality-range identity-center\ncontrastive (MRIC) loss is proposed to reduce the distances between the VIS,\nIR, and middle features, smoothing the training process. Extensive experiments\non the SYSU-MM01, RegDB, and LLCM datasets show that our HOS-Net achieves\nsuperior state-of-the-art performance. Our code is available at\n\\url{https://github.com/Jaulaucoeng/HOS-Net}.\n","authors":["Liuxiang Qiu","Si Chen","Yan Yan","Jing-Hao Xue","Da-Han Wang","Shunzhi Zhu"],"pdf_url":"https://arxiv.org/pdf/2312.07853v2.pdf","comment":"Accepted by AAAI-2024"},{"id":"http://arxiv.org/abs/2312.07533v2","updated":"2023-12-14T03:47:52Z","published":"2023-12-12T18:58:18Z","title":"VILA: On Pre-training for Visual Language Models","summary":"  Visual language models (VLMs) rapidly progressed with the recent success of\nlarge language models. There have been growing efforts on visual instruction\ntuning to extend the LLM with visual inputs, but lacks an in-depth study of the\nvisual language pre-training process, where the model learns to perform joint\nmodeling on both modalities. In this work, we examine the design options for\nVLM pre-training by augmenting LLM towards VLM through step-by-step\ncontrollable comparisons. We introduce three main findings: (1) freezing LLMs\nduring pre-training can achieve decent zero-shot performance, but lack\nin-context learning capability, which requires unfreezing the LLM; (2)\ninterleaved pre-training data is beneficial whereas image-text pairs alone are\nnot optimal; (3) re-blending text-only instruction data to image-text data\nduring instruction fine-tuning not only remedies the degradation of text-only\ntasks, but also boosts VLM task accuracy. With an enhanced pre-training recipe\nwe build VILA, a Visual Language model family that consistently outperforms the\nstate-of-the-art models, e.g., LLaVA-1.5, across main benchmarks without bells\nand whistles. Multi-modal pre-training also helps unveil appealing properties\nof VILA, including multi-image reasoning, enhanced in-context learning, and\nbetter world knowledge.\n","authors":["Ji Lin","Hongxu Yin","Wei Ping","Yao Lu","Pavlo Molchanov","Andrew Tao","Huizi Mao","Jan Kautz","Mohammad Shoeybi","Song Han"],"pdf_url":"https://arxiv.org/pdf/2312.07533v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09398v1","updated":"2023-12-14T23:37:19Z","published":"2023-12-14T23:37:19Z","title":"Relightable Neural Assets","summary":"  High-fidelity 3D assets with materials composed of fibers (including hair),\ncomplex layered material shaders, or fine scattering geometry are ubiquitous in\nhigh-end realistic rendering applications. Rendering such models is\ncomputationally expensive due to heavy shaders and long scattering paths.\nMoreover, implementing the shading and scattering models is non-trivial and has\nto be done not only in the 3D content authoring software (which is necessarily\ncomplex), but also in all downstream rendering solutions. For example, web and\nmobile viewers for complex 3D assets are desirable, but frequently cannot\nsupport the full shading complexity allowed by the authoring application. Our\ngoal is to design a neural representation for 3D assets with complex shading\nthat supports full relightability and full integration into existing renderers.\nWe provide an end-to-end shading solution at the first intersection of a ray\nwith the underlying geometry. All shading and scattering is precomputed and\nincluded in the neural asset; no multiple scattering paths need to be traced,\nand no complex shading models need to be implemented to render our assets,\nbeyond a single neural architecture. We combine an MLP decoder with a feature\ngrid. Shading consists of querying a feature vector, followed by an MLP\nevaluation producing the final reflectance value. Our method provides\nhigh-fidelity shading, close to the ground-truth Monte Carlo estimate even at\nclose-up views. We believe our neural assets could be used in practical\nrenderers, providing significant speed-ups and simplifying renderer\nimplementations.\n","authors":["Krishna Mullia","Fujun Luan","Xin Sun","Miloš Hašan"],"pdf_url":"https://arxiv.org/pdf/2312.09398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09387v1","updated":"2023-12-14T23:00:11Z","published":"2023-12-14T23:00:11Z","title":"High-Resolution Maps of Left Atrial Displacements and Strains Estimated\n  with 3D CINE MRI and Unsupervised Neural Networks","summary":"  The functional analysis of the left atrium (LA) is important for evaluating\ncardiac health and understanding diseases like atrial fibrillation. Cine MRI is\nideally placed for the detailed 3D characterisation of LA motion and\ndeformation, but it is lacking appropriate acquisition and analysis tools. In\nthis paper, we present Analysis for Left Atrial Displacements and Deformations\nusing unsupervIsed neural Networks, \\textit{Aladdin}, to automatically and\nreliably characterise regional LA deformations from high-resolution 3D Cine\nMRI. The tool includes: an online few-shot segmentation network (Aladdin-S), an\nonline unsupervised image registration network (Aladdin-R), and a strain\ncalculations pipeline tailored to the LA. We create maps of LA Displacement\nVector Field (DVF) magnitude and LA principal strain values from images of 10\nhealthy volunteers and 8 patients with cardiovascular disease (CVD). We\nadditionally create an atlas of these biomarkers using the data from the\nhealthy volunteers. Aladdin is able to accurately track the LA wall across the\ncardiac cycle and characterize its motion and deformation. The overall DVF\nmagnitude and principal strain values are significantly higher in the healthy\ngroup vs CVD patients: $2.85 \\pm 1.59~mm$ and $0.09 \\pm 0.05$ vs $1.96 \\pm\n0.74~mm$ and $0.03 \\pm 0.04$, respectively. The time course of these metrics is\nalso different in the two groups, with a more marked active contraction phase\nobserved in the healthy cohort. Finally, utilizing the LA atlas allows us to\nidentify regional deviations from the population distribution that may indicate\nfocal tissue abnormalities. The proposed tool for the quantification of novel\nregional LA deformation biomarkers should have important clinical applications.\nThe source code, anonymized images, generated maps and atlas are publicly\navailable: https://github.com/cgalaz01/aladdin_cmr_la.\n","authors":["Christoforos Galazis","Samuel Shepperd","Emma Brouwer","Sandro Queirós","Ebraham Alskaf","Mustafa Anjari","Amedeo Chiribiri","Jack Lee","Anil A. Bharath","Marta Varela"],"pdf_url":"https://arxiv.org/pdf/2312.09387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09367v1","updated":"2023-12-14T22:04:22Z","published":"2023-12-14T22:04:22Z","title":"Text-Guided Face Recognition using Multi-Granularity Cross-Modal\n  Contrastive Learning","summary":"  State-of-the-art face recognition (FR) models often experience a significant\nperformance drop when dealing with facial images in surveillance scenarios\nwhere images are in low quality and often corrupted with noise. Leveraging\nfacial characteristics, such as freckles, scars, gender, and ethnicity, becomes\nhighly beneficial in improving FR performance in such scenarios. In this paper,\nwe introduce text-guided face recognition (TGFR) to analyze the impact of\nintegrating facial attributes in the form of natural language descriptions. We\nhypothesize that adding semantic information into the loop can significantly\nimprove the image understanding capability of an FR algorithm compared to other\nsoft biometrics. However, learning a discriminative joint embedding within the\nmultimodal space poses a considerable challenge due to the semantic gap in the\nunaligned image-text representations, along with the complexities arising from\nambiguous and incoherent textual descriptions of the face. To address these\nchallenges, we introduce a face-caption alignment module (FCAM), which\nincorporates cross-modal contrastive losses across multiple granularities to\nmaximize the mutual information between local and global features of the\nface-caption pair. Within FCAM, we refine both facial and textual features for\nlearning aligned and discriminative features. We also design a face-caption\nfusion module (FCFM) that applies fine-grained interactions and coarse-grained\nassociations among cross-modal features. Through extensive experiments\nconducted on three face-caption datasets, proposed TGFR demonstrates remarkable\nimprovements, particularly on low-quality images, over existing FR models and\noutperforms other related methods and benchmarks.\n","authors":["Md Mahedi Hasan","Shoaib Meraj Sami","Nasser Nasrabadi"],"pdf_url":"https://arxiv.org/pdf/2312.09367v1.pdf","comment":"Accepted at IEEE/CVF Winter Conference on Applications of Computer\n  Vision (WACV), 2024"},{"id":"http://arxiv.org/abs/2312.09361v1","updated":"2023-12-14T21:51:06Z","published":"2023-12-14T21:51:06Z","title":"RTRA: Rapid Training of Regularization-based Approaches in Continual\n  Learning","summary":"  Catastrophic forgetting(CF) is a significant challenge in continual learning\n(CL). In regularization-based approaches to mitigate CF, modifications to\nimportant training parameters are penalized in subsequent tasks using an\nappropriate loss function. We propose the RTRA, a modification to the widely\nused Elastic Weight Consolidation (EWC) regularization scheme, using the\nNatural Gradient for loss function optimization. Our approach improves the\ntraining of regularization-based methods without sacrificing test-data\nperformance. We compare the proposed RTRA approach against EWC using the\niFood251 dataset. We show that RTRA has a clear edge over the state-of-the-art\napproaches.\n","authors":["Sahil Nokhwal","Nirman Kumar"],"pdf_url":"https://arxiv.org/pdf/2312.09361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09360v1","updated":"2023-12-14T21:48:56Z","published":"2023-12-14T21:48:56Z","title":"The Expert Knowledge combined with AI outperforms AI Alone in Seizure\n  Onset Zone Localization using resting state fMRI","summary":"  We evaluated whether integration of expert guidance on seizure onset zone\n(SOZ) identification from resting state functional MRI (rs-fMRI) connectomics\ncombined with deep learning (DL) techniques enhances the SOZ delineation in\npatients with refractory epilepsy (RE), compared to utilizing DL alone. Rs-fMRI\nwere collected from 52 children with RE who had subsequently undergone ic-EEG\nand then, if indicated, surgery for seizure control (n = 25). The resting state\nfunctional connectomics data were previously independently classified by two\nexpert epileptologists, as indicative of measurement noise, typical resting\nstate network connectivity, or SOZ. An expert knowledge integrated deep network\nwas trained on functional connectomics data to identify SOZ. Expert knowledge\nintegrated with DL showed a SOZ localization accuracy of 84.8& and F1 score,\nharmonic mean of positive predictive value and sensitivity, of 91.7%.\nConversely, a DL only model yielded an accuracy of less than 50% (F1 score\n63%). Activations that initiate in gray matter, extend through white matter and\nend in vascular regions are seen as the most discriminative expert identified\nSOZ characteristics. Integration of expert knowledge of functional connectomics\ncan not only enhance the performance of DL in localizing SOZ in RE, but also\nlead toward potentially useful explanations of prevalent co-activation patterns\nin SOZ. RE with surgical outcomes and pre-operative rs-fMRI studies can yield\nexpert knowledge most salient for SOZ identification.\n","authors":["Payal Kamboj","Ayan Banerjee","Varina L. Boerwinkle","Sandeep K. S. Gupta"],"pdf_url":"https://arxiv.org/pdf/2312.09360v1.pdf","comment":"Accepted in Frontiers in Neurology journal, section Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2312.09337v1","updated":"2023-12-14T21:00:56Z","published":"2023-12-14T21:00:56Z","title":"Promptable Behaviors: Personalizing Multi-Objective Rewards from Human\n  Preferences","summary":"  Customizing robotic behaviors to be aligned with diverse human preferences is\nan underexplored challenge in the field of embodied AI. In this paper, we\npresent Promptable Behaviors, a novel framework that facilitates efficient\npersonalization of robotic agents to diverse human preferences in complex\nenvironments. We use multi-objective reinforcement learning to train a single\npolicy adaptable to a broad spectrum of preferences. We introduce three\ndistinct methods to infer human preferences by leveraging different types of\ninteractions: (1) human demonstrations, (2) preference feedback on trajectory\ncomparisons, and (3) language instructions. We evaluate the proposed method in\npersonalized object-goal navigation and flee navigation tasks in ProcTHOR and\nRoboTHOR, demonstrating the ability to prompt agent behaviors to satisfy human\npreferences in various scenarios. Project page:\nhttps://promptable-behaviors.github.io\n","authors":["Minyoung Hwang","Luca Weihs","Chanwoo Park","Kimin Lee","Aniruddha Kembhavi","Kiana Ehsani"],"pdf_url":"https://arxiv.org/pdf/2312.09337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.13859v2","updated":"2023-12-14T20:34:32Z","published":"2023-10-20T23:47:01Z","title":"Not all Fake News is Written: A Dataset and Analysis of Misleading Video\n  Headlines","summary":"  Polarization and the marketplace for impressions have conspired to make\nnavigating information online difficult for users, and while there has been a\nsignificant effort to detect false or misleading text, multimodal datasets have\nreceived considerably less attention. To complement existing resources, we\npresent multimodal Video Misleading Headline (VMH), a dataset that consists of\nvideos and whether annotators believe the headline is representative of the\nvideo's contents. After collecting and annotating this dataset, we analyze\nmultimodal baselines for detecting misleading headlines. Our annotation process\nalso focuses on why annotators view a video as misleading, allowing us to\nbetter understand the interplay of annotators' background and the content of\nthe videos.\n","authors":["Yoo Yeon Sung","Jordan Boyd-Graber","Naeemul Hassan"],"pdf_url":"https://arxiv.org/pdf/2310.13859v2.pdf","comment":"EMNLP 2023 Main Paper"},{"id":"http://arxiv.org/abs/2312.09313v1","updated":"2023-12-14T19:38:06Z","published":"2023-12-14T19:38:06Z","title":"LatentEditor: Text Driven Local Editing of 3D Scenes","summary":"  While neural fields have made significant strides in view synthesis and scene\nreconstruction, editing them poses a formidable challenge due to their implicit\nencoding of geometry and texture information from multi-view inputs. In this\npaper, we introduce \\textsc{LatentEditor}, an innovative framework designed to\nempower users with the ability to perform precise and locally controlled\nediting of neural fields using text prompts. Leveraging denoising diffusion\nmodels, we successfully embed real-world scenes into the latent space,\nresulting in a faster and more adaptable NeRF backbone for editing compared to\ntraditional methods. To enhance editing precision, we introduce a delta score\nto calculate the 2D mask in the latent space that serves as a guide for local\nmodifications while preserving irrelevant regions. Our novel pixel-level\nscoring approach harnesses the power of InstructPix2Pix (IP2P) to discern the\ndisparity between IP2P conditional and unconditional noise predictions in the\nlatent space. The edited latents conditioned on the 2D masks are then\niteratively updated in the training set to achieve 3D local editing. Our\napproach achieves faster editing speeds and superior output quality compared to\nexisting 3D editing models, bridging the gap between textual instructions and\nhigh-quality 3D scene editing in latent space. We show the superiority of our\napproach on four benchmark 3D datasets, LLFF, IN2N, NeRFStudio and NeRF-Art.\n","authors":["Umar Khalid","Hasan Iqbal","Nazmul Karim","Jing Hua","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2312.09313v1.pdf","comment":"Project Page: https://latenteditor.github.io/"},{"id":"http://arxiv.org/abs/2001.01258v3","updated":"2023-12-14T19:27:50Z","published":"2020-01-05T15:30:23Z","title":"The troublesome kernel -- On hallucinations, no free lunches and the\n  accuracy-stability trade-off in inverse problems","summary":"  Methods inspired by Artificial Intelligence (AI) are starting to\nfundamentally change computational science and engineering through breakthrough\nperformances on challenging problems. However, reliability and trustworthiness\nof such techniques is becoming a major concern. In inverse problems in imaging,\nthe focus of this paper, there is increasing empirical evidence that methods\nmay suffer from hallucinations, i.e., false, but realistic-looking artifacts;\ninstability, i.e., sensitivity to perturbations in the data; and unpredictable\ngeneralization, i.e., excellent performance on some images, but significant\ndeterioration on others. This paper presents a theoretical foundation for these\nphenomena. We give a mathematical framework describing how and when such\neffects arise in arbitrary reconstruction methods, not just AI-inspired\ntechniques. Several of our results take the form of `no free lunch' theorems.\nSpecifically, we show that (i) methods that overperform on a single image can\nwrongly transfer details from one image to another, creating a hallucination,\n(ii) methods that overperform on two or more images can hallucinate or be\nunstable, (iii) optimizing the accuracy-stability trade-off is generally\ndifficult, (iv) hallucinations and instabilities, if they occur, are not rare\nevents, and may be encouraged by standard training, (v) it may be impossible to\nconstruct optimal reconstruction maps for certain problems. Our results trace\nthese effects to the kernel of the forward operator whenever it is nontrivial,\nbut also extend to the case when the forward operator is ill-conditioned. Based\non these insights, our work aims to spur research into new ways to develop\nrobust and reliable AI-inspired methods for inverse problems in imaging.\n","authors":["Nina M. Gottschling","Vegard Antun","Anders C. Hansen","Ben Adcock"],"pdf_url":"https://arxiv.org/pdf/2001.01258v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09305v1","updated":"2023-12-14T19:18:38Z","published":"2023-12-14T19:18:38Z","title":"Stable Score Distillation for High-Quality 3D Generation","summary":"  Score Distillation Sampling (SDS) has exhibited remarkable performance in\nconditional 3D content generation. However, a comprehensive understanding of\nthe SDS formulation is still lacking, hindering the development of 3D\ngeneration. In this work, we present an interpretation of SDS as a combination\nof three functional components: mode-disengaging, mode-seeking and\nvariance-reducing terms, and analyze the properties of each. We show that\nproblems such as over-smoothness and color-saturation result from the intrinsic\ndeficiency of the supervision terms and reveal that the variance-reducing term\nintroduced by SDS is sub-optimal. Additionally, we shed light on the adoption\nof large Classifier-Free Guidance (CFG) scale for 3D generation. Based on the\nanalysis, we propose a simple yet effective approach named Stable Score\nDistillation (SSD) which strategically orchestrates each term for high-quality\n3D generation. Extensive experiments validate the efficacy of our approach,\ndemonstrating its ability to generate high-fidelity 3D content without\nsuccumbing to issues such as over-smoothness and over-saturation, even under\nlow CFG conditions with the most challenging NeRF representation.\n","authors":["Boshi Tang","Jianan Wang","Zhiyong Wu","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.09305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09299v1","updated":"2023-12-14T19:08:56Z","published":"2023-12-14T19:08:56Z","title":"Weight subcloning: direct initialization of transformers using larger\n  pretrained ones","summary":"  Training large transformer models from scratch for a target task requires\nlots of data and is computationally demanding. The usual practice of transfer\nlearning overcomes this challenge by initializing the model with weights of a\npretrained model of the same size and specification to increase the convergence\nand training speed. However, what if no pretrained model of the required size\nis available? In this paper, we introduce a simple yet effective technique to\ntransfer the knowledge of a pretrained model to smaller variants. Our approach\ncalled weight subcloning expedites the training of scaled-down transformers by\ninitializing their weights from larger pretrained models.\n  Weight subcloning involves an operation on the pretrained model to obtain the\nequivalent initialized scaled-down model. It consists of two key steps: first,\nwe introduce neuron importance ranking to decrease the embedding dimension per\nlayer in the pretrained model. Then, we remove blocks from the transformer\nmodel to match the number of layers in the scaled-down network. The result is a\nnetwork ready to undergo training, which gains significant improvements in\ntraining speed compared to random initialization. For instance, we achieve 4x\nfaster training for vision transformers in image classification and language\nmodels designed for next token prediction.\n","authors":["Mohammad Samragh","Mehrdad Farajtabar","Sachin Mehta","Raviteja Vemulapalli","Fartash Faghri","Devang Naik","Oncel Tuzel","Mohammad Rastegari"],"pdf_url":"https://arxiv.org/pdf/2312.09299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09256v1","updated":"2023-12-14T18:59:59Z","published":"2023-12-14T18:59:59Z","title":"LIME: Localized Image Editing via Attention Regularization in Diffusion\n  Models","summary":"  Diffusion models (DMs) have gained prominence due to their ability to\ngenerate high-quality, varied images, with recent advancements in text-to-image\ngeneration. The research focus is now shifting towards the controllability of\nDMs. A significant challenge within this domain is localized editing, where\nspecific areas of an image are modified without affecting the rest of the\ncontent. This paper introduces LIME for localized image editing in diffusion\nmodels that do not require user-specified regions of interest (RoI) or\nadditional text input. Our method employs features from pre-trained methods and\na simple clustering technique to obtain precise semantic segmentation maps.\nThen, by leveraging cross-attention maps, it refines these segments for\nlocalized edits. Finally, we propose a novel cross-attention regularization\ntechnique that penalizes unrelated cross-attention scores in the RoI during the\ndenoising steps, ensuring localized edits. Our approach, without re-training\nand fine-tuning, consistently improves the performance of existing methods in\nvarious editing benchmarks.\n","authors":["Enis Simsar","Alessio Tonioni","Yongqin Xian","Thomas Hofmann","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2312.09256v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09254v1","updated":"2023-12-14T18:59:58Z","published":"2023-12-14T18:59:58Z","title":"Revisiting Depth Completion from a Stereo Matching Perspective for\n  Cross-domain Generalization","summary":"  This paper proposes a new framework for depth completion robust against\ndomain-shifting issues. It exploits the generalization capability of modern\nstereo networks to face depth completion, by processing fictitious stereo pairs\nobtained through a virtual pattern projection paradigm. Any stereo network or\ntraditional stereo matcher can be seamlessly plugged into our framework,\nallowing for the deployment of a virtual stereo setup that is future-proof\nagainst advancement in the stereo field. Exhaustive experiments on cross-domain\ngeneralization support our claims. Hence, we argue that our framework can help\ndepth completion to reach new deployment scenarios.\n","authors":["Luca Bartolomei","Matteo Poggi","Andrea Conti","Fabio Tosi","Stefano Mattoccia"],"pdf_url":"https://arxiv.org/pdf/2312.09254v1.pdf","comment":"3DV 2024. Code: https://github.com/bartn8/vppdc - Project page:\n  https://vppdc.github.io/"},{"id":"http://arxiv.org/abs/2312.09251v1","updated":"2023-12-14T18:59:43Z","published":"2023-12-14T18:59:43Z","title":"VL-GPT: A Generative Pre-trained Transformer for Vision and Language\n  Understanding and Generation","summary":"  In this work, we introduce Vision-Language Generative Pre-trained Transformer\n(VL-GPT), a transformer model proficient at concurrently perceiving and\ngenerating visual and linguistic data. VL-GPT achieves a unified pre-training\napproach for both image and text modalities by employing a straightforward\nauto-regressive objective, thereby enabling the model to process image and text\nas seamlessly as a language model processes text. To accomplish this, we\ninitially propose a novel image tokenizer-detokenizer framework for visual\ndata, specifically designed to transform raw images into a sequence of\ncontinuous embeddings and reconstruct them accordingly. In combination with the\nexisting text tokenizer and detokenizer, this framework allows for the encoding\nof interleaved image-text data into a multimodal sequence, which can\nsubsequently be fed into the transformer model. Consequently, VL-GPT can\nperform large-scale pre-training on multimodal corpora utilizing a unified\nauto-regressive objective (i.e., next-token prediction). Upon completion of\npre-training, VL-GPT exhibits remarkable zero-shot and few-shot performance\nacross a diverse range of vision and language understanding and generation\ntasks, including image captioning, visual question answering, text-to-image\ngeneration, and more. Additionally, the pre-trained model retrains in-context\nlearning capabilities when provided with multimodal prompts. We further conduct\ninstruction tuning on our VL-GPT, highlighting its exceptional potential for\nmultimodal assistance. The source code and model weights shall be released.\n","authors":["Jinguo Zhu","Xiaohan Ding","Yixiao Ge","Yuying Ge","Sijie Zhao","Hengshuang Zhao","Xiaohua Wang","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2312.09251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09252v1","updated":"2023-12-14T18:59:43Z","published":"2023-12-14T18:59:43Z","title":"FineControlNet: Fine-level Text Control for Image Generation with\n  Spatially Aligned Text Control Injection","summary":"  Recently introduced ControlNet has the ability to steer the text-driven image\ngeneration process with geometric input such as human 2D pose, or edge\nfeatures. While ControlNet provides control over the geometric form of the\ninstances in the generated image, it lacks the capability to dictate the visual\nappearance of each instance. We present FineControlNet to provide fine control\nover each instance's appearance while maintaining the precise pose control\ncapability. Specifically, we develop and demonstrate FineControlNet with\ngeometric control via human pose images and appearance control via\ninstance-level text prompts. The spatial alignment of instance-specific text\nprompts and 2D poses in latent space enables the fine control capabilities of\nFineControlNet. We evaluate the performance of FineControlNet with rigorous\ncomparison against state-of-the-art pose-conditioned text-to-image diffusion\nmodels. FineControlNet achieves superior performance in generating images that\nfollow the user-provided instance-specific text prompts and poses compared with\nexisting methods. Project webpage:\nhttps://samsunglabs.github.io/FineControlNet-project-page\n","authors":["Hongsuk Choi","Isaac Kasahara","Selim Engin","Moritz Graule","Nikhil Chavan-Dafle","Volkan Isler"],"pdf_url":"https://arxiv.org/pdf/2312.09252v1.pdf","comment":"Hongsuk Choi and Isaac Kasahara have eqaul contributions. 19 pages,\n  15 figures, 3 tables"},{"id":"http://arxiv.org/abs/2312.09250v1","updated":"2023-12-14T18:59:36Z","published":"2023-12-14T18:59:36Z","title":"Single Mesh Diffusion Models with Field Latents for Texture Generation","summary":"  We introduce a framework for intrinsic latent diffusion models operating\ndirectly on the surfaces of 3D shapes, with the goal of synthesizing\nhigh-quality textures. Our approach is underpinned by two contributions: field\nlatents, a latent representation encoding textures as discrete vector fields on\nthe mesh vertices, and field latent diffusion models, which learn to denoise a\ndiffusion process in the learned latent space on the surface. We consider a\nsingle-textured-mesh paradigm, where our models are trained to generate\nvariations of a given texture on a mesh. We show the synthesized textures are\nof superior fidelity compared those from existing single-textured-mesh\ngenerative models. Our models can also be adapted for user-controlled editing\ntasks such as inpainting and label-guided generation. The efficacy of our\napproach is due in part to the equivariance of our proposed framework under\nisometries, allowing our models to seamlessly reproduce details across locally\nsimilar regions and opening the door to a notion of generative texture\ntransfer.\n","authors":["Thomas W. Mitchel","Carlos Esteves","Ameesh Makadia"],"pdf_url":"https://arxiv.org/pdf/2312.09250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09249v1","updated":"2023-12-14T18:59:32Z","published":"2023-12-14T18:59:32Z","title":"ZeroRF: Fast Sparse View 360° Reconstruction with Zero Pretraining","summary":"  We present ZeroRF, a novel per-scene optimization method addressing the\nchallenge of sparse view 360{\\deg} reconstruction in neural field\nrepresentations. Current breakthroughs like Neural Radiance Fields (NeRF) have\ndemonstrated high-fidelity image synthesis but struggle with sparse input\nviews. Existing methods, such as Generalizable NeRFs and per-scene optimization\napproaches, face limitations in data dependency, computational cost, and\ngeneralization across diverse scenarios. To overcome these challenges, we\npropose ZeroRF, whose key idea is to integrate a tailored Deep Image Prior into\na factorized NeRF representation. Unlike traditional methods, ZeroRF\nparametrizes feature grids with a neural network generator, enabling efficient\nsparse view 360{\\deg} reconstruction without any pretraining or additional\nregularization. Extensive experiments showcase ZeroRF's versatility and\nsuperiority in terms of both quality and speed, achieving state-of-the-art\nresults on benchmark datasets. ZeroRF's significance extends to applications in\n3D content generation and editing. Project page:\nhttps://sarahweiii.github.io/zerorf/\n","authors":["Ruoxi Shi","Xinyue Wei","Cheng Wang","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2312.09249v1.pdf","comment":"Project page: https://sarahweiii.github.io/zerorf/"},{"id":"http://arxiv.org/abs/2312.09246v1","updated":"2023-12-14T18:59:06Z","published":"2023-12-14T18:59:06Z","title":"SHAP-EDITOR: Instruction-guided Latent 3D Editing in Seconds","summary":"  We propose a novel feed-forward 3D editing framework called Shap-Editor.\nPrior research on editing 3D objects primarily concentrated on editing\nindividual objects by leveraging off-the-shelf 2D image editing networks. This\nis achieved via a process called distillation, which transfers knowledge from\nthe 2D network to 3D assets. Distillation necessitates at least tens of minutes\nper asset to attain satisfactory editing results, and is thus not very\npractical. In contrast, we ask whether 3D editing can be carried out directly\nby a feed-forward network, eschewing test-time optimisation. In particular, we\nhypothesise that editing can be greatly simplified by first encoding 3D objects\nin a suitable latent space. We validate this hypothesis by building upon the\nlatent space of Shap-E. We demonstrate that direct 3D editing in this space is\npossible and efficient by building a feed-forward editor network that only\nrequires approximately one second per edit. Our experiments show that\nShap-Editor generalises well to both in-distribution and out-of-distribution 3D\nassets with different prompts, exhibiting comparable performance with methods\nthat carry out test-time optimisation for each edited instance.\n","authors":["Minghao Chen","Junyu Xie","Iro Laina","Andrea Vedaldi"],"pdf_url":"https://arxiv.org/pdf/2312.09246v1.pdf","comment":"Project Page: https://silent-chen.github.io/Shap-Editor/"},{"id":"http://arxiv.org/abs/2312.09245v1","updated":"2023-12-14T18:59:05Z","published":"2023-12-14T18:59:05Z","title":"DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral\n  Planning States for Autonomous Driving","summary":"  Large language models (LLMs) have opened up new possibilities for intelligent\nagents, endowing them with human-like thinking and cognitive abilities. In this\nwork, we delve into the potential of large language models (LLMs) in autonomous\ndriving (AD). We introduce DriveMLM, an LLM-based AD framework that can perform\nclose-loop autonomous driving in realistic simulators. To this end, (1) we\nbridge the gap between the language decisions and the vehicle control commands\nby standardizing the decision states according to the off-the-shelf motion\nplanning module. (2) We employ a multi-modal LLM (MLLM) to model the behavior\nplanning module of a module AD system, which uses driving rules, user commands,\nand inputs from various sensors (e.g., camera, lidar) as input and makes\ndriving decisions and provide explanations; This model can plug-and-play in\nexisting AD systems such as Apollo for close-loop driving. (3) We design an\neffective data engine to collect a dataset that includes decision state and\ncorresponding explanation annotation for model training and evaluation. We\nconduct extensive experiments and show that our model achieves 76.1 driving\nscore on the CARLA Town05 Long, and surpasses the Apollo baseline by 4.7 points\nunder the same settings, demonstrating the effectiveness of our model. We hope\nthis work can serve as a baseline for autonomous driving with LLMs. Code and\nmodels shall be released at https://github.com/OpenGVLab/DriveMLM.\n","authors":["Wenhai Wang","Jiangwei Xie","ChuanYang Hu","Haoming Zou","Jianan Fan","Wenwen Tong","Yang Wen","Silei Wu","Hanming Deng","Zhiqi Li","Hao Tian","Lewei Lu","Xizhou Zhu","Xiaogang Wang","Yu Qiao","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2312.09245v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2312.09243v1","updated":"2023-12-14T18:58:52Z","published":"2023-12-14T18:58:52Z","title":"OccNeRF: Self-Supervised Multi-Camera Occupancy Prediction with Neural\n  Radiance Fields","summary":"  As a fundamental task of vision-based perception, 3D occupancy prediction\nreconstructs 3D structures of surrounding environments. It provides detailed\ninformation for autonomous driving planning and navigation. However, most\nexisting methods heavily rely on the LiDAR point clouds to generate occupancy\nground truth, which is not available in the vision-based system. In this paper,\nwe propose an OccNeRF method for self-supervised multi-camera occupancy\nprediction. Different from bounded 3D occupancy labels, we need to consider\nunbounded scenes with raw image supervision. To solve the issue, we\nparameterize the reconstructed occupancy fields and reorganize the sampling\nstrategy. The neural rendering is adopted to convert occupancy fields to\nmulti-camera depth maps, supervised by multi-frame photometric consistency.\nMoreover, for semantic occupancy prediction, we design several strategies to\npolish the prompts and filter the outputs of a pretrained open-vocabulary 2D\nsegmentation model. Extensive experiments for both self-supervised depth\nestimation and semantic occupancy prediction tasks on nuScenes dataset\ndemonstrate the effectiveness of our method.\n","authors":["Chubin Zhang","Juncheng Yan","Yi Wei","Jiaxin Li","Li Liu","Yansong Tang","Yueqi Duan","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2312.09243v1.pdf","comment":"Code: https://github.com/LinShan-Bin/OccNeRF"},{"id":"http://arxiv.org/abs/2312.09242v1","updated":"2023-12-14T18:58:47Z","published":"2023-12-14T18:58:47Z","title":"Text2Immersion: Generative Immersive Scene with 3D Gaussians","summary":"  We introduce Text2Immersion, an elegant method for producing high-quality 3D\nimmersive scenes from text prompts. Our proposed pipeline initiates by\nprogressively generating a Gaussian cloud using pre-trained 2D diffusion and\ndepth estimation models. This is followed by a refining stage on the Gaussian\ncloud, interpolating and refining it to enhance the details of the generated\nscene. Distinct from prevalent methods that focus on single object or indoor\nscenes, or employ zoom-out trajectories, our approach generates diverse scenes\nwith various objects, even extending to the creation of imaginary scenes.\nConsequently, Text2Immersion can have wide-ranging implications for various\napplications such as virtual reality, game development, and automated content\ncreation. Extensive evaluations demonstrate that our system surpasses other\nmethods in rendering quality and diversity, further progressing towards\ntext-driven 3D scene generation. We will make the source code publicly\naccessible at the project page.\n","authors":["Hao Ouyang","Kathryn Heal","Stephen Lombardi","Tiancheng Sun"],"pdf_url":"https://arxiv.org/pdf/2312.09242v1.pdf","comment":"Project page: https://ken-ouyang.github.io/text2immersion/index.html"},{"id":"http://arxiv.org/abs/2312.09238v1","updated":"2023-12-14T18:58:12Z","published":"2023-12-14T18:58:12Z","title":"Auto MC-Reward: Automated Dense Reward Design with Large Language Models\n  for Minecraft","summary":"  Traditional reinforcement-learning-based agents rely on sparse rewards that\noften only use binary values to indicate task completion or failure. The\nchallenge in exploration efficiency makes it difficult to effectively learn\ncomplex tasks in Minecraft. To address this, this paper introduces an advanced\nlearning system, named Auto MC-Reward, that leverages Large Language Models\n(LLMs) to automatically design dense reward functions, thereby enhancing the\nlearning efficiency. Auto MC-Reward consists of three important components:\nReward Designer, Reward Critic, and Trajectory Analyzer. Given the environment\ninformation and task descriptions, the Reward Designer first design the reward\nfunction by coding an executable Python function with predefined observation\ninputs. Then, our Reward Critic will be responsible for verifying the code,\nchecking whether the code is self-consistent and free of syntax and semantic\nerrors. Further, the Trajectory Analyzer summarizes possible failure causes and\nprovides refinement suggestions according to collected trajectories. In the\nnext round, Reward Designer will take further refine and iterate the dense\nreward function based on feedback. Experiments demonstrate a significant\nimprovement in the success rate and learning efficiency of our agents in\ncomplex tasks in Minecraft, such as obtaining diamond with the efficient\nability to avoid lava, and efficiently explore trees and animals that are\nsparse on the plains biome.\n","authors":["Hao Li","Xue Yang","Zhaokai Wang","Xizhou Zhu","Jie Zhou","Yu Qiao","Xiaogang Wang","Hongsheng Li","Lewei Lu","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2312.09238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09237v1","updated":"2023-12-14T18:57:58Z","published":"2023-12-14T18:57:58Z","title":"Pixel Aligned Language Models","summary":"  Large language models have achieved great success in recent years, so as\ntheir variants in vision. Existing vision-language models can describe images\nin natural languages, answer visual-related questions, or perform complex\nreasoning about the image. However, it is yet unclear how localization tasks,\nsuch as word grounding or referring localization, can be performed using large\nlanguage models. In this work, we aim to develop a vision-language model that\ncan take locations, for example, a set of points or boxes, as either inputs or\noutputs. When taking locations as inputs, the model performs\nlocation-conditioned captioning, which generates captions for the indicated\nobject or region. When generating locations as outputs, our model regresses\npixel coordinates for each output word generated by the language model, and\nthus performs dense word grounding. Our model is pre-trained on the Localized\nNarrative dataset, which contains pixel-word-aligned captioning from human\nattention. We show our model can be applied to various location-aware\nvision-language tasks, including referring localization, location-conditioned\ncaptioning, and dense object captioning, archiving state-of-the-art performance\non RefCOCO and Visual Genome. Project page: https://jerryxu.net/PixelLLM .\n","authors":["Jiarui Xu","Xingyi Zhou","Shen Yan","Xiuye Gu","Anurag Arnab","Chen Sun","Xiaolong Wang","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2312.09237v1.pdf","comment":"Project page: https://jerryxu.net/PixelLLM"},{"id":"http://arxiv.org/abs/2312.09232v1","updated":"2023-12-14T18:56:54Z","published":"2023-12-14T18:56:54Z","title":"DVQI: A Multi-task, Hardware-integrated Artificial Intelligence System\n  for Automated Visual Inspection in Electronics Manufacturing","summary":"  As electronics manufacturers continue to face pressure to increase production\nefficiency amid difficulties with supply chains and labour shortages, many\nprinted circuit board assembly (PCBA) manufacturers have begun to invest in\nautomation and technological innovations to remain competitive. One such method\nis to leverage artificial intelligence (AI) to greatly augment existing\nmanufacturing processes. In this paper, we present the DarwinAI Visual Quality\nInspection (DVQI) system, a hardware-integration artificial intelligence system\nfor the automated inspection of printed circuit board assembly defects in an\nelectronics manufacturing environment. The DVQI system enables multi-task\ninspection via minimal programming and setup for manufacturing engineers while\nimproving cycle time relative to manual inspection. We also present a case\nstudy of the deployed DVQI system's performance and impact for a top\nelectronics manufacturer.\n","authors":["Audrey Chung","Francis Li","Jeremy Ward","Andrew Hryniowski","Alexander Wong"],"pdf_url":"https://arxiv.org/pdf/2312.09232v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2312.09231v1","updated":"2023-12-14T18:56:07Z","published":"2023-12-14T18:56:07Z","title":"Reliability in Semantic Segmentation: Can We Use Synthetic Data?","summary":"  Assessing the reliability of perception models to covariate shifts and\nout-of-distribution (OOD) detection is crucial for safety-critical applications\nsuch as autonomous vehicles. By nature of the task, however, the relevant data\nis difficult to collect and annotate. In this paper, we challenge cutting-edge\ngenerative models to automatically synthesize data for assessing reliability in\nsemantic segmentation. By fine-tuning Stable Diffusion, we perform zero-shot\ngeneration of synthetic data in OOD domains or inpainted with OOD objects.\nSynthetic data is employed to provide an initial assessment of pretrained\nsegmenters, thereby offering insights into their performance when confronted\nwith real edge cases. Through extensive experiments, we demonstrate a high\ncorrelation between the performance on synthetic data and the performance on\nreal OOD data, showing the validity approach. Furthermore, we illustrate how\nsynthetic data can be utilized to enhance the calibration and OOD detection\ncapabilities of segmenters.\n","authors":["Thibaut Loiseau","Tuan-Hung Vu","Mickael Chen","Patrick Pérez","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2312.09231v1.pdf","comment":"Project Page: https://valeoai.github.io/blog/publications/GenVal"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2312.08221v2","updated":"2023-12-14T09:38:28Z","published":"2023-12-13T15:42:14Z","title":"Curriculum-Enhanced Residual Soft An-Isotropic Normalization for\n  Over-smoothness in Deep GNNs","summary":"  Despite Graph neural networks' significant performance gain over many classic\ntechniques in various graph-related downstream tasks, their successes are\nrestricted in shallow models due to over-smoothness and the difficulties of\noptimizations among many other issues. In this paper, to alleviate the\nover-smoothing issue, we propose a soft graph normalization method to preserve\nthe diversities of node embeddings and prevent indiscrimination due to possible\nover-closeness. Combined with residual connections, we analyze the reason why\nthe method can effectively capture the knowledge in both input graph structures\nand node features even with deep networks. Additionally, inspired by Curriculum\nLearning that learns easy examples before the hard ones, we propose a novel\nlabel-smoothing-based learning framework to enhance the optimization of deep\nGNNs, which iteratively smooths labels in an auxiliary graph and constructs\nmany gradual non-smooth tasks for extracting increasingly complex knowledge and\ngradually discriminating nodes from coarse to fine. The method arguably reduces\nthe risk of overfitting and generalizes better results. Finally, extensive\nexperiments are carried out to demonstrate the effectiveness and potential of\nthe proposed model and learning framework through comparison with twelve\nexisting baselines including the state-of-the-art methods on twelve real-world\nnode classification benchmarks.\n","authors":["Jin Li","Qirong Zhang","Shuling Xu","Xinlong Chen","Longkun Guo","Yang-Geng Fu"],"pdf_url":"https://arxiv.org/pdf/2312.08221v2.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2307.12971v3","updated":"2023-12-14T17:30:47Z","published":"2023-07-24T17:49:05Z","title":"Big Data - Supply Chain Management Framework for Forecasting: Data\n  Preprocessing and Machine Learning Techniques","summary":"  This article intends to systematically identify and comparatively analyze\nstate-of-the-art supply chain (SC) forecasting strategies and technologies. A\nnovel framework has been proposed incorporating Big Data Analytics in SC\nManagement (problem identification, data sources, exploratory data analysis,\nmachine-learning model training, hyperparameter tuning, performance evaluation,\nand optimization), forecasting effects on human-workforce, inventory, and\noverall SC. Initially, the need to collect data according to SC strategy and\nhow to collect them has been discussed. The article discusses the need for\ndifferent types of forecasting according to the period or SC objective. The SC\nKPIs and the error-measurement systems have been recommended to optimize the\ntop-performing model. The adverse effects of phantom inventory on forecasting\nand the dependence of managerial decisions on the SC KPIs for determining model\nperformance parameters and improving operations management, transparency, and\nplanning efficiency have been illustrated. The cyclic connection within the\nframework introduces preprocessing optimization based on the post-process KPIs,\noptimizing the overall control process (inventory management, workforce\ndetermination, cost, production and capacity planning). The contribution of\nthis research lies in the standard SC process framework proposal, recommended\nforecasting data analysis, forecasting effects on SC performance, machine\nlearning algorithms optimization followed, and in shedding light on future\nresearch.\n","authors":["Md Abrar Jahin","Md Sakib Hossain Shovon","Jungpil Shin","Istiyaque Ahmed Ridoy","Yoichi Tomioka","M. F. Mridha"],"pdf_url":"https://arxiv.org/pdf/2307.12971v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07987v2","updated":"2023-12-14T06:35:33Z","published":"2023-12-13T09:00:21Z","title":"SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention","summary":"  The costly self-attention layers in modern Transformers require memory and\ncompute quadratic in sequence length. Existing approximation methods usually\nunderperform and fail to obtain significant speedups in practice. Here we\npresent SwitchHead - a novel method that reduces both compute and memory\nrequirements and achieves wall-clock speedup, while matching the language\nmodeling performance of baseline Transformers with the same parameter budget.\nSwitchHead uses Mixture-of-Experts (MoE) layers for the value and output\nprojections and requires 4 to 8 times fewer attention matrices than standard\nTransformers. Our novel attention can also be combined with MoE MLP layers,\nresulting in an efficient fully-MoE \"SwitchAll\" Transformer model. Our code is\npublic.\n","authors":["Róbert Csordás","Piotr Piękos","Kazuki Irie","Jürgen Schmidhuber"],"pdf_url":"https://arxiv.org/pdf/2312.07987v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07953v2","updated":"2023-12-14T06:01:47Z","published":"2023-12-13T08:00:26Z","title":"Enhancing Robotic Navigation: An Evaluation of Single and\n  Multi-Objective Reinforcement Learning Strategies","summary":"  This study presents a comparative analysis between single-objective and\nmulti-objective reinforcement learning methods for training a robot to navigate\neffectively to an end goal while efficiently avoiding obstacles. Traditional\nreinforcement learning techniques, namely Deep Q-Network (DQN), Deep\nDeterministic Policy Gradient (DDPG), and Twin Delayed DDPG (TD3), have been\nevaluated using the Gazebo simulation framework in a variety of environments\nwith parameters such as random goal and robot starting locations. These methods\nprovide a numerical reward to the robot, offering an indication of action\nquality in relation to the goal. However, their limitations become apparent in\ncomplex settings where multiple, potentially conflicting, objectives are\npresent. To address these limitations, we propose an approach employing\nMulti-Objective Reinforcement Learning (MORL). By modifying the reward function\nto return a vector of rewards, each pertaining to a distinct objective, the\nrobot learns a policy that effectively balances the different goals, aiming to\nachieve a Pareto optimal solution. This comparative study highlights the\npotential for MORL in complex, dynamic robotic navigation tasks, setting the\nstage for future investigations into more adaptable and robust robotic\nbehaviors.\n","authors":["Vicki Young","Jumman Hossain","Nirmalya Roy"],"pdf_url":"https://arxiv.org/pdf/2312.07953v2.pdf","comment":"REU program project (work in progress)"},{"id":"http://arxiv.org/abs/2312.07851v2","updated":"2023-12-14T02:17:47Z","published":"2023-12-13T02:39:10Z","title":"Noise in the reverse process improves the approximation capabilities of\n  diffusion models","summary":"  In Score based Generative Modeling (SGMs), the state-of-the-art in generative\nmodeling, stochastic reverse processes are known to perform better than their\ndeterministic counterparts. This paper delves into the heart of this\nphenomenon, comparing neural ordinary differential equations (ODEs) and neural\nstochastic differential equations (SDEs) as reverse processes. We use a control\ntheoretic perspective by posing the approximation of the reverse process as a\ntrajectory tracking problem. We analyze the ability of neural SDEs to\napproximate trajectories of the Fokker-Planck equation, revealing the\nadvantages of stochasticity. First, neural SDEs exhibit a powerful regularizing\neffect, enabling $L^2$ norm trajectory approximation surpassing the Wasserstein\nmetric approximation achieved by neural ODEs under similar conditions, even\nwhen the reference vector field or score function is not Lipschitz. Applying\nthis result, we establish the class of distributions that can be sampled using\nscore matching in SGMs, relaxing the Lipschitz requirement on the gradient of\nthe data distribution in existing literature. Second, we show that this\napproximation property is preserved when network width is limited to the input\ndimension of the network. In this limited width case, the weights act as\ncontrol inputs, framing our analysis as a controllability problem for neural\nSDEs in probability density space. This sheds light on how noise helps to steer\nthe system towards the desired solution and illuminates the empirical success\nof stochasticity in generative modeling.\n","authors":["Karthik Elamvazhuthi","Samet Oymak","Fabio Pasqualetti"],"pdf_url":"https://arxiv.org/pdf/2312.07851v2.pdf","comment":"Extended preprint for submission to Learning for Dynamics & Control\n  Conference"}],"Multimedia":[{"id":"http://arxiv.org/abs/2308.09300v4","updated":"2023-12-14T00:15:14Z","published":"2023-08-18T04:49:38Z","title":"V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by\n  Connecting Foundation Models","summary":"  Building artificial intelligence (AI) systems on top of a set of foundation\nmodels (FMs) is becoming a new paradigm in AI research. Their representative\nand generative abilities learnt from vast amounts of data can be easily adapted\nand transferred to a wide range of downstream tasks without extra training from\nscratch. However, leveraging FMs in cross-modal generation remains\nunder-researched when audio modality is involved. On the other hand,\nautomatically generating semantically-relevant sound from visual input is an\nimportant problem in cross-modal generation studies. To solve this\nvision-to-audio (V2A) generation problem, existing methods tend to design and\nbuild complex systems from scratch using modestly sized datasets. In this\npaper, we propose a lightweight solution to this problem by leveraging\nfoundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate\nthe domain gap between the latent space of the visual CLIP and the auditory\nCLAP models. Then we propose a simple yet effective mapper mechanism\n(V2A-Mapper) to bridge the domain gap by translating the visual input between\nCLIP and CLAP spaces. Conditioned on the translated CLAP embedding, pretrained\naudio generative FM AudioLDM is adopted to produce high-fidelity and\nvisually-aligned sound. Compared to previous approaches, our method only\nrequires a quick training of the V2A-Mapper. We further analyze and conduct\nextensive experiments on the choice of the V2A-Mapper and show that a\ngenerative mapper is better at fidelity and variability (FD) while a regression\nmapper is slightly better at relevance (CS). Both objective and subjective\nevaluation on two V2A datasets demonstrate the superiority of our proposed\nmethod compared to current state-of-the-art approaches - trained with 86% fewer\nparameters but achieving 53% and 19% improvement in FD and CS, respectively.\n","authors":["Heng Wang","Jianbo Ma","Santiago Pascual","Richard Cartwright","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2308.09300v4.pdf","comment":"AAAI 2024. Demo page: https://v2a-mapper.github.io/"},{"id":"http://arxiv.org/abs/2312.09334v1","updated":"2023-12-14T20:48:26Z","published":"2023-12-14T20:48:26Z","title":"ArchiGuesser -- AI Art Architecture Educational Game","summary":"  The use of generative AI in education is a controversial topic. Current\ntechnology offers the potential to create educational content from text,\nspeech, to images based on simple input prompts. This can enhance productivity\nby summarizing knowledge and improving communication, quickly adjusting to\ndifferent types of learners. Moreover, generative AI holds the promise of\nmaking the learning itself more fun, by responding to user inputs and\ndynamically generating high-quality creative material. In this paper we present\nthe multisensory educational game ArchiGuesser that combines various AI\ntechnologies from large language models, image generation, to computer vision\nto serve a single purpose: Teaching students in a playful way the diversity of\nour architectural history and how generative AI works.\n","authors":["Joern Ploennigs","Markus Berger","Eva Carnein"],"pdf_url":"https://arxiv.org/pdf/2312.09334v1.pdf","comment":"NeurIPS, Creative AI track"},{"id":"http://arxiv.org/abs/2312.08984v1","updated":"2023-12-14T14:29:53Z","published":"2023-12-14T14:29:53Z","title":"CL2CM: Improving Cross-Lingual Cross-Modal Retrieval via Cross-Lingual\n  Knowledge Transfer","summary":"  Cross-lingual cross-modal retrieval has garnered increasing attention\nrecently, which aims to achieve the alignment between vision and target\nlanguage (V-T) without using any annotated V-T data pairs. Current methods\nemploy machine translation (MT) to construct pseudo-parallel data pairs, which\nare then used to learn a multi-lingual and multi-modal embedding space that\naligns visual and target-language representations. However, the large\nheterogeneous gap between vision and text, along with the noise present in\ntarget language translations, poses significant challenges in effectively\naligning their representations. To address these challenges, we propose a\ngeneral framework, Cross-Lingual to Cross-Modal (CL2CM), which improves the\nalignment between vision and target language using cross-lingual transfer. This\napproach allows us to fully leverage the merits of multi-lingual pre-trained\nmodels (e.g., mBERT) and the benefits of the same modality structure, i.e.,\nsmaller gap, to provide reliable and comprehensive semantic correspondence\n(knowledge) for the cross-modal network. We evaluate our proposed approach on\ntwo multilingual image-text datasets, Multi30K and MSCOCO, and one video-text\ndataset, VATEX. The results clearly demonstrate the effectiveness of our\nproposed method and its high potential for large-scale retrieval.\n","authors":["Yabing Wang","Fan Wang","Jianfeng Dong","Hao Luo"],"pdf_url":"https://arxiv.org/pdf/2312.08984v1.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2310.05804v2","updated":"2023-12-14T13:07:45Z","published":"2023-10-09T15:43:07Z","title":"Learning Language-guided Adaptive Hyper-modality Representation for\n  Multimodal Sentiment Analysis","summary":"  Though Multimodal Sentiment Analysis (MSA) proves effective by utilizing rich\ninformation from multiple sources (e.g., language, video, and audio), the\npotential sentiment-irrelevant and conflicting information across modalities\nmay hinder the performance from being further improved. To alleviate this, we\npresent Adaptive Language-guided Multimodal Transformer (ALMT), which\nincorporates an Adaptive Hyper-modality Learning (AHL) module to learn an\nirrelevance/conflict-suppressing representation from visual and audio features\nunder the guidance of language features at different scales. With the obtained\nhyper-modality representation, the model can obtain a complementary and joint\nrepresentation through multimodal fusion for effective MSA. In practice, ALMT\nachieves state-of-the-art performance on several popular datasets (e.g., MOSI,\nMOSEI and CH-SIMS) and an abundance of ablation demonstrates the validity and\nnecessity of our irrelevance/conflict suppression mechanism.\n","authors":["Haoyu Zhang","Yu Wang","Guanghao Yin","Kejun Liu","Yuanyuan Liu","Tianshu Yu"],"pdf_url":"https://arxiv.org/pdf/2310.05804v2.pdf","comment":"Published in EMNLP 2023"},{"id":"http://arxiv.org/abs/2312.08720v1","updated":"2023-12-14T08:05:09Z","published":"2023-12-14T08:05:09Z","title":"Panel Transitions for Genre Analysis in Visual Narratives","summary":"  Understanding how humans communicate and perceive narratives is important for\nmedia technology research and development. This is particularly important in\ncurrent times when there are tools and algorithms that are easily available for\namateur users to create high-quality content. Narrative media develops over\ntime a set of recognizable patterns of features across similar artifacts. Genre\nis one such grouping of artifacts for narrative media with similar patterns,\ntropes, and story structures. While much work has been done on genre-based\nclassifications in text and video, we present a novel approach to do a\nmulti-modal analysis of genre based on comics and manga-style visual\nnarratives. We present a systematic feature analysis of an annotated dataset\nthat includes a variety of western and eastern visual books with annotations\nfor high-level narrative patterns. We then present a detailed analysis of the\ncontributions of high-level features to genre classification for this medium.\nWe highlight some of the limitations and challenges of our existing\ncomputational approaches in modeling subjective labels. Our contributions to\nthe community are: a dataset of annotated manga books, a multi-modal analysis\nof visual panels and text in a constrained and popular medium through\nhigh-level features, and a systematic process for incorporating subjective\nnarrative patterns in computational models.\n","authors":["Yi-Chun Chen","Arnav Jhala"],"pdf_url":"https://arxiv.org/pdf/2312.08720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08695v1","updated":"2023-12-14T07:26:18Z","published":"2023-12-14T07:26:18Z","title":"CPST: Comprehension-Preserving Style Transfer for Multi-Modal Narratives","summary":"  We investigate the challenges of style transfer in multi-modal visual\nnarratives. Among static visual narratives such as comics and manga, there are\ndistinct visual styles in terms of presentation. They include style features\nacross multiple dimensions, such as panel layout, size, shape, and color. They\ninclude both visual and text media elements. The layout of both text and media\nelements is also significant in terms of narrative communication. The\nsequential transitions between panels are where readers make inferences about\nthe narrative world. These feature differences provide an interesting challenge\nfor style transfer in which there are distinctions between the processing of\nfeatures for each modality. We introduce the notion of comprehension-preserving\nstyle transfer (CPST) in such multi-modal domains. CPST requires not only\ntraditional metrics of style transfer but also metrics of narrative\ncomprehension. To spur further research in this area, we present an annotated\ndataset of comics and manga and an initial set of algorithms that utilize\nseparate style transfer modules for the visual, textual, and layout parameters.\nTo test whether the style transfer preserves narrative semantics, we evaluate\nthis algorithm through visual story cloze tests inspired by work in\ncomputational cognition of narrative systems. Understanding the connection\nbetween style and narrative semantics provides insight for applications ranging\nfrom informational brochure designs to data storytelling.\n","authors":["Yi-Chun Chen","Arnav Jhala"],"pdf_url":"https://arxiv.org/pdf/2312.08695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08600v1","updated":"2023-12-14T01:54:38Z","published":"2023-12-14T01:54:38Z","title":"CartoMark: a benchmark dataset for map pattern recognition and 1 map\n  content retrieval with machine intelligence","summary":"  Maps are fundamental medium to visualize and represent the real word in a\nsimple and 16 philosophical way. The emergence of the 3rd wave information has\nmade a proportion of maps are available to be generated ubiquitously, which\nwould significantly enrich the dimensions and perspectives to understand the\ncharacteristics of the real world. However, a majority of map dataset have\nnever been discovered, acquired and effectively used, and the map data used in\nmany applications might not be completely fitted for the authentic demands of\nthese applications. This challenge is emerged due to the lack of numerous\nwell-labelled benchmark datasets for implementing the deep learning approaches\ninto identifying complicated map content. Thus, we develop a large-scale\nbenchmark dataset that includes well-labelled dataset for map text annotation\nrecognition, map scene classification, map super-resolution reconstruction, and\nmap style transferring. Furthermore, these well-labelled datasets would\nfacilitate the state-of-the-art machine intelligence technologies to conduct\nmap feature detection, map pattern recognition and map content retrieval. We\nhope our efforts would be useful for AI-enhanced cartographical applications.\n","authors":["Xiran Zhou","Yi Wen","Honghao Li","Kaiyuan Li","Zhenfeng Shao","Zhigang Yan","Xiao Xie"],"pdf_url":"https://arxiv.org/pdf/2312.08600v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2312.09312v1","updated":"2023-12-14T19:34:32Z","published":"2023-12-14T19:34:32Z","title":"O Contract, Where Art Thou? Contract Management as a SharePoint Oddity","summary":"  For many legal operations teams, the management of the contracts and\nagreements that their organization are negotiating or have been executed is an\nencompassing and time-consuming task. This has resulted in specialized tools\nfor Contract Lifecycle Management (CLM) have grown steadily in demand over the\nlast decade. Transitioning to such tools can itself be an arduous and costly\nprocess and so a logical step would be to augment existing storage solutions.\nIn this paper, we present the analysis of 26 semi-structured interviews with\nlegal operations professionals about their trials and tribulations with using\nMicrosoft SharePoint for contract management. We find that while there is\npromise, too much of what is needed to be successful requires more technical\nprowess than might be easily available to those empowered to put it in place.\n","authors":["Sasha Vtyurina","Adam Roegiest"],"pdf_url":"https://arxiv.org/pdf/2312.09312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09207v1","updated":"2023-12-14T18:38:02Z","published":"2023-12-14T18:38:02Z","title":"WikiMuTe: A web-sourced dataset of semantic descriptions for music audio","summary":"  Multi-modal deep learning techniques for matching free-form text with music\nhave shown promising results in the field of Music Information Retrieval (MIR).\nPrior work is often based on large proprietary data while publicly available\ndatasets are few and small in size. In this study, we present WikiMuTe, a new\nand open dataset containing rich semantic descriptions of music. The data is\nsourced from Wikipedia's rich catalogue of articles covering musical works.\nUsing a dedicated text-mining pipeline, we extract both long and short-form\ndescriptions covering a wide range of topics related to music content such as\ngenre, style, mood, instrumentation, and tempo. To show the use of this data,\nwe train a model that jointly learns text and audio representations and\nperforms cross-modal retrieval. The model is evaluated on two tasks: tag-based\nmusic retrieval and music auto-tagging. The results show that while our\napproach has state-of-the-art performance on multiple tasks, but still observe\na difference in performance depending on the data used for training.\n","authors":["Benno Weck","Holger Kirchhoff","Peter Grosche","Xavier Serra"],"pdf_url":"https://arxiv.org/pdf/2312.09207v1.pdf","comment":"Submitted to 30th International Conference on MultiMedia Modeling\n  (MMM2024). This preprint has not undergone peer review or any post-submission\n  improvements or corrections"},{"id":"http://arxiv.org/abs/2312.08995v1","updated":"2023-12-14T14:41:37Z","published":"2023-12-14T14:41:37Z","title":"FrameFinder: Explorative Multi-Perspective Framing Extraction from News\n  Headlines","summary":"  Revealing the framing of news articles is an important yet neglected task in\ninformation seeking and retrieval. In the present work, we present FrameFinder,\nan open tool for extracting and analyzing frames in textual data. FrameFinder\nvisually represents the frames of text from three perspectives, i.e., (i) frame\nlabels, (ii) frame dimensions, and (iii) frame structure. By analyzing the\nwell-established gun violence frame corpus, we demonstrate the merits of our\nproposed solution to support social science research and call for subsequent\nintegration into information interactions.\n","authors":["Markus Reiter-Haas","Beate Klösch","Markus Hadler","Elisabeth Lex"],"pdf_url":"https://arxiv.org/pdf/2312.08995v1.pdf","comment":"Accepted for publication at CHIIR'24"},{"id":"http://arxiv.org/abs/2308.03443v3","updated":"2023-12-14T08:52:01Z","published":"2023-08-07T10:00:07Z","title":"Doubly Robust Estimator for Off-Policy Evaluation with Large Action\n  Spaces","summary":"  We study Off-Policy Evaluation (OPE) in contextual bandit settings with large\naction spaces. The benchmark estimators suffer from severe bias and variance\ntradeoffs. Parametric approaches suffer from bias due to difficulty specifying\nthe correct model, whereas ones with importance weight suffer from variance. To\novercome these limitations, Marginalized Inverse Propensity Scoring (MIPS) was\nproposed to mitigate the estimator's variance via embeddings of an action.\nNevertheless, MIPS is unbiased under the no direct effect, which assumes that\nthe action embedding completely mediates the effect of an action on a reward.\nTo overcome the dependency on these unrealistic assumptions, we propose a\nMarginalized Doubly Robust (MDR) estimator. Theoretical analysis shows that the\nproposed estimator is unbiased under weaker assumptions than MIPS while\nreducing the variance against MIPS. The empirical experiment verifies the\nsupremacy of MDR against existing estimators with large action spaces.\n","authors":["Tatsuhiro Shimizu","Laura Forastiere"],"pdf_url":"https://arxiv.org/pdf/2308.03443v3.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2312.08727v1","updated":"2023-12-14T08:15:02Z","published":"2023-12-14T08:15:02Z","title":"Calibration-compatible Listwise Distillation of Privileged Features for\n  CTR Prediction","summary":"  In machine learning systems, privileged features refer to the features that\nare available during offline training but inaccessible for online serving.\nPrevious studies have recognized the importance of privileged features and\nexplored ways to tackle online-offline discrepancies. A typical practice is\nprivileged features distillation (PFD): train a teacher model using all\nfeatures (including privileged ones) and then distill the knowledge from the\nteacher model using a student model (excluding the privileged features), which\nis then employed for online serving. In practice, the pointwise cross-entropy\nloss is often adopted for PFD. However, this loss is insufficient to distill\nthe ranking ability for CTR prediction. First, it does not consider the\nnon-i.i.d. characteristic of the data distribution, i.e., other items on the\nsame page significantly impact the click probability of the candidate item.\nSecond, it fails to consider the relative item order ranked by the teacher\nmodel's predictions, which is essential to distill the ranking ability. To\naddress these issues, we first extend the pointwise-based PFD to the\nlistwise-based PFD. We then define the calibration-compatible property of\ndistillation loss and show that commonly used listwise losses do not satisfy\nthis property when employed as distillation loss, thus compromising the model's\ncalibration ability, which is another important measure for CTR prediction. To\ntackle this dilemma, we propose Calibration-compatible LIstwise Distillation\n(CLID), which employs carefully-designed listwise distillation loss to achieve\nbetter ranking ability than the pointwise-based PFD while preserving the\nmodel's calibration ability. We theoretically prove it is\ncalibration-compatible. Extensive experiments on public datasets and a\nproduction dataset collected from the display advertising system of Alibaba\nfurther demonstrate the effectiveness of CLID.\n","authors":["Xiaoqiang Gui","Yueyao Cheng","Xiang-Rong Sheng","Yunfeng Zhao","Guoxian Yu","Shuguang Han","Yuning Jiang","Jian Xu","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2312.08727v1.pdf","comment":"This paper has been accepted by WSDM'24"},{"id":"http://arxiv.org/abs/2312.08584v1","updated":"2023-12-14T01:11:41Z","published":"2023-12-14T01:11:41Z","title":"Hybrid Content Dynamic Recommendation System Based in Adapted Tags and\n  Applied to Digital Library","summary":"  The technological evolution of the library in the academic environment\nbrought a lot of information and documents that are available to access, but\nthese systems do not always have mechanisms to search in an integrated way the\nrelevant information for the user. To alleviate this problem, we propose a\nrecommendation system that generates the user profile through tags that are\nreshaped over time. To trace the user profile the system uses information from\nyour lending history stored in the library database and it collects their\nopinions (feedback) through a list of recommendations. These data are\nintegrated with the document base of institutional repository.Thus, the\nrecommendation system assists users in identifying relevant items and makes\nsuggestions for content in an integrated environment that contains\ninstitutional repository documents and the university library database. The\nproposed recommendation system uses a hybrid approach being applied in an\nacademic environment with the participation of the users.\n","authors":["Thiago Bellotti Furtado","Ahmed Esmin"],"pdf_url":"https://arxiv.org/pdf/2312.08584v1.pdf","comment":"35 pages, 8 figures"},{"id":"http://arxiv.org/abs/2312.10096v1","updated":"2023-12-14T16:03:49Z","published":"2023-12-14T16:03:49Z","title":"Open Government Data Programs and Information Privacy Concerns: A\n  Literature Review","summary":"  This study presents a narrative review of the literature on privacy concerns\nof Open Government Data (OGD) programs and identifies suggested technical,\nprocedural, and legal remedies. Peer-reviewed articles were identified and\nanalysed from major bibliographic databases, including Web of Science, Digital\nACM Library, IEEE Explore Digital Library and Science Direct. Included articles\nfocus on identifying individual information privacy concerns from the viewpoint\nof OGD stakeholders or providing solutions for mitigating concerns and risks.\nPapers that discussed and focused on general privacy issues or privacy concerns\nof open data in general or open science privacy concerns were excluded. Three\nstreams of research were identified: 1) exploring privacy concerns and balance\nwith OGD value propositions, 2) proposing solutions for mitigating privacy\nconcerns, and 3) developing risk-based frameworks for the OGD program at\ndifferent governmental levels. Findings suggest that contradictions with Fair\nInformation Practices, reidentification risks, conflicts with OGD value\npropositions, and smart city data practices are significant privacy concerns in\nthe literature. Proposed solutions include technical, legal, and procedural\nmeasures to mitigate privacy concerns. Building on the findings, practical\nimplications and suggested future research directions are provided.\n","authors":["Mehdi Barati"],"pdf_url":"https://arxiv.org/pdf/2312.10096v1.pdf","comment":"51 pages, 2 figures"},{"id":"http://arxiv.org/abs/2312.10094v1","updated":"2023-12-14T15:40:51Z","published":"2023-12-14T15:40:51Z","title":"Evaluative Item-Contrastive Explanations in Rankings","summary":"  The remarkable success of Artificial Intelligence in advancing automated\ndecision-making is evident both in academia and industry. Within the plethora\nof applications, ranking systems hold significant importance in various\ndomains. This paper advocates for the application of a specific form of\nExplainable AI -- namely, contrastive explanations -- as particularly\nwell-suited for addressing ranking problems. This approach is especially potent\nwhen combined with an Evaluative AI methodology, which conscientiously\nevaluates both positive and negative aspects influencing a potential ranking.\nTherefore, the present work introduces Evaluative Item-Contrastive Explanations\ntailored for ranking systems and illustrates its application and\ncharacteristics through an experiment conducted on publicly available data.\n","authors":["Alessandro Castelnovo","Riccardo Crupi","Nicolò Mombelli","Gabriele Nanino","Daniele Regoli"],"pdf_url":"https://arxiv.org/pdf/2312.10094v1.pdf","comment":"12 pages, 3 figures, 1 table"}]},"2023-12-15T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2312.10007v1","updated":"2023-12-15T18:23:50Z","published":"2023-12-15T18:23:50Z","title":"Faithful Persona-based Conversational Dataset Generation with Large\n  Language Models","summary":"  High-quality conversational datasets are essential for developing AI models\nthat can communicate with users. One way to foster deeper interactions between\na chatbot and its user is through personas, aspects of the user's character\nthat provide insights into their personality, motivations, and behaviors.\nTraining Natural Language Processing (NLP) models on a diverse and\ncomprehensive persona-based dataset can lead to conversational models that\ncreate a deeper connection with the user, and maintain their engagement. In\nthis paper, we leverage the power of Large Language Models (LLMs) to create a\nlarge, high-quality conversational dataset from a seed dataset. We propose a\nGenerator-Critic architecture framework to expand the initial dataset, while\nimproving the quality of its conversations. The Generator is an LLM prompted to\noutput conversations. The Critic consists of a mixture of expert LLMs that\ncontrol the quality of the generated conversations. These experts select the\nbest generated conversations, which we then use to improve the Generator. We\nrelease Synthetic-Persona-Chat, consisting of 20k conversations seeded from\nPersona-Chat. We evaluate the quality of Synthetic-Persona-Chat and our\ngeneration framework on different dimensions through extensive experiments, and\nobserve that the losing rate of Synthetic-Persona-Chat against Persona-Chat\nduring Turing test decreases from 17.2% to 8.8% over three iterations.\n","authors":["Pegah Jandaghi","XiangHai Sheng","Xinyi Bai","Jay Pujara","Hakim Sidahmed"],"pdf_url":"https://arxiv.org/pdf/2312.10007v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10003v1","updated":"2023-12-15T18:20:15Z","published":"2023-12-15T18:20:15Z","title":"ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent","summary":"  Answering complex natural language questions often necessitates multi-step\nreasoning and integrating external information. Several systems have combined\nknowledge retrieval with a large language model (LLM) to answer such questions.\nThese systems, however, suffer from various failure cases, and we cannot\ndirectly train them end-to-end to fix such failures, as interaction with\nexternal knowledge is non-differentiable. To address these deficiencies, we\ndefine a ReAct-style LLM agent with the ability to reason and act upon external\nknowledge. We further refine the agent through a ReST-like method that\niteratively trains on previous trajectories, employing growing-batch\nreinforcement learning with AI feedback for continuous self-improvement and\nself-distillation. Starting from a prompted large model and after just two\niterations of the algorithm, we can produce a fine-tuned small model that\nachieves comparable performance on challenging compositional question-answering\nbenchmarks with two orders of magnitude fewer parameters.\n","authors":["Renat Aksitov","Sobhan Miryoosefi","Zonglin Li","Daliang Li","Sheila Babayan","Kavya Kopparapu","Zachary Fisher","Ruiqi Guo","Sushant Prakash","Pranesh Srinivasan","Manzil Zaheer","Felix Yu","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2312.10003v1.pdf","comment":"19 pages, 4 figures, 4 tables, 8 listings"},{"id":"http://arxiv.org/abs/2312.09993v1","updated":"2023-12-15T18:06:22Z","published":"2023-12-15T18:06:22Z","title":"LLaMAntino: LLaMA 2 Models for Effective Text Generation in Italian\n  Language","summary":"  Large Language Models represent state-of-the-art linguistic models designed\nto equip computers with the ability to comprehend natural language. With its\nexceptional capacity to capture complex contextual relationships, the LLaMA\n(Large Language Model Meta AI) family represents a novel advancement in the\nfield of natural language processing by releasing foundational models designed\nto improve the natural language understanding abilities of the transformer\narchitecture thanks to their large amount of trainable parameters (7, 13, and\n70 billion parameters). In many natural language understanding tasks, these\nmodels obtain the same performances as private company models such as OpenAI\nChat-GPT with the advantage to make publicly available weights and code for\nresearch and commercial uses. In this work, we investigate the possibility of\nLanguage Adaptation for LLaMA models, explicitly focusing on addressing the\nchallenge of Italian Language coverage. Adopting an open science approach, we\nexplore various tuning approaches to ensure a high-quality text generated in\nItalian suitable for common tasks in this underrepresented language in the\noriginal models' datasets. We aim to release effective text generation models\nwith strong linguistic properties for many tasks that seem challenging using\nmultilingual or general-purpose LLMs. By leveraging an open science philosophy,\nthis study contributes to Language Adaptation strategies for the Italian\nlanguage by introducing the novel LLaMAntino family of Italian LLMs.\n","authors":["Pierpaolo Basile","Elio Musacchio","Marco Polignano","Lucia Siciliani","Giuseppe Fiameni","Giovanni Semeraro"],"pdf_url":"https://arxiv.org/pdf/2312.09993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09979v1","updated":"2023-12-15T17:45:06Z","published":"2023-12-15T17:45:06Z","title":"The Art of Balancing: Revolutionizing Mixture of Experts for Maintaining\n  World Knowledge in Language Model Alignment","summary":"  Supervised fine-tuning (SFT) is a crucial step for large language models\n(LLMs), enabling them to align with human instructions and enhance their\ncapabilities in downstream tasks. When the models are required to align with a\nbroader range of downstream tasks, or there is a desire to notably improve the\nperformance on a specific task, a substantial increase in fine-tuning data\noften emerges as the solution. However, we find that large-scale increases in\ninstruction data can disrupt the world knowledge previously stored in the LLMs,\ni.e., world knowledge forgetting. In this paper, we introduce LoRAMoE to\naddress above challenge. The LoRAMoE is a plugin version of Mixture of Experts\n(MoE). The plugin-form ensures the integrity of world knowledge by freezing the\nbackbone model during the training phase. And we propose the use of localized\nbalancing constraints to coordinate parts of experts for task utilization,\nmeanwhile enables other experts to to fully leverage the world knowledge stored\nin the models. Experimental results demonstrate that LoRAMoE can reasonly\ncoordinate experts based on data type during inference, and even dramatically\nincreasing instruction data does not result in knowledge forgetting. Moreover,\nLoRAMoE provides additional benefits for the performance of downstream tasks,\nindicating the potential of our approach for multi-task learning.\n","authors":["Shihan Dou","Enyu Zhou","Yan Liu","Songyang Gao","Jun Zhao","Wei Shen","Yuhao Zhou","Zhiheng Xi","Xiao Wang","Xiaoran Fan","Shiliang Pu","Jiang Zhu","Rui Zheng","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2312.09979v1.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2312.09966v1","updated":"2023-12-15T17:23:33Z","published":"2023-12-15T17:23:33Z","title":"Data and Approaches for German Text simplification -- towards an\n  Accessibility-enhanced Communication","summary":"  This paper examines the current state-of-the-art of German text\nsimplification, focusing on parallel and monolingual German corpora. It reviews\nneural language models for simplifying German texts and assesses their\nsuitability for legal texts and accessibility requirements. Our findings\nhighlight the need for additional training data and more appropriate approaches\nthat consider the specific linguistic characteristics of German, as well as the\nimportance of the needs and preferences of target groups with cognitive or\nlanguage impairments. The authors launched the interdisciplinary OPEN-LS\nproject in April 2023 to address these research gaps. The project aims to\ndevelop a framework for text formats tailored to individuals with low literacy\nlevels, integrate legal texts, and enhance comprehensibility for those with\nlinguistic or cognitive impairments. It will also explore cost-effective ways\nto enhance the data with audience-specific illustrations using image-generating\nAI.\n  For more and up-to-date information, please visit our project homepage\nhttps://open-ls.entavis.com\n","authors":["Thorben Schomacker","Michael Gille","Jörg von der Hülls","Marina Tropmann-Frick"],"pdf_url":"https://arxiv.org/pdf/2312.09966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11844v2","updated":"2023-12-15T17:18:48Z","published":"2023-11-20T15:34:45Z","title":"How to Use Large Language Models for Text Coding: The Case of Fatherhood\n  Roles in Public Policy Documents","summary":"  Recent advances in large language models (LLMs) like GPT-3 and GPT-4 have\nopened up new opportunities for text analysis in political science. They\npromise automation with better results and less programming. In this study, we\nevaluate LLMs on three original coding tasks of non-English political science\ntexts, and we provide a detailed description of a general workflow for using\nLLMs for text coding in political science research. Our use case offers a\npractical guide for researchers looking to incorporate LLMs into their research\non text analysis. We find that, when provided with detailed label definitions\nand coding examples, an LLM can be as good as or even better than a human\nannotator while being much faster (up to hundreds of times), considerably\ncheaper (costing up to 60% less than human coding), and much easier to scale to\nlarge amounts of text. Overall, LLMs present a viable option for most text\ncoding projects.\n","authors":["Lorenzo Lupo","Oscar Magnusson","Dirk Hovy","Elin Naurin","Lena Wängnerud"],"pdf_url":"https://arxiv.org/pdf/2311.11844v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14160v3","updated":"2023-12-15T16:48:15Z","published":"2023-05-23T15:26:20Z","title":"Label Words are Anchors: An Information Flow Perspective for\n  Understanding In-Context Learning","summary":"  In-context learning (ICL) emerges as a promising capability of large language\nmodels (LLMs) by providing them with demonstration examples to perform diverse\ntasks. However, the underlying mechanism of how LLMs learn from the provided\ncontext remains under-explored. In this paper, we investigate the working\nmechanism of ICL through an information flow lens. Our findings reveal that\nlabel words in the demonstration examples function as anchors: (1) semantic\ninformation aggregates into label word representations during the shallow\ncomputation layers' processing; (2) the consolidated information in label words\nserves as a reference for LLMs' final predictions. Based on these insights, we\nintroduce an anchor re-weighting method to improve ICL performance, a\ndemonstration compression technique to expedite inference, and an analysis\nframework for diagnosing ICL errors in GPT2-XL. The promising applications of\nour findings again validate the uncovered ICL working mechanism and pave the\nway for future studies.\n","authors":["Lean Wang","Lei Li","Damai Dai","Deli Chen","Hao Zhou","Fandong Meng","Jie Zhou","Xu Sun"],"pdf_url":"https://arxiv.org/pdf/2305.14160v3.pdf","comment":"Accepted by EMNLP 2023"},{"id":"http://arxiv.org/abs/2312.09932v1","updated":"2023-12-15T16:41:48Z","published":"2023-12-15T16:41:48Z","title":"RDR: the Recap, Deliberate, and Respond Method for Enhanced Language\n  Understanding","summary":"  Natural language understanding (NLU) using neural network pipelines often\nrequires additional context that is not solely present in the input data.\nThrough Prior research, it has been evident that NLU benchmarks are susceptible\nto manipulation by neural models, wherein these models exploit statistical\nartifacts within the encoded external knowledge to artificially inflate\nperformance metrics for downstream tasks. Our proposed approach, known as the\nRecap, Deliberate, and Respond (RDR) paradigm, addresses this issue by\nincorporating three distinct objectives within the neural network pipeline.\nFirstly, the Recap objective involves paraphrasing the input text using a\nparaphrasing model in order to summarize and encapsulate its essence. Secondly,\nthe Deliberation objective entails encoding external graph information related\nto entities mentioned in the input text, utilizing a graph embedding model.\nFinally, the Respond objective employs a classification head model that\nutilizes representations from the Recap and Deliberation modules to generate\nthe final prediction. By cascading these three models and minimizing a combined\nloss, we mitigate the potential for gaming the benchmark and establish a robust\nmethod for capturing the underlying semantic patterns, thus enabling accurate\npredictions. To evaluate the effectiveness of the RDR method, we conduct tests\non multiple GLUE benchmark tasks. Our results demonstrate improved performance\ncompared to competitive baselines, with an enhancement of up to 2\\% on standard\nmetrics. Furthermore, we analyze the observed evidence for semantic\nunderstanding exhibited by RDR models, emphasizing their ability to avoid\ngaming the benchmark and instead accurately capture the true underlying\nsemantic patterns.\n","authors":["Yuxin Zi","Hariram Veeramani","Kaushik Roy","Amit Sheth"],"pdf_url":"https://arxiv.org/pdf/2312.09932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09917v1","updated":"2023-12-15T16:25:56Z","published":"2023-12-15T16:25:56Z","title":"Red AI? Inconsistent Responses from GPT3.5 Models on Political Issues in\n  the US and China","summary":"  The rising popularity of ChatGPT and other AI-powered large language models\n(LLMs) has led to increasing studies highlighting their susceptibility to\nmistakes and biases. However, most of these studies focus on models trained on\nEnglish texts. Taking an innovative approach, this study investigates political\nbiases in GPT's multilingual models. We posed the same question about\nhigh-profile political issues in the United States and China to GPT in both\nEnglish and simplified Chinese, and our analysis of the bilingual responses\nrevealed that GPT's bilingual models' political \"knowledge\" (content) and the\npolitical \"attitude\" (sentiment) are significantly more inconsistent on\npolitical issues in China. The simplified Chinese GPT models not only tended to\nprovide pro-China information but also presented the least negative sentiment\ntowards China's problems, whereas the English GPT was significantly more\nnegative towards China. This disparity may stem from Chinese state censorship\nand US-China geopolitical tensions, which influence the training corpora of GPT\nbilingual models. Moreover, both Chinese and English models tended to be less\ncritical towards the issues of \"their own\" represented by the language used,\nthan the issues of \"the other.\" This suggests that GPT multilingual models\ncould potentially develop a \"political identity\" and an associated sentiment\nbias based on their training language. We discussed the implications of our\nfindings for information transmission and communication in an increasingly\ndivided world.\n","authors":["Di Zhou","Yinxian Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.09917v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09907v1","updated":"2023-12-15T16:10:44Z","published":"2023-12-15T16:10:44Z","title":"Exploring Automatic Text Simplification of German Narrative Documents","summary":"  In this paper, we apply transformer-based Natural Language Generation (NLG)\ntechniques to the problem of text simplification. Currently, there are only a\nfew German datasets available for text simplification, even fewer with larger\nand aligned documents, and not a single one with narrative texts. In this\npaper, we explore to which degree modern NLG techniques can be applied to\nGerman narrative text simplifications. We use Longformer attention and a\npre-trained mBART model. Our findings indicate that the existing approaches for\nGerman are not able to solve the task properly. We conclude on a few directions\nfor future research to address this problem.\n","authors":["Thorben Schomacker","Tillmann Dönicke","Marina Tropmann-Frick"],"pdf_url":"https://arxiv.org/pdf/2312.09907v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09895v1","updated":"2023-12-15T15:46:02Z","published":"2023-12-15T15:46:02Z","title":"Generative Context-aware Fine-tuning of Self-supervised Speech Models","summary":"  When performing tasks like automatic speech recognition or spoken language\nunderstanding for a given utterance, access to preceding text or audio provides\ncontextual information can improve performance. Considering the recent advances\nin generative large language models (LLM), we hypothesize that an LLM could\ngenerate useful context information using the preceding text. With appropriate\nprompts, LLM could generate a prediction of the next sentence or abstractive\ntext like titles or topics. In this paper, we study the use of LLM-generated\ncontext information and propose an approach to distill the generated\ninformation during fine-tuning of self-supervised speech models, which we refer\nto as generative context-aware fine-tuning. This approach allows the fine-tuned\nmodel to make improved predictions without access to the true surrounding\nsegments or to the LLM at inference time, while requiring only a very small\nadditional context module. We evaluate the proposed approach using the SLUE and\nLibri-light benchmarks for several downstream tasks: automatic speech\nrecognition, named entity recognition, and sentiment analysis. The results show\nthat generative context-aware fine-tuning outperforms a context injection\nfine-tuning approach that accesses the ground-truth previous text, and is\ncompetitive with a generative context injection fine-tuning approach that\nrequires the LLM at inference time.\n","authors":["Suwon Shon","Kwangyoun Kim","Prashant Sridhar","Yi-Te Hsu","Shinji Watanabe","Karen Livescu"],"pdf_url":"https://arxiv.org/pdf/2312.09895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18333v3","updated":"2023-12-15T15:45:51Z","published":"2023-10-20T14:18:40Z","title":"She had Cobalt Blue Eyes: Prompt Testing to Create Aligned and\n  Sustainable Language Models","summary":"  As the use of large language models (LLMs) increases within society, as does\nthe risk of their misuse. Appropriate safeguards must be in place to ensure LLM\noutputs uphold the ethical standards of society, highlighting the positive role\nthat artificial intelligence technologies can have. Recent events indicate\nethical concerns around conventionally trained LLMs, leading to overall unsafe\nuser experiences. This motivates our research question: how do we ensure LLM\nalignment? In this work, we introduce a test suite of unique prompts to foster\nthe development of aligned LLMs that are fair, safe, and robust. We show that\nprompting LLMs at every step of the development pipeline, including data\ncuration, pre-training, and fine-tuning, will result in an overall more\nresponsible model. Our test suite evaluates outputs from four state-of-the-art\nlanguage models: GPT-3.5, GPT-4, OPT, and LLaMA-2. The assessment presented in\nthis paper highlights a gap between societal alignment and the capabilities of\ncurrent LLMs. Additionally, implementing a test suite such as ours lowers the\nenvironmental overhead of making models safe and fair.\n","authors":["Veronica Chatrath","Oluwanifemi Bamgbose","Shaina Raza"],"pdf_url":"https://arxiv.org/pdf/2310.18333v3.pdf","comment":"Accepted as Oral at the AAAI 2nd Workshop on Sustainable AI"},{"id":"http://arxiv.org/abs/2312.09890v1","updated":"2023-12-15T15:41:52Z","published":"2023-12-15T15:41:52Z","title":"Grammatical information in BERT sentence embeddings as two-dimensional\n  arrays","summary":"  Sentence embeddings induced with various transformer architectures encode\nmuch semantic and syntactic information in a distributed manner in a\none-dimensional array. We investigate whether specific grammatical information\ncan be accessed in these distributed representations. Using data from a task\ndeveloped to test rule-like generalizations, our experiments on detecting\nsubject-verb agreement yield several promising results. First, we show that\nwhile the usual sentence representations encoded as one-dimensional arrays do\nnot easily support extraction of rule-like regularities, a two-dimensional\nreshaping of these vectors allows various learning architectures to access such\ninformation. Next, we show that various architectures can detect patterns in\nthese two-dimensional reshaped sentence embeddings and successfully learn a\nmodel based on smaller amounts of simpler training data, which performs well on\nmore complex test data. This indicates that current sentence embeddings contain\ninformation that is regularly distributed, and which can be captured when the\nembeddings are reshaped into higher dimensional arrays. Our results cast light\non representations produced by language models and help move towards developing\nfew-shot learning approaches.\n","authors":["Vivi Nastase","Paola Merlo"],"pdf_url":"https://arxiv.org/pdf/2312.09890v1.pdf","comment":"Published in RepL4NLP 2023"},{"id":"http://arxiv.org/abs/2312.09211v2","updated":"2023-12-15T14:46:53Z","published":"2023-12-14T18:41:32Z","title":"Mitigating Outlier Activations in Low-Precision Fine-Tuning of Language\n  Models","summary":"  Low-precision fine-tuning of language models has gained prominence as a\ncost-effective and energy-efficient approach to deploying large-scale models in\nvarious applications. However, this approach is susceptible to the existence of\noutlier values in activation. The outlier values in the activation can\nnegatively affect the performance of fine-tuning language models in the\nlow-precision regime since they affect the scaling factor and thus make\nrepresenting smaller values harder. This paper investigates techniques for\nmitigating outlier activation in low-precision integer fine-tuning of the\nlanguage models. Our proposed novel approach enables us to represent the\noutlier activation values in 8-bit integers instead of floating-point (FP16)\nvalues. The benefit of using integers for outlier values is that it enables us\nto use operator tiling to avoid performing 16-bit integer matrix multiplication\nto address this problem effectively. We provide theoretical analysis and\nsupporting experiments to demonstrate the effectiveness of our approach in\nimproving the robustness and performance of low-precision fine-tuned language\nmodels.\n","authors":["Alireza Ghaffari","Justin Yu","Mahsa Ghazvini Nejad","Masoud Asgharian","Boxing Chen","Vahid Partovi Nia"],"pdf_url":"https://arxiv.org/pdf/2312.09211v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09818v1","updated":"2023-12-15T14:17:45Z","published":"2023-12-15T14:17:45Z","title":"SMILE: Multimodal Dataset for Understanding Laughter in Video with\n  Language Models","summary":"  Despite the recent advances of the artificial intelligence, building social\nintelligence remains a challenge. Among social signals, laughter is one of the\ndistinctive expressions that occurs during social interactions between humans.\nIn this work, we tackle a new challenge for machines to understand the\nrationale behind laughter in video, Video Laugh Reasoning. We introduce this\nnew task to explain why people laugh in a particular video and a dataset for\nthis task. Our proposed dataset, SMILE, comprises video clips and language\ndescriptions of why people laugh. We propose a baseline by leveraging the\nreasoning capacity of large language models (LLMs) with textual video\nrepresentation. Experiments show that our baseline can generate plausible\nexplanations for laughter. We further investigate the scalability of our\nbaseline by probing other video understanding tasks and in-the-wild videos. We\nrelease our dataset, code, and model checkpoints on\nhttps://github.com/SMILE-data/SMILE.\n","authors":["Lee Hyun","Kim Sung-Bin","Seungju Han","Youngjae Yu","Tae-Hyun Oh"],"pdf_url":"https://arxiv.org/pdf/2312.09818v1.pdf","comment":"18 pages, 14 figures"},{"id":"http://arxiv.org/abs/2312.09806v1","updated":"2023-12-15T14:04:23Z","published":"2023-12-15T14:04:23Z","title":"Improving Biomedical Entity Linking with Retrieval-enhanced Learning","summary":"  Biomedical entity linking (BioEL) has achieved remarkable progress with the\nhelp of pre-trained language models. However, existing BioEL methods usually\nstruggle to handle rare and difficult entities due to long-tailed distribution.\nTo address this limitation, we introduce a new scheme $k$NN-BioEL, which\nprovides a BioEL model with the ability to reference similar instances from the\nentire training corpus as clues for prediction, thus improving the\ngeneralization capabilities. Moreover, we design a contrastive learning\nobjective with dynamic hard negative sampling (DHNS) that improves the quality\nof the retrieved neighbors during inference. Extensive experimental results\nshow that $k$NN-BioEL outperforms state-of-the-art baselines on several\ndatasets.\n","authors":["Zhenxi Lin","Ziheng Zhang","Xian Wu","Yefeng Zheng"],"pdf_url":"https://arxiv.org/pdf/2312.09806v1.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.09801v1","updated":"2023-12-15T14:01:46Z","published":"2023-12-15T14:01:46Z","title":"ProCoT: Stimulating Critical Thinking and Writing of Students through\n  Engagement with Large Language Models (LLMs)","summary":"  We introduce a novel writing method called Probing Chain of Thought (ProCoT),\nwhich prevents students from cheating using a Large Language Model (LLM), such\nas ChatGPT, while enhancing their active learning through such models. LLMs\nhave disrupted education and many other feilds. For fear of students cheating,\nmany educationists have resorted to banning their use, as their outputs can be\nhuman-like and hard to detect in some cases. These LLMs are also known for\nhallucinations (i.e. fake facts). We conduct studies with ProCoT in two\ndifferent courses with a combined total of about 66 students. The students in\neach course were asked to prompt an LLM of their choice with one question from\na set of four and required to affirm or refute statements in the LLM output by\nusing peer reviewed references. The results show two things: (1) ProCoT\nstimulates creative/critical thinking and writing of students through\nengagement with LLMs when we compare the LLM solely output to ProCoT output and\n(2) ProCoT can prevent cheating because of clear limitations in existing LLMs\nwhen we compare students ProCoT output to LLM ProCoT output. We also discover\nthat most students prefer to give answers in fewer words than LLMs, which are\ntypically verbose. The average word counts for students, ChatGPT (v3.5) and\nPhind (v8) are 208, 391 and 383, respectively.\n","authors":["Tosin Adewumi","Lama Alkhaled","Claudia Buck","Sergio Hernandez","Saga Brilioth","Mkpe Kekung","Yelvin Ragimov","Elisa Barney"],"pdf_url":"https://arxiv.org/pdf/2312.09801v1.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2312.09785v1","updated":"2023-12-15T13:40:25Z","published":"2023-12-15T13:40:25Z","title":"RJUA-QA: A Comprehensive QA Dataset for Urology","summary":"  We introduce RJUA-QA, a novel medical dataset for question answering (QA) and\nreasoning with clinical evidence, contributing to bridge the gap between\ngeneral large language models (LLMs) and medical-specific LLM applications.\nRJUA-QA is derived from realistic clinical scenarios and aims to facilitate\nLLMs in generating reliable diagnostic and advice. The dataset contains 2,132\ncurated Question-Context-Answer pairs, corresponding about 25,000 diagnostic\nrecords and clinical cases. The dataset covers 67 common urological disease\ncategories, where the disease coverage exceeds 97.6\\% of the population seeking\nmedical services in urology. Each data instance in RJUA-QA comprises: (1) a\nquestion mirroring real patient to inquiry about clinical symptoms and medical\nconditions, (2) a context including comprehensive expert knowledge, serving as\na reference for medical examination and diagnosis, (3) a doctor response\noffering the diagnostic conclusion and suggested examination guidance, (4) a\ndiagnosed clinical disease as the recommended diagnostic outcome, and (5)\nclinical advice providing recommendations for medical examination. RJUA-QA is\nthe first medical QA dataset for clinical reasoning over the patient inquiries,\nwhere expert-level knowledge and experience are required for yielding\ndiagnostic conclusions and medical examination advice. A comprehensive\nevaluation is conducted to evaluate the performance of both medical-specific\nand general LLMs on the RJUA-QA dataset.\n","authors":["Shiwei Lyu","Chenfei Chi","Hongbo Cai","Lei Shi","Xiaoyan Yang","Lei Liu","Xiang Chen","Deng Zhao","Zhiqiang Zhang","Xianguo Lyu","Ming Zhang","Fangzhou Li","Xiaowei Ma","Yue Shen","Jinjie Gu","Wei Xue","Yiran Huang"],"pdf_url":"https://arxiv.org/pdf/2312.09785v1.pdf","comment":"An initial version"},{"id":"http://arxiv.org/abs/2312.09781v1","updated":"2023-12-15T13:33:18Z","published":"2023-12-15T13:33:18Z","title":"GSQA: An End-to-End Model for Generative Spoken Question Answering","summary":"  In recent advancements in spoken question answering (QA), end-to-end models\nhave made significant strides. However, previous research has primarily focused\non extractive span selection. While this extractive-based approach is effective\nwhen answers are present directly within the input, it falls short in\naddressing abstractive questions, where answers are not directly extracted but\ninferred from the given information. To bridge this gap, we introduce the first\nend-to-end Generative Spoken Question Answering (GSQA) model that empowers the\nsystem to engage in abstractive reasoning. The challenge in training our GSQA\nmodel lies in the absence of a spoken abstractive QA dataset. We propose using\ntext models for initialization and leveraging the extractive QA dataset to\ntransfer knowledge from the text generative model to the spoken generative\nmodel. Experimental results indicate that our model surpasses the previous\nextractive model by 3% on extractive QA datasets. Furthermore, the GSQA model\nhas only been fine-tuned on the spoken extractive QA dataset. Despite not\nhaving seen any spoken abstractive QA data, it can still closely match the\nperformance of the cascade model. In conclusion, our GSQA model shows the\npotential to generalize to a broad spectrum of questions, thus further\nexpanding spoken question answering capabilities of abstractive QA. Our code is\navailable at\n\\href{https://voidful.github.io/GSQA}{https://voidful.github.io/GSQA}\n","authors":["Min-Han Shih","Ho-Lam Chung","Yu-Chi Pai","Ming-Hao Hsu","Guan-Ting Lin","Shang-Wen Li","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2312.09781v1.pdf","comment":"5 pages, 2 figures, submitted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.08078v3","updated":"2023-12-15T13:22:51Z","published":"2023-12-13T11:47:28Z","title":"Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic\n  Image-Report Generation","summary":"  To address these issues, we propose a novel Adaptive patch-word Matching\n(AdaMatch) model to correlate chest X-ray (CXR) image regions with words in\nmedical reports and apply it to CXR-report generation to provide explainability\nfor the generation process. AdaMatch exploits the fine-grained relation between\nadaptive patches and words to provide explanations of specific image regions\nwith corresponding words. To capture the abnormal regions of varying sizes and\npositions, we introduce the Adaptive Patch extraction (AdaPatch) module to\nacquire the adaptive patches for these regions adaptively. In order to provide\nexplicit explainability for CXR-report generation task, we propose an\nAdaMatch-based bidirectional large language model for Cyclic CXR-report\ngeneration (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywords\nfor CXR images and `keypatches' for medical reports as hints to guide\nCXR-report generation. Extensive experiments on two publicly available CXR\ndatasets prove the effectiveness of our method and its superior performance to\nexisting methods.\n","authors":["Wenting Chen","Xiang Li","Linlin Shen","Yixuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2312.08078v3.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2310.03985v2","updated":"2023-12-15T13:07:44Z","published":"2023-10-06T03:04:11Z","title":"Dementia Assessment Using Mandarin Speech with an Attention-based Speech\n  Recognition Encoder","summary":"  Dementia diagnosis requires a series of different testing methods, which is\ncomplex and time-consuming. Early detection of dementia is crucial as it can\nprevent further deterioration of the condition. This paper utilizes a speech\nrecognition model to construct a dementia assessment system tailored for\nMandarin speakers during the picture description task. By training an\nattention-based speech recognition model on voice data closely resembling\nreal-world scenarios, we have significantly enhanced the model's recognition\ncapabilities. Subsequently, we extracted the encoder from the speech\nrecognition model and added a linear layer for dementia assessment. We\ncollected Mandarin speech data from 99 subjects and acquired their clinical\nassessments from a local hospital. We achieved an accuracy of 92.04% in\nAlzheimer's disease detection and a mean absolute error of 9% in clinical\ndementia rating score prediction.\n","authors":["Zih-Jyun Lin","Yi-Ju Chen","Po-Chih Kuo","Likai Huang","Chaur-Jong Hu","Cheng-Yu Chen"],"pdf_url":"https://arxiv.org/pdf/2310.03985v2.pdf","comment":"Accepted to IEEE ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.09736v1","updated":"2023-12-15T12:20:24Z","published":"2023-12-15T12:20:24Z","title":"HEAR: Hearing Enhanced Audio Response for Video-grounded Dialogue","summary":"  Video-grounded Dialogue (VGD) aims to answer questions regarding a given\nmulti-modal input comprising video, audio, and dialogue history. Although there\nhave been numerous efforts in developing VGD systems to improve the quality of\ntheir responses, existing systems are competent only to incorporate the\ninformation in the video and text and tend to struggle in extracting the\nnecessary information from the audio when generating appropriate responses to\nthe question. The VGD system seems to be deaf, and thus, we coin this symptom\nof current systems' ignoring audio data as a deaf response. To overcome the\ndeaf response problem, Hearing Enhanced Audio Response (HEAR) framework is\nproposed to perform sensible listening by selectively attending to audio\nwhenever the question requires it. The HEAR framework enhances the accuracy and\naudibility of VGD systems in a model-agnostic manner. HEAR is validated on VGD\ndatasets (i.e., AVSD@DSTC7 and AVSD@DSTC8) and shows effectiveness with various\nVGD systems.\n","authors":["Sunjae Yoon","Dahyun Kim","Eunseop Yoon","Hee Suk Yoon","Junyeong Kim","Chnag D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2312.09736v1.pdf","comment":"EMNLP 2023, 14 pages, 13 figures"},{"id":"http://arxiv.org/abs/2312.09718v1","updated":"2023-12-15T11:45:42Z","published":"2023-12-15T11:45:42Z","title":"Discovering Highly Influential Shortcut Reasoning: An Automated\n  Template-Free Approach","summary":"  Shortcut reasoning is an irrational process of inference, which degrades the\nrobustness of an NLP model. While a number of previous work has tackled the\nidentification of shortcut reasoning, there are still two major limitations:\n(i) a method for quantifying the severity of the discovered shortcut reasoning\nis not provided; (ii) certain types of shortcut reasoning may be missed. To\naddress these issues, we propose a novel method for identifying shortcut\nreasoning. The proposed method quantifies the severity of the shortcut\nreasoning by leveraging out-of-distribution data and does not make any\nassumptions about the type of tokens triggering the shortcut reasoning. Our\nexperiments on Natural Language Inference and Sentiment Analysis demonstrate\nthat our framework successfully discovers known and unknown shortcut reasoning\nin the previous work.\n","authors":["Daichi Haraguchi","Kiyoaki Shirai","Naoya Inoue","Natthawut Kertkeidkachorn"],"pdf_url":"https://arxiv.org/pdf/2312.09718v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09670v1","updated":"2023-12-15T10:31:36Z","published":"2023-12-15T10:31:36Z","title":"Probing Pretrained Language Models with Hierarchy Properties","summary":"  Since Pretrained Language Models (PLMs) are the cornerstone of the most\nrecent Information Retrieval (IR) models, the way they encode semantic\nknowledge is particularly important. However, little attention has been given\nto studying the PLMs' capability to capture hierarchical semantic knowledge.\nTraditionally, evaluating such knowledge encoded in PLMs relies on their\nperformance on a task-dependent evaluation approach based on proxy tasks, such\nas hypernymy detection. Unfortunately, this approach potentially ignores other\nimplicit and complex taxonomic relations. In this work, we propose a\ntask-agnostic evaluation method able to evaluate to what extent PLMs can\ncapture complex taxonomy relations, such as ancestors and siblings. The\nevaluation is based on intrinsic properties that capture the hierarchical\nnature of taxonomies. Our experimental evaluation shows that the\nlexico-semantic knowledge implicitly encoded in PLMs does not always capture\nhierarchical relations. We further demonstrate that the proposed properties can\nbe injected into PLMs to improve their understanding of hierarchy. Through\nevaluations on taxonomy reconstruction, hypernym discovery and reading\ncomprehension tasks, we show that the knowledge about hierarchy is moderately\nbut not systematically transferable across tasks.\n","authors":["Jesús Lovón-Melgarejo","Jose G. Moreno","Romaric Besançon","Olivier Ferret","Lynda Tamine"],"pdf_url":"https://arxiv.org/pdf/2312.09670v1.pdf","comment":"Accepted at ECIR 2024"},{"id":"http://arxiv.org/abs/2308.10822v2","updated":"2023-12-15T09:30:43Z","published":"2023-08-14T03:20:28Z","title":"A Novel Ehanced Move Recognition Algorithm Based on Pre-trained Models\n  with Positional Embeddings","summary":"  The recognition of abstracts is crucial for effectively locating the content\nand clarifying the article. Existing move recognition algorithms lack the\nability to learn word position information to obtain contextual semantics. This\npaper proposes a novel enhanced move recognition algorithm with an improved\npre-trained model and a gated network with attention mechanism for unstructured\nabstracts of Chinese scientific and technological papers. The proposed\nalgorithm first performs summary data segmentation and vocabulary training. The\nEP-ERNIE$\\_$AT-GRU framework is leveraged to incorporate word positional\ninformation, facilitating deep semantic learning and targeted feature\nextraction. Experimental results demonstrate that the proposed algorithm\nachieves 13.37$\\%$ higher accuracy on the split dataset than on the original\ndataset and a 7.55$\\%$ improvement in accuracy over the basic comparison model.\n","authors":["Hao Wen","Jie Wang","Xiaodong Qiao"],"pdf_url":"https://arxiv.org/pdf/2308.10822v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09625v1","updated":"2023-12-15T09:08:14Z","published":"2023-12-15T09:08:14Z","title":"Weakly-Supervised 3D Visual Grounding based on Visual Linguistic\n  Alignment","summary":"  Learning to ground natural language queries to target objects or regions in\n3D point clouds is quite essential for 3D scene understanding. Nevertheless,\nexisting 3D visual grounding approaches require a substantial number of\nbounding box annotations for text queries, which is time-consuming and\nlabor-intensive to obtain. In this paper, we propose \\textbf{3D-VLA}, a weakly\nsupervised approach for \\textbf{3D} visual grounding based on \\textbf{V}isual\n\\textbf{L}inguistic \\textbf{A}lignment. Our 3D-VLA exploits the superior\nability of current large-scale vision-language models (VLMs) on aligning the\nsemantics between texts and 2D images, as well as the naturally existing\ncorrespondences between 2D images and 3D point clouds, and thus implicitly\nconstructs correspondences between texts and 3D point clouds with no need for\nfine-grained box annotations in the training procedure. During the inference\nstage, the learned text-3D correspondence will help us ground the text queries\nto the 3D target objects even without 2D images. To the best of our knowledge,\nthis is the first work to investigate 3D visual grounding in a weakly\nsupervised manner by involving large scale vision-language models, and\nextensive experiments on ReferIt3D and ScanRefer datasets demonstrate that our\n3D-VLA achieves comparable and even superior results over the fully supervised\nmethods.\n","authors":["Xiaoxu Xu","Yitian Yuan","Qiudan Zhang","Wenhui Wu","Zequn Jie","Lin Ma","Xu Wang"],"pdf_url":"https://arxiv.org/pdf/2312.09625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15786v3","updated":"2023-12-15T08:50:33Z","published":"2023-11-27T13:01:59Z","title":"YUAN 2.0: A Large Language Model with Localized Filtering-based\n  Attention","summary":"  In this work, we develop and release Yuan 2.0, a series of large language\nmodels with parameters ranging from 2.1 billion to 102.6 billion. The Localized\nFiltering-based Attention (LFA) is introduced to incorporate prior knowledge of\nlocal dependencies of natural language into Attention. A data filtering and\ngenerating system is presented to build pre-training and fine-tuning dataset in\nhigh quality. A distributed training method with non-uniform pipeline parallel,\ndata parallel, and optimizer parallel is proposed, which greatly reduces the\nbandwidth requirements of intra-node communication, and achieves good\nperformance in large-scale distributed training. Yuan 2.0 models display\nimpressive ability in code generation, math problem-solving, and chatting\ncompared with existing models. The latest version of YUAN 2.0, including model\nweights and source code, is accessible at Github.\n","authors":["Shaohua Wu","Xudong Zhao","Shenling Wang","Jiangang Luo","Lingjun Li","Xi Chen","Bing Zhao","Wei Wang","Tong Yu","Rongguo Zhang","Jiahua Zhang","Chao Wang"],"pdf_url":"https://arxiv.org/pdf/2311.15786v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.16230v4","updated":"2023-12-15T08:44:33Z","published":"2022-03-30T12:06:32Z","title":"Evaluation of semantic relations impact in query expansion-based\n  retrieval systems","summary":"  With the increasing demand of intelligent systems capable of operating in\ndifferent contexts (e.g. users on the move) the correct interpretation of the\nuser-need by such systems has become crucial to give consistent answers to the\nuser questions. The most effective applications addressing such task are in the\nfields of natural language processing and semantic expansion of terms. These\ntechniques are aimed at estimating the goal of an input query reformulating it\nas an intent, commonly relying on textual resources built exploiting different\nsemantic relations like \\emph{synonymy}, \\emph{antonymy} and many others. The\naim of this paper is to generate such resources using the labels of a given\ntaxonomy as source of information. The obtained resources are integrated into a\nplain classifier for reformulating a set of input queries as intents and\ntracking the effect of each relation, in order to quantify the impact of each\nsemantic relation on the classification. As an extension to this, the best\ntradeoff between improvement and noise introduction when combining such\nrelations is evaluated. The assessment is made generating the resources and\ntheir combinations and using them for tuning the classifier which is used to\nreformulate the user questions as labels. The evaluation employs a wide and\nvaried taxonomy as a use-case, exploiting its labels as basis for the semantic\nexpansion and producing several corpora with the purpose of enhancing the\npseudo-queries estimation.\n","authors":["Lorenzo Massai"],"pdf_url":"https://arxiv.org/pdf/2203.16230v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09601v1","updated":"2023-12-15T08:32:28Z","published":"2023-12-15T08:32:28Z","title":"Binary Code Summarization: Benchmarking ChatGPT/GPT-4 and Other Large\n  Language Models","summary":"  Binary code summarization, while invaluable for understanding code semantics,\nis challenging due to its labor-intensive nature. This study delves into the\npotential of large language models (LLMs) for binary code comprehension. To\nthis end, we present BinSum, a comprehensive benchmark and dataset of over 557K\nbinary functions and introduce a novel method for prompt synthesis and\noptimization. To more accurately gauge LLM performance, we also propose a new\nsemantic similarity metric that surpasses traditional exact-match approaches.\nOur extensive evaluation of prominent LLMs, including ChatGPT, GPT-4, Llama 2,\nand Code Llama, reveals 10 pivotal insights. This evaluation generates 4\nbillion inference tokens, incurred a total expense of 11,418 US dollars and 873\nNVIDIA A100 GPU hours. Our findings highlight both the transformative potential\nof LLMs in this field and the challenges yet to be overcome.\n","authors":["Xin Jin","Jonathan Larson","Weiwei Yang","Zhiqiang Lin"],"pdf_url":"https://arxiv.org/pdf/2312.09601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14360v4","updated":"2023-12-15T08:19:59Z","published":"2023-10-22T17:03:56Z","title":"Is ChatGPT a game changer for geocoding -- a benchmark for geocoding\n  address parsing techniques","summary":"  The remarkable success of GPT models across various tasks, including toponymy\nrecognition motivates us to assess the performance of the GPT-3 model in the\ngeocoding address parsing task. To ensure that the evaluation more accurately\nmirrors performance in real-world scenarios with diverse user input qualities\nand resolve the pressing need for a 'gold standard' evaluation dataset for\ngeocoding systems, we introduce a benchmark dataset of low-quality address\ndescriptions synthesized based on human input patterns mining from actual input\nlogs of a geocoding system in production. This dataset has 21 different input\nerrors and variations; contains over 239,000 address records that are uniquely\nselected from streets across all U.S. 50 states and D.C.; and consists of three\nsubsets to be used as training, validation, and testing sets. Building on this,\nwe train and gauge the performance of the GPT-3 model in extracting address\ncomponents, contrasting its performance with transformer-based and LSTM-based\nmodels. The evaluation results indicate that Bidirectional LSTM-CRF model has\nachieved the best performance over these transformer-based models and GPT-3\nmodel. Transformer-based models demonstrate very comparable results compared to\nthe Bidirectional LSTM-CRF model. The GPT-3 model, though trailing in\nperformance, showcases potential in the address parsing task with few-shot\nexamples, exhibiting room for improvement with additional fine-tuning. We open\nsource the code and data of this presented benchmark so that researchers can\nutilize it for future model development or extend it to evaluate similar tasks,\nsuch as document geocoding.\n","authors":["Zhengcong Yin","Diya Li","Daniel W. Goldberg"],"pdf_url":"https://arxiv.org/pdf/2310.14360v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09583v1","updated":"2023-12-15T07:46:35Z","published":"2023-12-15T07:46:35Z","title":"Leveraging Language ID to Calculate Intermediate CTC Loss for Enhanced\n  Code-Switching Speech Recognition","summary":"  In recent years, end-to-end speech recognition has emerged as a technology\nthat integrates the acoustic, pronunciation dictionary, and language model\ncomponents of the traditional Automatic Speech Recognition model. It is\npossible to achieve human-like recognition without the need to build a\npronunciation dictionary in advance. However, due to the relative scarcity of\ntraining data on code-switching, the performance of ASR models tends to degrade\ndrastically when encountering this phenomenon. Most past studies have\nsimplified the learning complexity of the model by splitting the code-switching\ntask into multiple tasks dealing with a single language and then learning the\ndomain-specific knowledge of each language separately. Therefore, in this\npaper, we attempt to introduce language identification information into the\nmiddle layer of the ASR model's encoder. We aim to generate acoustic features\nthat imply language distinctions in a more implicit way, reducing the model's\nconfusion when dealing with language switching.\n","authors":["Tzu-Ting Yang","Hsin-Wei Wang","Berlin Chen"],"pdf_url":"https://arxiv.org/pdf/2312.09583v1.pdf","comment":"Accepted to The 28th International Conference on Technologies and\n  Applications of Artificial Intelligence (TAAI), in Chinese language"},{"id":"http://arxiv.org/abs/2312.09582v1","updated":"2023-12-15T07:37:09Z","published":"2023-12-15T07:37:09Z","title":"Phoneme-aware Encoding for Prefix-tree-based Contextual ASR","summary":"  In speech recognition applications, it is important to recognize\ncontext-specific rare words, such as proper nouns. Tree-constrained Pointer\nGenerator (TCPGen) has shown promise for this purpose, which efficiently biases\nsuch words with a prefix tree. While the original TCPGen relies on\ngrapheme-based encoding, we propose extending it with phoneme-aware encoding to\nbetter recognize words of unusual pronunciations. As TCPGen handles biasing\nwords as subword units, we propose obtaining subword-level phoneme-aware\nencoding by using alignment between phonemes and subwords. Furthermore, we\npropose injecting phoneme-level predictions from CTC into queries of TCPGen so\nthat the model better interprets the phoneme-aware encodings. We conducted ASR\nexperiments with TCPGen for RNN transducer. We observed that proposed\nphoneme-aware encoding outperformed ordinary grapheme-based encoding on both\nthe English LibriSpeech and Japanese CSJ datasets, demonstrating the robustness\nof our approach across linguistically diverse languages.\n","authors":["Hayato Futami","Emiru Tsunoo","Yosuke Kashiwagi","Hiroaki Ogawa","Siddhant Arora","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2312.09582v1.pdf","comment":"Accepted to ICASSP2024"},{"id":"http://arxiv.org/abs/2312.08274v3","updated":"2023-12-15T07:25:34Z","published":"2023-12-13T16:43:41Z","title":"High-throughput Biomedical Relation Extraction for Semi-Structured Web\n  Articles Empowered by Large Language Models","summary":"  Objective: To develop a high-throughput biomedical relation extraction system\nthat takes advantage of the large language models' (LLMs) reading comprehension\nability and biomedical world knowledge in a scalable and evidential manner.\nMethods: We formulate the relation extraction task as a simple binary\nclassification problem for large language models such as ChatGPT. Specifically,\nLLMs make the decision based on the external corpus and its world knowledge,\ngiving the reason for the judgment to factual verification. This method is\ntailored for semi-structured web articles, wherein we designate the main title\nas the tail entity and explicitly incorporate it into the context, and the\npotential head entities are matched based on a biomedical thesaurus. Moreover,\nlengthy contents are sliced into text chunks, embedded, and retrieved with\nadditional embedding models, ensuring compatibility with the context window\nsize constraints of available open-source LLMs. Results: Using an open-source\nLLM, we extracted 304315 relation triplets of three distinct relation types\nfrom four reputable biomedical websites. To assess the efficacy of the basic\npipeline employed for biomedical relation extraction, we curated a benchmark\ndataset annotated by a medical expert. Evaluation results indicate that the\npipeline exhibits performance comparable to that of GPT-4. Case studies further\nilluminate challenges faced by contemporary LLMs in the context of biomedical\nrelation extraction for semi-structured web articles. Conclusion: The proposed\nmethod has demonstrated its effectiveness in leveraging the strengths of LLMs\nfor high-throughput biomedical relation extraction. Its adaptability is\nevident, as it can be seamlessly extended to diverse semi-structured biomedical\nwebsites, facilitating the extraction of various types of biomedical relations\nwith ease.\n","authors":["Songchi Zhou","Sheng Yu"],"pdf_url":"https://arxiv.org/pdf/2312.08274v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09572v1","updated":"2023-12-15T07:04:40Z","published":"2023-12-15T07:04:40Z","title":"IR-UWB Radar-Based Contactless Silent Speech Recognition of Vowels,\n  Consonants, Words, and Phrases","summary":"  Several sensing techniques have been proposed for silent speech recognition\n(SSR); however, many of these methods require invasive processes or sensor\nattachment to the skin using adhesive tape or glue, rendering them unsuitable\nfor frequent use in daily life. By contrast, impulse radio ultra-wideband\n(IR-UWB) radar can operate without physical contact with users' articulators\nand related body parts, offering several advantages for SSR. These advantages\ninclude high range resolution, high penetrability, low power consumption,\nrobustness to external light or sound interference, and the ability to be\nembedded in space-constrained handheld devices. This study demonstrated IR-UWB\nradar-based contactless SSR using four types of speech stimuli (vowels,\nconsonants, words, and phrases). To achieve this, a novel speech feature\nextraction algorithm specifically designed for IR-UWB radar-based SSR is\nproposed. Each speech stimulus is recognized by applying a classification\nalgorithm to the extracted speech features. Two different algorithms,\nmultidimensional dynamic time warping (MD-DTW) and deep neural network-hidden\nMarkov model (DNN-HMM), were compared for the classification task.\nAdditionally, a favorable radar antenna position, either in front of the user's\nlips or below the user's chin, was determined to achieve higher recognition\naccuracy. Experimental results demonstrated the efficacy of the proposed speech\nfeature extraction algorithm combined with DNN-HMM for classifying vowels,\nconsonants, words, and phrases. Notably, this study represents the first\ndemonstration of phoneme-level SSR using contactless radar.\n","authors":["Sunghwa Lee","Younghoon Shin","Myungjong Kim","Jiwon Seo"],"pdf_url":"https://arxiv.org/pdf/2312.09572v1.pdf","comment":"Submitted to IEEE Access"},{"id":"http://arxiv.org/abs/2312.09571v1","updated":"2023-12-15T07:04:33Z","published":"2023-12-15T07:04:33Z","title":"Extending Context Window of Large Language Models via Semantic\n  Compression","summary":"  Transformer-based Large Language Models (LLMs) often impose limitations on\nthe length of the text input to ensure the generation of fluent and relevant\nresponses. This constraint restricts their applicability in scenarios involving\nlong texts. We propose a novel semantic compression method that enables\ngeneralization to texts that are 6-8 times longer, without incurring\nsignificant computational costs or requiring fine-tuning. Our proposed\nframework draws inspiration from source coding in information theory and\nemploys a pre-trained model to reduce the semantic redundancy of long inputs\nbefore passing them to the LLMs for downstream tasks. Experimental results\ndemonstrate that our method effectively extends the context window of LLMs\nacross a range of tasks including question answering, summarization, few-shot\nlearning, and information retrieval. Furthermore, the proposed semantic\ncompression method exhibits consistent fluency in text generation while\nreducing the associated computational overhead.\n","authors":["Weizhi Fei","Xueyan Niu","Pingyi Zhou","Lu Hou","Bo Bai","Lei Deng","Wei Han"],"pdf_url":"https://arxiv.org/pdf/2312.09571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.15539v2","updated":"2023-12-15T06:40:41Z","published":"2023-10-24T06:04:28Z","title":"SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code\n  Translation","summary":"  With the recent focus on Large Language Models (LLMs), both StarCoder (Li et\nal., 2023) and Code Llama (Rozi\\`ere et al., 2023) have demonstrated remarkable\nperformance in code generation. However, there is still a need for improvement\nin code translation functionality with efficient training techniques. In\nresponse to this, we introduce SteloCoder, a decoder-only StarCoder-based LLM\ndesigned specifically for multi-programming language-to-Python code\ntranslation. In particular, SteloCoder achieves C++, C#, JavaScript, Java, or\nPHP-to-Python code translation without specifying the input programming\nlanguage. We modified StarCoder model architecture by incorporating a\nMixture-of-Experts (MoE) technique featuring five experts and a gating network\nfor multi-task handling. Experts are obtained by StarCoder fine-tuning.\nSpecifically, we use a Low-Rank Adaptive Method (LoRA) technique, limiting each\nexpert size as only 0.06% of number of StarCoder's parameters. At the same\ntime, to enhance training efficiency in terms of time, we adopt curriculum\nlearning strategy and use self-instruct data for efficient fine-tuning. As a\nresult, each expert takes only 6 hours to train on one single 80Gb A100 HBM.\nWith experiments on XLCoST datasets, SteloCoder achieves an average of 73.76\nCodeBLEU score in multi-programming language-to-Python translation, surpassing\nthe top performance from the leaderboard by at least 3.5. This accomplishment\nis attributed to only 45M extra parameters with StarCoder as the backbone and\n32 hours of valid training on one 80GB A100 HBM. The source code is release\nhere: https://github.com/sade-adrien/SteloCoder.\n","authors":["Jialing Pan","Adrien Sadé","Jin Kim","Eric Soriano","Guillem Sole","Sylvain Flamant"],"pdf_url":"https://arxiv.org/pdf/2310.15539v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.04308v2","updated":"2023-12-15T06:26:05Z","published":"2023-06-07T10:14:17Z","title":"Personality testing of GPT-3: Limited temporal reliability, but\n  highlighted social desirability of GPT-3's personality instruments results","summary":"  As AI-bots continue to gain popularity due to their human-like traits and the\nintimacy they offer to users, their societal impact inevitably expands. This\nleads to the rising necessity for comprehensive studies to fully understand\nAI-bots and reveal their potential opportunities, drawbacks, and overall\nsocietal impact. With that in mind, this research conducted an extensive\ninvestigation into ChatGPT3, a renowned AI bot, aiming to assess the temporal\nreliability of its personality profile. Psychological questionnaires were\nadministered to the chatbot on two separate occasions, followed by a comparison\nof the responses to human normative data. The findings revealed varying levels\nof agreement in chatbot's responses over time, with some scales displaying\nexcellent agreement while others demonstrated poor agreement. Overall,\nDavinci-003 displayed a socially desirable and pro-social personality profile,\nparticularly in the domain of communion. However, the underlying basis of the\nchatbot's responses-whether driven by conscious self reflection or\npredetermined algorithms-remains uncertain.\n","authors":["Bojana Bodroza","Bojana M. Dinic","Ljubisa Bojic"],"pdf_url":"https://arxiv.org/pdf/2306.04308v2.pdf","comment":"21 pages, 1 table"},{"id":"http://arxiv.org/abs/2312.09545v1","updated":"2023-12-15T05:40:15Z","published":"2023-12-15T05:40:15Z","title":"GPT-4 Surpassing Human Performance in Linguistic Pragmatics","summary":"  As Large Language Models (LLMs) become increasingly integrated into everyday\nlife, their capabilities to understand and emulate human cognition are under\nsteady examination. This study investigates the ability of LLMs to comprehend\nand interpret linguistic pragmatics, an aspect of communication that considers\ncontext and implied meanings. Using Grice's communication principles, LLMs and\nhuman subjects (N=76) were evaluated based on their responses to various\ndialogue-based tasks. The findings revealed the superior performance and speed\nof LLMs, particularly GPT4, over human subjects in interpreting pragmatics.\nGPT4 also demonstrated accuracy in the pre-testing of human-written samples,\nindicating its potential in text analysis. In a comparative analysis of LLMs\nusing human individual and average scores, the models exhibited significant\nchronological improvement. The models were ranked from lowest to highest score,\nwith GPT2 positioned at 78th place, GPT3 ranking at 23rd, Bard at 10th, GPT3.5\nplacing 5th, Best Human scoring 2nd, and GPT4 achieving the top spot. The\nfindings highlight the remarkable progress made in the development and\nperformance of these LLMs. Future studies should consider diverse subjects,\nmultiple languages, and other cognitive aspects to fully comprehend the\ncapabilities of LLMs. This research holds significant implications for the\ndevelopment and application of AI-based models in communication-centered\nsectors.\n","authors":["Ljubisa Bojic","Predrag Kovacevic","Milan Cabarkapa"],"pdf_url":"https://arxiv.org/pdf/2312.09545v1.pdf","comment":"19 pages, 1 figure, 2 tables"},{"id":"http://arxiv.org/abs/2312.09542v1","updated":"2023-12-15T05:30:14Z","published":"2023-12-15T05:30:14Z","title":"Marathon: A Race Through the Realm of Long Context with Large Language\n  Models","summary":"  Although there are currently many benchmarks available for evaluating the\nlong context understanding and reasoning capability of large language models,\nwith the expansion of the context window in these models, the existing long\ncontext benchmarks are no longer sufficient for evaluating the long context\nunderstanding and reasoning capability of large language models. In this paper,\nwe have developed a fresh long context evaluation benchmark, which we name it\nMarathon in the form of multiple choice questions, inspired by benchmarks such\nas MMLU, for assessing the long context comprehension capability of large\nlanguage models quickly, accurately, and objectively. We have evaluated several\nof the latest and most popular large language models, as well as three recent\nand effective long context optimization methods, on our benchmark. This\nshowcases the long context reasoning and comprehension capabilities of these\nlarge language models and validates the effectiveness of these optimization\nmethods. Marathon is available at\nhttps://huggingface.co/datasets/Lemoncoke/Marathon.\n","authors":["Lei Zhang","Yunshui Li","Ziqiang Liu","Jiaxi yang","Junhao Liu","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2312.09542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09541v1","updated":"2023-12-15T05:27:24Z","published":"2023-12-15T05:27:24Z","title":"Picking the Underused Heads: A Network Pruning Perspective of Attention\n  Head Selection for Fusing Dialogue Coreference Information","summary":"  The Transformer-based models with the multi-head self-attention mechanism are\nwidely used in natural language processing, and provide state-of-the-art\nresults. While the pre-trained language backbones are shown to implicitly\ncapture certain linguistic knowledge, explicitly incorporating structure-aware\nfeatures can bring about further improvement on the downstream tasks. However,\nsuch enhancement often requires additional neural components and increases\ntraining parameter size. In this work, we investigate the attention head\nselection and manipulation strategy for feature injection from a network\npruning perspective, and conduct a case study on dialogue summarization. We\nfirst rank attention heads in a Transformer-based summarizer with layer-wise\nimportance. We then select the underused heads through extensive analysis, and\ninject structure-aware features by manipulating the selected heads.\nExperimental results show that the importance-based head selection is effective\nfor feature injection, and dialogue summarization can be improved by\nincorporating coreference information via head manipulation.\n","authors":["Zhengyuan Liu","Nancy F. Chen"],"pdf_url":"https://arxiv.org/pdf/2312.09541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07597v2","updated":"2023-12-15T05:18:11Z","published":"2023-09-14T10:57:50Z","title":"C-Pack: Packaged Resources To Advance General Chinese Embedding","summary":"  We introduce C-Pack, a package of resources that significantly advance the\nfield of general Chinese embeddings. C-Pack includes three critical resources.\n1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6\ntasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated\nfrom labeled and unlabeled Chinese corpora for training embedding models. 3)\nC-TEM is a family of embedding models covering multiple sizes. Our models\noutperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the\ntime of the release. We also integrate and optimize the entire suite of\ntraining methods for C-TEM. Along with our resources on general Chinese\nembedding, we release our data and models for English text embeddings. The\nEnglish models achieve state-of-the-art performance on MTEB benchmark;\nmeanwhile, our released English data is 2 times larger than the Chinese data.\nAll these resources are made publicly available at\nhttps://github.com/FlagOpen/FlagEmbedding.\n","authors":["Shitao Xiao","Zheng Liu","Peitian Zhang","Niklas Muennighoff"],"pdf_url":"https://arxiv.org/pdf/2309.07597v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09536v1","updated":"2023-12-15T05:03:24Z","published":"2023-12-15T05:03:24Z","title":"Riveter: Measuring Power and Social Dynamics Between Entities","summary":"  Riveter provides a complete easy-to-use pipeline for analyzing verb\nconnotations associated with entities in text corpora. We prepopulate the\npackage with connotation frames of sentiment, power, and agency, which have\ndemonstrated usefulness for capturing social phenomena, such as gender bias, in\na broad range of corpora. For decades, lexical frameworks have been\nfoundational tools in computational social science, digital humanities, and\nnatural language processing, facilitating multifaceted analysis of text\ncorpora. But working with verb-centric lexica specifically requires natural\nlanguage processing skills, reducing their accessibility to other researchers.\nBy organizing the language processing pipeline, providing complete lexicon\nscores and visualizations for all entities in a corpus, and providing\nfunctionality for users to target specific research questions, Riveter greatly\nimproves the accessibility of verb lexica and can facilitate a broad range of\nfuture research.\n","authors":["Maria Antoniak","Anjalie Field","Jimin Mun","Melanie Walsh","Lauren F. Klein","Maarten Sap"],"pdf_url":"https://arxiv.org/pdf/2312.09536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.14132v3","updated":"2023-12-15T04:13:45Z","published":"2023-07-26T11:59:14Z","title":"CIF-T: A Novel CIF-based Transducer Architecture for Automatic Speech\n  Recognition","summary":"  RNN-T models are widely used in ASR, which rely on the RNN-T loss to achieve\nlength alignment between input audio and target sequence. However, the\nimplementation complexity and the alignment-based optimization target of RNN-T\nloss lead to computational redundancy and a reduced role for predictor network,\nrespectively. In this paper, we propose a novel model named CIF-Transducer\n(CIF-T) which incorporates the Continuous Integrate-and-Fire (CIF) mechanism\nwith the RNN-T model to achieve efficient alignment. In this way, the RNN-T\nloss is abandoned, thus bringing a computational reduction and allowing the\npredictor network a more significant role. We also introduce Funnel-CIF,\nContext Blocks, Unified Gating and Bilinear Pooling joint network, and\nauxiliary training strategy to further improve performance. Experiments on the\n178-hour AISHELL-1 and 10000-hour WenetSpeech datasets show that CIF-T achieves\nstate-of-the-art results with lower computational overhead compared to RNN-T\nmodels.\n","authors":["Tian-Hao Zhang","Dinghao Zhou","Guiping Zhong","Jiaming Zhou","Baoxiang Li"],"pdf_url":"https://arxiv.org/pdf/2307.14132v3.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.09508v1","updated":"2023-12-15T03:19:53Z","published":"2023-12-15T03:19:53Z","title":"IndicIRSuite: Multilingual Dataset and Neural Information Models for\n  Indian Languages","summary":"  In this paper, we introduce Neural Information Retrieval resources for 11\nwidely spoken Indian Languages (Assamese, Bengali, Gujarati, Hindi, Kannada,\nMalayalam, Marathi, Oriya, Punjabi, Tamil, and Telugu) from two major Indian\nlanguage families (Indo-Aryan and Dravidian). These resources include (a)\nINDIC-MARCO, a multilingual version of the MSMARCO dataset in 11 Indian\nLanguages created using Machine Translation, and (b) Indic-ColBERT, a\ncollection of 11 distinct Monolingual Neural Information Retrieval models, each\ntrained on one of the 11 languages in the INDIC-MARCO dataset. To the best of\nour knowledge, IndicIRSuite is the first attempt at building large-scale Neural\nInformation Retrieval resources for a large number of Indian languages, and we\nhope that it will help accelerate research in Neural IR for Indian Languages.\nExperiments demonstrate that Indic-ColBERT achieves 47.47% improvement in the\nMRR@10 score averaged over the INDIC-MARCO baselines for all 11 Indian\nlanguages except Oriya, 12.26% improvement in the NDCG@10 score averaged over\nthe MIRACL Bengali and Hindi Language baselines, and 20% improvement in the\nMRR@100 Score over the Mr.Tydi Bengali Language baseline. IndicIRSuite is\navailable at https://github.com/saifulhaq95/IndicIRSuite\n","authors":["Saiful Haq","Ashutosh Sharma","Pushpak Bhattacharyya"],"pdf_url":"https://arxiv.org/pdf/2312.09508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.01528v4","updated":"2023-12-15T03:08:59Z","published":"2021-11-02T12:10:58Z","title":"Effective and Imperceptible Adversarial Textual Attack via\n  Multi-objectivization","summary":"  The field of adversarial textual attack has significantly grown over the last\nfew years, where the commonly considered objective is to craft adversarial\nexamples (AEs) that can successfully fool the target model. However, the\nimperceptibility of attacks, which is also essential for practical attackers,\nis often left out by previous studies. In consequence, the crafted AEs tend to\nhave obvious structural and semantic differences from the original\nhuman-written text, making them easily perceptible. In this work, we advocate\nleveraging multi-objectivization to address such issue. Specifically, we\nreformulate the problem of crafting AEs as a multi-objective optimization\nproblem, where the attack imperceptibility is considered as an auxiliary\nobjective. Then, we propose a simple yet effective evolutionary algorithm,\ndubbed HydraText, to solve this problem. To the best of our knowledge,\nHydraText is currently the only approach that can be effectively applied to\nboth score-based and decision-based attack settings. Exhaustive experiments\ninvolving 44237 instances demonstrate that HydraText consistently achieves\ncompetitive attack success rates and better attack imperceptibility than the\nrecently proposed attack approaches. A human evaluation study also shows that\nthe AEs crafted by HydraText are more indistinguishable from human-written\ntext. Finally, these AEs exhibit good transferability and can bring notable\nrobustness improvement to the target model by adversarial training.\n","authors":["Shengcai Liu","Ning Lu","Wenjing Hong","Chao Qian","Ke Tang"],"pdf_url":"https://arxiv.org/pdf/2111.01528v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04103v2","updated":"2023-12-15T03:04:04Z","published":"2023-12-07T07:37:15Z","title":"Enhancing the Rationale-Input Alignment for Self-explaining\n  Rationalization","summary":"  Rationalization empowers deep learning models with self-explaining\ncapabilities through a cooperative game, where a generator selects a\nsemantically consistent subset of the input as a rationale, and a subsequent\npredictor makes predictions based on the selected rationale. In this paper, we\ndiscover that rationalization is prone to a problem named \\emph{rationale\nshift}, which arises from the algorithmic bias of the cooperative game.\nRationale shift refers to a situation where the semantics of the selected\nrationale may deviate from the original input, but the predictor still produces\naccurate predictions based on the deviation, resulting in a compromised\ngenerator with misleading feedback.\n  To address this issue, we first demonstrate the importance of the alignment\nbetween the rationale and the full input through both empirical observations\nand theoretical analysis. Subsequently, we introduce a novel approach called\nDAR (\\textbf{D}iscriminatively \\textbf{A}ligned \\textbf{R}ationalization),\nwhich utilizes an auxiliary module pretrained on the full input to\ndiscriminatively align the selected rationale and the original input. We\ntheoretically illustrate how DAR accomplishes the desired alignment, thereby\novercoming the rationale shift problem. The experiments on two widely used\nreal-world benchmarks show that the proposed method significantly improves the\nexplanation quality (measured by the overlap between the model-selected\nexplanation and the human-annotated rationale) as compared to state-of-the-art\ntechniques. Additionally, results on two synthetic settings further validate\nthe effectiveness of DAR in addressing the rationale shift problem.\n","authors":["Wei Liu","Haozhao Wang","Jun Wang","Zhiying Deng","YuanKai Zhang","Cheng Wang","Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2312.04103v2.pdf","comment":"Accept at ICDE 2024"},{"id":"http://arxiv.org/abs/2310.08903v2","updated":"2023-12-15T03:03:16Z","published":"2023-10-13T07:18:53Z","title":"SeqXGPT: Sentence-Level AI-Generated Text Detection","summary":"  Widely applied large language models (LLMs) can generate human-like content,\nraising concerns about the abuse of LLMs. Therefore, it is important to build\nstrong AI-generated text (AIGT) detectors. Current works only consider\ndocument-level AIGT detection, therefore, in this paper, we first introduce a\nsentence-level detection challenge by synthesizing a dataset that contains\ndocuments that are polished with LLMs, that is, the documents contain sentences\nwritten by humans and sentences modified by LLMs. Then we propose\n\\textbf{Seq}uence \\textbf{X} (Check) \\textbf{GPT}, a novel method that utilizes\nlog probability lists from white-box LLMs as features for sentence-level AIGT\ndetection. These features are composed like \\textit{waves} in speech processing\nand cannot be studied by LLMs. Therefore, we build SeqXGPT based on convolution\nand self-attention networks. We test it in both sentence and document-level\ndetection challenges. Experimental results show that previous methods struggle\nin solving sentence-level AIGT detection, while our method not only\nsignificantly surpasses baseline methods in both sentence and document-level\ndetection challenges but also exhibits strong generalization capabilities.\n","authors":["Pengyu Wang","Linyang Li","Ke Ren","Botian Jiang","Dong Zhang","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2310.08903v2.pdf","comment":"Accepted by EMNLP2023"},{"id":"http://arxiv.org/abs/2312.09494v1","updated":"2023-12-15T02:42:05Z","published":"2023-12-15T02:42:05Z","title":"No-Skim: Towards Efficiency Robustness Evaluation on Skimming-based\n  Language Models","summary":"  To reduce the computation cost and the energy consumption in large language\nmodels (LLM), skimming-based acceleration dynamically drops unimportant tokens\nof the input sequence progressively along layers of the LLM while preserving\nthe tokens of semantic importance. However, our work for the first time reveals\nthe acceleration may be vulnerable to Denial-of-Service (DoS) attacks. In this\npaper, we propose No-Skim, a general framework to help the owners of\nskimming-based LLM to understand and measure the robustness of their\nacceleration scheme. Specifically, our framework searches minimal and\nunnoticeable perturbations at character-level and token-level to generate\nadversarial inputs that sufficiently increase the remaining token ratio, thus\nincreasing the computation cost and energy consumption. We systematically\nevaluate the vulnerability of the skimming acceleration in various LLM\narchitectures including BERT and RoBERTa on the GLUE benchmark. In the worst\ncase, the perturbation found by No-Skim substantially increases the running\ncost of LLM by over 145% on average. Moreover, No-Skim extends the evaluation\nframework to various scenarios, making the evaluation conductible with\ndifferent level of knowledge.\n","authors":["Shengyao Zhang","Mi Zhang","Xudong Pan","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2312.09494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18396v3","updated":"2023-12-15T02:03:10Z","published":"2023-05-28T13:08:13Z","title":"LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly\n  Transformers","summary":"  The community explored to build private inference frameworks for\ntransformer-based large language models (LLMs) in a server-client setting,\nwhere the server holds the model parameters and the client inputs its private\ndata (or prompt) for inference. However, these frameworks impose significant\noverhead when the private inputs are forward propagated through the original\nLLMs. In this paper, we show that substituting the computation- and\ncommunication-heavy operators in the transformer architecture with\nprivacy-computing friendly approximations can greatly reduce the private\ninference costs while incurring very minor impact on model performance.\nCompared to state-of-the-art Iron (NeurIPS 2022), our privacy-computing\nfriendly model inference pipeline achieves a $5\\times$ acceleration in\ncomputation and an 80% reduction in communication overhead, while retaining\nnearly identical accuracy.\n","authors":["Xuanqi Liu","Zhuotao Liu"],"pdf_url":"https://arxiv.org/pdf/2305.18396v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08688v2","updated":"2023-12-15T01:42:20Z","published":"2023-12-14T07:05:42Z","title":"TigerBot: An Open Multilingual Multitask LLM","summary":"  We release and introduce the TigerBot family of large language models (LLMs),\nconsisting of base and chat models, sized from 7, 13, 70 and 180 billion\nparameters. We develop our models embarking from Llama-2 and BLOOM, and push\nthe boundary further in data, training algorithm, infrastructure, and\napplication tools. Our models yield meaningful performance gain over SOTA\nopen-source models, e.g., Llama-2, specifically 6% gain in English and 20% gain\nin Chinese. TigerBot model family also achieves leading performance in major\nacademic and industrial benchmarks and leaderboards. We believe that TigerBot\nrepresents just a snapshot of lightning-fast progression in LLM open-source\ncommunity. Therefore, we are thrilled to give back by publicly releasing our\nmodels and reporting our approach behind, with additional emphases on building\nSOTA LLMs in a democratized way and making LLMs of use in real-world\napplications.\n","authors":["Ye Chen","Wei Cai","Liangmin Wu","Xiaowei Li","Zhanxuan Xin","Cong Fu"],"pdf_url":"https://arxiv.org/pdf/2312.08688v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08282v2","updated":"2023-12-15T01:28:10Z","published":"2023-12-13T16:57:31Z","title":"Prompting LLMs with content plans to enhance the summarization of\n  scientific articles","summary":"  This paper presents novel prompting techniques to improve the performance of\nautomatic summarization systems for scientific articles. Scientific article\nsummarization is highly challenging due to the length and complexity of these\ndocuments. We conceive, implement, and evaluate prompting techniques that\nprovide additional contextual information to guide summarization systems.\nSpecifically, we feed summarizers with lists of key terms extracted from\narticles, such as author keywords or automatically generated keywords. Our\ntechniques are tested with various summarization models and input texts.\nResults show performance gains, especially for smaller models summarizing\nsections separately. This evidences that prompting is a promising approach to\novercoming the limitations of less powerful systems. Our findings introduce a\nnew research direction of using prompts to aid smaller models.\n","authors":["Aldan Creo","Manuel Lama","Juan C. Vidal"],"pdf_url":"https://arxiv.org/pdf/2312.08282v2.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2312.09411v1","updated":"2023-12-15T00:22:55Z","published":"2023-12-15T00:22:55Z","title":"OTOv3: Automatic Architecture-Agnostic Neural Network Training and\n  Compression from Structured Pruning to Erasing Operators","summary":"  Compressing a predefined deep neural network (DNN) into a compact sub-network\nwith competitive performance is crucial in the efficient machine learning\nrealm. This topic spans various techniques, from structured pruning to neural\narchitecture search, encompassing both pruning and erasing operators\nperspectives. Despite advancements, existing methods suffers from complex,\nmulti-stage processes that demand substantial engineering and domain knowledge,\nlimiting their broader applications. We introduce the third-generation\nOnly-Train-Once (OTOv3), which first automatically trains and compresses a\ngeneral DNN through pruning and erasing operations, creating a compact and\ncompetitive sub-network without the need of fine-tuning. OTOv3 simplifies and\nautomates the training and compression process, minimizes the engineering\nefforts required from users. It offers key technological advancements: (i)\nautomatic search space construction for general DNNs based on dependency graph\nanalysis; (ii) Dual Half-Space Projected Gradient (DHSPG) and its enhanced\nversion with hierarchical search (H2SPG) to reliably solve (hierarchical)\nstructured sparsity problems and ensure sub-network validity; and (iii)\nautomated sub-network construction using solutions from DHSPG/H2SPG and\ndependency graphs. Our empirical results demonstrate the efficacy of OTOv3\nacross various benchmarks in structured pruning and neural architecture search.\nOTOv3 produces sub-networks that match or exceed the state-of-the-arts. The\nsource code will be available at https://github.com/tianyic/only_train_once.\n","authors":["Tianyi Chen","Tianyu Ding","Zhihui Zhu","Zeyu Chen","HsiangTao Wu","Ilya Zharkov","Luming Liang"],"pdf_url":"https://arxiv.org/pdf/2312.09411v1.pdf","comment":"39 pages. Due to the page dim limitation, the full appendix is\n  attached here https://tinyurl.com/otov3appendix. Recommend to zoom-in for\n  finer details. arXiv admin note: text overlap with arXiv:2305.18030"},{"id":"http://arxiv.org/abs/2312.10259v1","updated":"2023-12-15T23:19:42Z","published":"2023-12-15T23:19:42Z","title":"CRNNet: Copy Recurrent Neural Network Structure Network","summary":"  The target of Electronic Health Record (EHR) coding is to find the diagnostic\ncodes according to the EHRs. In previous research, researchers have preferred\nto do multi-classification on the EHR coding task; most of them encode the EHR\nfirst and then process it to get the probability of each code based on the EHR\nrepresentation. However, the question of complicating diseases is neglected\namong all these methods. In this paper, we propose a novel EHR coding\nframework, which is the first attempt at detecting complicating diseases,\ncalled Copy Recurrent Neural Network Structure Network (CRNNet). This method\nrefers to the idea of adversarial learning; a Path Generator and a Path\nDiscriminator are designed to more efficiently finish the task of EHR coding.\nWe propose a copy module to detect complicating diseases; by the proposed copy\nmodule and the adversarial learning strategy, we identify complicating diseases\nefficiently. Extensive experiments show that our method achieves a 57.30\\%\nratio of complicating diseases in predictions, demonstrating the effectiveness\nof our proposed model. According to the ablation study, the proposed copy\nmechanism plays a crucial role in detecting complicating diseases.\n","authors":["Xiaofan Zhou","Xunzhu Tang"],"pdf_url":"https://arxiv.org/pdf/2312.10259v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2305.13250"},{"id":"http://arxiv.org/abs/2312.10253v1","updated":"2023-12-15T23:11:45Z","published":"2023-12-15T23:11:45Z","title":"Catwalk: A Unified Language Model Evaluation Framework for Many Datasets","summary":"  The success of large language models has shifted the evaluation paradigms in\nnatural language processing (NLP). The community's interest has drifted towards\ncomparing NLP models across many tasks, domains, and datasets, often at an\nextreme scale. This imposes new engineering challenges: efforts in constructing\ndatasets and models have been fragmented, and their formats and interfaces are\nincompatible. As a result, it often takes extensive (re)implementation efforts\nto make fair and controlled comparisons at scale.\n  Catwalk aims to address these issues. Catwalk provides a unified interface to\na broad range of existing NLP datasets and models, ranging from both canonical\nsupervised training and fine-tuning, to more modern paradigms like in-context\nlearning. Its carefully-designed abstractions allow for easy extensions to many\nothers. Catwalk substantially lowers the barriers to conducting controlled\nexperiments at scale. For example, we finetuned and evaluated over 64 models on\nover 86 datasets with a single command, without writing any code. Maintained by\nthe AllenNLP team at the Allen Institute for Artificial Intelligence (AI2),\nCatwalk is an ongoing open-source effort: https://github.com/allenai/catwalk.\n","authors":["Dirk Groeneveld","Anas Awadalla","Iz Beltagy","Akshita Bhagia","Ian Magnusson","Hao Peng","Oyvind Tafjord","Pete Walsh","Kyle Richardson","Jesse Dodge"],"pdf_url":"https://arxiv.org/pdf/2312.10253v1.pdf","comment":"technical report, work in progress"},{"id":"http://arxiv.org/abs/2312.07492v3","updated":"2023-12-15T22:30:56Z","published":"2023-12-12T18:27:44Z","title":"SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in\n  Generative Language Models","summary":"  Current datasets for unwanted social bias auditing are limited to studying\nprotected demographic features such as race and gender. In this work, we\nintroduce a comprehensive benchmark that is meant to capture the amplification\nof social bias, via stigmas, in generative language models. Taking inspiration\nfrom social science research, we start with a documented list of 93 US-centric\nstigmas and curate a question-answering (QA) dataset which involves simple\nsocial situations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts,\nwith a variety of prompt styles, carefully constructed to systematically test\nfor both social bias and model robustness. We present results for\nSocialStigmaQA with two open source generative language models and we find that\nthe proportion of socially biased output ranges from 45% to 59% across a\nvariety of decoding strategies and prompting styles. We demonstrate that the\ndeliberate design of the templates in our benchmark (e.g., adding biasing text\nto the prompt or using different verbs that change the answer that indicates\nbias) impacts the model tendencies to generate socially biased output.\nAdditionally, through manual evaluation, we discover problematic patterns in\nthe generated chain-of-thought output that range from subtle bias to lack of\nreasoning.\n  Warning: This paper contains examples of text which are toxic, biased, and\npotentially harmful.\n","authors":["Manish Nagireddy","Lamogha Chiazor","Moninder Singh","Ioana Baldini"],"pdf_url":"https://arxiv.org/pdf/2312.07492v3.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2312.10210v1","updated":"2023-12-15T21:09:34Z","published":"2023-12-15T21:09:34Z","title":"VK-G2T: Vision and Context Knowledge enhanced Gloss2Text","summary":"  Existing sign language translation methods follow a two-stage pipeline: first\nconverting the sign language video to a gloss sequence (i.e. Sign2Gloss) and\nthen translating the generated gloss sequence into a spoken language sentence\n(i.e. Gloss2Text). While previous studies have focused on boosting the\nperformance of the Sign2Gloss stage, we emphasize the optimization of the\nGloss2Text stage. However, this task is non-trivial due to two distinct\nfeatures of Gloss2Text: (1) isolated gloss input and (2) low-capacity gloss\nvocabulary. To address these issues, we propose a vision and context knowledge\nenhanced Gloss2Text model, named VK-G2T, which leverages the visual content of\nthe sign language video to learn the properties of the target sentence and\nexploit the context knowledge to facilitate the adaptive translation of gloss\nwords. Extensive experiments conducted on a Chinese benchmark validate the\nsuperiority of our model.\n","authors":["Liqiang Jing","Xuemeng Song","Xinxing Zu","Na Zheng","Zhongzhou Zhao","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2312.10210v1.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.10202v1","updated":"2023-12-15T20:59:17Z","published":"2023-12-15T20:59:17Z","title":"Low-resource classification of mobility functioning information in\n  clinical sentences using large language models","summary":"  Objective: Function is increasingly recognized as an important indicator of\nwhole-person health. This study evaluates the ability of publicly available\nlarge language models (LLMs) to accurately identify the presence of functioning\ninformation from clinical notes. We explore various strategies to improve the\nperformance on this task. Materials and Methods: We collect a balanced binary\nclassification dataset of 1000 sentences from the Mobility NER dataset, which\nwas curated from n2c2 clinical notes. For evaluation, we construct zero-shot\nand few-shot prompts to query the LLMs whether a given sentence contains\nmobility functioning information. Two sampling techniques, random sampling and\nk-nearest neighbor (kNN)-based sampling, are used to select the few-shot\nexamples. Furthermore, we apply a parameter-efficient prompt-based fine-tuning\nmethod to the LLMs and evaluate their performance under various training\nsettings. Results: Flan-T5-xxl outperforms all other models in both zero-shot\nand few-shot settings, achieving a F1 score of 0.865 with a single\ndemonstrative example selected by kNN sampling. In prompt-based fine-tuning\nexperiments, this foundation model also demonstrates superior performance\nacross all low-resource settings, particularly achieving an impressive F1 score\nof 0.922 using the full training dataset. The smaller model, Flan-T5-xl,\nrequires fine-tuning with only 2.3M additional parameters to achieve comparable\nperformance to the fully fine-tuned Gatortron-base model, both surpassing 0.9\nF1 score. Conclusion: Open-source instruction-tuned LLMs demonstrate impressive\nin-context learning capability in the mobility functioning classification task.\nThe performance of these models can be further improved by continuing\nfine-tuning on a task-specific dataset.\n","authors":["Tuan Dung Le","Thanh Duong","Thanh Thieu"],"pdf_url":"https://arxiv.org/pdf/2312.10202v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10185v1","updated":"2023-12-15T20:21:45Z","published":"2023-12-15T20:21:45Z","title":"Student as an Inherent Denoiser of Noisy Teacher","summary":"  Knowledge distillation (KD) has been widely employed to transfer knowledge\nfrom a large language model (LLM) to a specialized model in low-data regimes\nthrough pseudo label learning. However, pseudo labels generated by teacher\nmodels are usually noisy and may influence KD performance. This study delves\ninto KD with noisy teachers and uncovers that the student model can already\ngenerate more accurate predictions than the teacher labels used to train it\nduring KD, indicating its inherent ability to denoise noisy teacher labels.\nMotivated by this finding, we propose Peer-Advised KD to improve vanilla KD\nfrom noisy teachers. Experiments show that Peer-Advised KD can outperform LLM\nby approximately 5% with 50 human-labeled data, and even competitive to\nstandard supervised finetuning with 750 human-labeled data.\n","authors":["Jiachen Zhao"],"pdf_url":"https://arxiv.org/pdf/2312.10185v1.pdf","comment":"The Third NeurIPS Workshop on Efficient Natural Language and Speech\n  Processing"},{"id":"http://arxiv.org/abs/2312.10171v1","updated":"2023-12-15T19:43:41Z","published":"2023-12-15T19:43:41Z","title":"Pipeline and Dataset Generation for Automated Fact-checking in Almost\n  Any Language","summary":"  This article presents a pipeline for automated fact-checking leveraging\npublicly available Language Models and data. The objective is to assess the\naccuracy of textual claims using evidence from a ground-truth evidence corpus.\nThe pipeline consists of two main modules -- the evidence retrieval and the\nclaim veracity evaluation. Our primary focus is on the ease of deployment in\nvarious languages that remain unexplored in the field of automated\nfact-checking. Unlike most similar pipelines, which work with evidence\nsentences, our pipeline processes data on a paragraph level, simplifying the\noverall architecture and data requirements. Given the high cost of annotating\nlanguage-specific fact-checking training data, our solution builds on the\nQuestion Answering for Claim Generation (QACG) method, which we adapt and use\nto generate the data for all models of the pipeline. Our strategy enables the\nintroduction of new languages through machine translation of only two fixed\ndatasets of moderate size. Subsequently, any number of training samples can be\ngenerated based on an evidence corpus in the target language. We provide open\naccess to all data and fine-tuned models for Czech, English, Polish, and Slovak\npipelines, as well as to our codebase that may be used to reproduce the\nresults.We comprehensively evaluate the pipelines for all four languages,\nincluding human annotations and per-sample difficulty assessment using\nPointwise V-information. The presented experiments are based on full Wikipedia\nsnapshots to promote reproducibility. To facilitate implementation and user\ninteraction, we develop the FactSearch application featuring the proposed\npipeline and the preliminary feedback on its performance.\n","authors":["Jan Drchal","Herbert Ullrich","Tomáš Mlynář","Václav Moravec"],"pdf_url":"https://arxiv.org/pdf/2312.10171v1.pdf","comment":"submitted to NCAA journal for review"},{"id":"http://arxiv.org/abs/2312.10169v1","updated":"2023-12-15T19:31:00Z","published":"2023-12-15T19:31:00Z","title":"Review of Unsupervised POS Tagging and Its Implications on Language\n  Acquisition","summary":"  An ability that underlies human syntactic knowledge is determining which\nwords can appear in the similar structures (i.e. grouping words by their\nsyntactic categories). These groupings enable humans to combine structures in\norder to communicate complex meanings. A foundational question is how do\nchildren acquire this ability underlying syntactic knowledge. In exploring this\nprocess, we will review various engineering approaches whose goal is similar to\nthat of a child's -- without prior syntactic knowledge, correctly identify the\nparts of speech (POS) of the words in a sample of text. In reviewing these\nunsupervised tagging efforts, we will discuss common themes that support the\nadvances in the models and their relevance for language acquisition. For\nexample, we discuss how each model judges success (evaluation metrics), the\n\"additional information\" that constrains the POS learning (such as orthographic\ninformation), and the context used to determine POS (only previous word, words\nbefore and after the target, etc). The identified themes pave the way for\nfuture investigations into the cognitive processes that underpin the\nacquisition of syntactic categories and provide a useful layout of current\nstate of the art unsupervised POS tagging models.\n","authors":["Niels Dickson"],"pdf_url":"https://arxiv.org/pdf/2312.10169v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10160v1","updated":"2023-12-15T19:16:21Z","published":"2023-12-15T19:16:21Z","title":"Do LVLMs Understand Charts? Analyzing and Correcting Factual Errors in\n  Chart Captioning","summary":"  Recent advancements in large vision-language models (LVLMs) have led to\nsignificant progress in generating natural language descriptions for visual\ncontent and thus enhancing various applications. One issue with these powerful\nmodels is that they sometimes produce texts that are factually inconsistent\nwith the visual input. While there has been some effort to mitigate such\ninconsistencies in natural image captioning, the factuality of generated\ncaptions for structured document images, such as charts, has not received as\nmuch scrutiny, posing a potential threat to information reliability in critical\napplications. This work delves into the factuality aspect by introducing a\ncomprehensive typology of factual errors in generated chart captions. A\nlarge-scale human annotation effort provides insight into the error patterns\nand frequencies in captions crafted by various chart captioning models,\nultimately forming the foundation of a novel dataset, CHOCOLATE. Our analysis\nreveals that even state-of-the-art models, including GPT-4V, frequently produce\ncaptions laced with factual inaccuracies. In response to this challenge, we\nestablish the new task of Chart Caption Factual Error Correction and introduce\nCHARTVE, a model for visual entailment that outperforms proprietary and\nopen-source LVLMs in evaluating factual consistency. Furthermore, we propose\nC2TFEC, an interpretable two-stage framework that excels at correcting factual\nerrors. This work inaugurates a new domain in factual error correction for\nchart captions, presenting a novel evaluation mechanism, and demonstrating an\neffective approach to ensuring the factuality of generated chart captions.\n","authors":["Kung-Hsiang Huang","Mingyang Zhou","Hou Pong Chan","Yi R. Fung","Zhenhailong Wang","Lingyu Zhang","Shih-Fu Chang","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2312.10160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10126v1","updated":"2023-12-15T14:26:06Z","published":"2023-12-15T14:26:06Z","title":"Do Text Simplification Systems Preserve Meaning? A Human Evaluation via\n  Reading Comprehension","summary":"  Automatic text simplification (TS) aims to automate the process of rewriting\ntext to make it easier for people to read. A pre-requisite for TS to be useful\nis that it should convey information that is consistent with the meaning of the\noriginal text. However, current TS evaluation protocols assess system outputs\nfor simplicity and meaning preservation without regard for the document context\nin which output sentences occur and for how people understand them. In this\nwork, we introduce a human evaluation framework to assess whether simplified\ntexts preserve meaning using reading comprehension questions. With this\nframework, we conduct a thorough human evaluation of texts by humans and by\nnine automatic systems. Supervised systems that leverage pre-training knowledge\nachieve the highest scores on the reading comprehension (RC) tasks amongst the\nautomatic controllable TS systems. However, even the best-performing supervised\nsystem struggles with at least 14% of the questions, marking them as\n\"unanswerable'' based on simplified content. We further investigate how\nexisting TS evaluation metrics and automatic question-answering systems\napproximate the human judgments we obtained.\n","authors":["Sweta Agrawal","Marine Carpuat"],"pdf_url":"https://arxiv.org/pdf/2312.10126v1.pdf","comment":"Accepted at TACL (a pre-MIT Press publication version)"},{"id":"http://arxiv.org/abs/2307.01003v2","updated":"2023-12-15T10:09:58Z","published":"2023-07-03T13:37:00Z","title":"Visual Instruction Tuning with Polite Flamingo","summary":"  Recent research has demonstrated that the multi-task fine-tuning of\nmulti-modal Large Language Models (LLMs) using an assortment of annotated\ndownstream vision-language datasets significantly enhances their performance.\nYet, during this process, a side effect, which we termed as the \"multi-modal\nalignment tax\", surfaces. This side effect negatively impacts the model's\nability to format responses appropriately -- for instance, its \"politeness\" --\ndue to the overly succinct and unformatted nature of raw annotations, resulting\nin reduced human preference. In this paper, we introduce Polite Flamingo, a\nmulti-modal response rewriter that transforms raw annotations into a more\nappealing, \"polite\" format. Polite Flamingo is trained to reconstruct\nhigh-quality responses from their automatically distorted counterparts and is\nsubsequently applied to a vast array of vision-language datasets for response\nrewriting. After rigorous filtering, we generate the PF-1M dataset and further\nvalidate its value by fine-tuning a multi-modal LLM with it. Combined with\nnovel methodologies including U-shaped multi-stage tuning and multi-turn\naugmentation, the resulting model, Clever Flamingo, demonstrates its advantages\nin both multi-modal understanding and response politeness according to\nautomated and human evaluations.\n","authors":["Delong Chen","Jianfeng Liu","Wenliang Dai","Baoyuan Wang"],"pdf_url":"https://arxiv.org/pdf/2307.01003v2.pdf","comment":"In AAAI-24"},{"id":"http://arxiv.org/abs/2312.10104v1","updated":"2023-12-15T03:11:03Z","published":"2023-12-15T03:11:03Z","title":"ICD-LM: Configuring Vision-Language In-Context Demonstrations by\n  Language Modeling","summary":"  This paper studies how to configure powerful In-Context Demonstration (ICD)\nsequences for a Large Vision-Language Model (LVLM) to solve Vision-Language\ntasks through In-Context Learning (ICL). After observing that configuring an\nICD sequence is a mirror process of composing a sentence, i.e., just as a\nsentence can be composed word by word via a Language Model, an ICD sequence can\nalso be configured one by one. Consequently, we introduce an ICD Language Model\n(ICD-LM) specifically designed to generate effective ICD sequences. This\ninvolves creating a dataset of hand-crafted ICD sequences for various query\nsamples and using it to train the ICD-LM. Our approach, diverging from\ntraditional methods in NLP that select and order ICDs separately, enables to\nsimultaneously learn how to select and order ICDs, enhancing the effect of the\nsequences. Moreover, during data construction, we use the LVLM intended for ICL\nimplementation to validate the strength of each ICD sequence, resulting in a\nmodel-specific dataset and the ICD-LM trained by this dataset is also\nmodel-specific. We validate our methodology through experiments in Visual\nQuestion Answering and Image Captioning, confirming the viability of using a\nLanguage Model for ICD configuration. Our comprehensive ablation studies\nfurther explore the impact of various dataset construction and ICD-LM\ndevelopment settings on the outcomes. The code is given in\nhttps://github.com/ForJadeForest/ICD-LM.\n","authors":["Yingzhe Peng","Xu Yang","Haoxuan Ma","Shuo Xu","Chi Zhang","Yucheng Han","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.10104v1.pdf","comment":"14 pages, 6 figures"},{"id":"http://arxiv.org/abs/2312.10101v1","updated":"2023-12-15T00:34:52Z","published":"2023-12-15T00:34:52Z","title":"A Review of Repository Level Prompting for LLMs","summary":"  As coding challenges become more complex, recent advancements in Large\nLanguage Models (LLMs) have led to notable successes, such as achieving a\n94.6\\% solve rate on the HumanEval benchmark. Concurrently, there is an\nincreasing commercial push for repository-level inline code completion tools,\nsuch as GitHub Copilot and Tab Nine, aimed at enhancing developer productivity.\nThis paper delves into the transition from individual coding problems to\nrepository-scale solutions, presenting a thorough review of the current\nliterature on effective LLM prompting for code generation at the repository\nlevel. We examine approaches that will work with black-box LLMs such that they\nwill be useful and applicable to commercial use cases, and their applicability\nin interpreting code at a repository scale. We juxtapose the Repository-Level\nPrompt Generation technique with RepoCoder, an iterative retrieval and\ngeneration method, to highlight the trade-offs inherent in each approach and to\nestablish best practices for their application in cutting-edge coding\nbenchmarks. The interplay between iterative refinement of prompts and the\ndevelopment of advanced retrieval systems forms the core of our discussion,\noffering a pathway to significantly improve LLM performance in code generation\ntasks. Insights from this study not only guide the application of these methods\nbut also chart a course for future research to integrate such techniques into\nbroader software engineering contexts.\n","authors":["Douglas Schonholtz"],"pdf_url":"https://arxiv.org/pdf/2312.10101v1.pdf","comment":"15 figures/charts, 7 pages, Submitted as an NLP project at\n  Northeastern. Focuses on comparing two papers, there are many more papers\n  that could be included"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2312.10035v1","updated":"2023-12-15T18:59:59Z","published":"2023-12-15T18:59:59Z","title":"Point Transformer V3: Simpler, Faster, Stronger","summary":"  This paper is not motivated to seek innovation within the attention\nmechanism. Instead, it focuses on overcoming the existing trade-offs between\naccuracy and efficiency within the context of point cloud processing,\nleveraging the power of scale. Drawing inspiration from recent advances in 3D\nlarge-scale representation learning, we recognize that model performance is\nmore influenced by scale than by intricate design. Therefore, we present Point\nTransformer V3 (PTv3), which prioritizes simplicity and efficiency over the\naccuracy of certain mechanisms that are minor to the overall performance after\nscaling, such as replacing the precise neighbor search by KNN with an efficient\nserialized neighbor mapping of point clouds organized with specific patterns.\nThis principle enables significant scaling, expanding the receptive field from\n16 to 1024 points while remaining efficient (a 3x increase in processing speed\nand a 10x improvement in memory efficiency compared with its predecessor,\nPTv2). PTv3 attains state-of-the-art results on over 20 downstream tasks that\nspan both indoor and outdoor scenarios. Further enhanced with multi-dataset\njoint training, PTv3 pushes these results to a higher level.\n","authors":["Xiaoyang Wu","Li Jiang","Peng-Shuai Wang","Zhijian Liu","Xihui Liu","Yu Qiao","Wanli Ouyang","Tong He","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2312.10035v1.pdf","comment":"Code available at Pointcept\n  (https://github.com/Pointcept/PointTransformerV3)"},{"id":"http://arxiv.org/abs/2312.10034v1","updated":"2023-12-15T18:59:55Z","published":"2023-12-15T18:59:55Z","title":"SlimmeRF: Slimmable Radiance Fields","summary":"  Neural Radiance Field (NeRF) and its variants have recently emerged as\nsuccessful methods for novel view synthesis and 3D scene reconstruction.\nHowever, most current NeRF models either achieve high accuracy using large\nmodel sizes, or achieve high memory-efficiency by trading off accuracy. This\nlimits the applicable scope of any single model, since high-accuracy models\nmight not fit in low-memory devices, and memory-efficient models might not\nsatisfy high-quality requirements. To this end, we present SlimmeRF, a model\nthat allows for instant test-time trade-offs between model size and accuracy\nthrough slimming, thus making the model simultaneously suitable for scenarios\nwith different computing budgets. We achieve this through a newly proposed\nalgorithm named Tensorial Rank Incrementation (TRaIn) which increases the rank\nof the model's tensorial representation gradually during training. We also\nobserve that our model allows for more effective trade-offs in sparse-view\nscenarios, at times even achieving higher accuracy after being slimmed. We\ncredit this to the fact that erroneous information such as floaters tend to be\nstored in components corresponding to higher ranks. Our implementation is\navailable at https://github.com/Shiran-Yuan/SlimmeRF.\n","authors":["Shiran Yuan","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2312.10034v1.pdf","comment":"3DV 2024 Oral, Project Page: https://shiran-yuan.github.io/SlimmeRF/,\n  Code: https://github.com/Shiran-Yuan/SlimmeRF/"},{"id":"http://arxiv.org/abs/2312.10032v1","updated":"2023-12-15T18:58:11Z","published":"2023-12-15T18:58:11Z","title":"Osprey: Pixel Understanding with Visual Instruction Tuning","summary":"  Multimodal large language models (MLLMs) have recently achieved impressive\ngeneral-purpose vision-language capabilities through visual instruction tuning.\nHowever, current MLLMs primarily focus on image-level or box-level\nunderstanding, falling short of achieving fine-grained vision-language\nalignment at the pixel level. Besides, the lack of mask-based instruction data\nlimits their advancements. In this paper, we propose Osprey, a mask-text\ninstruction tuning approach, to extend MLLMs by incorporating fine-grained mask\nregions into language instruction, aiming at achieving pixel-wise visual\nunderstanding. To achieve this goal, we first meticulously curate a mask-based\nregion-text dataset with 724K samples, and then design a vision-language model\nby injecting pixel-level representation into LLM. Especially, Osprey adopts a\nconvolutional CLIP backbone as the vision encoder and employs a mask-aware\nvisual extractor to extract precise visual mask features from high resolution\ninput. Experimental results demonstrate Osprey's superiority in various region\nunderstanding tasks, showcasing its new capability for pixel-level instruction\ntuning. In particular, Osprey can be integrated with Segment Anything Model\n(SAM) seamlessly to obtain multi-granularity semantics. The source code,\ndataset and demo can be found at https://github.com/CircleRadon/Osprey.\n","authors":["Yuqian Yuan","Wentong Li","Jian Liu","Dongqi Tang","Xinjie Luo","Chi Qin","Lei Zhang","Jianke Zhu"],"pdf_url":"https://arxiv.org/pdf/2312.10032v1.pdf","comment":"20 pages, Code and Demo link:https://github.com/CircleRadon/Osprey"},{"id":"http://arxiv.org/abs/2208.09602v2","updated":"2023-12-15T18:41:43Z","published":"2022-08-20T04:14:27Z","title":"Exploring Adversarial Robustness of Vision Transformers in the Spectral\n  Perspective","summary":"  The Vision Transformer has emerged as a powerful tool for image\nclassification tasks, surpassing the performance of convolutional neural\nnetworks (CNNs). Recently, many researchers have attempted to understand the\nrobustness of Transformers against adversarial attacks. However, previous\nresearches have focused solely on perturbations in the spatial domain. This\npaper proposes an additional perspective that explores the adversarial\nrobustness of Transformers against frequency-selective perturbations in the\nspectral domain. To facilitate comparison between these two domains, an attack\nframework is formulated as a flexible tool for implementing attacks on images\nin the spatial and spectral domains. The experiments reveal that Transformers\nrely more on phase and low frequency information, which can render them more\nvulnerable to frequency-selective attacks than CNNs. This work offers new\ninsights into the properties and adversarial robustness of Transformers.\n","authors":["Gihyun Kim","Juyeop Kim","Jong-Seok Lee"],"pdf_url":"https://arxiv.org/pdf/2208.09602v2.pdf","comment":"Accepted in IEEE/CVF Winter Conference on Applications of Computer\n  Vision (WACV) 2024"},{"id":"http://arxiv.org/abs/2312.08782v2","updated":"2023-12-15T18:25:48Z","published":"2023-12-14T10:02:55Z","title":"Toward General-Purpose Robots via Foundation Models: A Survey and\n  Meta-Analysis","summary":"  Building general-purpose robots that can operate seamlessly, in any\nenvironment, with any object, and utilizing various skills to complete diverse\ntasks has been a long-standing goal in Artificial Intelligence. Unfortunately,\nhowever, most existing robotic systems have been constrained - having been\ndesigned for specific tasks, trained on specific datasets, and deployed within\nspecific environments. These systems usually require extensively-labeled data,\nrely on task-specific models, have numerous generalization issues when deployed\nin real-world scenarios, and struggle to remain robust to distribution shifts.\nMotivated by the impressive open-set performance and content generation\ncapabilities of web-scale, large-capacity pre-trained models (i.e., foundation\nmodels) in research fields such as Natural Language Processing (NLP) and\nComputer Vision (CV), we devote this survey to exploring (i) how these existing\nfoundation models from NLP and CV can be applied to the field of robotics, and\nalso exploring (ii) what a robotics-specific foundation model would look like.\nWe begin by providing an overview of what constitutes a conventional robotic\nsystem and the fundamental barriers to making it universally applicable. Next,\nwe establish a taxonomy to discuss current work exploring ways to leverage\nexisting foundation models for robotics and develop ones catered to robotics.\nFinally, we discuss key challenges and promising future directions in using\nfoundation models for enabling general-purpose robotic systems. We encourage\nreaders to view our living GitHub repository of resources, including papers\nreviewed in this survey as well as related projects and repositories for\ndeveloping foundation models for robotics.\n","authors":["Yafei Hu","Quanting Xie","Vidhi Jain","Jonathan Francis","Jay Patrikar","Nikhil Keetha","Seungchan Kim","Yaqi Xie","Tianyi Zhang","Shibo Zhao","Yu Quan Chong","Chen Wang","Katia Sycara","Matthew Johnson-Roberson","Dhruv Batra","Xiaolong Wang","Sebastian Scherer","Zsolt Kira","Fei Xia","Yonatan Bisk"],"pdf_url":"https://arxiv.org/pdf/2312.08782v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09997v1","updated":"2023-12-15T18:15:20Z","published":"2023-12-15T18:15:20Z","title":"One Self-Configurable Model to Solve Many Abstract Visual Reasoning\n  Problems","summary":"  Abstract Visual Reasoning (AVR) comprises a wide selection of various\nproblems similar to those used in human IQ tests. Recent years have brought\ndynamic progress in solving particular AVR tasks, however, in the contemporary\nliterature AVR problems are largely dealt with in isolation, leading to highly\nspecialized task-specific methods. With the aim of developing universal\nlearning systems in the AVR domain, we propose the unified model for solving\nSingle-Choice Abstract visual Reasoning tasks (SCAR), capable of solving\nvarious single-choice AVR tasks, without making any a priori assumptions about\nthe task structure, in particular the number and location of panels. The\nproposed model relies on a novel Structure-Aware dynamic Layer (SAL), which\nadapts its weights to the structure of the considered AVR problem. Experiments\nconducted on Raven's Progressive Matrices, Visual Analogy Problems, and Odd One\nOut problems show that SCAR (SAL-based models, in general) effectively solves\ndiverse AVR tasks, and its performance is on par with the state-of-the-art\ntask-specific baselines. What is more, SCAR demonstrates effective knowledge\nreuse in multi-task and transfer learning settings. To our knowledge, this work\nis the first successful attempt to construct a general single-choice AVR solver\nrelying on self-configurable architecture and unified solving method. With this\nwork we aim to stimulate and foster progress on task-independent research paths\nin the AVR domain, with the long-term goal of development of a general AVR\nsolver.\n","authors":["Mikołaj Małkiński","Jacek Mańdziuk"],"pdf_url":"https://arxiv.org/pdf/2312.09997v1.pdf","comment":"Accepted to The 38th Annual AAAI Conference on Artificial\n  Intelligence (AAAI 2024)"},{"id":"http://arxiv.org/abs/2312.09988v1","updated":"2023-12-15T18:01:47Z","published":"2023-12-15T18:01:47Z","title":"Towards Architecture-Insensitive Untrained Network Priors for\n  Accelerated MRI Reconstruction","summary":"  Untrained neural networks pioneered by Deep Image Prior (DIP) have recently\nenabled MRI reconstruction without requiring fully-sampled measurements for\ntraining. Their success is widely attributed to the implicit regularization\ninduced by suitable network architectures. However, the lack of understanding\nof such architectural priors results in superfluous design choices and\nsub-optimal outcomes. This work aims to simplify the architectural design\ndecisions for DIP-MRI to facilitate its practical deployment. We observe that\ncertain architectural components are more prone to causing overfitting\nregardless of the number of parameters, incurring severe reconstruction\nartifacts by hindering accurate extrapolation on the un-acquired measurements.\nWe interpret this phenomenon from a frequency perspective and find that the\narchitectural characteristics favoring low frequencies, i.e., deep and narrow\nwith unlearnt upsampling, can lead to enhanced generalization and hence better\nreconstruction. Building on this insight, we propose two architecture-agnostic\nremedies: one to constrain the frequency range of the white-noise input and the\nother to penalize the Lipschitz constants of the network. We demonstrate that\neven with just one extra line of code on the input, the performance gap between\nthe ill-designed models and the high-performing ones can be closed. These\nresults signify that for the first time, architectural biases on untrained MRI\nreconstruction can be mitigated without architectural modifications.\n","authors":["Yilin Liu","Yunkui Pang","Jiang Li","Yong Chen","Pew-Thian Yap"],"pdf_url":"https://arxiv.org/pdf/2312.09988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04787v2","updated":"2023-12-15T17:35:37Z","published":"2023-10-07T12:26:56Z","title":"HI-SLAM: Monocular Real-time Dense Mapping with Hybrid Implicit Fields","summary":"  In this letter, we present a neural field-based real-time monocular mapping\nframework for accurate and dense Simultaneous Localization and Mapping (SLAM).\nRecent neural mapping frameworks show promising results, but rely on RGB-D or\npose inputs, or cannot run in real-time. To address these limitations, our\napproach integrates dense-SLAM with neural implicit fields. Specifically, our\ndense SLAM approach runs parallel tracking and global optimization, while a\nneural field-based map is constructed incrementally based on the latest SLAM\nestimates. For the efficient construction of neural fields, we employ\nmulti-resolution grid encoding and signed distance function (SDF)\nrepresentation. This allows us to keep the map always up-to-date and adapt\ninstantly to global updates via loop closing. For global consistency, we\npropose an efficient Sim(3)-based pose graph bundle adjustment (PGBA) approach\nto run online loop closing and mitigate the pose and scale drift. To enhance\ndepth accuracy further, we incorporate learned monocular depth priors. We\npropose a novel joint depth and scale adjustment (JDSA) module to solve the\nscale ambiguity inherent in depth priors. Extensive evaluations across\nsynthetic and real-world datasets validate that our approach outperforms\nexisting methods in accuracy and map completeness while preserving real-time\nperformance.\n","authors":["Wei Zhang","Tiecheng Sun","Sen Wang","Qing Cheng","Norbert Haala"],"pdf_url":"https://arxiv.org/pdf/2310.04787v2.pdf","comment":"Accepted by IEEE Robotics and Automation Letters"},{"id":"http://arxiv.org/abs/2312.09968v1","updated":"2023-12-15T17:26:01Z","published":"2023-12-15T17:26:01Z","title":"Human Perception-Inspired Grain Segmentation Refinement Using\n  Conditional Random Fields","summary":"  Accurate segmentation of interconnected line networks, such as grain\nboundaries in polycrystalline material microstructures, poses a significant\nchallenge due to the fragmented masks produced by conventional computer vision\nalgorithms, including convolutional neural networks. These algorithms struggle\nwith thin masks, often necessitating intricate post-processing for effective\ncontour closure and continuity. Addressing this issue, this paper introduces a\nfast, high-fidelity post-processing technique, leveraging domain knowledge\nabout grain boundary connectivity and employing conditional random fields and\nperceptual grouping rules. This approach significantly enhances segmentation\nmask accuracy, achieving a 79% segment identification accuracy in validation\nwith a U-Net model on electron microscopy images of a polycrystalline oxide.\nAdditionally, a novel grain alignment metric is introduced, showing a 51%\nimprovement in grain alignment, providing a more detailed assessment of\nsegmentation performance for complex microstructures. This method not only\nenables rapid and accurate segmentation but also facilitates an unprecedented\nlevel of data analysis, significantly improving the statistical representation\nof grain boundary networks, making it suitable for a range of disciplines where\nprecise segmentation of interconnected line networks is essential.\n","authors":["Doruk Aksoy","Huolin L. Xin","Timothy J. Rupert","William J. Bowman"],"pdf_url":"https://arxiv.org/pdf/2312.09968v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09955v1","updated":"2023-12-15T17:05:32Z","published":"2023-12-15T17:05:32Z","title":"DHFormer: A Vision Transformer-Based Attention Module for Image Dehazing","summary":"  Images acquired in hazy conditions have degradations induced in them.\nDehazing such images is a vexed and ill-posed problem. Scores of prior-based\nand learning-based approaches have been proposed to mitigate the effect of haze\nand generate haze-free images. Many conventional methods are constrained by\ntheir lack of awareness regarding scene depth and their incapacity to capture\nlong-range dependencies. In this paper, a method that uses residual learning\nand vision transformers in an attention module is proposed. It essentially\ncomprises two networks: In the first one, the network takes the ratio of a hazy\nimage and the approximated transmission matrix to estimate a residual map. The\nsecond network takes this residual image as input and passes it through\nconvolution layers before superposing it on the generated feature maps. It is\nthen passed through global context and depth-aware transformer encoders to\nobtain channel attention. The attention module then infers the spatial\nattention map before generating the final haze-free image. Experimental\nresults, including several quantitative metrics, demonstrate the efficiency and\nscalability of the suggested methodology.\n","authors":["Abdul Wasi","O. Jeba Shiney"],"pdf_url":"https://arxiv.org/pdf/2312.09955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09935v1","updated":"2023-12-15T16:44:38Z","published":"2023-12-15T16:44:38Z","title":"LogoStyleFool: Vitiating Video Recognition Systems via Logo Style\n  Transfer","summary":"  Video recognition systems are vulnerable to adversarial examples. Recent\nstudies show that style transfer-based and patch-based unrestricted\nperturbations can effectively improve attack efficiency. These attacks,\nhowever, face two main challenges: 1) Adding large stylized perturbations to\nall pixels reduces the naturalness of the video and such perturbations can be\neasily detected. 2) Patch-based video attacks are not extensible to targeted\nattacks due to the limited search space of reinforcement learning that has been\nwidely used in video attacks recently. In this paper, we focus on the video\nblack-box setting and propose a novel attack framework named LogoStyleFool by\nadding a stylized logo to the clean video. We separate the attack into three\nstages: style reference selection, reinforcement-learning-based logo style\ntransfer, and perturbation optimization. We solve the first challenge by\nscaling down the perturbation range to a regional logo, while the second\nchallenge is addressed by complementing an optimization stage after\nreinforcement learning. Experimental results substantiate the overall\nsuperiority of LogoStyleFool over three state-of-the-art patch-based attacks in\nterms of attack performance and semantic preservation. Meanwhile, LogoStyleFool\nstill maintains its performance against two existing patch-based defense\nmethods. We believe that our research is beneficial in increasing the attention\nof the security community to such subregional style transfer attacks.\n","authors":["Yuxin Cao","Ziyu Zhao","Xi Xiao","Derui Wang","Minhui Xue","Jin Lu"],"pdf_url":"https://arxiv.org/pdf/2312.09935v1.pdf","comment":"13 pages, 3 figures. Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2308.01262v2","updated":"2023-12-15T16:33:17Z","published":"2023-08-02T16:30:18Z","title":"Incorporating Season and Solar Specificity into Renderings made by a\n  NeRF Architecture using Satellite Images","summary":"  As a result of Shadow NeRF and Sat-NeRF, it is possible to take the solar\nangle into account in a NeRF-based framework for rendering a scene from a novel\nviewpoint using satellite images for training. Our work extends those\ncontributions and shows how one can make the renderings season-specific. Our\nmain challenge was creating a Neural Radiance Field (NeRF) that could render\nseasonal features independently of viewing angle and solar angle while still\nbeing able to render shadows. We teach our network to render seasonal features\nby introducing one more input variable -- time of the year. However, the small\ntraining datasets typical of satellite imagery can introduce ambiguities in\ncases where shadows are present in the same location for every image of a\nparticular season. We add additional terms to the loss function to discourage\nthe network from using seasonal features for accounting for shadows. We show\nthe performance of our network on eight Areas of Interest containing images\ncaptured by the Maxar WorldView-3 satellite. This evaluation includes tests\nmeasuring the ability of our framework to accurately render novel views,\ngenerate height maps, predict shadows, and specify seasonal features\nindependently from shadows. Our ablation studies justify the choices made for\nnetwork design parameters.\n","authors":["Michael Gableman","Avinash Kak"],"pdf_url":"https://arxiv.org/pdf/2308.01262v2.pdf","comment":"18 pages, 17 figures, 10 tables"},{"id":"http://arxiv.org/abs/2312.09925v1","updated":"2023-12-15T16:31:17Z","published":"2023-12-15T16:31:17Z","title":"CNC-Net: Self-Supervised Learning for CNC Machining Operations","summary":"  CNC manufacturing is a process that employs computer numerical control (CNC)\nmachines to govern the movements of various industrial tools and machinery,\nencompassing equipment ranging from grinders and lathes to mills and CNC\nrouters. However, the reliance on manual CNC programming has become a\nbottleneck, and the requirement for expert knowledge can result in significant\ncosts. Therefore, we introduce a pioneering approach named CNC-Net,\nrepresenting the use of deep neural networks (DNNs) to simulate CNC machines\nand grasp intricate operations when supplied with raw materials. CNC-Net\nconstitutes a self-supervised framework that exclusively takes an input 3D\nmodel and subsequently generates the essential operation parameters required by\nthe CNC machine to construct the object. Our method has the potential to\ntransformative automation in manufacturing by offering a cost-effective\nalternative to the high costs of manual CNC programming while maintaining\nexceptional precision in 3D object production. Our experiments underscore the\neffectiveness of our CNC-Net in constructing the desired 3D objects through the\nutilization of CNC operations. Notably, it excels in preserving finer local\ndetails, exhibiting a marked enhancement in precision compared to the\nstate-of-the-art 3D CAD reconstruction approaches.\n","authors":["Mohsen Yavartanoo","Sangmin Hong","Reyhaneh Neshatavar","Kyoung Mu Lee"],"pdf_url":"https://arxiv.org/pdf/2312.09925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09922v1","updated":"2023-12-15T16:30:59Z","published":"2023-12-15T16:30:59Z","title":"A Unifying Tensor View for Lightweight CNNs","summary":"  Despite the decomposition of convolutional kernels for lightweight CNNs being\nwell studied, existing works that rely on tensor network diagrams or\nhyperdimensional abstraction lack geometry intuition. This work devises a new\nperspective by linking a 3D-reshaped kernel tensor to its various slice-wise\nand rank-1 decompositions, permitting a straightforward connection between\nvarious tensor approximations and efficient CNN modules. Specifically, it is\ndiscovered that a pointwise-depthwise-pointwise (PDP) configuration constitutes\na viable construct for lightweight CNNs. Moreover, a novel link to the latest\nShiftNet is established, inspiring a first-ever shift layer pruning that\nachieves nearly 50% compression with < 1% drop in accuracy for ShiftResNet.\n","authors":["Jason Chun Lok Li","Rui Lin","Jiajun Zhou","Edmund Yin Mun Lam","Ngai Wong"],"pdf_url":"https://arxiv.org/pdf/2312.09922v1.pdf","comment":"4 pages, 3 figures, accepted in 2023 IEEE 15th International\n  Conference on ASIC (ASICON 2023)"},{"id":"http://arxiv.org/abs/2304.07213v3","updated":"2023-12-15T16:28:21Z","published":"2023-04-14T15:52:57Z","title":"Very high resolution canopy height maps from RGB imagery using\n  self-supervised vision transformer and convolutional decoder trained on\n  Aerial Lidar","summary":"  Vegetation structure mapping is critical for understanding the global carbon\ncycle and monitoring nature-based approaches to climate adaptation and\nmitigation. Repeated measurements of these data allow for the observation of\ndeforestation or degradation of existing forests, natural forest regeneration,\nand the implementation of sustainable agricultural practices like agroforestry.\nAssessments of tree canopy height and crown projected area at a high spatial\nresolution are also important for monitoring carbon fluxes and assessing\ntree-based land uses, since forest structures can be highly spatially\nheterogeneous, especially in agroforestry systems. Very high resolution\nsatellite imagery (less than one meter (1m) Ground Sample Distance) makes it\npossible to extract information at the tree level while allowing monitoring at\na very large scale. This paper presents the first high-resolution canopy height\nmap concurrently produced for multiple sub-national jurisdictions.\nSpecifically, we produce very high resolution canopy height maps for the states\nof California and Sao Paulo, a significant improvement in resolution over the\nten meter (10m) resolution of previous Sentinel / GEDI based worldwide maps of\ncanopy height. The maps are generated by the extraction of features from a\nself-supervised model trained on Maxar imagery from 2017 to 2020, and the\ntraining of a dense prediction decoder against aerial lidar maps. We also\nintroduce a post-processing step using a convolutional network trained on GEDI\nobservations. We evaluate the proposed maps with set-aside validation lidar\ndata as well as by comparing with other remotely sensed maps and\nfield-collected data, and find our model produces an average Mean Absolute\nError (MAE) of 2.8 meters and Mean Error (ME) of 0.6 meters.\n","authors":["Jamie Tolan","Hung-I Yang","Ben Nosarzewski","Guillaume Couairon","Huy Vo","John Brandt","Justine Spore","Sayantan Majumdar","Daniel Haziza","Janaki Vamaraju","Theo Moutakanni","Piotr Bojanowski","Tracy Johns","Brian White","Tobias Tiecke","Camille Couprie"],"pdf_url":"https://arxiv.org/pdf/2304.07213v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09913v1","updated":"2023-12-15T16:23:42Z","published":"2023-12-15T16:23:42Z","title":"LAENeRF: Local Appearance Editing for Neural Radiance Fields","summary":"  Due to the omnipresence of Neural Radiance Fields (NeRFs), the interest\ntowards editable implicit 3D representations has surged over the last years.\nHowever, editing implicit or hybrid representations as used for NeRFs is\ndifficult due to the entanglement of appearance and geometry encoded in the\nmodel parameters. Despite these challenges, recent research has shown first\npromising steps towards photorealistic and non-photorealistic appearance edits.\nThe main open issues of related work include limited interactivity, a lack of\nsupport for local edits and large memory requirements, rendering them less\nuseful in practice. We address these limitations with LAENeRF, a unified\nframework for photorealistic and non-photorealistic appearance editing of\nNeRFs. To tackle local editing, we leverage a voxel grid as starting point for\nregion selection. We learn a mapping from expected ray terminations to final\noutput color, which can optionally be supervised by a style loss, resulting in\na framework which can perform photorealistic and non-photorealistic appearance\nediting of selected regions. Relying on a single point per ray for our mapping,\nwe limit memory requirements and enable fast optimization. To guarantee\ninteractivity, we compose the output color using a set of learned, modifiable\nbase colors, composed with additive layer mixing. Compared to concurrent work,\nLAENeRF enables recoloring and stylization while keeping processing time low.\nFurthermore, we demonstrate that our approach surpasses baseline methods both\nquantitatively and qualitatively.\n","authors":["Lukas Radl","Michael Steiner","Andreas Kurz","Markus Steinberger"],"pdf_url":"https://arxiv.org/pdf/2312.09913v1.pdf","comment":"Project website: https://r4dl.github.io/LAENeRF/"},{"id":"http://arxiv.org/abs/2312.09909v1","updated":"2023-12-15T16:17:34Z","published":"2023-12-15T16:17:34Z","title":"TMP: Temporal Motion Propagation for Online Video Super-Resolution","summary":"  Online video super-resolution (online-VSR) highly relies on an effective\nalignment module to aggregate temporal information, while the strict latency\nrequirement makes accurate and efficient alignment very challenging. Though\nmuch progress has been achieved, most of the existing online-VSR methods\nestimate the motion fields of each frame separately to perform alignment, which\nis computationally redundant and ignores the fact that the motion fields of\nadjacent frames are correlated. In this work, we propose an efficient Temporal\nMotion Propagation (TMP) method, which leverages the continuity of motion field\nto achieve fast pixel-level alignment among consecutive frames. Specifically,\nwe first propagate the offsets from previous frames to the current frame, and\nthen refine them in the neighborhood, which significantly reduces the matching\nspace and speeds up the offset estimation process. Furthermore, to enhance the\nrobustness of alignment, we perform spatial-wise weighting on the warped\nfeatures, where the positions with more precise offsets are assigned higher\nimportance. Experiments on benchmark datasets demonstrate that the proposed TMP\nmethod achieves leading online-VSR accuracy as well as inference speed. The\nsource code of TMP can be found at\n\\href{https://github.com/xtudbxk/TMP}{https://github.com/xtudbxk/TMP}.\n","authors":["Zhengqiang Zhang","Ruihuang Li","Shi Guo","Yang Cao","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.09909v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.20092v3","updated":"2023-12-15T16:09:51Z","published":"2023-10-31T00:12:14Z","title":"The Missing U for Efficient Diffusion Models","summary":"  Diffusion Probabilistic Models stand as a critical tool in generative\nmodelling, enabling the generation of complex data distributions. This family\nof generative models yields record-breaking performance in tasks such as image\nsynthesis, video generation, and molecule design. Despite their capabilities,\ntheir efficiency, especially in the reverse process, remains a challenge due to\nslow convergence rates and high computational costs. In this paper, we\nintroduce an approach that leverages continuous dynamical systems to design a\nnovel denoising network for diffusion models that is more parameter-efficient,\nexhibits faster convergence, and demonstrates increased noise robustness.\nExperimenting with Denoising Diffusion Probabilistic Models (DDPMs), our\nframework operates with approximately a quarter of the parameters, and $\\sim$\n30\\% of the Floating Point Operations (FLOPs) compared to standard U-Nets in\nDDPMs. Furthermore, our model is notably faster in inference than the baseline\nwhen measured in fair and equal conditions. We also provide a mathematical\nintuition as to why our proposed reverse process is faster as well as a\nmathematical discussion of the empirical tradeoffs in the denoising downstream\ntask. Finally, we argue that our method is compatible with existing performance\nenhancement techniques, enabling further improvements in efficiency, quality,\nand speed.\n","authors":["Sergio Calvo-Ordonez","Chun-Wun Cheng","Jiahao Huang","Lipei Zhang","Guang Yang","Carola-Bibiane Schonlieb","Angelica I Aviles-Rivero"],"pdf_url":"https://arxiv.org/pdf/2310.20092v3.pdf","comment":"20 pages, 14 figures, Under review at Transactions of Machine\n  Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2204.04968v2","updated":"2023-12-15T16:08:46Z","published":"2022-04-11T09:34:34Z","title":"Bimodal Camera Pose Prediction for Endoscopy","summary":"  Deducing the 3D structure of endoscopic scenes from images is exceedingly\nchallenging. In addition to deformation and view-dependent lighting, tubular\nstructures like the colon present problems stemming from their self-occluding\nand repetitive anatomical structure. In this paper, we propose SimCol, a\nsynthetic dataset for camera pose estimation in colonoscopy, and a novel method\nthat explicitly learns a bimodal distribution to predict the endoscope pose.\nOur dataset replicates real colonoscope motion and highlights the drawbacks of\nexisting methods. We publish 18k RGB images from simulated colonoscopy with\ncorresponding depth and camera poses and make our data generation environment\nin Unity publicly available. We evaluate different camera pose prediction\nmethods and demonstrate that, when trained on our data, they generalize to real\ncolonoscopy sequences, and our bimodal approach outperforms prior unimodal\nwork.\n","authors":["Anita Rau","Binod Bhattarai","Lourdes Agapito","Danail Stoyanov"],"pdf_url":"https://arxiv.org/pdf/2204.04968v2.pdf","comment":"This article has been accepted for publication in IEEE Transactions\n  on Medical Robotics and Bionics. This is the author's version which has not\n  been fully edited and content may change prior to final publication. Citation\n  information: DOI 10.1109/TMRB.2023.3320267"},{"id":"http://arxiv.org/abs/2312.08054v2","updated":"2023-12-15T16:03:13Z","published":"2023-12-13T11:01:40Z","title":"Semantic Complete Scene Forecasting from a 4D Dynamic Point Cloud\n  Sequence","summary":"  We study a new problem of semantic complete scene forecasting (SCSF) in this\nwork. Given a 4D dynamic point cloud sequence, our goal is to forecast the\ncomplete scene corresponding to the future next frame along with its semantic\nlabels. To tackle this challenging problem, we properly model the synergetic\nrelationship between future forecasting and semantic scene completion through a\nnovel network named SCSFNet. SCSFNet leverages a hybrid geometric\nrepresentation for high-resolution complete scene forecasting. To leverage\nmulti-frame observation as well as the understanding of scene dynamics to ease\nthe completion task, SCSFNet introduces an attention-based skip connection\nscheme. To ease the need to model occlusion variations and to better focus on\nthe occluded part, SCSFNet utilizes auxiliary visibility grids to guide the\nforecasting task. To evaluate the effectiveness of SCSFNet, we conduct\nexperiments on various benchmarks including two large-scale indoor benchmarks\nwe contributed and the outdoor SemanticKITTI benchmark. Extensive experiments\nshow SCSFNet outperforms baseline methods on multiple metrics by a large\nmargin, and also prove the synergy between future forecasting and semantic\nscene completion.\n","authors":["Zifan Wang","Zhuorui Ye","Haoran Wu","Junyu Chen","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2312.08054v2.pdf","comment":"AAAI 2024, see https://scsfnet.github.io/"},{"id":"http://arxiv.org/abs/2212.07786v2","updated":"2023-12-15T15:59:44Z","published":"2022-12-14T17:34:03Z","title":"Convergent Data-driven Regularizations for CT Reconstruction","summary":"  The reconstruction of images from their corresponding noisy Radon transform\nis a typical example of an ill-posed linear inverse problem as arising in the\napplication of computerized tomography (CT). As the (naive) solution does not\ndepend on the measured data continuously, regularization is needed to\nre-establish a continuous dependence. In this work, we investigate simple, but\nyet still provably convergent approaches to learning linear regularization\nmethods from data. More specifically, we analyze two approaches: One generic\nlinear regularization that learns how to manipulate the singular values of the\nlinear operator in an extension of our previous work, and one tailored approach\nin the Fourier domain that is specific to CT-reconstruction. We prove that such\napproaches become convergent regularization methods as well as the fact that\nthe reconstructions they provide are typically much smoother than the training\ndata they were trained on. Finally, we compare the spectral as well as the\nFourier-based approaches for CT-reconstruction numerically, discuss their\nadvantages and disadvantages and investigate the effect of discretization\nerrors at different resolutions.\n","authors":["Samira Kabri","Alexander Auras","Danilo Riccio","Hartmut Bauermeister","Martin Benning","Michael Moeller","Martin Burger"],"pdf_url":"https://arxiv.org/pdf/2212.07786v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09899v1","updated":"2023-12-15T15:49:53Z","published":"2023-12-15T15:49:53Z","title":"SQA-SAM: Segmentation Quality Assessment for Medical Images Utilizing\n  the Segment Anything Model","summary":"  Segmentation quality assessment (SQA) plays a critical role in the deployment\nof a medical image based AI system. Users need to be informed/alerted whenever\nan AI system generates unreliable/incorrect predictions. With the introduction\nof the Segment Anything Model (SAM), a general foundation segmentation model,\nnew research opportunities emerged in how one can utilize SAM for medical image\nsegmentation. In this paper, we propose a novel SQA method, called SQA-SAM,\nwhich exploits SAM to enhance the accuracy of quality assessment for medical\nimage segmentation. When a medical image segmentation model (MedSeg) produces\npredictions for a test image, we generate visual prompts based on the\npredictions, and SAM is utilized to generate segmentation maps corresponding to\nthe visual prompts. How well MedSeg's segmentation aligns with SAM's\nsegmentation indicates how well MedSeg's segmentation aligns with the general\nperception of objectness and image region partition. We develop a score measure\nfor such alignment. In experiments, we find that the generated scores exhibit\nmoderate to strong positive correlation (in Pearson correlation and Spearman\ncorrelation) with Dice coefficient scores reflecting the true segmentation\nquality.\n","authors":["Yizhe Zhang","Shuo Wang","Tao Zhou","Qi Dou","Danny Z. Chen"],"pdf_url":"https://arxiv.org/pdf/2312.09899v1.pdf","comment":"Work in progress;"},{"id":"http://arxiv.org/abs/2312.09894v1","updated":"2023-12-15T15:45:52Z","published":"2023-12-15T15:45:52Z","title":"PathoDuet: Foundation Models for Pathological Slide Analysis of H&E and\n  IHC Stains","summary":"  Large amounts of digitized histopathological data display a promising future\nfor developing pathological foundation models via self-supervised learning\nmethods. Foundation models pretrained with these methods serve as a good basis\nfor downstream tasks. However, the gap between natural and histopathological\nimages hinders the direct application of existing methods. In this work, we\npresent PathoDuet, a series of pretrained models on histopathological images,\nand a new self-supervised learning framework in histopathology. The framework\nis featured by a newly-introduced pretext token and later task raisers to\nexplicitly utilize certain relations between images, like multiple\nmagnifications and multiple stains. Based on this, two pretext tasks,\ncross-scale positioning and cross-stain transferring, are designed to pretrain\nthe model on Hematoxylin and Eosin (H\\&E) images and transfer the model to\nimmunohistochemistry (IHC) images, respectively. To validate the efficacy of\nour models, we evaluate the performance over a wide variety of downstream\ntasks, including patch-level colorectal cancer subtyping and whole slide image\n(WSI)-level classification in H\\&E field, together with expression level\nprediction of IHC marker and tumor identification in IHC field. The\nexperimental results show the superiority of our models over most tasks and the\nefficacy of proposed pretext tasks. The codes and models are available at\nhttps://github.com/openmedlab/PathoDuet.\n","authors":["Shengyi Hua","Fang Yan","Tianle Shen","Xiaofan Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.09894v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04998v2","updated":"2023-12-15T15:44:28Z","published":"2023-03-09T02:43:10Z","title":"Rethinking Visual Prompt Learning as Masked Visual Token Modeling","summary":"  Prompt learning has achieved great success in efficiently exploiting\nlarge-scale pre-trained models in natural language processing (NLP). It\nreformulates the downstream tasks as the generative pre-training ones to\nachieve consistency, thus improving the performance stably. However, when\ntransferring it to the vision area, current visual prompt learning methods are\nalmost designed on discriminative pre-trained models, and there is also a lack\nof careful design to unify the forms of pre-training and downstream tasks. To\nexplore prompt learning on the generative pre-trained visual model, as well as\nkeeping the task consistency, we propose Visual Prompt learning as masked\nvisual Token Modeling (VPTM) to transform the downstream visual classification\ninto the pre-trained masked visual token prediction. In addition, we develop\nthe prototypical verbalizer for mapping the predicted visual token with\nimplicit semantics to explicit downstream labels. To our best knowledge, VPTM\nis the first visual prompt method on the generative pre-trained visual model,\nwhich achieves consistency between pre-training and downstream visual\nclassification by task reformulation. Experiments show that VPTM outperforms\nother visual prompt methods and achieves excellent efficiency. Moreover, the\ntask consistency of VPTM contributes to the robustness against prompt location,\nprompt length and prototype dimension, and could be deployed uniformly.\n","authors":["Ning Liao","Bowen Shi","Xiaopeng Zhang","Min Cao","Junchi Yan","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2303.04998v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09880v1","updated":"2023-12-15T15:27:41Z","published":"2023-12-15T15:27:41Z","title":"Information Extraction from Unstructured data using Augmented-AI and\n  Computer Vision","summary":"  Process of information extraction (IE) is often used to extract meaningful\ninformation from unstructured and unlabeled data. Conventional methods of data\nextraction including application of OCR and passing extraction engine, are\ninefficient on large data and have their limitation. In this paper, a peculiar\ntechnique of information extraction is proposed using A2I and computer vision\ntechnologies, which also includes NLP.\n","authors":["Aditya Parikh"],"pdf_url":"https://arxiv.org/pdf/2312.09880v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09876v1","updated":"2023-12-15T15:24:49Z","published":"2023-12-15T15:24:49Z","title":"Automatic Image Colourizer","summary":"  In this project we have designed and described a model which colourize a\ngray-scale image, with no human intervention. We propose a fully automatic\nprocess of colouring and re-colouring faded or gray-scale image with vibrant\nand pragmatic colours. We have used Convolutional Neural Network to hallucinate\ninput images and feed-forwarded by training thousands of images. This approach\nresults in trailblazing results.\n","authors":["Aditya Parikh"],"pdf_url":"https://arxiv.org/pdf/2312.09876v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09866v1","updated":"2023-12-15T15:09:30Z","published":"2023-12-15T15:09:30Z","title":"PLGSLAM: Progressive Neural Scene Represenation with Local to Global\n  Bundle Adjustment","summary":"  Neural implicit scene representations have recently shown encouraging results\nin dense visual SLAM. However, existing methods produce low-quality scene\nreconstruction and low-accuracy localization performance when scaling up to\nlarge indoor scenes and long sequences. These limitations are mainly due to\ntheir single, global radiance field with finite capacity, which does not adapt\nto large scenarios. Their end-to-end pose networks are also not robust enough\nwith the growth of cumulative errors in large scenes. To this end, we present\nPLGSLAM, a neural visual SLAM system which performs high-fidelity surface\nreconstruction and robust camera tracking in real time. To handle large-scale\nindoor scenes, PLGSLAM proposes a progressive scene representation method which\ndynamically allocates new local scene representation trained with frames within\na local sliding window. This allows us to scale up to larger indoor scenes and\nimproves robustness (even under pose drifts). In local scene representation,\nPLGSLAM utilizes tri-planes for local high-frequency features. We also\nincorporate multi-layer perceptron (MLP) networks for the low-frequency\nfeature, smoothness, and scene completion in unobserved areas. Moreover, we\npropose local-to-global bundle adjustment method with a global keyframe\ndatabase to address the increased pose drifts on long sequences. Experimental\nresults demonstrate that PLGSLAM achieves state-of-the-art scene reconstruction\nresults and tracking performance across various datasets and scenarios (both in\nsmall and large-scale indoor environments). The code will be open-sourced upon\npaper acceptance.\n","authors":["Tianchen Deng","Guole Shen","Tong Qin","Jianyu Wang","Wentao Zhao","Jingchuan Wang","Danwei Wang","Weidong Chen"],"pdf_url":"https://arxiv.org/pdf/2312.09866v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09854v1","updated":"2023-12-15T15:01:41Z","published":"2023-12-15T15:01:41Z","title":"Q-Segment: Segmenting Images In-Sensor for Vessel-Based Medical\n  Diagnosis","summary":"  This paper addresses the growing interest in deploying deep learning models\ndirectly in-sensor. We present \"Q-Segment\", a quantized real-time segmentation\nalgorithm, and conduct a comprehensive evaluation on two low-power edge vision\nplatforms, namely Sony IMX500, which has an in-sensors processor, and Sony\nSpresense, a low-power multi-core ARM Cortex-M microcontroller. One of the main\ngoals of the model is to achieve end-to-end image segmentation for vessel-based\nmedical diagnosis. Deployed on the IMX500 platform, Q-Segment achieves\nultra-low inference time in-sensor of only 1.9 ms and energy consumption of\nonly 5.7 mJ. We compare the proposed network with outperforming existing\nnetworks on various platforms by a factor of 75x (compared to ERFNet). The\nnetwork architecture employs an encoder-decoder structure with skip\nconnections, and results in a binary accuracy of 97.25% and an Area Under the\nReceiver Operating Characteristic Curve (AUC) of 96.97% on the CHASE dataset.\nThis research contributes valuable insights into edge-based image segmentation,\nlaying the foundation for efficient algorithms tailored to low-power\nenvironments.\n","authors":["Pietro Bonazzi","Julian Moosmann","Yawei Li","Sizhen Bian","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2312.09854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15316v3","updated":"2023-12-15T14:40:00Z","published":"2023-08-29T14:02:27Z","title":"3D-MuPPET: 3D Multi-Pigeon Pose Estimation and Tracking","summary":"  Markerless methods for animal posture tracking have been rapidly developing\nrecently, but frameworks and benchmarks for tracking large animal groups in 3D\nare still lacking. To overcome this gap in the literature, we present\n3D-MuPPET, a framework to estimate and track 3D poses of up to 10 pigeons at\ninteractive speed using multiple camera views. We train a pose estimator to\ninfer 2D keypoints and bounding boxes of multiple pigeons, then triangulate the\nkeypoints to 3D. For identity matching of individuals in all views, we first\ndynamically match 2D detections to global identities in the first frame, then\nuse a 2D tracker to maintain IDs across views in subsequent frames. We achieve\ncomparable accuracy to a state of the art 3D pose estimator in terms of median\nerror and Percentage of Correct Keypoints. Additionally, we benchmark the\ninference speed of 3D-MuPPET, with up to 9.45 fps in 2D and 1.89 fps in 3D, and\nperform quantitative tracking evaluation, which yields encouraging results.\nFinally, we showcase two novel applications for 3D-MuPPET. First, we train a\nmodel with data of single pigeons and achieve comparable results in 2D and 3D\nposture estimation for up to 5 pigeons. Second, we show that 3D-MuPPET also\nworks in outdoors without additional annotations from natural environments.\nBoth use cases simplify the domain shift to new species and environments,\nlargely reducing annotation effort needed for 3D posture tracking. To the best\nof our knowledge we are the first to present a framework for 2D/3D animal\nposture and trajectory tracking that works in both indoor and outdoor\nenvironments for up to 10 individuals. We hope that the framework can open up\nnew opportunities in studying animal collective behaviour and encourages\nfurther developments in 3D multi-animal posture tracking.\n","authors":["Urs Waldmann","Alex Hoi Hang Chan","Hemal Naik","Máté Nagy","Iain D. Couzin","Oliver Deussen","Bastian Goldluecke","Fumihiro Kano"],"pdf_url":"https://arxiv.org/pdf/2308.15316v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.08265v3","updated":"2023-12-15T14:39:13Z","published":"2023-07-17T06:14:19Z","title":"Extreme Image Compression using Fine-tuned VQGANs","summary":"  Recent advances in generative compression methods have demonstrated\nremarkable progress in enhancing the perceptual quality of compressed data,\nespecially in scenarios with low bitrates. However, their efficacy and\napplicability to achieve extreme compression ratios ($<0.05$ bpp) remain\nconstrained. In this work, we propose a simple yet effective coding framework\nby introducing vector quantization (VQ)--based generative models into the image\ncompression domain. The main insight is that the codebook learned by the VQGAN\nmodel yields a strong expressive capacity, facilitating efficient compression\nof continuous information in the latent space while maintaining reconstruction\nquality. Specifically, an image can be represented as VQ-indices by finding the\nnearest codeword, which can be encoded using lossless compression methods into\nbitstreams. We propose clustering a pre-trained large-scale codebook into\nsmaller codebooks through the K-means algorithm, yielding variable bitrates and\ndifferent levels of reconstruction quality within the coding framework.\nFurthermore, we introduce a transformer to predict lost indices and restore\nimages in unstable environments. Extensive qualitative and quantitative\nexperiments on various benchmark datasets demonstrate that the proposed\nframework outperforms state-of-the-art codecs in terms of perceptual\nquality-oriented metrics and human perception at extremely low bitrates ($\\le\n0.04$ bpp). Remarkably, even with the loss of up to $20\\%$ of indices, the\nimages can be effectively restored with minimal perceptual loss.\n","authors":["Qi Mao","Tinghan Yang","Yinuo Zhang","Zijian Wang","Meng Wang","Shiqi Wang","Siwei Ma"],"pdf_url":"https://arxiv.org/pdf/2307.08265v3.pdf","comment":"Generative Compression, Extreme Compression, VQGANs, Low Bitrate"},{"id":"http://arxiv.org/abs/2301.06132v2","updated":"2023-12-15T14:26:39Z","published":"2023-01-15T16:19:18Z","title":"Deep Diversity-Enhanced Feature Representation of Hyperspectral Images","summary":"  In this paper, we study the problem of efficiently and effectively embedding\nthe high-dimensional spatio-spectral information of hyperspectral (HS) images,\nguided by feature diversity. Specifically, based on the theoretical formulation\nthat feature diversity is correlated with the rank of the unfolded kernel\nmatrix, we rectify 3D convolution by modifying its topology to enhance the rank\nupper-bound. This modification yields a rank-enhanced spatial-spectral\nsymmetrical convolution set (ReS$^3$-ConvSet), which not only learns diverse\nand powerful feature representations but also saves network parameters.\nAdditionally, we also propose a novel diversity-aware regularization (DA-Reg)\nterm that directly acts on the feature maps to maximize independence among\nelements. To demonstrate the superiority of the proposed ReS$^3$-ConvSet and\nDA-Reg, we apply them to various HS image processing and analysis tasks,\nincluding denoising, spatial super-resolution, and classification. Extensive\nexperiments show that the proposed approaches outperform state-of-the-art\nmethods both quantitatively and qualitatively to a significant extent. The code\nis publicly available at https://github.com/jinnh/ReSSS-ConvSet.\n","authors":["Jinhui Hou","Zhiyu Zhu","Junhui Hou","Hui Liu","Huanqiang Zeng","Deyu Meng"],"pdf_url":"https://arxiv.org/pdf/2301.06132v2.pdf","comment":"15 pages, 12 figures. arXiv admin note: substantial text overlap with\n  arXiv:2207.04266"},{"id":"http://arxiv.org/abs/2312.09821v1","updated":"2023-12-15T14:20:16Z","published":"2023-12-15T14:20:16Z","title":"Fragility, Robustness and Antifragility in Deep Learning","summary":"  We propose a systematic analysis of deep neural networks (DNNs) based on a\nsignal processing technique for network parameter removal, in the form of\nsynaptic filters that identifies the fragility, robustness and antifragility\ncharacteristics of DNN parameters. Our proposed analysis investigates if the\nDNN performance is impacted negatively, invariantly, or positively on both\nclean and adversarially perturbed test datasets when the DNN undergoes synaptic\nfiltering. We define three \\textit{filtering scores} for quantifying the\nfragility, robustness and antifragility characteristics of DNN parameters based\non the performances for (i) clean dataset, (ii) adversarial dataset, and (iii)\nthe difference in performances of clean and adversarial datasets. We validate\nthe proposed systematic analysis on ResNet-18, ResNet-50, SqueezeNet-v1.1 and\nShuffleNet V2 x1.0 network architectures for MNIST, CIFAR10 and Tiny ImageNet\ndatasets. The filtering scores, for a given network architecture, identify\nnetwork parameters that are invariant in characteristics across different\ndatasets over learning epochs. Vice-versa, for a given dataset, the filtering\nscores identify the parameters that are invariant in characteristics across\ndifferent network architectures. We show that our synaptic filtering method\nimproves the test accuracy of ResNet and ShuffleNet models on adversarial\ndatasets when only the robust and antifragile parameters are selectively\nretrained at any given epoch, thus demonstrating applications of the proposed\nstrategy in improving model robustness.\n","authors":["Chandresh Pravin","Ivan Martino","Giuseppe Nicosia","Varun Ojha"],"pdf_url":"https://arxiv.org/pdf/2312.09821v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09812v1","updated":"2023-12-15T14:10:21Z","published":"2023-12-15T14:10:21Z","title":"Structural Information Guided Multimodal Pre-training for\n  Vehicle-centric Perception","summary":"  Understanding vehicles in images is important for various applications such\nas intelligent transportation and self-driving system. Existing vehicle-centric\nworks typically pre-train models on large-scale classification datasets and\nthen fine-tune them for specific downstream tasks. However, they neglect the\nspecific characteristics of vehicle perception in different tasks and might\nthus lead to sub-optimal performance. To address this issue, we propose a novel\nvehicle-centric pre-training framework called VehicleMAE, which incorporates\nthe structural information including the spatial structure from vehicle profile\ninformation and the semantic structure from informative high-level natural\nlanguage descriptions for effective masked vehicle appearance reconstruction.\nTo be specific, we explicitly extract the sketch lines of vehicles as a form of\nthe spatial structure to guide vehicle reconstruction. The more comprehensive\nknowledge distilled from the CLIP big model based on the similarity between the\npaired/unpaired vehicle image-text sample is further taken into consideration\nto help achieve a better understanding of vehicles. A large-scale dataset is\nbuilt to pre-train our model, termed Autobot1M, which contains about 1M vehicle\nimages and 12693 text information. Extensive experiments on four vehicle-based\ndownstream tasks fully validated the effectiveness of our VehicleMAE. The\nsource code and pre-trained models will be released at\nhttps://github.com/Event-AHU/VehicleMAE.\n","authors":["Xiao Wang","Wentao Wu","Chenglong Li","Zhicheng Zhao","Zhe Chen","Yukai Shi","Jin Tang"],"pdf_url":"https://arxiv.org/pdf/2312.09812v1.pdf","comment":"Accepted by AAAI-2024"},{"id":"http://arxiv.org/abs/2312.09800v1","updated":"2023-12-15T14:00:00Z","published":"2023-12-15T14:00:00Z","title":"Deep Event Visual Odometry","summary":"  Event cameras offer the exciting possibility of tracking the camera's pose\nduring high-speed motion and in adverse lighting conditions. Despite this\npromise, existing event-based monocular visual odometry (VO) approaches\ndemonstrate limited performance on recent benchmarks. To address this\nlimitation, some methods resort to additional sensors such as IMUs, stereo\nevent cameras, or frame-based cameras. Nonetheless, these additional sensors\nlimit the application of event cameras in real-world devices since they\nincrease cost and complicate system requirements. Moreover, relying on a\nframe-based camera makes the system susceptible to motion blur and HDR. To\nremove the dependency on additional sensors and to push the limits of using\nonly a single event camera, we present Deep Event VO (DEVO), the first\nmonocular event-only system with strong performance on a large number of\nreal-world benchmarks. DEVO sparsely tracks selected event patches over time. A\nkey component of DEVO is a novel deep patch selection mechanism tailored to\nevent data. We significantly decrease the pose tracking error on seven\nreal-world benchmarks by up to 97% compared to event-only methods and often\nsurpass or are close to stereo or inertial methods. Code is available at\nhttps://github.com/tum-vision/DEVO\n","authors":["Simon Klenk","Marvin Motzet","Lukas Koestler","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2312.09800v1.pdf","comment":"Accepted by 3DV 2024"},{"id":"http://arxiv.org/abs/2312.09799v1","updated":"2023-12-15T13:58:10Z","published":"2023-12-15T13:58:10Z","title":"IQNet: Image Quality Assessment Guided Just Noticeable Difference\n  Prefiltering For Versatile Video Coding","summary":"  Image prefiltering with just noticeable distortion (JND) improves coding\nefficiency in a visual lossless way by filtering the perceptually redundant\ninformation prior to compression. However, real JND cannot be well modeled with\ninaccurate masking equations in traditional approaches or image-level subject\ntests in deep learning approaches. Thus, this paper proposes a fine-grained JND\nprefiltering dataset guided by image quality assessment for accurate\nblock-level JND modeling. The dataset is constructed from decoded images to\ninclude coding effects and is also perceptually enhanced with block overlap and\nedge preservation. Furthermore, based on this dataset, we propose a lightweight\nJND prefiltering network, IQNet, which can be applied directly to different\nquantization cases with the same model and only needs 3K parameters. The\nexperimental results show that the proposed approach to Versatile Video Coding\ncould yield maximum/average bitrate savings of 41\\%/15\\% and 53\\%/19\\% for\nall-intra and low-delay P configurations, respectively, with negligible\nsubjective quality loss. Our method demonstrates higher perceptual quality and\na model size that is an order of magnitude smaller than previous deep learning\nmethods.\n","authors":["Yu-Han Sun","Chiang Lo-Hsuan Lee","Tian-Sheuan Chang"],"pdf_url":"https://arxiv.org/pdf/2312.09799v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09797v1","updated":"2023-12-15T13:54:48Z","published":"2023-12-15T13:54:48Z","title":"Part Representation Learning with Teacher-Student Decoder for Occluded\n  Person Re-identification","summary":"  Occluded person re-identification (ReID) is a very challenging task due to\nthe occlusion disturbance and incomplete target information. Leveraging\nexternal cues such as human pose or parsing to locate and align part features\nhas been proven to be very effective in occluded person ReID. Meanwhile, recent\nTransformer structures have a strong ability of long-range modeling.\nConsidering the above facts, we propose a Teacher-Student Decoder (TSD)\nframework for occluded person ReID, which utilizes the Transformer decoder with\nthe help of human parsing. More specifically, our proposed TSD consists of a\nParsing-aware Teacher Decoder (PTD) and a Standard Student Decoder (SSD). PTD\nemploys human parsing cues to restrict Transformer's attention and imparts this\ninformation to SSD through feature distillation. Thereby, SSD can learn from\nPTD to aggregate information of body parts automatically. Moreover, a mask\ngenerator is designed to provide discriminative regions for better ReID. In\naddition, existing occluded person ReID benchmarks utilize occluded samples as\nqueries, which will amplify the role of alleviating occlusion interference and\nunderestimate the impact of the feature absence issue. Contrastively, we\npropose a new benchmark with non-occluded queries, serving as a complement to\nthe existing benchmark. Extensive experiments demonstrate that our proposed\nmethod is superior and the new benchmark is essential. The source codes are\navailable at https://github.com/hh23333/TSD.\n","authors":["Shang Gao","Chenyang Yu","Pingping Zhang","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2312.09797v1.pdf","comment":"Accepted by ICASSP2024"},{"id":"http://arxiv.org/abs/2211.11727v4","updated":"2023-12-15T13:53:14Z","published":"2022-11-21T18:47:11Z","title":"Parametric Classification for Generalized Category Discovery: A Baseline\n  Study","summary":"  Generalized Category Discovery (GCD) aims to discover novel categories in\nunlabelled datasets using knowledge learned from labelled samples. Previous\nstudies argued that parametric classifiers are prone to overfitting to seen\ncategories, and endorsed using a non-parametric classifier formed with\nsemi-supervised k-means. However, in this study, we investigate the failure of\nparametric classifiers, verify the effectiveness of previous design choices\nwhen high-quality supervision is available, and identify unreliable\npseudo-labels as a key problem. We demonstrate that two prediction biases\nexist: the classifier tends to predict seen classes more often, and produces an\nimbalanced distribution across seen and novel categories. Based on these\nfindings, we propose a simple yet effective parametric classification method\nthat benefits from entropy regularisation, achieves state-of-the-art\nperformance on multiple GCD benchmarks and shows strong robustness to unknown\nclass numbers. We hope the investigation and proposed simple framework can\nserve as a strong baseline to facilitate future studies in this field. Our code\nis available at: https://github.com/CVMI-Lab/SimGCD.\n","authors":["Xin Wen","Bingchen Zhao","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2211.11727v4.pdf","comment":"v3: ICCV'23 version; v4: updated the dataset table"},{"id":"http://arxiv.org/abs/2312.09792v1","updated":"2023-12-15T13:48:55Z","published":"2023-12-15T13:48:55Z","title":"Latent Diffusion Models with Image-Derived Annotations for Enhanced\n  AI-Assisted Cancer Diagnosis in Histopathology","summary":"  Artificial Intelligence (AI) based image analysis has an immense potential to\nsupport diagnostic histopathology, including cancer diagnostics. However,\ndeveloping supervised AI methods requires large-scale annotated datasets. A\npotentially powerful solution is to augment training data with synthetic data.\nLatent diffusion models, which can generate high-quality, diverse synthetic\nimages, are promising. However, the most common implementations rely on\ndetailed textual descriptions, which are not generally available in this\ndomain. This work proposes a method that constructs structured textual prompts\nfrom automatically extracted image features. We experiment with the PCam\ndataset, composed of tissue patches only loosely annotated as healthy or\ncancerous. We show that including image-derived features in the prompt, as\nopposed to only healthy and cancerous labels, improves the Fr\\'echet Inception\nDistance (FID) from 178.8 to 90.2. We also show that pathologists find it\nchallenging to detect synthetic images, with a median sensitivity/specificity\nof 0.55/0.55. Finally, we show that synthetic data effectively trains AI\nmodels.\n","authors":["Pedro Osorio","Guillermo Jimenez-Perez","Javier Montalt-Tordera","Jens Hooge","Guillem Duran-Ballester","Shivam Singh","Moritz Radbruch","Ute Bach","Sabrina Schroeder","Krystyna Siudak","Julia Vienenkoetter","Bettina Lawrenz","Sadegh Mohammadi"],"pdf_url":"https://arxiv.org/pdf/2312.09792v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09788v1","updated":"2023-12-15T13:43:24Z","published":"2023-12-15T13:43:24Z","title":"Collaborating Foundation models for Domain Generalized Semantic\n  Segmentation","summary":"  Domain Generalized Semantic Segmentation (DGSS) deals with training a model\non a labeled source domain with the aim of generalizing to unseen domains\nduring inference. Existing DGSS methods typically effectuate robust features by\nmeans of Domain Randomization (DR). Such an approach is often limited as it can\nonly account for style diversification and not content. In this work, we take\nan orthogonal approach to DGSS and propose to use an assembly of CoLlaborative\nFOUndation models for Domain Generalized Semantic Segmentation (CLOUDS). In\ndetail, CLOUDS is a framework that integrates FMs of various kinds: (i) CLIP\nbackbone for its robust feature representation, (ii) generative models to\ndiversify the content, thereby covering various modes of the possible target\ndistribution, and (iii) Segment Anything Model (SAM) for iteratively refining\nthe predictions of the segmentation model. Extensive experiments show that our\nCLOUDS excels in adapting from synthetic to real DGSS benchmarks and under\nvarying weather conditions, notably outperforming prior methods by 5.6% and\n6.7% on averaged miou, respectively. The code is available at :\nhttps://github.com/yasserben/CLOUDS\n","authors":["Yasser Benigmim","Subhankar Roy","Slim Essid","Vicky Kalogeiton","Stéphane Lathuilière"],"pdf_url":"https://arxiv.org/pdf/2312.09788v1.pdf","comment":"https://github.com/yasserben/CLOUDS"},{"id":"http://arxiv.org/abs/2312.09228v2","updated":"2023-12-15T13:39:07Z","published":"2023-12-14T18:54:32Z","title":"3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting","summary":"  We introduce an approach that creates animatable human avatars from monocular\nvideos using 3D Gaussian Splatting (3DGS). Existing methods based on neural\nradiance fields (NeRFs) achieve high-quality novel-view/novel-pose image\nsynthesis but often require days of training, and are extremely slow at\ninference time. Recently, the community has explored fast grid structures for\nefficient training of clothed avatars. Albeit being extremely fast at training,\nthese methods can barely achieve an interactive rendering frame rate with\naround 15 FPS. In this paper, we use 3D Gaussian Splatting and learn a\nnon-rigid deformation network to reconstruct animatable clothed human avatars\nthat can be trained within 30 minutes and rendered at real-time frame rates\n(50+ FPS). Given the explicit nature of our representation, we further\nintroduce as-isometric-as-possible regularizations on both the Gaussian mean\nvectors and the covariance matrices, enhancing the generalization of our model\non highly articulated unseen poses. Experimental results show that our method\nachieves comparable and even better performance compared to state-of-the-art\napproaches on animatable avatar creation from a monocular input, while being\n400x and 250x faster in training and inference, respectively.\n","authors":["Zhiyin Qian","Shaofei Wang","Marko Mihajlovic","Andreas Geiger","Siyu Tang"],"pdf_url":"https://arxiv.org/pdf/2312.09228v2.pdf","comment":"Project page: https://neuralbodies.github.io/3DGS-Avatar"},{"id":"http://arxiv.org/abs/2306.10941v2","updated":"2023-12-15T13:37:03Z","published":"2023-06-19T14:01:47Z","title":"Synthetic optical coherence tomography angiographs for detailed retinal\n  vessel segmentation without human annotations","summary":"  Optical coherence tomography angiography (OCTA) is a non-invasive imaging\nmodality that can acquire high-resolution volumes of the retinal vasculature\nand aid the diagnosis of ocular, neurological and cardiac diseases. Segmenting\nthe visible blood vessels is a common first step when extracting quantitative\nbiomarkers from these images. Classical segmentation algorithms based on\nthresholding are strongly affected by image artifacts and limited\nsignal-to-noise ratio. The use of modern, deep learning-based segmentation\nmethods has been inhibited by a lack of large datasets with detailed\nannotations of the blood vessels. To address this issue, recent work has\nemployed transfer learning, where a segmentation network is trained on\nsynthetic OCTA images and is then applied to real data. However, the previously\nproposed simulations fail to faithfully model the retinal vasculature and do\nnot provide effective domain adaptation. Because of this, current methods are\nunable to fully segment the retinal vasculature, in particular the smallest\ncapillaries. In this work, we present a lightweight simulation of the retinal\nvascular network based on space colonization for faster and more realistic OCTA\nsynthesis. We then introduce three contrast adaptation pipelines to decrease\nthe domain gap between real and artificial images. We demonstrate the superior\nsegmentation performance of our approach in extensive quantitative and\nqualitative experiments on three public datasets that compare our method to\ntraditional computer vision algorithms and supervised training using human\nannotations. Finally, we make our entire pipeline publicly available, including\nthe source code, pretrained models, and a large dataset of synthetic OCTA\nimages.\n","authors":["Linus Kreitner","Johannes C. Paetzold","Nikolaus Rauch","Chen Chen","Ahmed M. Hagag","Alaa E. Fayed","Sobha Sivaprasad","Sebastian Rausch","Julian Weichsel","Bjoern H. Menze","Matthias Harders","Benjamin Knier","Daniel Rueckert","Martin J. Menten"],"pdf_url":"https://arxiv.org/pdf/2306.10941v2.pdf","comment":"Currently under review"},{"id":"http://arxiv.org/abs/2312.09783v1","updated":"2023-12-15T13:36:54Z","published":"2023-12-15T13:36:54Z","title":"Keep the Faith: Faithful Explanations in Convolutional Neural Networks\n  for Case-Based Reasoning","summary":"  Explaining predictions of black-box neural networks is crucial when applied\nto decision-critical tasks. Thus, attribution maps are commonly used to\nidentify important image regions, despite prior work showing that humans prefer\nexplanations based on similar examples. To this end, ProtoPNet learns a set of\nclass-representative feature vectors (prototypes) for case-based reasoning.\nDuring inference, similarities of latent features to prototypes are linearly\nclassified to form predictions and attribution maps are provided to explain the\nsimilarity. In this work, we evaluate whether architectures for case-based\nreasoning fulfill established axioms required for faithful explanations using\nthe example of ProtoPNet. We show that such architectures allow the extraction\nof faithful explanations. However, we prove that the attribution maps used to\nexplain the similarities violate the axioms. We propose a new procedure to\nextract explanations for trained ProtoPNets, named ProtoPFaith. Conceptually,\nthese explanations are Shapley values, calculated on the similarity scores of\neach prototype. They allow to faithfully answer which prototypes are present in\nan unseen image and quantify each pixel's contribution to that presence,\nthereby complying with all axioms. The theoretical violations of ProtoPNet\nmanifest in our experiments on three datasets (CUB-200-2011, Stanford Dogs,\nRSNA) and five architectures (ConvNet, ResNet, ResNet50, WideResNet50,\nResNeXt50). Our experiments show a qualitative difference between the\nexplanations given by ProtoPNet and ProtoPFaith. Additionally, we quantify the\nexplanations with the Area Over the Perturbation Curve, on which ProtoPFaith\noutperforms ProtoPNet on all experiments by a factor $>10^3$.\n","authors":["Tom Nuno Wolf","Fabian Bongratz","Anne-Marie Rickmann","Sebastian Pölsterl","Christian Wachinger"],"pdf_url":"https://arxiv.org/pdf/2312.09783v1.pdf","comment":"To be published in proceedings of AAAI Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2312.09780v1","updated":"2023-12-15T13:33:09Z","published":"2023-12-15T13:33:09Z","title":"RANRAC: Robust Neural Scene Representations via Random Ray Consensus","summary":"  We introduce RANRAC, a robust reconstruction algorithm for 3D objects\nhandling occluded and distracted images, which is a particularly challenging\nscenario that prior robust reconstruction methods cannot deal with. Our\nsolution supports single-shot reconstruction by involving light-field networks,\nand is also applicable to photo-realistic, robust, multi-view reconstruction\nfrom real-world images based on neural radiance fields. While the algorithm\nimposes certain limitations on the scene representation and, thereby, the\nsupported scene types, it reliably detects and excludes inconsistent\nperspectives, resulting in clean images without floating artifacts. Our\nsolution is based on a fuzzy adaption of the random sample consensus paradigm,\nenabling its application to large scale models. We interpret the minimal number\nof samples to determine the model parameters as a tunable hyperparameter. This\nis applicable, as a cleaner set of samples improves reconstruction quality.\nFurther, this procedure also handles outliers. Especially for conditioned\nmodels, it can result in the same local minimum in the latent space as would be\nobtained with a completely clean set. We report significant improvements for\nnovel-view synthesis in occluded scenarios, of up to 8dB PSNR compared to the\nbaseline.\n","authors":["Benno Buschmann","Andreea Dogaru","Elmar Eisemann","Michael Weinmann","Bernhard Egger"],"pdf_url":"https://arxiv.org/pdf/2312.09780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08078v3","updated":"2023-12-15T13:22:51Z","published":"2023-12-13T11:47:28Z","title":"Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic\n  Image-Report Generation","summary":"  To address these issues, we propose a novel Adaptive patch-word Matching\n(AdaMatch) model to correlate chest X-ray (CXR) image regions with words in\nmedical reports and apply it to CXR-report generation to provide explainability\nfor the generation process. AdaMatch exploits the fine-grained relation between\nadaptive patches and words to provide explanations of specific image regions\nwith corresponding words. To capture the abnormal regions of varying sizes and\npositions, we introduce the Adaptive Patch extraction (AdaPatch) module to\nacquire the adaptive patches for these regions adaptively. In order to provide\nexplicit explainability for CXR-report generation task, we propose an\nAdaMatch-based bidirectional large language model for Cyclic CXR-report\ngeneration (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywords\nfor CXR images and `keypatches' for medical reports as hints to guide\nCXR-report generation. Extensive experiments on two publicly available CXR\ndatasets prove the effectiveness of our method and its superior performance to\nexisting methods.\n","authors":["Wenting Chen","Xiang Li","Linlin Shen","Yixuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2312.08078v3.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2312.09767v1","updated":"2023-12-15T13:15:42Z","published":"2023-12-15T13:15:42Z","title":"DreamTalk: When Expressive Talking Head Generation Meets Diffusion\n  Probabilistic Models","summary":"  Diffusion models have shown remarkable success in a variety of downstream\ngenerative tasks, yet remain under-explored in the important and challenging\nexpressive talking head generation. In this work, we propose a DreamTalk\nframework to fulfill this gap, which employs meticulous design to unlock the\npotential of diffusion models in generating expressive talking heads.\nSpecifically, DreamTalk consists of three crucial components: a denoising\nnetwork, a style-aware lip expert, and a style predictor. The diffusion-based\ndenoising network is able to consistently synthesize high-quality audio-driven\nface motions across diverse expressions. To enhance the expressiveness and\naccuracy of lip motions, we introduce a style-aware lip expert that can guide\nlip-sync while being mindful of the speaking styles. To eliminate the need for\nexpression reference video or text, an extra diffusion-based style predictor is\nutilized to predict the target expression directly from the audio. By this\nmeans, DreamTalk can harness powerful diffusion models to generate expressive\nfaces effectively and reduce the reliance on expensive style references.\nExperimental results demonstrate that DreamTalk is capable of generating\nphoto-realistic talking faces with diverse speaking styles and achieving\naccurate lip motions, surpassing existing state-of-the-art counterparts.\n","authors":["Yifeng Ma","Shiwei Zhang","Jiayu Wang","Xiang Wang","Yingya Zhang","Zhidong Deng"],"pdf_url":"https://arxiv.org/pdf/2312.09767v1.pdf","comment":"Project Page: https://dreamtalk-project.github.io"},{"id":"http://arxiv.org/abs/2311.03054v4","updated":"2023-12-15T12:51:01Z","published":"2023-11-06T12:10:43Z","title":"AnyText: Multilingual Visual Text Generation And Editing","summary":"  Diffusion model based Text-to-Image has achieved impressive achievements\nrecently. Although current technology for synthesizing images is highly\nadvanced and capable of generating images with high fidelity, it is still\npossible to give the show away when focusing on the text area in the generated\nimage. To address this issue, we introduce AnyText, a diffusion-based\nmultilingual visual text generation and editing model, that focuses on\nrendering accurate and coherent text in the image. AnyText comprises a\ndiffusion pipeline with two primary elements: an auxiliary latent module and a\ntext embedding module. The former uses inputs like text glyph, position, and\nmasked image to generate latent features for text generation or editing. The\nlatter employs an OCR model for encoding stroke data as embeddings, which blend\nwith image caption embeddings from the tokenizer to generate texts that\nseamlessly integrate with the background. We employed text-control diffusion\nloss and text perceptual loss for training to further enhance writing accuracy.\nAnyText can write characters in multiple languages, to the best of our\nknowledge, this is the first work to address multilingual visual text\ngeneration. It is worth mentioning that AnyText can be plugged into existing\ndiffusion models from the community for rendering or editing text accurately.\nAfter conducting extensive evaluation experiments, our method has outperformed\nall other approaches by a significant margin. Additionally, we contribute the\nfirst large-scale multilingual text images dataset, AnyWord-3M, containing 3\nmillion image-text pairs with OCR annotations in multiple languages. Based on\nAnyWord-3M dataset, we propose AnyText-benchmark for the evaluation of visual\ntext generation accuracy and quality. Our project will be open-sourced on\nhttps://github.com/tyxsspa/AnyText to improve and promote the development of\ntext generation technology.\n","authors":["Yuxiang Tuo","Wangmeng Xiang","Jun-Yan He","Yifeng Geng","Xuansong Xie"],"pdf_url":"https://arxiv.org/pdf/2311.03054v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09754v1","updated":"2023-12-15T12:49:08Z","published":"2023-12-15T12:49:08Z","title":"PPFM: Image denoising in photon-counting CT using single-step posterior\n  sampling Poisson flow generative models","summary":"  Diffusion and Poisson flow models have shown impressive performance in a wide\nrange of generative tasks, including low-dose CT image denoising. However, one\nlimitation in general, and for clinical applications in particular, is slow\nsampling. Due to their iterative nature, the number of function evaluations\n(NFE) required is usually on the order of $10-10^3$, both for conditional and\nunconditional generation. In this paper, we present posterior sampling Poisson\nflow generative models (PPFM), a novel image denoising technique for low-dose\nand photon-counting CT that produces excellent image quality whilst keeping\nNFE=1. Updating the training and sampling processes of Poisson flow generative\nmodels (PFGM)++, we learn a conditional generator which defines a trajectory\nbetween the prior noise distribution and the posterior distribution of\ninterest. We additionally hijack and regularize the sampling process to achieve\nNFE=1. Our results shed light on the benefits of the PFGM++ framework compared\nto diffusion models. In addition, PPFM is shown to perform favorably compared\nto current state-of-the-art diffusion-style models with NFE=1, consistency\nmodels, as well as popular deep learning and non-deep learning-based image\ndenoising techniques, on clinical low-dose CT images and clinical images from a\nprototype photon-counting CT system.\n","authors":["Dennis Hein","Staffan Holmin","Timothy Szczykutowicz","Jonathan S Maltz","Mats Danielsson","Ge Wang","Mats Persson"],"pdf_url":"https://arxiv.org/pdf/2312.09754v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09750v1","updated":"2023-12-15T12:45:11Z","published":"2023-12-15T12:45:11Z","title":"Attention-Based VR Facial Animation with Visual Mouth Camera Guidance\n  for Immersive Telepresence Avatars","summary":"  Facial animation in virtual reality environments is essential for\napplications that necessitate clear visibility of the user's face and the\nability to convey emotional signals. In our scenario, we animate the face of an\noperator who controls a robotic Avatar system. The use of facial animation is\nparticularly valuable when the perception of interacting with a specific\nindividual, rather than just a robot, is intended. Purely keypoint-driven\nanimation approaches struggle with the complexity of facial movements. We\npresent a hybrid method that uses both keypoints and direct visual guidance\nfrom a mouth camera. Our method generalizes to unseen operators and requires\nonly a quick enrolment step with capture of two short videos. Multiple source\nimages are selected with the intention to cover different facial expressions.\nGiven a mouth camera frame from the HMD, we dynamically construct the target\nkeypoints and apply an attention mechanism to determine the importance of each\nsource image. To resolve keypoint ambiguities and animate a broader range of\nmouth expressions, we propose to inject visual mouth camera information into\nthe latent space. We enable training on large-scale speaking head datasets by\nsimulating the mouth camera input with its perspective differences and facial\ndeformations. Our method outperforms a baseline in quality, capability, and\ntemporal consistency. In addition, we highlight how the facial animation\ncontributed to our victory at the ANA Avatar XPRIZE Finals.\n","authors":["Andre Rochow","Max Schwarz","Sven Behnke"],"pdf_url":"https://arxiv.org/pdf/2312.09750v1.pdf","comment":"Published in IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2023"},{"id":"http://arxiv.org/abs/2305.08946v6","updated":"2023-12-15T12:38:37Z","published":"2023-05-15T18:35:47Z","title":"Image Matching by Bare Homography","summary":"  This paper presents Slime, a novel non-deep image matching framework which\nmodels the scene as rough local overlapping planes. This intermediate\nrepresentation sits in-between the local affine approximation of the keypoint\npatches and the global matching based on both spatial and similarity\nconstraints, providing a progressive pruning of the correspondences, as planes\nare easier to handle with respect to general scenes.\n  Slime decomposes the images into overlapping regions at different scales and\ncomputes loose planar homographies. Planes are mutually extended by compatible\nmatches and the images are split into fixed tiles, with only the best\nhomographies retained for each pair of tiles. Stable matches are identified\naccording to the consensus of the admissible stereo configurations provided by\npairwise homographies. Within tiles, the rough planes are then merged according\nto their overlap in terms of matches and further consistent correspondences are\nextracted.\n  The whole process only involves homography constraints. As a result, both the\ncoverage and the stability of correct matches over the scene are amplified,\ntogether with the ability to spot matches in challenging scenes, allowing\ntraditional hybrid matching pipelines to make up lost ground against recent\nend-to-end deep matching methods.\n  In addition, the paper gives a thorough comparative analysis of recent\nstate-of-the-art in image matching represented by end-to-end deep networks and\nhybrid pipelines. The evaluation considers both planar and non-planar scenes,\ntaking into account critical and challenging scenarios including abrupt\ntemporal image changes and strong variations in relative image rotations.\nAccording to this analysis, although the impressive progress done in this\nfield, there is still a wide room for improvements to be investigated in future\nresearch.\n","authors":["Fabio Bellavia"],"pdf_url":"https://arxiv.org/pdf/2305.08946v6.pdf","comment":"re-fixed bars in fig. 10"},{"id":"http://arxiv.org/abs/2312.09743v1","updated":"2023-12-15T12:31:20Z","published":"2023-12-15T12:31:20Z","title":"SLS4D: Sparse Latent Space for 4D Novel View Synthesis","summary":"  Neural radiance field (NeRF) has achieved great success in novel view\nsynthesis and 3D representation for static scenarios. Existing dynamic NeRFs\nusually exploit a locally dense grid to fit the deformation field; however,\nthey fail to capture the global dynamics and concomitantly yield models of\nheavy parameters. We observe that the 4D space is inherently sparse. Firstly,\nthe deformation field is sparse in spatial but dense in temporal due to the\ncontinuity of of motion. Secondly, the radiance field is only valid on the\nsurface of the underlying scene, usually occupying a small fraction of the\nwhole space. We thus propose to represent the 4D scene using a learnable sparse\nlatent space, a.k.a. SLS4D. Specifically, SLS4D first uses dense learnable time\nslot features to depict the temporal space, from which the deformation field is\nfitted with linear multi-layer perceptions (MLP) to predict the displacement of\na 3D position at any time. It then learns the spatial features of a 3D position\nusing another sparse latent space. This is achieved by learning the adaptive\nweights of each latent code with the attention mechanism. Extensive experiments\ndemonstrate the effectiveness of our SLS4D: it achieves the best 4D novel view\nsynthesis using only about $6\\%$ parameters of the most recent work.\n","authors":["Qi-Yuan Feng","Hao-Xiang Chen","Qun-Ce Xu","Tai-Jiang Mu"],"pdf_url":"https://arxiv.org/pdf/2312.09743v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2311.10329v2","updated":"2023-12-15T12:23:41Z","published":"2023-11-17T05:03:53Z","title":"High-fidelity Person-centric Subject-to-Image Synthesis","summary":"  Current subject-driven image generation methods encounter significant\nchallenges in person-centric image generation. The reason is that they learn\nthe semantic scene and person generation by fine-tuning a common pre-trained\ndiffusion, which involves an irreconcilable training imbalance. Precisely, to\ngenerate realistic persons, they need to sufficiently tune the pre-trained\nmodel, which inevitably causes the model to forget the rich semantic scene\nprior and makes scene generation over-fit to the training data. Moreover, even\nwith sufficient fine-tuning, these methods can still not generate high-fidelity\npersons since joint learning of the scene and person generation also lead to\nquality compromise. In this paper, we propose Face-diffuser, an effective\ncollaborative generation pipeline to eliminate the above training imbalance and\nquality compromise. Specifically, we first develop two specialized pre-trained\ndiffusion models, i.e., Text-driven Diffusion Model (TDM) and Subject-augmented\nDiffusion Model (SDM), for scene and person generation, respectively. The\nsampling process is divided into three sequential stages, i.e., semantic scene\nconstruction, subject-scene fusion, and subject enhancement. The first and last\nstages are performed by TDM and SDM respectively. The subject-scene fusion\nstage, that is the collaboration achieved through a novel and highly effective\nmechanism, Saliency-adaptive Noise Fusion (SNF). Specifically, it is based on\nour key observation that there exists a robust link between classifier-free\nguidance responses and the saliency of generated images. In each time step, SNF\nleverages the unique strengths of each model and allows for the spatial\nblending of predicted noises from both models automatically in a saliency-aware\nmanner. Extensive experiments confirm the impressive effectiveness and\nrobustness of the Face-diffuser.\n","authors":["Yibin Wang","Weizhong Zhang","Jianwei Zheng","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2311.10329v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16835v3","updated":"2023-12-15T12:19:34Z","published":"2023-11-28T14:51:08Z","title":"Unified-modal Salient Object Detection via Adaptive Prompt Learning","summary":"  Existing single-modal and multi-modal salient object detection (SOD) methods\nfocus on designing specific architectures tailored for their respective tasks.\nHowever, developing completely different models for different tasks leads to\nlabor and time consumption, as well as high computational and practical\ndeployment costs. In this paper, we make the first attempt to address both\nsingle-modal and multi-modal SOD in a unified framework called UniSOD.\nNevertheless, assigning appropriate strategies to modality variable inputs is\nchallenging. To this end, UniSOD learns modality-aware prompts with\ntask-specific hints through adaptive prompt learning, which are plugged into\nthe proposed pre-trained baseline SOD model to handle corresponding tasks,\nwhile only requiring few learnable parameters compared to training the entire\nmodel. Each modality-aware prompt is generated from a switchable prompt\ngeneration block, which performs structural switching solely relied on\nsingle-modal and multi-modal inputs. UniSOD achieves consistent performance\nimprovement on 14 benchmark datasets for RGB, RGB-D, and RGB-T SOD, which\ndemonstrates that our method effectively and efficiently unifies single-modal\nand multi-modal SOD tasks.\n","authors":["Kunpeng Wang","Chenglong Li","Zhengzheng Tu","Bin Luo"],"pdf_url":"https://arxiv.org/pdf/2311.16835v3.pdf","comment":"15 pages, 10 figures"},{"id":"http://arxiv.org/abs/2303.10891v3","updated":"2023-12-15T12:12:03Z","published":"2023-03-20T06:16:22Z","title":"Non-Exemplar Online Class-incremental Continual Learning via\n  Dual-prototype Self-augment and Refinement","summary":"  This paper investigates a new, practical, but challenging problem named\nNon-exemplar Online Class-incremental continual Learning (NO-CL), which aims to\npreserve the discernibility of base classes without buffering data examples and\nefficiently learn novel classes continuously in a single-pass (i.e., online)\ndata stream. The challenges of this task are mainly two-fold: (1) Both base and\nnovel classes suffer from severe catastrophic forgetting as no previous samples\nare available for replay. (2) As the online data can only be observed once,\nthere is no way to fully re-train the whole model, e.g., re-calibrate the\ndecision boundaries via prototype alignment or feature distillation. In this\npaper, we propose a novel Dual-prototype Self-augment and Refinement method\n(DSR) for NO-CL problem, which consists of two strategies: 1) Dual class\nprototypes: vanilla and high-dimensional prototypes are exploited to utilize\nthe pre-trained information and obtain robust quasi-orthogonal representations\nrather than example buffers for both privacy preservation and memory reduction.\n2) Self-augment and refinement: Instead of updating the whole network, we\noptimize high-dimensional prototypes alternatively with the extra projection\nmodule based on self-augment vanilla prototypes, through a bi-level\noptimization problem. Extensive experiments demonstrate the effectiveness and\nsuperiority of the proposed DSR in NO-CL.\n","authors":["Fushuo Huo","Wenchao Xu","Jingcai Guo","Haozhao Wang","Yunfeng Fan","Song Guo"],"pdf_url":"https://arxiv.org/pdf/2303.10891v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04008v4","updated":"2023-12-15T12:06:36Z","published":"2023-12-07T02:55:46Z","title":"Natural-language-driven Simulation Benchmark and Copilot for Efficient\n  Production of Object Interactions in Virtual Road Scenes","summary":"  We advocate the idea of the natural-language-driven(NLD) simulation to\nefficiently produce the object interactions between multiple objects in the\nvirtual road scenes, for teaching and testing the autonomous driving systems\nthat should take quick action to avoid collision with obstacles with\nunpredictable motions. The NLD simulation allows the brief natural-language\ndescription to control the object interactions, significantly reducing the\nhuman efforts for creating a large amount of interaction data. To facilitate\nthe research of NLD simulation, we collect the Language-to-Interaction(L2I)\nbenchmark dataset with 120,000 natural-language descriptions of object\ninteractions in 6 common types of road topologies. Each description is\nassociated with the programming code, which the graphic render can use to\nvisually reconstruct the object interactions in the virtual scenes. As a\nmethodology contribution, we design SimCopilot to translate the interaction\ndescriptions to the renderable code. We use the L2I dataset to evaluate\nSimCopilot's abilities to control the object motions, generate complex\ninteractions, and generalize interactions across road topologies. The L2I\ndataset and the evaluation results motivate the relevant research of the NLD\nsimulation.\n","authors":["Kairui Yang","Zihao Guo","Gengjie Lin","Haotian Dong","Die Zuo","Jibin Peng","Zhao Huang","Zhecheng Xu","Fupeng Li","Ziyun Bai","Di Lin"],"pdf_url":"https://arxiv.org/pdf/2312.04008v4.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2312.09727v1","updated":"2023-12-15T12:04:24Z","published":"2023-12-15T12:04:24Z","title":"LiteVSR: Efficient Visual Speech Recognition by Learning from Speech\n  Representations of Unlabeled Data","summary":"  This paper proposes a novel, resource-efficient approach to Visual Speech\nRecognition (VSR) leveraging speech representations produced by any trained\nAutomatic Speech Recognition (ASR) model. Moving away from the\nresource-intensive trends prevalent in recent literature, our method distills\nknowledge from a trained Conformer-based ASR model, achieving competitive\nperformance on standard VSR benchmarks with significantly less resource\nutilization. Using unlabeled audio-visual data only, our baseline model\nachieves a word error rate (WER) of 47.4% and 54.7% on the LRS2 and LRS3 test\nbenchmarks, respectively. After fine-tuning the model with limited labeled\ndata, the word error rate reduces to 35% (LRS2) and 45.7% (LRS3). Our model can\nbe trained on a single consumer-grade GPU within a few days and is capable of\nperforming real-time end-to-end VSR on dated hardware, suggesting a path\ntowards more accessible and resource-efficient VSR methodologies.\n","authors":["Hendrik Laux","Emil Mededovic","Ahmed Hallawa","Lukas Martin","Arne Peine","Anke Schmeink"],"pdf_url":"https://arxiv.org/pdf/2312.09727v1.pdf","comment":"Accepted for publication at ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.09723v1","updated":"2023-12-15T11:53:17Z","published":"2023-12-15T11:53:17Z","title":"Tracking Skiers from the Top to the Bottom","summary":"  Skiing is a popular winter sport discipline with a long history of\ncompetitive events. In this domain, computer vision has the potential to\nenhance the understanding of athletes' performance, but its application lags\nbehind other sports due to limited studies and datasets. This paper makes a\nstep forward in filling such gaps. A thorough investigation is performed on the\ntask of skier tracking in a video capturing his/her complete performance.\nObtaining continuous and accurate skier localization is preemptive for further\nhigher-level performance analyses. To enable the study, the largest and most\nannotated dataset for computer vision in skiing, SkiTB, is introduced. Several\nvisual object tracking algorithms, including both established methodologies and\na newly introduced skier-optimized baseline algorithm, are tested using the\ndataset. The results provide valuable insights into the applicability of\ndifferent tracking methods for vision-based skiing analysis. SkiTB, code, and\nresults are available at https://machinelearning.uniud.it/datasets/skitb.\n","authors":["Matteo Dunnhofer","Luca Sordi","Niki Martinel","Christian Micheloni"],"pdf_url":"https://arxiv.org/pdf/2312.09723v1.pdf","comment":"IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)\n  2024"},{"id":"http://arxiv.org/abs/2211.12417v4","updated":"2023-12-15T11:50:32Z","published":"2022-11-19T10:09:46Z","title":"ProCC: Progressive Cross-primitive Compatibility for Open-World\n  Compositional Zero-Shot Learning","summary":"  Open-World Compositional Zero-shot Learning (OW-CZSL) aims to recognize novel\ncompositions of state and object primitives in images with no priors on the\ncompositional space, which induces a tremendously large output space containing\nall possible state-object compositions. Existing works either learn the joint\ncompositional state-object embedding or predict simple primitives with separate\nclassifiers. However, the former heavily relies on external word embedding\nmethods, and the latter ignores the interactions of interdependent primitives,\nrespectively. In this paper, we revisit the primitive prediction approach and\npropose a novel method, termed Progressive Cross-primitive Compatibility\n(ProCC), to mimic the human learning process for OW-CZSL tasks. Specifically,\nthe cross-primitive compatibility module explicitly learns to model the\ninteractions of state and object features with the trainable memory units,\nwhich efficiently acquires cross-primitive visual attention to reason\nhigh-feasibility compositions, without the aid of external knowledge. Moreover,\nconsidering the partial-supervision setting (pCZSL) as well as the imbalance\nissue of multiple task prediction, we design a progressive training paradigm to\nenable the primitive classifiers to interact to obtain discriminative\ninformation in an easy-to-hard manner. Extensive experiments on three widely\nused benchmark datasets demonstrate that our method outperforms other\nrepresentative methods on both OW-CZSL and pCZSL settings by large margins.\n","authors":["Fushuo Huo","Wenchao Xu","Song Guo","Jingcai Guo","Haozhao Wang","Ziming Liu","Xiaocheng Lu"],"pdf_url":"https://arxiv.org/pdf/2211.12417v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09719v1","updated":"2023-12-15T11:46:29Z","published":"2023-12-15T11:46:29Z","title":"On the calibration of neural networks for histological slide-level\n  classification","summary":"  Deep Neural Networks have shown promising classification performance when\npredicting certain biomarkers from Whole Slide Images in digital pathology.\nHowever, the calibration of the networks' output probabilities is often not\nevaluated. Communicating uncertainty by providing reliable confidence scores is\nof high relevance in the medical context. In this work, we compare three neural\nnetwork architectures that combine feature representations on patch-level to a\nslide-level prediction with respect to their classification performance and\nevaluate their calibration. As slide-level classification task, we choose the\nprediction of Microsatellite Instability from Colorectal Cancer tissue\nsections. We observe that Transformers lead to good results in terms of\nclassification performance and calibration. When evaluating the classification\nperformance on a separate dataset, we observe that Transformers generalize\nbest. The investigation of reliability diagrams provides additional insights to\nthe Expected Calibration Error metric and we observe that especially\nTransformers push the output probabilities to extreme values, which results in\noverconfident predictions.\n","authors":["Alexander Kurz","Hendrik A. Mehrtens","Tabea-Clara Bucher","Titus J. Brinker"],"pdf_url":"https://arxiv.org/pdf/2312.09719v1.pdf","comment":"7 pages, 2 figures, 2 tables"},{"id":"http://arxiv.org/abs/2312.09716v1","updated":"2023-12-15T11:43:56Z","published":"2023-12-15T11:43:56Z","title":"Let All be Whitened: Multi-teacher Distillation for Efficient Visual\n  Retrieval","summary":"  Visual retrieval aims to search for the most relevant visual items, e.g.,\nimages and videos, from a candidate gallery with a given query item. Accuracy\nand efficiency are two competing objectives in retrieval tasks. Instead of\ncrafting a new method pursuing further improvement on accuracy, in this paper\nwe propose a multi-teacher distillation framework Whiten-MTD, which is able to\ntransfer knowledge from off-the-shelf pre-trained retrieval models to a\nlightweight student model for efficient visual retrieval. Furthermore, we\ndiscover that the similarities obtained by different retrieval models are\ndiversified and incommensurable, which makes it challenging to jointly distill\nknowledge from multiple models. Therefore, we propose to whiten the output of\nteacher models before fusion, which enables effective multi-teacher\ndistillation for retrieval models. Whiten-MTD is conceptually simple and\npractically effective. Extensive experiments on two landmark image retrieval\ndatasets and one video retrieval dataset demonstrate the effectiveness of our\nproposed method, and its good balance of retrieval performance and efficiency.\nOur source code is released at https://github.com/Maryeon/whiten_mtd.\n","authors":["Zhe Ma","Jianfeng Dong","Shouling Ji","Zhenguang Liu","Xuhong Zhang","Zonghui Wang","Sifeng He","Feng Qian","Xiaobo Zhang","Lei Yang"],"pdf_url":"https://arxiv.org/pdf/2312.09716v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.09709v1","updated":"2023-12-15T11:32:11Z","published":"2023-12-15T11:32:11Z","title":"ParsNets: A Parsimonious Orthogonal and Low-Rank Linear Networks for\n  Zero-Shot Learning","summary":"  This paper provides a novel parsimonious yet efficient design for zero-shot\nlearning (ZSL), dubbed ParsNets, where we are interested in learning a\ncomposition of on-device friendly linear networks, each with orthogonality and\nlow-rankness properties, to achieve equivalent or even better performance\nagainst existing deep models. Concretely, we first refactor the core module of\nZSL, i.e., visual-semantics mapping function, into several base linear networks\nthat correspond to diverse components of the semantic space, where the complex\nnonlinearity can be collapsed into simple local linearities. Then, to\nfacilitate the generalization of local linearities, we construct a maximal\nmargin geometry on the learned features by enforcing low-rank constraints on\nintra-class samples and high-rank constraints on inter-class samples, resulting\nin orthogonal subspaces for different classes and each subspace lies on a\ncompact manifold. To enhance the model's adaptability and counterbalance\nover/under-fittings in ZSL, a set of sample-wise indicators is employed to\nselect a sparse subset from these base linear networks to form a composite\nsemantic predictor for each sample. Notably, maximal margin geometry can\nguarantee the diversity of features, and meanwhile, local linearities guarantee\nefficiency. Thus, our ParsNets can generalize better to unseen classes and can\nbe deployed flexibly on resource-constrained devices. Theoretical explanations\nand extensive experiments are conducted to verify the effectiveness of the\nproposed method.\n","authors":["Jingcai Guo","Qihua Zhou","Ruibing Li","Xiaocheng Lu","Ziming Liu","Junyang Chen","Xin Xie","Jie Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.09709v1.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2312.08774v2","updated":"2023-12-15T11:28:30Z","published":"2023-12-14T09:50:09Z","title":"VSFormer: Visual-Spatial Fusion Transformer for Correspondence Pruning","summary":"  Correspondence pruning aims to find correct matches (inliers) from an initial\nset of putative correspondences, which is a fundamental task for many\napplications. The process of finding is challenging, given the varying inlier\nratios between scenes/image pairs due to significant visual differences.\nHowever, the performance of the existing methods is usually limited by the\nproblem of lacking visual cues (\\eg texture, illumination, structure) of\nscenes. In this paper, we propose a Visual-Spatial Fusion Transformer\n(VSFormer) to identify inliers and recover camera poses accurately. Firstly, we\nobtain highly abstract visual cues of a scene with the cross attention between\nlocal features of two-view images. Then, we model these visual cues and\ncorrespondences by a joint visual-spatial fusion module, simultaneously\nembedding visual cues into correspondences for pruning. Additionally, to mine\nthe consistency of correspondences, we also design a novel module that combines\nthe KNN-based graph and the transformer, effectively capturing both local and\nglobal contexts. Extensive experiments have demonstrated that the proposed\nVSFormer outperforms state-of-the-art methods on outdoor and indoor benchmarks.\n","authors":["Tangfei Liao","Xiaoqin Zhang","Li Zhao","Tao Wang","Guobao Xiao"],"pdf_url":"https://arxiv.org/pdf/2312.08774v2.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2312.09682v1","updated":"2023-12-15T10:56:07Z","published":"2023-12-15T10:56:07Z","title":"Exploring the Feasibility of Generating Realistic 3D Models of\n  Endangered Species Using DreamGaussian: An Analysis of Elevation Angle's\n  Impact on Model Generation","summary":"  Many species face the threat of extinction. It's important to study these\nspecies and gather information about them as much as possible to preserve\nbiodiversity. Due to the rarity of endangered species, there is a limited\namount of data available, making it difficult to apply data requiring\ngenerative AI methods to this domain. We aim to study the feasibility of\ngenerating consistent and real-like 3D models of endangered animals using\nlimited data. Such a phenomenon leads us to utilize zero-shot stable diffusion\nmodels that can generate a 3D model out of a single image of the target\nspecies. This paper investigates the intricate relationship between elevation\nangle and the output quality of 3D model generation, focusing on the innovative\napproach presented in DreamGaussian. DreamGaussian, a novel framework utilizing\nGenerative Gaussian Splatting along with novel mesh extraction and refinement\nalgorithms, serves as the focal point of our study. We conduct a comprehensive\nanalysis, analyzing the effect of varying elevation angles on DreamGaussian's\nability to reconstruct 3D scenes accurately. Through an empirical evaluation,\nwe demonstrate how changes in elevation angle impact the generated images'\nspatial coherence, structural integrity, and perceptual realism. We observed\nthat giving a correct elevation angle with the input image significantly\naffects the result of the generated 3D model. We hope this study to be\ninfluential for the usability of AI to preserve endangered animals; while the\npenultimate aim is to obtain a model that can output biologically consistent 3D\nmodels via small samples, the qualitative interpretation of an existing\nstate-of-the-art model such as DreamGaussian will be a step forward in our\ngoal.\n","authors":["Selcuk Anil Karatopak","Deniz Sen"],"pdf_url":"https://arxiv.org/pdf/2312.09682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09681v1","updated":"2023-12-15T10:53:09Z","published":"2023-12-15T10:53:09Z","title":"Urban Region Embedding via Multi-View Contrastive Prediction","summary":"  Recently, learning urban region representations utilizing multi-modal data\n(information views) has become increasingly popular, for deep understanding of\nthe distributions of various socioeconomic features in cities. However,\nprevious methods usually blend multi-view information in a posteriors stage,\nfalling short in learning coherent and consistent representations across\ndifferent views. In this paper, we form a new pipeline to learn consistent\nrepresentations across varying views, and propose the multi-view Contrastive\nPrediction model for urban Region embedding (ReCP), which leverages the\nmultiple information views from point-of-interest (POI) and human mobility\ndata. Specifically, ReCP comprises two major modules, namely an intra-view\nlearning module utilizing contrastive learning and feature reconstruction to\ncapture the unique information from each single view, and inter-view learning\nmodule that perceives the consistency between the two views using a contrastive\nprediction learning scheme. We conduct thorough experiments on two downstream\ntasks to assess the proposed model, i.e., land use clustering and region\npopularity prediction. The experimental results demonstrate that our model\noutperforms state-of-the-art baseline methods significantly in urban region\nrepresentation learning.\n","authors":["Zechen Li","Weiming Huang","Kai Zhao","Min Yang","Yongshun Gong","Meng Chen"],"pdf_url":"https://arxiv.org/pdf/2312.09681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.10839v3","updated":"2023-12-15T10:49:49Z","published":"2023-04-21T09:30:22Z","title":"Multi-frame-based Cross-domain Denoising for Low-dose Spiral Computed\n  Tomography","summary":"  Computed tomography (CT) has been used worldwide as a non-invasive test in\nassisting diagnosis. However, the ionizing nature of X-ray exposure raises\nconcerns about potential health risks such as cancer. The desire for lower\nradiation doses has driven researchers to improve reconstruction quality.\nAlthough previous studies on low-dose computed tomography (LDCT) denoising have\ndemonstrated the effectiveness of learning-based methods, most were developed\non the simulated data collected using the Radon transform. However, the\nreal-world scenario differs significantly from the simulation domain,\nespecially when using the multi-slice spiral scanner geometry. This paper\nproposes a two-stage method for the commercially available third-generation\nmulti-slice spiral CT scanners that better exploits the complete reconstruction\npipeline for LDCT denoising across different domains. Our approach makes good\nuse of the high redundancy of the multi-slice projections and the volumetric\nreconstructions while leveraging the over-smoothing of high-frequency\ninformation in conventional cascaded frameworks due to aggressive denoising.\nThe dedicated design also provides a more explicit interpretation of the data\nflow. Extensive experiments on various datasets showed that the proposed method\ncould remove up to 70% of noise without compromised spatial resolution, while\nsubjective evaluations by two radiologists further supported its superior\nperformance against state-of-the-art methods in clinical practice.\n","authors":["Yucheng Lu","Zhixin Xu","Moon Hyung Choi","Jimin Kim","Seung-Won Jung"],"pdf_url":"https://arxiv.org/pdf/2304.10839v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.05421v2","updated":"2023-12-15T10:48:54Z","published":"2023-05-09T13:13:53Z","title":"DC3DCD: unsupervised learning for multiclass 3D point cloud change\n  detection","summary":"  In a constant evolving world, change detection is of prime importance to keep\nupdated maps. To better sense areas with complex geometry (urban areas in\nparticular), considering 3D data appears to be an interesting alternative to\nclassical 2D images. In this context, 3D point clouds (PCs), whether obtained\nthrough LiDAR or photogrammetric techniques, provide valuable information.\nWhile recent studies showed the considerable benefit of using deep\nlearning-based methods to detect and characterize changes into raw 3D PCs,\nthese studies rely on large annotated training data to obtain accurate results.\nThe collection of these annotations are tricky and time-consuming. The\navailability of unsupervised or weakly supervised approaches is then of prime\ninterest. In this paper, we propose an unsupervised method, called DeepCluster\n3D Change Detection (DC3DCD), to detect and categorize multiclass changes at\npoint level. We classify our approach in the unsupervised family given the fact\nthat we extract in a completely unsupervised way a number of clusters\nassociated with potential changes. Let us precise that in the end of the\nprocess, the user has only to assign a label to each of these clusters to\nderive the final change map. Our method builds upon the DeepCluster approach,\noriginally designed for image classification, to handle complex raw 3D PCs and\nperform change segmentation task. An assessment of the method on both simulated\nand real public dataset is provided. The proposed method allows to outperform\nfully-supervised traditional machine learning algorithm and to be competitive\nwith fully-supervised deep learning networks applied on rasterization of 3D PCs\nwith a mean of IoU over classes of change of 57.06\\% and 66.69\\% for the\nsimulated and the real datasets, respectively.\n","authors":["Iris de Gélis","Sébastien Lefèvre","Thomas Corpetti"],"pdf_url":"https://arxiv.org/pdf/2305.05421v2.pdf","comment":"This work has been accepted to Elsevier for publication. Copyright\n  may be transferred without notice, after which this version may no longer be\n  accessible"},{"id":"http://arxiv.org/abs/2312.09676v1","updated":"2023-12-15T10:40:34Z","published":"2023-12-15T10:40:34Z","title":"nuScenes Knowledge Graph -- A comprehensive semantic representation of\n  traffic scenes for trajectory prediction","summary":"  Trajectory prediction in traffic scenes involves accurately forecasting the\nbehaviour of surrounding vehicles. To achieve this objective it is crucial to\nconsider contextual information, including the driving path of vehicles, road\ntopology, lane dividers, and traffic rules. Although studies demonstrated the\npotential of leveraging heterogeneous context for improving trajectory\nprediction, state-of-the-art deep learning approaches still rely on a limited\nsubset of this information. This is mainly due to the limited availability of\ncomprehensive representations. This paper presents an approach that utilizes\nknowledge graphs to model the diverse entities and their semantic connections\nwithin traffic scenes. Further, we present nuScenes Knowledge Graph (nSKG), a\nknowledge graph for the nuScenes dataset, that models explicitly all scene\nparticipants and road elements, as well as their semantic and spatial\nrelationships. To facilitate the usage of the nSKG via graph neural networks\nfor trajectory prediction, we provide the data in a format, ready-to-use by the\nPyG library. All artefacts can be found here:\nhttps://github.com/boschresearch/nuScenes_Knowledge_Graph\n","authors":["Leon Mlodzian","Zhigang Sun","Hendrik Berkemeyer","Sebastian Monka","Zixu Wang","Stefan Dietze","Lavdim Halilaj","Juergen Luettin"],"pdf_url":"https://arxiv.org/pdf/2312.09676v1.pdf","comment":"Accepted to the 2023 IEEE/CVF International Converence on Computer\n  Vision (ICCV) workshop on Scene Graphs and Graph Representation Learning\n  (SG2RL)"},{"id":"http://arxiv.org/abs/2312.09673v1","updated":"2023-12-15T10:35:30Z","published":"2023-12-15T10:35:30Z","title":"Style Generation in Robot Calligraphy with Deep Generative Adversarial\n  Networks","summary":"  Robot calligraphy is an emerging exploration of artificial intelligence in\nthe fields of art and education. Traditional calligraphy generation researches\nmainly focus on methods such as tool-based image processing, generative models,\nand style transfer. Unlike the English alphabet, the number of Chinese\ncharacters is tens of thousands, which leads to difficulties in the generation\nof a style consistent Chinese calligraphic font with over 6000 characters. Due\nto the lack of high-quality data sets, formal definitions of calligraphy\nknowledge, and scientific art evaluation methods, The results generated are\nfrequently of low quality and falls short of professional-level requirements.\nTo address the above problem, this paper proposes an automatic calligraphy\ngeneration model based on deep generative adversarial networks (deepGAN) that\ncan generate style calligraphy fonts with professional standards. The key\nhighlights of the proposed method include: (1) The datasets use a\nhigh-precision calligraphy synthesis method to ensure its high quality and\nsufficient quantity; (2) Professional calligraphers are invited to conduct a\nseries of Turing tests to evaluate the gap between model generation results and\nhuman artistic level; (3) Experimental results indicate that the proposed model\nis the state-of-the-art among current calligraphy generation methods. The\nTuring tests and similarity evaluations validate the effectiveness of the\nproposed method.\n","authors":["Xiaoming Wang","Zhiguo Gong"],"pdf_url":"https://arxiv.org/pdf/2312.09673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.11681v3","updated":"2023-12-15T09:42:25Z","published":"2023-08-22T14:58:36Z","title":"VadCLIP: Adapting Vision-Language Models for Weakly Supervised Video\n  Anomaly Detection","summary":"  The recent contrastive language-image pre-training (CLIP) model has shown\ngreat success in a wide range of image-level tasks, revealing remarkable\nability for learning powerful visual representations with rich semantics. An\nopen and worthwhile problem is efficiently adapting such a strong model to the\nvideo domain and designing a robust video anomaly detector. In this work, we\npropose VadCLIP, a new paradigm for weakly supervised video anomaly detection\n(WSVAD) by leveraging the frozen CLIP model directly without any pre-training\nand fine-tuning process. Unlike current works that directly feed extracted\nfeatures into the weakly supervised classifier for frame-level binary\nclassification, VadCLIP makes full use of fine-grained associations between\nvision and language on the strength of CLIP and involves dual branch. One\nbranch simply utilizes visual features for coarse-grained binary\nclassification, while the other fully leverages the fine-grained language-image\nalignment. With the benefit of dual branch, VadCLIP achieves both\ncoarse-grained and fine-grained video anomaly detection by transferring\npre-trained knowledge from CLIP to WSVAD task. We conduct extensive experiments\non two commonly-used benchmarks, demonstrating that VadCLIP achieves the best\nperformance on both coarse-grained and fine-grained WSVAD, surpassing the\nstate-of-the-art methods by a large margin. Specifically, VadCLIP achieves\n84.51% AP and 88.02% AUC on XD-Violence and UCF-Crime, respectively. Code and\nfeatures are released at https://github.com/nwpu-zxr/VadCLIP.\n","authors":["Peng Wu","Xuerong Zhou","Guansong Pang","Lingru Zhou","Qingsen Yan","Peng Wang","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2308.11681v3.pdf","comment":"Accept to AAAI2024"},{"id":"http://arxiv.org/abs/2309.16137v2","updated":"2023-12-15T09:32:37Z","published":"2023-09-28T03:35:25Z","title":"Context-I2W: Mapping Images to Context-dependent Words for Accurate\n  Zero-Shot Composed Image Retrieval","summary":"  Different from Composed Image Retrieval task that requires expensive labels\nfor training task-specific models, Zero-Shot Composed Image Retrieval (ZS-CIR)\ninvolves diverse tasks with a broad range of visual content manipulation intent\nthat could be related to domain, scene, object, and attribute. The key\nchallenge for ZS-CIR tasks is to learn a more accurate image representation\nthat has adaptive attention to the reference image for various manipulation\ndescriptions. In this paper, we propose a novel context-dependent mapping\nnetwork, named Context-I2W, for adaptively converting description-relevant\nImage information into a pseudo-word token composed of the description for\naccurate ZS-CIR. Specifically, an Intent View Selector first dynamically learns\na rotation rule to map the identical image to a task-specific manipulation\nview. Then a Visual Target Extractor further captures local information\ncovering the main targets in ZS-CIR tasks under the guidance of multiple\nlearnable queries. The two complementary modules work together to map an image\nto a context-dependent pseudo-word token without extra supervision. Our model\nshows strong generalization ability on four ZS-CIR tasks, including domain\nconversion, object composition, object manipulation, and attribute\nmanipulation. It obtains consistent and significant performance boosts ranging\nfrom 1.88% to 3.60% over the best methods and achieves new state-of-the-art\nresults on ZS-CIR. Our code is available at\nhttps://github.com/Pter61/context-i2w.\n","authors":["Yuanmin Tang","Jing Yu","Keke Gai","Jiamin Zhuang","Gang Xiong","Yue Hu","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2309.16137v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09641v1","updated":"2023-12-15T09:30:47Z","published":"2023-12-15T09:30:47Z","title":"Ins-HOI: Instance Aware Human-Object Interactions Recovery","summary":"  Recovering detailed interactions between humans/hands and objects is an\nappealing yet challenging task. Existing methods typically use template-based\nrepresentations to track human/hand and objects in interactions. Despite the\nprogress, they fail to handle the invisible contact surfaces. In this paper, we\npropose Ins-HOI, an end-to-end solution to recover human/hand-object\nreconstruction via instance-level implicit reconstruction. To this end, we\nintroduce an instance-level occupancy field to support simultaneous human/hand\nand object representation, and a complementary training strategy to handle the\nlack of instance-level ground truths. Such a representation enables learning a\ncontact prior implicitly from sparse observations. During the complementary\ntraining, we augment the real-captured data with synthesized data by randomly\ncomposing individual scans of humans/hands and objects and intentionally\nallowing for penetration. In this way, our network learns to recover individual\nshapes as completely as possible from the synthesized data, while being aware\nof the contact constraints and overall reasonability based on real-captured\nscans. As demonstrated in experiments, our method Ins-HOI can produce\nreasonable and realistic non-visible contact surfaces even in cases of\nextremely close interaction. To facilitate the research of this task, we\ncollect a large-scale, high-fidelity 3D scan dataset, including 5.2k\nhigh-quality scans with real-world human-chair and hand-object interactions. We\nwill release our dataset and source codes. Data examples and the video results\nof our method can be found on the project page.\n","authors":["Jiajun Zhang","Yuxiang Zhang","Hongwen Zhang","Boyao Zhou","Ruizhi Shao","Zonghai Hu","Yebin Liu"],"pdf_url":"https://arxiv.org/pdf/2312.09641v1.pdf","comment":"Project Page: https://jiajunzhang16.github.io/ins-hoi/ , Code and\n  Dataset Page: https://github.com/jiajunzhang16/ins-hoi"},{"id":"http://arxiv.org/abs/2312.09630v1","updated":"2023-12-15T09:19:00Z","published":"2023-12-15T09:19:00Z","title":"Pixel-Superpixel Contrastive Learning and Pseudo-Label Correction for\n  Hyperspectral Image Clustering","summary":"  Hyperspectral image (HSI) clustering is gaining considerable attention owing\nto recent methods that overcome the inefficiency and misleading results from\nthe absence of supervised information. Contrastive learning methods excel at\nexisting pixel level and super pixel level HSI clustering tasks. The\npixel-level contrastive learning method can effectively improve the ability of\nthe model to capture fine features of HSI but requires a large time overhead.\nThe super pixel-level contrastive learning method utilizes the homogeneity of\nHSI and reduces computing resources; however, it yields rough classification\nresults. To exploit the strengths of both methods, we present a pixel super\npixel contrastive learning and pseudo-label correction (PSCPC) method for the\nHSI clustering. PSCPC can reasonably capture domain-specific and fine-grained\nfeatures through super pixels and the comparative learning of a small number of\npixels within the super pixels. To improve the clustering performance of super\npixels, this paper proposes a pseudo-label correction module that aligns the\nclustering pseudo-labels of pixels and super-pixels. In addition, pixel-level\nclustering results are used to supervise super pixel-level clustering,\nimproving the generalization ability of the model. Extensive experiments\ndemonstrate the effectiveness and efficiency of PSCPC.\n","authors":["Renxiang Guan","Zihao Li","Xianju Li","Chang Tang"],"pdf_url":"https://arxiv.org/pdf/2312.09630v1.pdf","comment":"Accepted at IEEE ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.09627v1","updated":"2023-12-15T09:10:05Z","published":"2023-12-15T09:10:05Z","title":"TF-CLIP: Learning Text-free CLIP for Video-based Person\n  Re-Identification","summary":"  Large-scale language-image pre-trained models (e.g., CLIP) have shown\nsuperior performances on many cross-modal retrieval tasks. However, the problem\nof transferring the knowledge learned from such models to video-based person\nre-identification (ReID) has barely been explored. In addition, there is a lack\nof decent text descriptions in current ReID benchmarks. To address these\nissues, in this work, we propose a novel one-stage text-free CLIP-based\nlearning framework named TF-CLIP for video-based person ReID. More\nspecifically, we extract the identity-specific sequence feature as the\nCLIP-Memory to replace the text feature. Meanwhile, we design a\nSequence-Specific Prompt (SSP) module to update the CLIP-Memory online. To\ncapture temporal information, we further propose a Temporal Memory Diffusion\n(TMD) module, which consists of two key components: Temporal Memory\nConstruction (TMC) and Memory Diffusion (MD). Technically, TMC allows the\nframe-level memories in a sequence to communicate with each other, and to\nextract temporal information based on the relations within the sequence. MD\nfurther diffuses the temporal memories to each token in the original features\nto obtain more robust sequence features. Extensive experiments demonstrate that\nour proposed method shows much better results than other state-of-the-art\nmethods on MARS, LS-VID and iLIDS-VID. The code is available at\nhttps://github.com/AsuradaYuci/TF-CLIP.\n","authors":["Chenyang Yu","Xuehu Liu","Yingquan Wang","Pingping Zhang","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2312.09627v1.pdf","comment":"This work is accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2312.09625v1","updated":"2023-12-15T09:08:14Z","published":"2023-12-15T09:08:14Z","title":"Weakly-Supervised 3D Visual Grounding based on Visual Linguistic\n  Alignment","summary":"  Learning to ground natural language queries to target objects or regions in\n3D point clouds is quite essential for 3D scene understanding. Nevertheless,\nexisting 3D visual grounding approaches require a substantial number of\nbounding box annotations for text queries, which is time-consuming and\nlabor-intensive to obtain. In this paper, we propose \\textbf{3D-VLA}, a weakly\nsupervised approach for \\textbf{3D} visual grounding based on \\textbf{V}isual\n\\textbf{L}inguistic \\textbf{A}lignment. Our 3D-VLA exploits the superior\nability of current large-scale vision-language models (VLMs) on aligning the\nsemantics between texts and 2D images, as well as the naturally existing\ncorrespondences between 2D images and 3D point clouds, and thus implicitly\nconstructs correspondences between texts and 3D point clouds with no need for\nfine-grained box annotations in the training procedure. During the inference\nstage, the learned text-3D correspondence will help us ground the text queries\nto the 3D target objects even without 2D images. To the best of our knowledge,\nthis is the first work to investigate 3D visual grounding in a weakly\nsupervised manner by involving large scale vision-language models, and\nextensive experiments on ReferIt3D and ScanRefer datasets demonstrate that our\n3D-VLA achieves comparable and even superior results over the fully supervised\nmethods.\n","authors":["Xiaoxu Xu","Yitian Yuan","Qiudan Zhang","Wenhui Wu","Zequn Jie","Lin Ma","Xu Wang"],"pdf_url":"https://arxiv.org/pdf/2312.09625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09612v1","updated":"2023-12-15T08:54:15Z","published":"2023-12-15T08:54:15Z","title":"TOP-ReID: Multi-spectral Object Re-Identification with Token Permutation","summary":"  Multi-spectral object Re-identification (ReID) aims to retrieve specific\nobjects by leveraging complementary information from different image spectra.\nIt delivers great advantages over traditional single-spectral ReID in complex\nvisual environment. However, the significant distribution gap among different\nimage spectra poses great challenges for effective multi-spectral feature\nrepresentations. In addition, most of current Transformer-based ReID methods\nonly utilize the global feature of class tokens to achieve the holistic\nretrieval, ignoring the local discriminative ones. To address the above issues,\nwe step further to utilize all the tokens of Transformers and propose a cyclic\ntoken permutation framework for multi-spectral object ReID, dubbled TOP-ReID.\nMore specifically, we first deploy a multi-stream deep network based on vision\nTransformers to preserve distinct information from different image spectra.\nThen, we propose a Token Permutation Module (TPM) for cyclic multi-spectral\nfeature aggregation. It not only facilitates the spatial feature alignment\nacross different image spectra, but also allows the class token of each\nspectrum to perceive the local details of other spectra. Meanwhile, we propose\na Complementary Reconstruction Module (CRM), which introduces dense token-level\nreconstruction constraints to reduce the distribution gap across different\nimage spectra. With the above modules, our proposed framework can generate more\ndiscriminative multi-spectral features for robust object ReID. Extensive\nexperiments on three ReID benchmarks (i.e., RGBNT201, RGBNT100 and MSVR310)\nverify the effectiveness of our methods. The code is available at\nhttps://github.com/924973292/TOP-ReID.\n","authors":["Yuhao Wang","Xuehu Liu","Pingping Zhang","Hu Lu","Zhengzheng Tu","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2312.09612v1.pdf","comment":"This work is accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2312.09609v1","updated":"2023-12-15T08:50:00Z","published":"2023-12-15T08:50:00Z","title":"Semantic-Aware Transformation-Invariant RoI Align","summary":"  Great progress has been made in learning-based object detection methods in\nthe last decade. Two-stage detectors often have higher detection accuracy than\none-stage detectors, due to the use of region of interest (RoI) feature\nextractors which extract transformation-invariant RoI features for different\nRoI proposals, making refinement of bounding boxes and prediction of object\ncategories more robust and accurate. However, previous RoI feature extractors\ncan only extract invariant features under limited transformations. In this\npaper, we propose a novel RoI feature extractor, termed Semantic RoI Align\n(SRA), which is capable of extracting invariant RoI features under a variety of\ntransformations for two-stage detectors. Specifically, we propose a semantic\nattention module to adaptively determine different sampling areas by leveraging\nthe global and local semantic relationship within the RoI. We also propose a\nDynamic Feature Sampler which dynamically samples features based on the RoI\naspect ratio to enhance the efficiency of SRA, and a new position embedding,\n\\ie Area Embedding, to provide more accurate position information for SRA\nthrough an improved sampling area representation. Experiments show that our\nmodel significantly outperforms baseline models with slight computational\noverhead. In addition, it shows excellent generalization ability and can be\nused to improve performance with various state-of-the-art backbones and\ndetection methods.\n","authors":["Guo-Ye Yang","George Kiyohiro Nakayama","Zi-Kai Xiao","Tai-Jiang Mu","Xiaolei Huang","Shi-Min Hu"],"pdf_url":"https://arxiv.org/pdf/2312.09609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09608v1","updated":"2023-12-15T08:46:43Z","published":"2023-12-15T08:46:43Z","title":"Faster Diffusion: Rethinking the Role of UNet Encoder in Diffusion\n  Models","summary":"  One of the key components within diffusion models is the UNet for noise\nprediction. While several works have explored basic properties of the UNet\ndecoder, its encoder largely remains unexplored. In this work, we conduct the\nfirst comprehensive study of the UNet encoder. We empirically analyze the\nencoder features and provide insights to important questions regarding their\nchanges at the inference process. In particular, we find that encoder features\nchange gently, whereas the decoder features exhibit substantial variations\nacross different time-steps. This finding inspired us to omit the encoder at\ncertain adjacent time-steps and reuse cyclically the encoder features in the\nprevious time-steps for the decoder. Further based on this observation, we\nintroduce a simple yet effective encoder propagation scheme to accelerate the\ndiffusion sampling for a diverse set of tasks. By benefiting from our\npropagation scheme, we are able to perform in parallel the decoder at certain\nadjacent time-steps. Additionally, we introduce a prior noise injection method\nto improve the texture details in the generated image. Besides the standard\ntext-to-image task, we also validate our approach on other tasks:\ntext-to-video, personalized generation and reference-guided generation. Without\nutilizing any knowledge distillation technique, our approach accelerates both\nthe Stable Diffusion (SD) and the DeepFloyd-IF models sampling by 41$\\%$ and\n24$\\%$ respectively, while maintaining high-quality generation performance. Our\ncode is available in\n\\href{https://github.com/hutaiHang/Faster-Diffusion}{FasterDiffusion}.\n","authors":["Senmao Li","Taihang Hu","Fahad Shahbaz Khan","Linxuan Li","Shiqi Yang","Yaxing Wang","Ming-Ming Cheng","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2312.09608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.13237v3","updated":"2023-12-15T08:42:04Z","published":"2023-09-23T02:40:28Z","title":"Spatial-Temporal Knowledge-Embedded Transformer for Video Scene Graph\n  Generation","summary":"  Video scene graph generation (VidSGG) aims to identify objects in visual\nscenes and infer their relationships for a given video. It requires not only a\ncomprehensive understanding of each object scattered on the whole scene but\nalso a deep dive into their temporal motions and interactions. Inherently,\nobject pairs and their relationships enjoy spatial co-occurrence correlations\nwithin each image and temporal consistency/transition correlations across\ndifferent images, which can serve as prior knowledge to facilitate VidSGG model\nlearning and inference. In this work, we propose a spatial-temporal\nknowledge-embedded transformer (STKET) that incorporates the prior\nspatial-temporal knowledge into the multi-head cross-attention mechanism to\nlearn more representative relationship representations. Specifically, we first\nlearn spatial co-occurrence and temporal transition correlations in a\nstatistical manner. Then, we design spatial and temporal knowledge-embedded\nlayers that introduce the multi-head cross-attention mechanism to fully explore\nthe interaction between visual representation and the knowledge to generate\nspatial- and temporal-embedded representations, respectively. Finally, we\naggregate these representations for each subject-object pair to predict the\nfinal semantic labels and their relationships. Extensive experiments show that\nSTKET outperforms current competing algorithms by a large margin, e.g.,\nimproving the mR@50 by 8.1%, 4.7%, and 2.1% on different settings over current\nalgorithms.\n","authors":["Tao Pu","Tianshui Chen","Hefeng Wu","Yongyi Lu","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2309.13237v3.pdf","comment":"Accepted at IEEE T-IP, 2024"},{"id":"http://arxiv.org/abs/2312.09598v1","updated":"2023-12-15T08:27:52Z","published":"2023-12-15T08:27:52Z","title":"CLAF: Contrastive Learning with Augmented Features for Imbalanced\n  Semi-Supervised Learning","summary":"  Due to the advantages of leveraging unlabeled data and learning meaningful\nrepresentations, semi-supervised learning and contrastive learning have been\nprogressively combined to achieve better performances in popular applications\nwith few labeled data and abundant unlabeled data. One common manner is\nassigning pseudo-labels to unlabeled samples and selecting positive and\nnegative samples from pseudo-labeled samples to apply contrastive learning.\nHowever, the real-world data may be imbalanced, causing pseudo-labels to be\nbiased toward the majority classes and further undermining the effectiveness of\ncontrastive learning. To address the challenge, we propose Contrastive Learning\nwith Augmented Features (CLAF). We design a class-dependent feature\naugmentation module to alleviate the scarcity of minority class samples in\ncontrastive learning. For each pseudo-labeled sample, we select positive and\nnegative samples from labeled data instead of unlabeled data to compute\ncontrastive loss. Comprehensive experiments on imbalanced image classification\ndatasets demonstrate the effectiveness of CLAF in the context of imbalanced\nsemi-supervised learning.\n","authors":["Bowen Tao","Lan Li","Xin-Chun Li","De-Chuan Zhan"],"pdf_url":"https://arxiv.org/pdf/2312.09598v1.pdf","comment":"Accepted to ICASSP'2024"},{"id":"http://arxiv.org/abs/2312.09595v1","updated":"2023-12-15T08:22:36Z","published":"2023-12-15T08:22:36Z","title":"Density Matters: Improved Core-set for Active Domain Adaptive\n  Segmentation","summary":"  Active domain adaptation has emerged as a solution to balance the expensive\nannotation cost and the performance of trained models in semantic segmentation.\nHowever, existing works usually ignore the correlation between selected samples\nand its local context in feature space, which leads to inferior usage of\nannotation budgets. In this work, we revisit the theoretical bound of the\nclassical Core-set method and identify that the performance is closely related\nto the local sample distribution around selected samples. To estimate the\ndensity of local samples efficiently, we introduce a local proxy estimator with\nDynamic Masked Convolution and develop a Density-aware Greedy algorithm to\noptimize the bound. Extensive experiments demonstrate the superiority of our\napproach. Moreover, with very few labels, our scheme achieves comparable\nperformance to the fully supervised counterpart.\n","authors":["Shizhan Liu","Zhengkai Jiang","Yuxi Li","Jinlong Peng","Yabiao Wang","Weiyao Lin"],"pdf_url":"https://arxiv.org/pdf/2312.09595v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13194v2","updated":"2023-12-15T08:12:32Z","published":"2023-11-22T06:46:37Z","title":"Towards Improving Document Understanding: An Exploration on\n  Text-Grounding via MLLMs","summary":"  In the field of document understanding, significant advances have been made\nin the fine-tuning of Multimodal Large Language Models (MLLMs) with\ninstruction-following data. Nevertheless, the potential of text-grounding\ncapability within text-rich scenarios remains underexplored. In this paper, we\npresent a text-grounding document understanding model, termed TGDoc, which\naddresses this deficiency by enhancing MLLMs with the ability to discern the\nspatial positioning of text within images. Empirical evidence suggests that\ntext-grounding improves the model's interpretation of textual content, thereby\nelevating its proficiency in comprehending text-rich images. Specifically, we\ncompile a dataset containing 99K PowerPoint presentations sourced from the\ninternet. We formulate instruction tuning tasks including text detection,\nrecognition, and spotting to facilitate the cohesive alignment between the\nvisual encoder and large language model. Moreover, we curate a collection of\ntext-rich images and prompt the text-only GPT-4 to generate 12K high-quality\nconversations, featuring textual locations within text-rich scenarios. By\nintegrating text location data into the instructions, TGDoc is adept at\ndiscerning text locations during the visual question process. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance\nacross multiple text-rich benchmarks, validating the effectiveness of our\nmethod.\n","authors":["Yonghui Wang","Wengang Zhou","Hao Feng","Keyi Zhou","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2311.13194v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09589v1","updated":"2023-12-15T07:54:46Z","published":"2023-12-15T07:54:46Z","title":"Improving Cross-domain Few-shot Classification with Multilayer\n  Perceptron","summary":"  Cross-domain few-shot classification (CDFSC) is a challenging and tough task\ndue to the significant distribution discrepancies across different domains. To\naddress this challenge, many approaches aim to learn transferable\nrepresentations. Multilayer perceptron (MLP) has shown its capability to learn\ntransferable representations in various downstream tasks, such as unsupervised\nimage classification and supervised concept generalization. However, its\npotential in the few-shot settings has yet to be comprehensively explored. In\nthis study, we investigate the potential of MLP to assist in addressing the\nchallenges of CDFSC. Specifically, we introduce three distinct frameworks\nincorporating MLP in accordance with three types of few-shot classification\nmethods to verify the effectiveness of MLP. We reveal that MLP can\nsignificantly enhance discriminative capabilities and alleviate distribution\nshifts, which can be supported by our expensive experiments involving 10\nbaseline models and 12 benchmark datasets. Furthermore, our method even\ncompares favorably against other state-of-the-art CDFSC algorithms.\n","authors":["Shuanghao Bai","Wanqi Zhou","Zhirong Luan","Donglin Wang","Badong Chen"],"pdf_url":"https://arxiv.org/pdf/2312.09589v1.pdf","comment":"5pages, 4 figures"},{"id":"http://arxiv.org/abs/2312.08733v2","updated":"2023-12-15T07:49:19Z","published":"2023-12-14T08:25:04Z","title":"VMT-Adapter: Parameter-Efficient Transfer Learning for Multi-Task Dense\n  Scene Understanding","summary":"  Large-scale pre-trained models have achieved remarkable success in various\ncomputer vision tasks. A standard approach to leverage these models is to\nfine-tune all model parameters for downstream tasks, which poses challenges in\nterms of computational and storage costs. Recently, inspired by Natural\nLanguage Processing (NLP), parameter-efficient transfer learning has been\nsuccessfully applied to vision tasks. However, most existing techniques\nprimarily focus on single-task adaptation, and despite limited research on\nmulti-task adaptation, these methods often exhibit suboptimal training and\ninference efficiency. In this paper, we first propose an once-for-all Vision\nMulti-Task Adapter (VMT-Adapter), which strikes approximately O(1) training and\ninference efficiency w.r.t task number. Concretely, VMT-Adapter shares the\nknowledge from multiple tasks to enhance cross-task interaction while preserves\ntask-specific knowledge via independent knowledge extraction modules. Notably,\nsince task-specific modules require few parameters, VMT-Adapter can handle an\narbitrary number of tasks with a negligible increase of trainable parameters.\nWe also propose VMT-Adapter-Lite, which further reduces the trainable\nparameters by learning shared parameters between down- and up-projections.\nExtensive experiments on four dense scene understanding tasks demonstrate the\nsuperiority of VMT-Adapter(-Lite), achieving a 3.96%(1.34%) relative\nimprovement compared to single-task full fine-tuning, while utilizing merely\n~1% (0.36%) trainable parameters of the pre-trained model.\n","authors":["Yi Xin","Junlong Du","Qiang Wang","Zhiwen Lin","Ke Yan"],"pdf_url":"https://arxiv.org/pdf/2312.08733v2.pdf","comment":"Accepted to AAAI2024"},{"id":"http://arxiv.org/abs/2312.09584v1","updated":"2023-12-15T07:46:44Z","published":"2023-12-15T07:46:44Z","title":"Multiscale Vision Transformer With Deep Clustering-Guided Refinement for\n  Weakly Supervised Object Localization","summary":"  This work addresses the task of weakly-supervised object localization. The\ngoal is to learn object localization using only image-level class labels, which\nare much easier to obtain compared to bounding box annotations. This task is\nimportant because it reduces the need for labor-intensive ground-truth\nannotations. However, methods for object localization trained using weak\nsupervision often suffer from limited accuracy in localization. To address this\nchallenge and enhance localization accuracy, we propose a multiscale object\nlocalization transformer (MOLT). It comprises multiple object localization\ntransformers that extract patch embeddings across various scales. Moreover, we\nintroduce a deep clustering-guided refinement method that further enhances\nlocalization accuracy by utilizing separately extracted image segments. These\nsegments are obtained by clustering pixels using convolutional neural networks.\nFinally, we demonstrate the effectiveness of our proposed method by conducting\nexperiments on the publicly available ILSVRC-2012 dataset.\n","authors":["David Kim","Sinhae Cha","Byeongkeun Kang"],"pdf_url":"https://arxiv.org/pdf/2312.09584v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2210.12381v3","updated":"2023-12-15T07:21:56Z","published":"2022-10-22T07:56:13Z","title":"S2WAT: Image Style Transfer via Hierarchical Vision Transformer using\n  Strips Window Attention","summary":"  Transformer's recent integration into style transfer leverages its\nproficiency in establishing long-range dependencies, albeit at the expense of\nattenuated local modeling. This paper introduces Strips Window Attention\nTransformer (S2WAT), a novel hierarchical vision transformer designed for style\ntransfer. S2WAT employs attention computation in diverse window shapes to\ncapture both short- and long-range dependencies. The merged dependencies\nutilize the \"Attn Merge\" strategy, which adaptively determines spatial weights\nbased on their relevance to the target. Extensive experiments on representative\ndatasets show the proposed method's effectiveness compared to state-of-the-art\n(SOTA) transformer-based and other approaches. The code and pre-trained models\nare available at https://github.com/AlienZhang1996/S2WAT.\n","authors":["Chiyu Zhang","Xiaogang Xu","Lei Wang","Zaiyan Dai","Jun Yang"],"pdf_url":"https://arxiv.org/pdf/2210.12381v3.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.09579v1","updated":"2023-12-15T07:21:12Z","published":"2023-12-15T07:21:12Z","title":"MobileSAMv2: Faster Segment Anything to Everything","summary":"  Segment anything model (SAM) addresses two practical yet challenging\nsegmentation tasks: \\textbf{segment anything (SegAny)}, which utilizes a\ncertain point to predict the mask for a single object of interest, and\n\\textbf{segment everything (SegEvery)}, which predicts the masks for all\nobjects on the image. What makes SegAny slow for SAM is its heavyweight image\nencoder, which has been addressed by MobileSAM via decoupled knowledge\ndistillation. The efficiency bottleneck of SegEvery with SAM, however, lies in\nits mask decoder because it needs to first generate numerous masks with\nredundant grid-search prompts and then perform filtering to obtain the final\nvalid masks. We propose to improve its efficiency by directly generating the\nfinal masks with only valid prompts, which can be obtained through object\ndiscovery. Our proposed approach not only helps reduce the total time on the\nmask decoder by at least 16 times but also achieves superior performance.\nSpecifically, our approach yields an average performance boost of 3.6\\% (42.5\\%\n\\textit{v.s.} 38.9\\%) for zero-shot object proposal on the LVIS dataset with\nthe mask AR@$K$ metric. Qualitative results show that our approach generates\nfine-grained masks while avoiding over-segmenting things. This project\ntargeting faster SegEvery than the original SAM is termed MobileSAMv2 to\ndifferentiate from MobileSAM which targets faster SegAny. Moreover, we\ndemonstrate that our new prompt sampling is also compatible with the distilled\nimage encoders in MobileSAM, contributing to a unified framework for efficient\nSegAny and SegEvery. The code is available at the same link as MobileSAM\nProject\n\\href{https://github.com/ChaoningZhang/MobileSAM}{\\textcolor{red}{https://github.com/ChaoningZhang/MobileSAM}}.\n\\end{abstract}\n","authors":["Chaoning Zhang","Dongshen Han","Sheng Zheng","Jinwoo Choi","Tae-Ho Kim","Choong Seon Hong"],"pdf_url":"https://arxiv.org/pdf/2312.09579v1.pdf","comment":"MobileSAM achieves faster segment anything, while MobileSAMv2\n  achieves faster segment everything"},{"id":"http://arxiv.org/abs/2312.09076v2","updated":"2023-12-15T07:11:28Z","published":"2023-12-14T16:11:42Z","title":"ProSGNeRF: Progressive Dynamic Neural Scene Graph with Frequency\n  Modulated Auto-Encoder in Urban Scenes","summary":"  Implicit neural representation has demonstrated promising results in view\nsynthesis for large and complex scenes. However, existing approaches either\nfail to capture the fast-moving objects or need to build the scene graph\nwithout camera ego-motions, leading to low-quality synthesized views of the\nscene. We aim to jointly solve the view synthesis problem of large-scale urban\nscenes and fast-moving vehicles, which is more practical and challenging. To\nthis end, we first leverage a graph structure to learn the local scene\nrepresentations of dynamic objects and the background. Then, we design a\nprogressive scheme that dynamically allocates a new local scene graph trained\nwith frames within a temporal window, allowing us to scale up the\nrepresentation to an arbitrarily large scene. Besides, the training views of\nurban scenes are relatively sparse, which leads to a significant decline in\nreconstruction accuracy for dynamic objects. Therefore, we design a frequency\nauto-encoder network to encode the latent code and regularize the frequency\nrange of objects, which can enhance the representation of dynamic objects and\naddress the issue of sparse image inputs. Additionally, we employ lidar point\nprojection to maintain geometry consistency in large-scale urban scenes.\nExperimental results demonstrate that our method achieves state-of-the-art view\nsynthesis accuracy, object manipulation, and scene roaming ability. The code\nwill be open-sourced upon paper acceptance.\n","authors":["Tianchen Deng","Siyang Liu","Xuan Wang","Yejia Liu","Danwei Wang","Weidong Chen"],"pdf_url":"https://arxiv.org/pdf/2312.09076v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09576v1","updated":"2023-12-15T07:08:38Z","published":"2023-12-15T07:08:38Z","title":"SegRap2023: A Benchmark of Organs-at-Risk and Gross Tumor Volume\n  Segmentation for Radiotherapy Planning of Nasopharyngeal Carcinoma","summary":"  Radiation therapy is a primary and effective NasoPharyngeal Carcinoma (NPC)\ntreatment strategy. The precise delineation of Gross Tumor Volumes (GTVs) and\nOrgans-At-Risk (OARs) is crucial in radiation treatment, directly impacting\npatient prognosis. Previously, the delineation of GTVs and OARs was performed\nby experienced radiation oncologists. Recently, deep learning has achieved\npromising results in many medical image segmentation tasks. However, for NPC\nOARs and GTVs segmentation, few public datasets are available for model\ndevelopment and evaluation. To alleviate this problem, the SegRap2023 challenge\nwas organized in conjunction with MICCAI2023 and presented a large-scale\nbenchmark for OAR and GTV segmentation with 400 Computed Tomography (CT) scans\nfrom 200 NPC patients, each with a pair of pre-aligned non-contrast and\ncontrast-enhanced CT scans. The challenge's goal was to segment 45 OARs and 2\nGTVs from the paired CT scans. In this paper, we detail the challenge and\nanalyze the solutions of all participants. The average Dice similarity\ncoefficient scores for all submissions ranged from 76.68\\% to 86.70\\%, and\n70.42\\% to 73.44\\% for OARs and GTVs, respectively. We conclude that the\nsegmentation of large-size OARs is well-addressed, and more efforts are needed\nfor GTVs and small-size or thin-structure OARs. The benchmark will remain\npublicly available here: https://segrap2023.grand-challenge.org\n","authors":["Xiangde Luo","Jia Fu","Yunxin Zhong","Shuolin Liu","Bing Han","Mehdi Astaraki","Simone Bendazzoli","Iuliana Toma-Dasu","Yiwen Ye","Ziyang Chen","Yong Xia","Yanzhou Su","Jin Ye","Junjun He","Zhaohu Xing","Hongqiu Wang","Lei Zhu","Kaixiang Yang","Xin Fang","Zhiwei Wang","Chan Woong Lee","Sang Joon Park","Jaehee Chun","Constantin Ulrich","Klaus H. Maier-Hein","Nchongmaje Ndipenoch","Alina Miron","Yongmin Li","Yimeng Zhang","Yu Chen","Lu Bai","Jinlong Huang","Chengyang An","Lisheng Wang","Kaiwen Huang","Yunqi Gu","Tao Zhou","Mu Zhou","Shichuan Zhang","Wenjun Liao","Guotai Wang","Shaoting Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.09576v1.pdf","comment":"A challenge report of SegRap2023 (organized in conjunction with\n  MICCAI2023)"},{"id":"http://arxiv.org/abs/2312.09570v1","updated":"2023-12-15T07:04:27Z","published":"2023-12-15T07:04:27Z","title":"CAGE: Controllable Articulation GEneration","summary":"  We address the challenge of generating 3D articulated objects in a\ncontrollable fashion. Currently, modeling articulated 3D objects is either\nachieved through laborious manual authoring, or using methods from prior work\nthat are hard to scale and control directly. We leverage the interplay between\npart shape, connectivity, and motion using a denoising diffusion-based method\nwith attention modules designed to extract correlations between part\nattributes. Our method takes an object category label and a part connectivity\ngraph as input and generates an object's geometry and motion parameters. The\ngenerated objects conform to user-specified constraints on the object category,\npart shape, and part articulation. Our experiments show that our method\noutperforms the state-of-the-art in articulated object generation, producing\nmore realistic objects while conforming better to user constraints.\n  Video Summary at: http://youtu.be/cH_rbKbyTpE\n","authors":["Jiayi Liu","Hou In Ivan Tam","Ali Mahdavi-Amiri","Manolis Savva"],"pdf_url":"https://arxiv.org/pdf/2312.09570v1.pdf","comment":"Project page: https://3dlg-hcvc.github.io/cage/"},{"id":"http://arxiv.org/abs/2306.14448v2","updated":"2023-12-15T06:44:01Z","published":"2023-06-26T06:34:53Z","title":"Progressive Energy-Based Cooperative Learning for Multi-Domain\n  Image-to-Image Translation","summary":"  This paper studies a novel energy-based cooperative learning framework for\nmulti-domain image-to-image translation. The framework consists of four\ncomponents: descriptor, translator, style encoder, and style generator. The\ndescriptor is a multi-head energy-based model that represents a multi-domain\nimage distribution. The components of translator, style encoder, and style\ngenerator constitute a diversified image generator. Specifically, given an\ninput image from a source domain, the translator turns it into a stylised\noutput image of the target domain according to a style code, which can be\ninferred by the style encoder from a reference image or produced by the style\ngenerator from a random noise. Since the style generator is represented as an\ndomain-specific distribution of style codes, the translator can provide a\none-to-many transformation (i.e., diversified generation) between source domain\nand target domain. To train our framework, we propose a likelihood-based\nmulti-domain cooperative learning algorithm to jointly train the multi-domain\ndescriptor and the diversified image generator (including translator, style\nencoder, and style generator modules) via multi-domain MCMC teaching, in which\nthe descriptor guides the diversified image generator to shift its probability\ndensity toward the data distribution, while the diversified image generator\nuses its randomly translated images to initialize the descriptor's Langevin\ndynamics process for efficient sampling.\n","authors":["Weinan Song","Yaxuan Zhu","Lei He","Yingnian Wu","Jianwen Xie"],"pdf_url":"https://arxiv.org/pdf/2306.14448v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04687v2","updated":"2023-12-15T06:42:15Z","published":"2023-10-07T05:24:42Z","title":"Understanding and Improving Adversarial Attacks on Latent Diffusion\n  Model","summary":"  Latent Diffusion Model (LDM) achieves state-of-the-art performances in image\ngeneration yet raising copyright and privacy concerns. Adversarial attacks on\nLDM are then born to protect unauthorized images from being used in LDM-driven\nfew-shot generation. However, these attacks suffer from moderate performance\nand excessive computational cost, especially in GPU memory. In this paper, we\npropose an effective adversarial attack on LDM that shows superior performance\nagainst state-of-the-art few-shot generation pipeline of LDM, for example,\nLoRA. We implement the attack with memory efficiency by introducing several\nmechanisms and decrease the memory cost of the attack to less than 6GB, which\nallows individual users to run the attack on a majority of consumer GPUs. Our\nproposed attack can be a practical tool for people facing the copyright and\nprivacy risk brought by LDM to protect themselves.\n","authors":["Boyang Zheng","Chumeng Liang","Xiaoyu Wu","Yan Liu"],"pdf_url":"https://arxiv.org/pdf/2310.04687v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09558v1","updated":"2023-12-15T06:33:14Z","published":"2023-12-15T06:33:14Z","title":"Towards Transferable Targeted 3D Adversarial Attack in the Physical\n  World","summary":"  Compared with transferable untargeted attacks, transferable targeted\nadversarial attacks could specify the misclassification categories of\nadversarial samples, posing a greater threat to security-critical tasks. In the\nmeanwhile, 3D adversarial samples, due to their potential of multi-view\nrobustness, can more comprehensively identify weaknesses in existing deep\nlearning systems, possessing great application value. However, the field of\ntransferable targeted 3D adversarial attacks remains vacant. The goal of this\nwork is to develop a more effective technique that could generate transferable\ntargeted 3D adversarial examples, filling the gap in this field. To achieve\nthis goal, we design a novel framework named TT3D that could rapidly\nreconstruct from few multi-view images into Transferable Targeted 3D textured\nmeshes. While existing mesh-based texture optimization methods compute\ngradients in the high-dimensional mesh space and easily fall into local optima,\nleading to unsatisfactory transferability and distinct distortions, TT3D\ninnovatively performs dual optimization towards both feature grid and\nMulti-layer Perceptron (MLP) parameters in the grid-based NeRF space, which\nsignificantly enhances black-box transferability while enjoying naturalness.\nExperimental results show that TT3D not only exhibits superior cross-model\ntransferability but also maintains considerable adaptability across different\nrenders and vision tasks. More importantly, we produce 3D adversarial examples\nwith 3D printing techniques in the real world and verify their robust\nperformance under various scenarios.\n","authors":["Yao Huang","Yinpeng Dong","Shouwei Ruan","Xiao Yang","Hang Su","Xingxing Wei"],"pdf_url":"https://arxiv.org/pdf/2312.09558v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2312.09554v1","updated":"2023-12-15T06:16:17Z","published":"2023-12-15T06:16:17Z","title":"Embodied Adversarial Attack: A Dynamic Robust Physical Attack in\n  Autonomous Driving","summary":"  As physical adversarial attacks become extensively applied in unearthing the\npotential risk of security-critical scenarios, especially in autonomous\ndriving, their vulnerability to environmental changes has also been brought to\nlight. The non-robust nature of physical adversarial attack methods brings\nless-than-stable performance consequently. To enhance the robustness of\nphysical adversarial attacks in the real world, instead of statically\noptimizing a robust adversarial example via an off-line training manner like\nthe existing methods, this paper proposes a brand new robust adversarial attack\nframework: Embodied Adversarial Attack (EAA) from the perspective of dynamic\nadaptation, which aims to employ the paradigm of embodied intelligence:\nPerception-Decision-Control to dynamically adjust the optimal attack strategy\naccording to the current situations in real time. For the perception module,\ngiven the challenge of needing simulation for the victim's viewpoint, EAA\ninnovatively devises a Perspective Transformation Network to estimate the\ntarget's transformation from the attacker's perspective. For the decision and\ncontrol module, EAA adopts the laser-a highly manipulable medium to implement\nphysical attacks, and further trains an attack agent with reinforcement\nlearning to make it capable of instantaneously determining the best attack\nstrategy based on the perceived information. Finally, we apply our framework to\nthe autonomous driving scenario. A variety of experiments verify the high\neffectiveness of our method under complex scenes.\n","authors":["Yitong Sun","Yao Huang","Xingxing Wei"],"pdf_url":"https://arxiv.org/pdf/2312.09554v1.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2312.09553v1","updated":"2023-12-15T06:15:04Z","published":"2023-12-15T06:15:04Z","title":"Prompt-based Distribution Alignment for Unsupervised Domain Adaptation","summary":"  Recently, despite the unprecedented success of large pre-trained\nvisual-language models (VLMs) on a wide range of downstream tasks, the\nreal-world unsupervised domain adaptation (UDA) problem is still not well\nexplored. Therefore, in this paper, we first experimentally demonstrate that\nthe unsupervised-trained VLMs can significantly reduce the distribution\ndiscrepancy between source and target domains, thereby improving the\nperformance of UDA. However, a major challenge for directly deploying such\nmodels on downstream UDA tasks is prompt engineering, which requires aligning\nthe domain knowledge of source and target domains, since the performance of UDA\nis severely influenced by a good domain-invariant representation. We further\npropose a Prompt-based Distribution Alignment (PDA) method to incorporate the\ndomain knowledge into prompt learning. Specifically, PDA employs a two-branch\nprompt-tuning paradigm, namely base branch and alignment branch. The base\nbranch focuses on integrating class-related representation into prompts,\nensuring discrimination among different classes. To further minimize domain\ndiscrepancy, for the alignment branch, we construct feature banks for both the\nsource and target domains and propose image-guided feature tuning (IFT) to make\nthe input attend to feature banks, which effectively integrates self-enhanced\nand cross-domain features into the model. In this way, these two branches can\nbe mutually promoted to enhance the adaptation of VLMs for UDA. We conduct\nextensive experiments on three benchmarks to demonstrate that our proposed PDA\nachieves state-of-the-art performance. The code is available at\nhttps://github.com/BaiShuanghao/Prompt-based-Distribution-Alignment.\n","authors":["Shuanghao Bai","Min Zhang","Wanqi Zhou","Siteng Huang","Zhirong Luan","Donglin Wang","Badong Chen"],"pdf_url":"https://arxiv.org/pdf/2312.09553v1.pdf","comment":"13pages,6figures"},{"id":"http://arxiv.org/abs/2312.09551v1","updated":"2023-12-15T06:04:42Z","published":"2023-12-15T06:04:42Z","title":"Learning-based Axial Motion Magnification","summary":"  Video motion magnification amplifies invisible small motions to be\nperceptible, which provides humans with spatially dense and holistic\nunderstanding about small motions from the scene of interest. This is based on\nthe premise that magnifying small motions enhances the legibility of the\nmotion. In the real world, however, vibrating objects often possess complex\nsystems, having complex natural frequencies, modes, and directions. Existing\nmotion magnification often fails to improve the legibility since the intricate\nmotions still retain complex characteristics even when magnified, which\ndistracts us from analyzing them. In this work, we focus on improving the\nlegibility by proposing a new concept, axial motion magnification, which\nmagnifies decomposed motions along the user-specified direction. Axial motion\nmagnification can be applied to various applications where motions of specific\naxes are critical, by providing simplified and easily readable motion\ninformation. We propose a novel learning-based axial motion magnification\nmethod with the Motion Separation Module that enables to disentangle and\nmagnify the motion representation along axes of interest. Further, we build a\nnew synthetic training dataset for the axial motion magnification task. Our\nproposed method improves the legibility of resulting motions along certain\naxes, while adding additional user controllability. Our method can be directly\nadopted to the generic motion magnification and achieves favorable performance\nagainst competing methods. Our project page is available at\nhttps://axial-momag.github.io/axial-momag/.\n","authors":["Kwon Byung-Ki","Oh Hyun-Bin","Kim Jun-Seong","Tae-Hyun Oh"],"pdf_url":"https://arxiv.org/pdf/2312.09551v1.pdf","comment":"main paper: 10 pages, supplementary: 4 pages, 17 figures, 1 table"},{"id":"http://arxiv.org/abs/2312.08887v2","updated":"2023-12-15T05:52:44Z","published":"2023-12-13T09:42:04Z","title":"SpeedUpNet: A Plug-and-Play Hyper-Network for Accelerating Text-to-Image\n  Diffusion Models","summary":"  Text-to-image diffusion models (SD) exhibit significant advancements while\nrequiring extensive computational resources. Though many acceleration methods\nhave been proposed, they suffer from generation quality degradation or extra\ntraining cost generalizing to new fine-tuned models. To address these\nlimitations, we propose a novel and universal Stable-Diffusion (SD)\nacceleration module called SpeedUpNet(SUN). SUN can be directly plugged into\nvarious fine-tuned SD models without extra training. This technique utilizes\ncross-attention layers to learn the relative offsets in the generated image\nresults between negative and positive prompts achieving classifier-free\nguidance distillation with negative prompts controllable, and introduces a\nMulti-Step Consistency (MSC) loss to ensure a harmonious balance between\nreducing inference steps and maintaining consistency in the generated output.\nConsequently, SUN significantly reduces the number of inference steps to just 4\nsteps and eliminates the need for classifier-free guidance. It leads to an\noverall speedup of more than 10 times for SD models compared to the\nstate-of-the-art 25-step DPM-solver++, and offers two extra advantages: (1)\nclassifier-free guidance distillation with controllable negative prompts and\n(2) seamless integration into various fine-tuned Stable-Diffusion models\nwithout training. The effectiveness of the SUN has been verified through\nextensive experimentation. Project Page:\nhttps://williechai.github.io/speedup-plugin-for-stable-diffusions.github.io\n","authors":["Weilong Chai","DanDan Zheng","Jiajiong Cao","Zhiquan Chen","Changbao Wang","Chenguang Ma"],"pdf_url":"https://arxiv.org/pdf/2312.08887v2.pdf","comment":"Table 1. shows the comparison with existing methods, but the lack of\n  experimental data of the LCM method under 12-step makes the table incomplete.\n  We need to temporarily withdraw the manuscript and conduct corresponding\n  experiments before resubmitting it"},{"id":"http://arxiv.org/abs/2312.04233v2","updated":"2023-12-15T05:50:55Z","published":"2023-12-07T11:39:11Z","title":"Fine-tuning vision foundation model for crack segmentation in civil\n  infrastructures","summary":"  Large-scale foundation models have become the mainstream deep learning\nmethod, while in civil engineering, the scale of AI models is strictly limited.\nIn this work, a vision foundation model is introduced for crack segmentation.\nTwo parameter-efficient fine-tuning methods, adapter and low-rank adaptation,\nare adopted to fine-tune the foundation model in semantic segmentation: the\nSegment Anything Model (SAM). The fine-tuned CrackSAM model is much larger than\nall the existing crack segmentation models but shows excellent performance. To\ntest the zero-shot performance of the proposed method, two unique datasets\nrelated to road and exterior wall cracks are collected, annotated and\nopen-sourced, for a total of 810 images. Comparative experiments are conducted\nwith twelve mature semantic segmentation models. On datasets with artificial\nnoise and previously unseen datasets, the performance of CrackSAM far exceeds\nthat of all state-of-the-art models. CrackSAM exhibits remarkable superiority,\nparticularly under challenging conditions such as dim lighting, shadows, road\nmarkings, construction joints, and other interference factors. These\ncross-scenario results demonstrate the outstanding zero-shot capability of\nfoundation models and provide new ideas for developing vision models in civil\nengineering.\n","authors":["Kang Ge","Chen Wang","Yutao Guo","Yansong Tang","Zhenzhong Hu"],"pdf_url":"https://arxiv.org/pdf/2312.04233v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09538v1","updated":"2023-12-15T05:09:08Z","published":"2023-12-15T05:09:08Z","title":"AEGIS-Net: Attention-guided Multi-Level Feature Aggregation for Indoor\n  Place Recognition","summary":"  We present AEGIS-Net, a novel indoor place recognition model that takes in\nRGB point clouds and generates global place descriptors by aggregating\nlower-level color, geometry features and higher-level implicit semantic\nfeatures. However, rather than simple feature concatenation, self-attention\nmodules are employed to select the most important local features that best\ndescribe an indoor place. Our AEGIS-Net is made of a semantic encoder, a\nsemantic decoder and an attention-guided feature embedding. The model is\ntrained in a 2-stage process with the first stage focusing on an auxiliary\nsemantic segmentation task and the second one on the place recognition task. We\nevaluate our AEGIS-Net on the ScanNetPR dataset and compare its performance\nwith a pre-deep-learning feature-based method and five state-of-the-art\ndeep-learning-based methods. Our AEGIS-Net achieves exceptional performance and\noutperforms all six methods.\n","authors":["Yuhang Ming","Jian Ma","Xingrui Yang","Weichen Dai","Yong Peng","Wanzeng Kong"],"pdf_url":"https://arxiv.org/pdf/2312.09538v1.pdf","comment":"Accepted by 2024 IEEE International Conference on Acoustics, Speech\n  and Signal Processing (ICASSP 2024)"},{"id":"http://arxiv.org/abs/2210.01272v2","updated":"2023-12-15T04:58:36Z","published":"2022-10-03T23:44:38Z","title":"A systematic review of the use of Deep Learning in Satellite Imagery for\n  Agriculture","summary":"  Agricultural research is essential for increasing food production to meet the\nrequirements of an increasing population in the coming decades. Recently,\nsatellite technology has been improving rapidly and deep learning has seen much\nsuccess in generic computer vision tasks and many application areas which\npresents an important opportunity to improve analysis of agricultural land.\nHere we present a systematic review of 150 studies to find the current uses of\ndeep learning on satellite imagery for agricultural research. Although we\nidentify 5 categories of agricultural monitoring tasks, the majority of the\nresearch interest is in crop segmentation and yield prediction. We found that,\nwhen used, modern deep learning methods consistently outperformed traditional\nmachine learning across most tasks; the only exception was that Long Short-Term\nMemory (LSTM) Recurrent Neural Networks did not consistently outperform Random\nForests (RF) for yield prediction. The reviewed studies have largely adopted\nmethodologies from generic computer vision, except for one major omission:\nbenchmark datasets are not utilised to evaluate models across studies, making\nit difficult to compare results. Additionally, some studies have specifically\nutilised the extra spectral resolution available in satellite imagery, but\nother divergent properties of satellite images - such as the hugely different\nscales of spatial patterns - are not being taken advantage of in the reviewed\nstudies.\n","authors":["Brandon Victor","Zhen He","Aiden Nibali"],"pdf_url":"https://arxiv.org/pdf/2210.01272v2.pdf","comment":"23 pages, 5 figures and 10 tables in main paper. Supplementary\n  materials section also included in main pdf. Update: All tables with specific\n  references have been moved to supplementary. Main text now uses only\n  aggregated information"},{"id":"http://arxiv.org/abs/2312.09534v1","updated":"2023-12-15T04:57:54Z","published":"2023-12-15T04:57:54Z","title":"WeatherProof: A Paired-Dataset Approach to Semantic Segmentation in\n  Adverse Weather","summary":"  The introduction of large, foundational models to computer vision has led to\ndrastically improved performance on the task of semantic segmentation. However,\nthese existing methods exhibit a large performance drop when testing on images\ndegraded by weather conditions such as rain, fog, or snow. We introduce a\ngeneral paired-training method that can be applied to all current foundational\nmodel architectures that leads to improved performance on images in adverse\nweather conditions. To this end, we create the WeatherProof Dataset, the first\nsemantic segmentation dataset with accurate clear and adverse weather image\npairs, which not only enables our new training paradigm, but also improves the\nevaluation of the performance gap between clear and degraded segmentation. We\nfind that training on these paired clear and adverse weather frames which share\nan underlying scene results in improved performance on adverse weather data.\nWith this knowledge, we propose a training pipeline which accentuates the\nadvantages of paired-data training using consistency losses and language\nguidance, which leads to performance improvements by up to 18.4% as compared to\nstandard training procedures.\n","authors":["Blake Gella","Howard Zhang","Rishi Upadhyay","Tiffany Chang","Matthew Waliman","Yunhao Ba","Alex Wong","Achuta Kadambi"],"pdf_url":"https://arxiv.org/pdf/2312.09534v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09533v1","updated":"2023-12-15T04:51:43Z","published":"2023-12-15T04:51:43Z","title":"Adversarial Robustness on Image Classification with $k$-means","summary":"  In this paper we explore the challenges and strategies for enhancing the\nrobustness of $k$-means clustering algorithms against adversarial\nmanipulations. We evaluate the vulnerability of clustering algorithms to\nadversarial attacks, emphasising the associated security risks. Our study\ninvestigates the impact of incremental attack strength on training, introduces\nthe concept of transferability between supervised and unsupervised models, and\nhighlights the sensitivity of unsupervised models to sample distributions. We\nadditionally introduce and evaluate an adversarial training method that\nimproves testing performance in adversarial scenarios, and we highlight the\nimportance of various parameters in the proposed training method, such as\ncontinuous learning, centroid initialisation, and adversarial step-count.\n","authors":["Rollin Omari","Junae Kim","Paul Montague"],"pdf_url":"https://arxiv.org/pdf/2312.09533v1.pdf","comment":"6 pages, 3 figures, 2 equations, 1 algorithm"},{"id":"http://arxiv.org/abs/2312.09529v1","updated":"2023-12-15T04:36:13Z","published":"2023-12-15T04:36:13Z","title":"Can Physician Judgment Enhance Model Trustworthiness? A Case Study on\n  Predicting Pathological Lymph Nodes in Rectal Cancer","summary":"  Explainability is key to enhancing artificial intelligence's trustworthiness\nin medicine. However, several issues remain concerning the actual benefit of\nexplainable models for clinical decision-making. Firstly, there is a lack of\nconsensus on an evaluation framework for quantitatively assessing the practical\nbenefits that effective explainability should provide to practitioners.\nSecondly, physician-centered evaluations of explainability are limited.\nThirdly, the utility of built-in attention mechanisms in transformer-based\nmodels as an explainability technique is unclear. We hypothesize that superior\nattention maps should align with the information that physicians focus on,\npotentially reducing prediction uncertainty and increasing model reliability.\nWe employed a multimodal transformer to predict lymph node metastasis in rectal\ncancer using clinical data and magnetic resonance imaging, exploring how well\nattention maps, visualized through a state-of-the-art technique, can achieve\nagreement with physician understanding. We estimated the model's uncertainty\nusing meta-level information like prediction probability variance and\nquantified agreement. Our assessment of whether this agreement reduces\nuncertainty found no significant effect. In conclusion, this case study did not\nconfirm the anticipated benefit of attention maps in enhancing model\nreliability. Superficial explanations could do more harm than good by\nmisleading physicians into relying on uncertain predictions, suggesting that\nthe current state of attention mechanisms in explainability should not be\noverestimated. Identifying explainability mechanisms truly beneficial for\nclinical decision-making remains essential.\n","authors":["Kazuma Kobayashi","Yasuyuki Takamizawa","Mototaka Miyake","Sono Ito","Lin Gu","Tatsuya Nakatsuka","Yu Akagi","Tatsuya Harada","Yukihide Kanemitsu","Ryuji Hamamoto"],"pdf_url":"https://arxiv.org/pdf/2312.09529v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09527v1","updated":"2023-12-15T04:23:20Z","published":"2023-12-15T04:23:20Z","title":"TIFace: Improving Facial Reconstruction through Tensorial Radiance\n  Fields and Implicit Surfaces","summary":"  This report describes the solution that secured the first place in the \"View\nSynthesis Challenge for Human Heads (VSCHH)\" at the ICCV 2023 workshop. Given\nthe sparse view images of human heads, the objective of this challenge is to\nsynthesize images from novel viewpoints. Due to the complexity of textures on\nthe face and the impact of lighting, the baseline method TensoRF yields results\nwith significant artifacts, seriously affecting facial reconstruction. To\naddress this issue, we propose TI-Face, which improves facial reconstruction\nthrough tensorial radiance fields (T-Face) and implicit surfaces (I-Face),\nrespectively. Specifically, we employ an SAM-based approach to obtain the\nforeground mask, thereby filtering out intense lighting in the background.\nAdditionally, we design mask-based constraints and sparsity constraints to\neliminate rendering artifacts effectively. The experimental results demonstrate\nthe effectiveness of the proposed improvements and superior performance of our\nmethod on face reconstruction. The code will be available at\nhttps://github.com/RuijieZhu94/TI-Face.\n","authors":["Ruijie Zhu","Jiahao Chang","Ziyang Song","Jiahuan Yu","Tianzhu Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.09527v1.pdf","comment":"1st place solution in the View Synthesis Challenge for Human Heads\n  (VSCHH) at the ICCV 2023 workshop"},{"id":"http://arxiv.org/abs/2312.09525v1","updated":"2023-12-15T04:13:21Z","published":"2023-12-15T04:13:21Z","title":"Hierarchical Graph Pattern Understanding for Zero-Shot VOS","summary":"  The optical flow guidance strategy is ideal for obtaining motion information\nof objects in the video. It is widely utilized in video segmentation tasks.\nHowever, existing optical flow-based methods have a significant dependency on\noptical flow, which results in poor performance when the optical flow\nestimation fails for a particular scene. The temporal consistency provided by\nthe optical flow could be effectively supplemented by modeling in a structural\nform. This paper proposes a new hierarchical graph neural network (GNN)\narchitecture, dubbed hierarchical graph pattern understanding (HGPU), for\nzero-shot video object segmentation (ZS-VOS). Inspired by the strong ability of\nGNNs in capturing structural relations, HGPU innovatively leverages motion cues\n(\\ie, optical flow) to enhance the high-order representations from the\nneighbors of target frames. Specifically, a hierarchical graph pattern encoder\nwith message aggregation is introduced to acquire different levels of motion\nand appearance features in a sequential manner. Furthermore, a decoder is\ndesigned for hierarchically parsing and understanding the transformed\nmulti-modal contexts to achieve more accurate and robust results. HGPU achieves\nstate-of-the-art performance on four publicly available benchmarks (DAVIS-16,\nYouTube-Objects, Long-Videos and DAVIS-17). Code and pre-trained model can be\nfound at \\url{https://github.com/NUST-Machine-Intelligence-Laboratory/HGPU}.\n","authors":["Gensheng Pei","Fumin Shen","Yazhou Yao","Tao Chen","Xian-Sheng Hua","Heng-Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2312.09525v1.pdf","comment":"accepted by IEEE Transactions on Image Processing"},{"id":"http://arxiv.org/abs/2312.09523v1","updated":"2023-12-15T04:06:52Z","published":"2023-12-15T04:06:52Z","title":"DriveTrack: A Benchmark for Long-Range Point Tracking in Real-World\n  Videos","summary":"  This paper presents DriveTrack, a new benchmark and data generation framework\nfor long-range keypoint tracking in real-world videos. DriveTrack is motivated\nby the observation that the accuracy of state-of-the-art trackers depends\nstrongly on visual attributes around the selected keypoints, such as texture\nand lighting. The problem is that these artifacts are especially pronounced in\nreal-world videos, but these trackers are unable to train on such scenes due to\na dearth of annotations. DriveTrack bridges this gap by building a framework to\nautomatically annotate point tracks on autonomous driving datasets. We release\na dataset consisting of 1 billion point tracks across 24 hours of video, which\nis seven orders of magnitude greater than prior real-world benchmarks and on\npar with the scale of synthetic benchmarks. DriveTrack unlocks new use cases\nfor point tracking in real-world videos. First, we show that fine-tuning\nkeypoint trackers on DriveTrack improves accuracy on real-world scenes by up to\n7%. Second, we analyze the sensitivity of trackers to visual artifacts in real\nscenes and motivate the idea of running assistive keypoint selectors alongside\ntrackers.\n","authors":["Arjun Balasingam","Joseph Chandler","Chenning Li","Zhoutong Zhang","Hari Balakrishnan"],"pdf_url":"https://arxiv.org/pdf/2312.09523v1.pdf","comment":"16 pages, 13 figures, 5 tables"},{"id":"http://arxiv.org/abs/2312.09520v1","updated":"2023-12-15T04:01:32Z","published":"2023-12-15T04:01:32Z","title":"SlowTrack: Increasing the Latency of Camera-based Perception in\n  Autonomous Driving Using Adversarial Examples","summary":"  In Autonomous Driving (AD), real-time perception is a critical component\nresponsible for detecting surrounding objects to ensure safe driving. While\nresearchers have extensively explored the integrity of AD perception due to its\nsafety and security implications, the aspect of availability (real-time\nperformance) or latency has received limited attention. Existing works on\nlatency-based attack have focused mainly on object detection, i.e., a component\nin camera-based AD perception, overlooking the entire camera-based AD\nperception, which hinders them to achieve effective system-level effects, such\nas vehicle crashes. In this paper, we propose SlowTrack, a novel framework for\ngenerating adversarial attacks to increase the execution time of camera-based\nAD perception. We propose a novel two-stage attack strategy along with the\nthree new loss function designs. Our evaluation is conducted on four popular\ncamera-based AD perception pipelines, and the results demonstrate that\nSlowTrack significantly outperforms existing latency-based attacks while\nmaintaining comparable imperceptibility levels. Furthermore, we perform the\nevaluation on Baidu Apollo, an industry-grade full-stack AD system, and LGSVL,\na production-grade AD simulator, with two scenarios to compare the system-level\neffects of SlowTrack and existing attacks. Our evaluation results show that the\nsystem-level effects can be significantly improved, i.e., the vehicle crash\nrate of SlowTrack is around 95% on average while existing works only have\naround 30%.\n","authors":["Chen Ma","Ningfei Wang","Qi Alfred Chen","Chao Shen"],"pdf_url":"https://arxiv.org/pdf/2312.09520v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2107.10984v4","updated":"2023-12-15T03:45:14Z","published":"2021-07-23T01:25:07Z","title":"Human Pose Transfer with Augmented Disentangled Feature Consistency","summary":"  Deep generative models have made great progress in synthesizing images with\narbitrary human poses and transferring poses of one person to others. Though\nmany different methods have been proposed to generate images with high visual\nfidelity, the main challenge remains and comes from two fundamental issues:\npose ambiguity and appearance inconsistency. To alleviate the current\nlimitations and improve the quality of the synthesized images, we propose a\npose transfer network with augmented Disentangled Feature Consistency (DFC-Net)\nto facilitate human pose transfer. Given a pair of images containing the source\nand target person, DFC-Net extracts pose and static information from the source\nand target respectively, then synthesizes an image of the target person with\nthe desired pose from the source. Moreover, DFC-Net leverages disentangled\nfeature consistency losses in the adversarial training to strengthen the\ntransfer coherence and integrates a keypoint amplifier to enhance the pose\nfeature extraction. With the help of the disentangled feature consistency\nlosses, we further propose a novel data augmentation scheme that introduces\nunpaired support data with the augmented consistency constraints to improve the\ngenerality and robustness of DFC-Net. Extensive experimental results on\nMixamo-Pose and EDN-10k have demonstrated DFC-Net achieves state-of-the-art\nperformance on pose transfer.\n","authors":["Kun Wu","Chengxiang Yin","Zhengping Che","Bo Jiang","Jian Tang","Zheng Guan","Gangyi Ding"],"pdf_url":"https://arxiv.org/pdf/2107.10984v4.pdf","comment":"22 pages, 6 figures"},{"id":"http://arxiv.org/abs/2302.00901v4","updated":"2023-12-15T03:44:59Z","published":"2023-02-02T06:38:00Z","title":"Longformer: Longitudinal Transformer for Alzheimer's Disease\n  Classification with Structural MRIs","summary":"  Structural magnetic resonance imaging (sMRI) is widely used for brain\nneurological disease diagnosis; while longitudinal MRIs are often collected to\nmonitor and capture disease progression, as clinically used in diagnosing\nAlzheimer's disease (AD). However, most current methods neglect AD's\nprogressive nature and only take a single sMRI for recognizing AD. In this\npaper, we consider the problem of leveraging the longitudinal MRIs of a subject\nfor AD identification. To capture longitudinal changes in sMRIs, we propose a\nnovel model Longformer, a spatiotemporal transformer network that performs\nattention mechanisms spatially on sMRIs at each time point and integrates brain\nregion features over time to obtain longitudinal embeddings for classification.\nOur Longformer achieves state-of-the-art performance on two binary\nclassification tasks of separating different stages of AD using the ADNI\ndataset. Our source code is available at https://github.com/Qybc/LongFormer.\n","authors":["Qiuhui Chen","Yi Hong"],"pdf_url":"https://arxiv.org/pdf/2302.00901v4.pdf","comment":"WACV2024"},{"id":"http://arxiv.org/abs/2308.02162v2","updated":"2023-12-15T03:42:52Z","published":"2023-08-04T06:50:52Z","title":"Learning Referring Video Object Segmentation from Weak Annotation","summary":"  Referring video object segmentation (RVOS) is a task that aims to segment the\ntarget object in all video frames based on a sentence describing the object.\nAlthough existing RVOS methods have achieved significant performance, they\ndepend on densely-annotated datasets, which are expensive and time-consuming to\nobtain. In this paper, we propose a new annotation scheme that reduces the\nannotation effort by 8 times, while providing sufficient supervision for RVOS.\nOur scheme only requires a mask for the frame where the object first appears\nand bounding boxes for the rest of the frames. Based on this scheme, we develop\na novel RVOS method that exploits weak annotations effectively. Specifically,\nwe build a simple but effective baseline model, SimRVOS, for RVOS with weak\nannotation. Then, we design a cross frame segmentation module, which uses the\nlanguage-guided dynamic filters from one frame to segment the target object in\nother frames to thoroughly leverage the valuable mask annotation and bounding\nboxes. Finally, we develop a bi-level contrastive learning method to enhance\nthe pixel-level discriminative representation of the model with weak\nannotation. We conduct extensive experiments to show that our method achieves\ncomparable or even superior performance to fully-supervised methods, without\nrequiring dense mask annotations.\n","authors":["Wangbo Zhao","Kepan Nan","Songyang Zhang","Kai Chen","Dahua Lin","Yang You"],"pdf_url":"https://arxiv.org/pdf/2308.02162v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09514v1","updated":"2023-12-15T03:33:08Z","published":"2023-12-15T03:33:08Z","title":"Single PW takes a shortcut to compound PW in US imaging","summary":"  Reconstruction of ultrasound (US) images from radio-frequency data can be\nconceptualized as a linear inverse problem. Traditional deep learning\napproaches, which aim to improve the quality of US images by directly learning\npriors, often encounter challenges in generalization. Recently, diffusion-based\ngenerative models have received significant attention within the research\ncommunity due to their robust performance in image reconstruction tasks.\nHowever, a limitation of these models is their inherent low speed in generating\nimage samples from pure Gaussian noise progressively. In this study, we exploit\nthe inherent similarity between the US images reconstructed from a single plane\nwave (PW) and PW compounding PWC). We hypothesize that a single PW can take a\nshortcut to reach the diffusion trajectory of PWC, removing the need to begin\nwith Gaussian noise. By employing an advanced diffusion model, we demonstrate\nits effectiveness in US image reconstruction, achieving a substantial reduction\nin sampling steps. In-vivo experimental results indicate that our approach can\nreduce sampling steps by 60%, while preserving comparable performance metrics\nwith the conventional diffusion model.\n","authors":["Zhiqiang Li","Hengrong Lan","Lijie Huang","Qiong He","Jianwen Luo"],"pdf_url":"https://arxiv.org/pdf/2312.09514v1.pdf","comment":"Submitted to ISBI 2024"},{"id":"http://arxiv.org/abs/2312.09510v1","updated":"2023-12-15T03:28:17Z","published":"2023-12-15T03:28:17Z","title":"Fast Sampling generative model for Ultrasound image reconstruction","summary":"  Image reconstruction from radio-frequency data is pivotal in ultrafast plane\nwave ultrasound imaging. Unlike the conventional delay-and-sum (DAS) technique,\nwhich relies on somewhat imprecise assumptions, deep learning-based methods\nperform image reconstruction by training on paired data, leading to a notable\nenhancement in image quality. Nevertheless, these strategies often exhibit\nlimited generalization capabilities. Recently, denoising diffusion models have\nbecome the preferred paradigm for image reconstruction tasks. However, their\nreliance on an iterative sampling procedure results in prolonged generation\ntime. In this paper, we propose a novel sampling framework that concurrently\nenforces data consistency of ultrasound signals and data-driven priors. By\nleveraging the advanced diffusion model, the generation of high-quality images\nis substantially expedited. Experimental evaluations on an in-vivo dataset\nindicate that our approach with a single plane wave surpasses DAS with spatial\ncoherent compounding of 75 plane waves.\n","authors":["Hengrong Lan","Zhiqiang Li","Qiong He","Jianwen Luo"],"pdf_url":"https://arxiv.org/pdf/2312.09510v1.pdf","comment":"submitted to ISBI 2024"},{"id":"http://arxiv.org/abs/2312.09507v1","updated":"2023-12-15T03:17:37Z","published":"2023-12-15T03:17:37Z","title":"WAVER: Writing-style Agnostic Video Retrieval via Distilling\n  Vision-Language Models Through Open-Vocabulary Knowledge","summary":"  Text-video retrieval, a prominent sub-field within the broader domain of\nmultimedia content management, has witnessed remarkable growth and innovation\nover the past decade. However, existing methods assume the video scenes are\nconsistent and the description annotators are unbiased. These limitations fail\nto align with fluid real-world scenarios, and descriptions can be influenced by\nannotator biases, diverse writing styles, and varying textual perspectives. To\novercome the aforementioned problems, we introduce WAVER, a cross-domain\nknowledge distillation mechanism designed to tackle the challenge of handling\nwriting-style agnostics. WAVER capitalizes on the open-vocabulary properties\ninherent in pre-trained vision-language models and employs an implicit\nknowledge distillation approach to transfer text-based knowledge from a teacher\nmodel to a vision-based student. Empirical studies conducted across four\nstandard benchmark datasets, encompassing various settings, provide compelling\nevidence that \\WAVER can achieve state-of-the-art performance in text-video\nretrieval tasks while handling writing-style variations.\n","authors":["Huy Le","Tung Kieu","Anh Nguyen","Ngan Le"],"pdf_url":"https://arxiv.org/pdf/2312.09507v1.pdf","comment":"Accepted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.09501v1","updated":"2023-12-15T02:55:24Z","published":"2023-12-15T02:55:24Z","title":"EDA: Evolving and Distinct Anchors for Multimodal Motion Prediction","summary":"  Motion prediction is a crucial task in autonomous driving, and one of its\nmajor challenges lands in the multimodality of future behaviors. Many\nsuccessful works have utilized mixture models which require identification of\npositive mixture components, and correspondingly fall into two main lines:\nprediction-based and anchor-based matching. The prediction clustering\nphenomenon in prediction-based matching makes it difficult to pick\nrepresentative trajectories for downstream tasks, while the anchor-based\nmatching suffers from a limited regression capability. In this paper, we\nintroduce a novel paradigm, named Evolving and Distinct Anchors (EDA), to\ndefine the positive and negative components for multimodal motion prediction\nbased on mixture models. We enable anchors to evolve and redistribute\nthemselves under specific scenes for an enlarged regression capacity.\nFurthermore, we select distinct anchors before matching them with the ground\ntruth, which results in impressive scoring performance. Our approach enhances\nall metrics compared to the baseline MTR, particularly with a notable relative\nreduction of 13.5% in Miss Rate, resulting in state-of-the-art performance on\nthe Waymo Open Motion Dataset. Code is available at\nhttps://github.com/Longzhong-Lin/EDA.\n","authors":["Longzhong Lin","Xuewu Lin","Tianwei Lin","Lichao Huang","Rong Xiong","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2312.09501v1.pdf","comment":"Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI2024)"},{"id":"http://arxiv.org/abs/2312.09093v2","updated":"2023-12-15T02:49:17Z","published":"2023-12-14T16:24:09Z","title":"Aleth-NeRF: Illumination Adaptive NeRF with Concealing Field Assumption","summary":"  The standard Neural Radiance Fields (NeRF) paradigm employs a viewer-centered\nmethodology, entangling the aspects of illumination and material reflectance\ninto emission solely from 3D points. This simplified rendering approach\npresents challenges in accurately modeling images captured under adverse\nlighting conditions, such as low light or over-exposure. Motivated by the\nancient Greek emission theory that posits visual perception as a result of rays\nemanating from the eyes, we slightly refine the conventional NeRF framework to\ntrain NeRF under challenging light conditions and generate normal-light\ncondition novel views unsupervised. We introduce the concept of a \"Concealing\nField,\" which assigns transmittance values to the surrounding air to account\nfor illumination effects. In dark scenarios, we assume that object emissions\nmaintain a standard lighting level but are attenuated as they traverse the air\nduring the rendering process. Concealing Field thus compel NeRF to learn\nreasonable density and colour estimations for objects even in dimly lit\nsituations. Similarly, the Concealing Field can mitigate over-exposed emissions\nduring the rendering stage. Furthermore, we present a comprehensive multi-view\ndataset captured under challenging illumination conditions for evaluation. Our\ncode and dataset available at https://github.com/cuiziteng/Aleth-NeRF\n","authors":["Ziteng Cui","Lin Gu","Xiao Sun","Xianzheng Ma","Yu Qiao","Tatsuya Harada"],"pdf_url":"https://arxiv.org/pdf/2312.09093v2.pdf","comment":"AAAI 2024, code available at https://github.com/cuiziteng/Aleth-NeRF\n  Modified version of previous paper arXiv:2303.05807"},{"id":"http://arxiv.org/abs/2312.09496v1","updated":"2023-12-15T02:43:30Z","published":"2023-12-15T02:43:30Z","title":"Image Deblurring using GAN","summary":"  In recent years, deep generative models, such as Generative Adversarial\nNetwork (GAN), has grabbed significant attention in the field of computer\nvision. This project focuses on the application of GAN in image deblurring with\nthe aim of generating clearer images from blurry inputs caused by factors such\nas motion blur. However, traditional image restoration techniques have\nlimitations in handling complex blurring patterns. Hence, a GAN-based framework\nis proposed as a solution to generate high-quality deblurred images. The\nproject defines a GAN model in Tensorflow and trains it with GoPRO dataset. The\nGenerator will intake blur images directly to create fake images to convince\nthe Discriminator which will receive clear images at the same time and\ndistinguish between the real image and the fake image. After obtaining the\ntrained parameters, the model was used to deblur motion-blur images taken in\ndaily life as well as testing set for validation. The result shows that the\npretrained network of GAN can obtain sharper pixels in image, achieving an\naverage of 29.3 Peak Signal-to-Noise Ratio (PSNR) and 0.72 Structural\nSimilarity Assessment (SSIM). This help to effectively address the challenges\nposed by image blurring, leading to the generation of visually pleasing and\nsharp images. By exploiting the adversarial learning framework, the proposed\napproach enhances the potential for real-world applications in image\nrestoration.\n","authors":["Zhengdong Li"],"pdf_url":"https://arxiv.org/pdf/2312.09496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07009v2","updated":"2023-12-15T02:40:29Z","published":"2023-12-12T06:45:19Z","title":"Vision-language Assisted Attribute Learning","summary":"  Attribute labeling at large scale is typically incomplete and partial, posing\nsignificant challenges to model optimization. Existing attribute learning\nmethods often treat the missing labels as negative or simply ignore them all\nduring training, either of which could hamper the model performance to a great\nextent. To overcome these limitations, in this paper we leverage the available\nvision-language knowledge to explicitly disclose the missing labels for\nenhancing model learning. Given an image, we predict the likelihood of each\nmissing attribute label assisted by an off-the-shelf vision-language model, and\nrandomly select to ignore those with high scores in training. Our strategy\nstrikes a good balance between fully ignoring and negatifying the missing\nlabels, as these high scores are found to be informative on revealing label\nambiguity. Extensive experiments show that our proposed vision-language\nassisted loss can achieve state-of-the-art performance on the newly cleaned VAW\ndataset. Qualitative evaluation demonstrates the ability of the proposed method\nin predicting more complete attributes.\n","authors":["Kongming Liang","Xinran Wang","Rui Wang","Donghui Gao","Ling Jin","Weidong Liu","Xiatian Zhu","Zhanyu Ma","Jun Guo"],"pdf_url":"https://arxiv.org/pdf/2312.07009v2.pdf","comment":"Accepted by IEEE IC-NIDC 2023"},{"id":"http://arxiv.org/abs/2312.05777v2","updated":"2023-12-15T02:18:09Z","published":"2023-12-10T05:52:36Z","title":"Negative Pre-aware for Noisy Cross-modal Matching","summary":"  Cross-modal noise-robust learning is a challenging task since noisy\ncorrespondence is hard to recognize and rectify. Due to the cumulative and\nunavoidable negative impact of unresolved noise, existing methods cannot\nmaintain a stable performance when the noise increases. In this paper, we\npresent a novel Negative Pre-aware Cross-modal (NPC) matching solution for\nlarge visual-language model fine-tuning on noisy downstream tasks. It is\nfeatured in two aspects: (1) For noise recognition and resistance, previous\nmethods usually directly filter out a noise subset, we propose to estimate the\nnegative impact of each sample. It does not need additional correction\nmechanisms that may predict unreliable correction results, leading to\nself-reinforcing error. We assign a confidence weight to each sample according\nto its negative impact in the training process. This adaptively adjusts the\ncontribution of each sample to avoid noisy accumulation. (2) For maintaining\nstable performance with increasing noise, we utilize the memorization effect of\nDNNs by maintaining a memory bank. Specifically, we apply GMM to select\nhigh-confident clean samples as the memory entry, where the memory entry is\nused to estimate the negative impact of each sample. Since clean samples are\neasier distinguished by GMM with increasing noise, the memory bank can still\nmaintain high quality at a high noise ratio. Compared to the correction\nmechanism focusing on noise samples, memory bank-based estimation is more\nrobust, which makes the model performance stable on noisy datasets. Extensive\nexperiments demonstrate that our method significantly improves matching\naccuracy and performance stability at increasing noise ratio. Our approach also\nsurpasses the state-of-the-art methods by a large margin. The code is available\nat: https://github.com/ZhangXu0963/NPC.\n","authors":["Xu Zhang","Hao Li","Mang Ye"],"pdf_url":"https://arxiv.org/pdf/2312.05777v2.pdf","comment":"9 pages, 5 figures, conference"},{"id":"http://arxiv.org/abs/2308.11932v3","updated":"2023-12-15T02:15:35Z","published":"2023-08-23T05:40:55Z","title":"Synergistic Multiscale Detail Refinement via Intrinsic Supervision for\n  Underwater Image Enhancement","summary":"  Visually restoring underwater scenes primarily involves mitigating\ninterference from underwater media. Existing methods ignore the inherent\nscale-related characteristics in underwater scenes. Therefore, we present the\nsynergistic multi-scale detail refinement via intrinsic supervision (SMDR-IS)\nfor enhancing underwater scene details, which contain multi-stages. The\nlow-degradation stage from the original images furnishes the original stage\nwith multi-scale details, achieved through feature propagation using the\nAdaptive Selective Intrinsic Supervised Feature (ASISF) module. By using\nintrinsic supervision, the ASISF module can precisely control and guide feature\ntransmission across multi-degradation stages, enhancing multi-scale detail\nrefinement and minimizing the interference from irrelevant information in the\nlow-degradation stage. In multi-degradation encoder-decoder framework of\nSMDR-IS, we introduce the Bifocal Intrinsic-Context Attention Module (BICA).\nBased on the intrinsic supervision principles, BICA efficiently exploits\nmulti-scale scene information in images. BICA directs higher-resolution spaces\nby tapping into the insights of lower-resolution ones, underscoring the pivotal\nrole of spatial contextual relationships in underwater image restoration.\nThroughout training, the inclusion of a multi-degradation loss function can\nenhance the network, allowing it to adeptly extract information across diverse\nscales. When benchmarked against state-of-the-art methods, SMDR-IS consistently\nshowcases superior performance. The code is publicly available at:\nhttps://github.com/zhoujingchun03/SMDR-IS.\n","authors":["Dehuan Zhang","Jingchun Zhou","ChunLe Guo","Weishi Zhang","Chongyi Li"],"pdf_url":"https://arxiv.org/pdf/2308.11932v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16973v2","updated":"2023-12-15T02:15:29Z","published":"2023-11-24T00:16:00Z","title":"DemoFusion: Democratising High-Resolution Image Generation With No $$$","summary":"  High-resolution image generation with Generative Artificial Intelligence\n(GenAI) has immense potential but, due to the enormous capital investment\nrequired for training, it is increasingly centralised to a few large\ncorporations, and hidden behind paywalls. This paper aims to democratise\nhigh-resolution GenAI by advancing the frontier of high-resolution generation\nwhile remaining accessible to a broad audience. We demonstrate that existing\nLatent Diffusion Models (LDMs) possess untapped potential for higher-resolution\nimage generation. Our novel DemoFusion framework seamlessly extends open-source\nGenAI models, employing Progressive Upscaling, Skip Residual, and Dilated\nSampling mechanisms to achieve higher-resolution image generation. The\nprogressive nature of DemoFusion requires more passes, but the intermediate\nresults can serve as \"previews\", facilitating rapid prompt iteration.\n","authors":["Ruoyi Du","Dongliang Chang","Timothy Hospedales","Yi-Zhe Song","Zhanyu Ma"],"pdf_url":"https://arxiv.org/pdf/2311.16973v2.pdf","comment":"Project Page: https://ruoyidu.github.io/demofusion/demofusion.html"},{"id":"http://arxiv.org/abs/2303.05754v2","updated":"2023-12-15T02:12:55Z","published":"2023-03-10T07:42:49Z","title":"Decomposed Diffusion Sampler for Accelerating Large-Scale Inverse\n  Problems","summary":"  Krylov subspace, which is generated by multiplying a given vector by the\nmatrix of a linear transformation and its successive powers, has been\nextensively studied in classical optimization literature to design algorithms\nthat converge quickly for large linear inverse problems. For example, the\nconjugate gradient method (CG), one of the most popular Krylov subspace\nmethods, is based on the idea of minimizing the residual error in the Krylov\nsubspace. However, with the recent advancement of high-performance diffusion\nsolvers for inverse problems, it is not clear how classical wisdom can be\nsynergistically combined with modern diffusion models. In this study, we\npropose a novel and efficient diffusion sampling strategy that synergistically\ncombine the diffusion sampling and Krylov subspace methods. Specifically, we\nprove that if the tangent space at a denoised sample by Tweedie's formula forms\na Krylov subspace, then the CG initialized with the denoised data ensures the\ndata consistency update to remain in the tangent space. This negates the need\nto compute the manifold-constrained gradient (MCG), leading to a more efficient\ndiffusion sampling method. Our method is applicable regardless of the\nparametrization and setting (i.e., VE, VP). Notably, we achieve\nstate-of-the-art reconstruction quality on challenging real-world medical\ninverse imaging problems, including multi-coil MRI reconstruction and 3D CT\nreconstruction. Moreover, our proposed method achieves more than 80 times\nfaster inference time than the previous state-of-the-art method.\n","authors":["Hyungjin Chung","Suhyeon Lee","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2303.05754v2.pdf","comment":"28 pages, 9 figures"},{"id":"http://arxiv.org/abs/2312.09095v2","updated":"2023-12-15T02:03:30Z","published":"2023-12-14T16:26:46Z","title":"ColNeRF: Collaboration for Generalizable Sparse Input Neural Radiance\n  Field","summary":"  Neural Radiance Fields (NeRF) have demonstrated impressive potential in\nsynthesizing novel views from dense input, however, their effectiveness is\nchallenged when dealing with sparse input. Existing approaches that incorporate\nadditional depth or semantic supervision can alleviate this issue to an extent.\nHowever, the process of supervision collection is not only costly but also\npotentially inaccurate, leading to poor performance and generalization ability\nin diverse scenarios. In our work, we introduce a novel model: the\nCollaborative Neural Radiance Fields (ColNeRF) designed to work with sparse\ninput. The collaboration in ColNeRF includes both the cooperation between\nsparse input images and the cooperation between the output of the neural\nradiation field. Through this, we construct a novel collaborative module that\naligns information from various views and meanwhile imposes self-supervised\nconstraints to ensure multi-view consistency in both geometry and appearance. A\nCollaborative Cross-View Volume Integration module (CCVI) is proposed to\ncapture complex occlusions and implicitly infer the spatial location of\nobjects. Moreover, we introduce self-supervision of target rays projected in\nmultiple directions to ensure geometric and color consistency in adjacent\nregions. Benefiting from the collaboration at the input and output ends,\nColNeRF is capable of capturing richer and more generalized scene\nrepresentation, thereby facilitating higher-quality results of the novel view\nsynthesis. Extensive experiments demonstrate that ColNeRF outperforms\nstate-of-the-art sparse input generalizable NeRF methods. Furthermore, our\napproach exhibits superiority in fine-tuning towards adapting to new scenes,\nachieving competitive performance compared to per-scene optimized NeRF-based\nmethods while significantly reducing computational costs. Our code is available\nat: https://github.com/eezkni/ColNeRF.\n","authors":["Zhangkai Ni","Peiqi Yang","Wenhan Yang","Hanli Wang","Lin Ma","Sam Kwong"],"pdf_url":"https://arxiv.org/pdf/2312.09095v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05836v4","updated":"2023-12-15T01:58:38Z","published":"2023-11-10T02:47:15Z","title":"UMedNeRF: Uncertainty-aware Single View Volumetric Rendering for Medical\n  Neural Radiance Fields","summary":"  In the field of clinical medicine, computed tomography (CT) is an effective\nmedical imaging modality for the diagnosis of various pathologies. Compared\nwith X-ray images, CT images can provide more information, including\nmulti-planar slices and three-dimensional structures for clinical diagnosis.\nHowever, CT imaging requires patients to be exposed to large doses of ionizing\nradiation for a long time, which may cause irreversible physical harm. In this\npaper, we propose an Uncertainty-aware MedNeRF (UMedNeRF) network based on\ngenerated radiation fields. The network can learn a continuous representation\nof CT projections from 2D X-ray images by obtaining the internal structure and\ndepth information and using adaptive loss weights to ensure the quality of the\ngenerated images. Our model is trained on publicly available knee and chest\ndatasets, and we show the results of CT projection rendering with a single\nX-ray and compare our method with other methods based on generated radiation\nfields.\n","authors":["Jing Hu","Qinrui Fan","Shu Hu","Siwei Lyu","Xi Wu","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2311.05836v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09486v1","updated":"2023-12-15T01:52:35Z","published":"2023-12-15T01:52:35Z","title":"Unraveling Batch Normalization for Realistic Test-Time Adaptation","summary":"  While recent test-time adaptations exhibit efficacy by adjusting batch\nnormalization to narrow domain disparities, their effectiveness diminishes with\nrealistic mini-batches due to inaccurate target estimation. As previous\nattempts merely introduce source statistics to mitigate this issue, the\nfundamental problem of inaccurate target estimation still persists, leaving the\nintrinsic test-time domain shifts unresolved. This paper delves into the\nproblem of mini-batch degradation. By unraveling batch normalization, we\ndiscover that the inexact target statistics largely stem from the substantially\nreduced class diversity in batch. Drawing upon this insight, we introduce a\nstraightforward tool, Test-time Exponential Moving Average (TEMA), to bridge\nthe class diversity gap between training and testing batches. Importantly, our\nTEMA adaptively extends the scope of typical methods beyond the current batch\nto incorporate a diverse set of class information, which in turn boosts an\naccurate target estimation. Built upon this foundation, we further design a\nnovel layer-wise rectification strategy to consistently promote test-time\nperformance. Our proposed method enjoys a unique advantage as it requires\nneither training nor tuning parameters, offering a truly hassle-free solution.\nIt significantly enhances model robustness against shifted domains and\nmaintains resilience in diverse real-world scenarios with various batch sizes,\nachieving state-of-the-art performance on several major benchmarks. Code is\navailable at \\url{https://github.com/kiwi12138/RealisticTTA}.\n","authors":["Zixian Su","Jingwei Guo","Kai Yao","Xi Yang","Qiufeng Wang","Kaizhu Huang"],"pdf_url":"https://arxiv.org/pdf/2312.09486v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.09481v1","updated":"2023-12-15T01:38:26Z","published":"2023-12-15T01:38:26Z","title":"Continual Adversarial Defense","summary":"  In response to the rapidly evolving nature of adversarial attacks on a\nmonthly basis, numerous defenses have been proposed to generalize against as\nmany known attacks as possible. However, designing a defense method that can\ngeneralize to all types of attacks, including unseen ones, is not realistic\nbecause the environment in which defense systems operate is dynamic and\ncomprises various unique attacks used by many attackers. The defense system\nneeds to upgrade itself by utilizing few-shot defense feedback and efficient\nmemory. Therefore, we propose the first continual adversarial defense (CAD)\nframework that adapts to any attacks in a dynamic scenario, where various\nattacks emerge stage by stage. In practice, CAD is modeled under four\nprinciples: (1) continual adaptation to new attacks without catastrophic\nforgetting, (2) few-shot adaptation, (3) memory-efficient adaptation, and (4)\nhigh accuracy on both clean and adversarial images. We leverage cutting-edge\ncontinual learning, few-shot learning, and ensemble learning techniques to\nqualify the principles. Experiments conducted on CIFAR-10 and ImageNet-100\nvalidate the effectiveness of our approach against multiple stages of 10 modern\nadversarial attacks and significant improvements over 10 baseline methods. In\nparticular, CAD is capable of quickly adapting with minimal feedback and a low\ncost of defense failure, while maintaining good performance against old\nattacks. Our research sheds light on a brand-new paradigm for continual defense\nadaptation against dynamic and evolving attacks.\n","authors":["Qian Wang","Yaoyao Liu","Hefei Ling","Yingwei Li","Qihao Liu","Ping Li","Jiazhong Chen","Alan Yuille","Ning Yu"],"pdf_url":"https://arxiv.org/pdf/2312.09481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09480v1","updated":"2023-12-15T01:37:29Z","published":"2023-12-15T01:37:29Z","title":"TAB: Text-Align Anomaly Backbone Model for Industrial Inspection Tasks","summary":"  In recent years, the focus on anomaly detection and localization in\nindustrial inspection tasks has intensified. While existing studies have\ndemonstrated impressive outcomes, they often rely heavily on extensive training\ndatasets or robust features extracted from pre-trained models trained on\ndiverse datasets like ImageNet. In this work, we propose a novel framework\nleveraging the visual-linguistic CLIP model to adeptly train a backbone model\ntailored to the manufacturing domain. Our approach concurrently considers\nvisual and text-aligned embedding spaces for normal and abnormal conditions.\nThe resulting pre-trained backbone markedly enhances performance in industrial\ndownstream tasks, particularly in anomaly detection and localization. Notably,\nthis improvement is substantiated through experiments conducted on multiple\ndatasets such as MVTecAD, BTAD, and KSDD2. Furthermore, using our pre-trained\nbackbone weights allows previous works to achieve superior performance in\nfew-shot scenarios with less training data. The proposed anomaly backbone\nprovides a foundation model for more precise anomaly detection and\nlocalization.\n","authors":["Ho-Weng Lee","Shang-Hong Lai"],"pdf_url":"https://arxiv.org/pdf/2312.09480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09411v1","updated":"2023-12-15T00:22:55Z","published":"2023-12-15T00:22:55Z","title":"OTOv3: Automatic Architecture-Agnostic Neural Network Training and\n  Compression from Structured Pruning to Erasing Operators","summary":"  Compressing a predefined deep neural network (DNN) into a compact sub-network\nwith competitive performance is crucial in the efficient machine learning\nrealm. This topic spans various techniques, from structured pruning to neural\narchitecture search, encompassing both pruning and erasing operators\nperspectives. Despite advancements, existing methods suffers from complex,\nmulti-stage processes that demand substantial engineering and domain knowledge,\nlimiting their broader applications. We introduce the third-generation\nOnly-Train-Once (OTOv3), which first automatically trains and compresses a\ngeneral DNN through pruning and erasing operations, creating a compact and\ncompetitive sub-network without the need of fine-tuning. OTOv3 simplifies and\nautomates the training and compression process, minimizes the engineering\nefforts required from users. It offers key technological advancements: (i)\nautomatic search space construction for general DNNs based on dependency graph\nanalysis; (ii) Dual Half-Space Projected Gradient (DHSPG) and its enhanced\nversion with hierarchical search (H2SPG) to reliably solve (hierarchical)\nstructured sparsity problems and ensure sub-network validity; and (iii)\nautomated sub-network construction using solutions from DHSPG/H2SPG and\ndependency graphs. Our empirical results demonstrate the efficacy of OTOv3\nacross various benchmarks in structured pruning and neural architecture search.\nOTOv3 produces sub-networks that match or exceed the state-of-the-arts. The\nsource code will be available at https://github.com/tianyic/only_train_once.\n","authors":["Tianyi Chen","Tianyu Ding","Zhihui Zhu","Zeyu Chen","HsiangTao Wu","Ilya Zharkov","Luming Liang"],"pdf_url":"https://arxiv.org/pdf/2312.09411v1.pdf","comment":"39 pages. Due to the page dim limitation, the full appendix is\n  attached here https://tinyurl.com/otov3appendix. Recommend to zoom-in for\n  finer details. arXiv admin note: text overlap with arXiv:2305.18030"},{"id":"http://arxiv.org/abs/2306.08707v3","updated":"2023-12-15T23:54:57Z","published":"2023-06-14T19:15:49Z","title":"VidEdit: Zero-Shot and Spatially Aware Text-Driven Video Editing","summary":"  Recently, diffusion-based generative models have achieved remarkable success\nfor image generation and edition. However, their use for video editing still\nfaces important limitations. This paper introduces VidEdit, a novel method for\nzero-shot text-based video editing ensuring strong temporal and spatial\nconsistency. Firstly, we propose to combine atlas-based and pre-trained\ntext-to-image diffusion models to provide a training-free and efficient editing\nmethod, which by design fulfills temporal smoothness. Secondly, we leverage\noff-the-shelf panoptic segmenters along with edge detectors and adapt their use\nfor conditioned diffusion-based atlas editing. This ensures a fine spatial\ncontrol on targeted regions while strictly preserving the structure of the\noriginal video. Quantitative and qualitative experiments show that VidEdit\noutperforms state-of-the-art methods on DAVIS dataset, regarding semantic\nfaithfulness, image preservation, and temporal consistency metrics. With this\nframework, processing a single video only takes approximately one minute, and\nit can generate multiple compatible edits based on a unique text prompt.\nProject web-page at https://videdit.github.io\n","authors":["Paul Couairon","Clément Rambour","Jean-Emmanuel Haugeard","Nicolas Thome"],"pdf_url":"https://arxiv.org/pdf/2306.08707v3.pdf","comment":"Project web-page at https://videdit.github.io"},{"id":"http://arxiv.org/abs/2309.06618v3","updated":"2023-12-15T23:54:04Z","published":"2023-09-12T22:21:14Z","title":"Multi-dimensional Fusion and Consistency for Semi-supervised Medical\n  Image Segmentation","summary":"  In this paper, we introduce a novel semi-supervised learning framework\ntailored for medical image segmentation. Central to our approach is the\ninnovative Multi-scale Text-aware ViT-CNN Fusion scheme. This scheme adeptly\ncombines the strengths of both ViTs and CNNs, capitalizing on the unique\nadvantages of both architectures as well as the complementary information in\nvision-language modalities. Further enriching our framework, we propose the\nMulti-Axis Consistency framework for generating robust pseudo labels, thereby\nenhancing the semisupervised learning process. Our extensive experiments on\nseveral widelyused datasets unequivocally demonstrate the efficacy of our\napproach.\n","authors":["Yixing Lu","Zhaoxin Fan","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2309.06618v3.pdf","comment":"Accepted by the 30th International Conference on MultiMedia Modeling"},{"id":"http://arxiv.org/abs/2312.10264v1","updated":"2023-12-15T23:46:03Z","published":"2023-12-15T23:46:03Z","title":"Progressive Painterly Image Harmonization from Low-level Styles to\n  High-level Styles","summary":"  Painterly image harmonization aims to harmonize a photographic foreground\nobject on the painterly background. Different from previous auto-encoder based\nharmonization networks, we develop a progressive multi-stage harmonization\nnetwork, which harmonizes the composite foreground from low-level styles (e.g.,\ncolor, simple texture) to high-level styles (e.g., complex texture). Our\nnetwork has better interpretability and harmonization performance. Moreover, we\ndesign an early-exit strategy to automatically decide the proper stage to exit,\nwhich can skip the unnecessary and even harmful late stages. Extensive\nexperiments on the benchmark dataset demonstrate the effectiveness of our\nprogressive harmonization network.\n","authors":["Li Niu","Yan Hong","Junyan Cao","Liqing Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.10264v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.10263v1","updated":"2023-12-15T23:36:44Z","published":"2023-12-15T23:36:44Z","title":"Painterly Image Harmonization by Learning from Painterly Objects","summary":"  Given a composite image with photographic object and painterly background,\npainterly image harmonization targets at stylizing the composite object to be\ncompatible with the background. Despite the competitive performance of existing\npainterly harmonization works, they did not fully leverage the painterly\nobjects in artistic paintings. In this work, we explore learning from painterly\nobjects for painterly image harmonization. In particular, we learn a mapping\nfrom background style and object information to object style based on painterly\nobjects in artistic paintings. With the learnt mapping, we can hallucinate the\ntarget style of composite object, which is used to harmonize encoder feature\nmaps to produce the harmonized image. Extensive experiments on the benchmark\ndataset demonstrate the effectiveness of our proposed method.\n","authors":["Li Niu","Junyan Cao","Yan Hong","Liqing Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.10263v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.10251v1","updated":"2023-12-15T22:50:12Z","published":"2023-12-15T22:50:12Z","title":"Advancing Surgical VQA with Scene Graph Knowledge","summary":"  Modern operating room is becoming increasingly complex, requiring innovative\nintra-operative support systems. While the focus of surgical data science has\nlargely been on video analysis, integrating surgical computer vision with\nlanguage capabilities is emerging as a necessity. Our work aims to advance\nVisual Question Answering (VQA) in the surgical context with scene graph\nknowledge, addressing two main challenges in the current surgical VQA systems:\nremoving question-condition bias in the surgical VQA dataset and incorporating\nscene-aware reasoning in the surgical VQA model design. First, we propose a\nSurgical Scene Graph-based dataset, SSG-QA, generated by employing segmentation\nand detection models on publicly available datasets. We build surgical scene\ngraphs using spatial and action information of instruments and anatomies. These\ngraphs are fed into a question engine, generating diverse QA pairs. Our SSG-QA\ndataset provides a more complex, diverse, geometrically grounded, unbiased, and\nsurgical action-oriented dataset compared to existing surgical VQA datasets. We\nthen propose SSG-QA-Net, a novel surgical VQA model incorporating a lightweight\nScene-embedded Interaction Module (SIM), which integrates geometric scene\nknowledge in the VQA model design by employing cross-attention between the\ntextual and the scene features. Our comprehensive analysis of the SSG-QA\ndataset shows that SSG-QA-Net outperforms existing methods across different\nquestion types and complexities. We highlight that the primary limitation in\nthe current surgical VQA systems is the lack of scene knowledge to answer\ncomplex queries. We present a novel surgical VQA dataset and model and show\nthat results can be significantly improved by incorporating geometric scene\nfeatures in the VQA model design. The source code and the dataset will be made\npublicly available at: https://github.com/CAMMA-public/SSG-QA\n","authors":["Kun Yuan","Manasi Kattel","Joel L. Lavanchy","Nassir Navab","Vinkle Srivastava","Nicolas Padoy"],"pdf_url":"https://arxiv.org/pdf/2312.10251v1.pdf","comment":"Conditional Accept by IPCAI 2024"},{"id":"http://arxiv.org/abs/2312.10246v1","updated":"2023-12-15T22:34:17Z","published":"2023-12-15T22:34:17Z","title":"Implicit Modeling of Non-rigid Objects with Cross-Category Signals","summary":"  Deep implicit functions (DIFs) have emerged as a potent and articulate means\nof representing 3D shapes. However, methods modeling object categories or\nnon-rigid entities have mainly focused on single-object scenarios. In this\nwork, we propose MODIF, a multi-object deep implicit function that jointly\nlearns the deformation fields and instance-specific latent codes for multiple\nobjects at once. Our emphasis is on non-rigid, non-interpenetrating entities\nsuch as organs. To effectively capture the interrelation between these entities\nand ensure precise, collision-free representations, our approach facilitates\nsignaling between category-specific fields to adequately rectify shapes. We\nalso introduce novel inter-object supervision: an attraction-repulsion loss is\nformulated to refine contact regions between objects. Our approach is\ndemonstrated on various medical benchmarks, involving modeling different groups\nof intricate anatomical entities. Experimental results illustrate that our\nmodel can proficiently learn the shape representation of each organ and their\nrelations to others, to the point that shapes missing from unseen instances can\nbe consistently recovered by our method. Finally, MODIF can also propagate\nsemantic information throughout the population via accurate point\ncorrespondences\n","authors":["Yuchun Liu","Benjamin Planche","Meng Zheng","Zhongpai Gao","Pierre Sibut-Bourde","Fan Yang","Terrence Chen","Ziyan Wu"],"pdf_url":"https://arxiv.org/pdf/2312.10246v1.pdf","comment":"Accepted at AAAI 2024. Paper + supplementary material"},{"id":"http://arxiv.org/abs/2312.10240v1","updated":"2023-12-15T22:18:38Z","published":"2023-12-15T22:18:38Z","title":"Rich Human Feedback for Text-to-Image Generation","summary":"  Recent Text-to-Image (T2I) generation models such as Stable Diffusion and\nImagen have made significant progress in generating high-resolution images\nbased on text descriptions. However, many generated images still suffer from\nissues such as artifacts/implausibility, misalignment with text descriptions,\nand low aesthetic quality. Inspired by the success of Reinforcement Learning\nwith Human Feedback (RLHF) for large language models, prior works collected\nhuman-provided scores as feedback on generated images and trained a reward\nmodel to improve the T2I generation. In this paper, we enrich the feedback\nsignal by (i) marking image regions that are implausible or misaligned with the\ntext, and (ii) annotating which words in the text prompt are misrepresented or\nmissing on the image. We collect such rich human feedback on 18K generated\nimages and train a multimodal transformer to predict the rich feedback\nautomatically. We show that the predicted rich human feedback can be leveraged\nto improve image generation, for example, by selecting high-quality training\ndata to finetune and improve the generative models, or by creating masks with\npredicted heatmaps to inpaint the problematic regions. Notably, the\nimprovements generalize to models (Muse) beyond those used to generate the\nimages on which human feedback data were collected (Stable Diffusion variants).\n","authors":["Youwei Liang","Junfeng He","Gang Li","Peizhao Li","Arseniy Klimovskiy","Nicholas Carolan","Jiao Sun","Jordi Pont-Tuset","Sarah Young","Feng Yang","Junjie Ke","Krishnamurthy Dj Dvijotham","Katie Collins","Yiwen Luo","Yang Li","Kai J Kohlhoff","Deepak Ramachandran","Vidhya Navalpakkam"],"pdf_url":"https://arxiv.org/pdf/2312.10240v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10237v1","updated":"2023-12-15T22:09:04Z","published":"2023-12-15T22:09:04Z","title":"Vertical Federated Alzheimer's Detection on Multimodal Data","summary":"  In the era of rapidly advancing medical technologies, the segmentation of\nmedical data has become inevitable, necessitating the development of privacy\npreserving machine learning algorithms that can train on distributed data.\nConsolidating sensitive medical data is not always an option particularly due\nto the stringent privacy regulations imposed by the Health Insurance\nPortability and Accountability Act (HIPAA). In this paper, we introduce a HIPAA\ncompliant framework that can train from distributed data. We then propose a\nmultimodal vertical federated model for Alzheimer's Disease (AD) detection, a\nserious neurodegenerative condition that can cause dementia, severely impairing\nbrain function and hindering simple tasks, especially without preventative\ncare. This vertical federated model offers a distributed architecture that\nenables collaborative learning across diverse sources of medical data while\nrespecting privacy constraints imposed by HIPAA. It is also able to leverage\nmultiple modalities of data, enhancing the robustness and accuracy of AD\ndetection. Our proposed model not only contributes to the advancement of\nfederated learning techniques but also holds promise for overcoming the hurdles\nposed by data segmentation in medical research. By using vertical federated\nlearning, this research strives to provide a framework that enables healthcare\ninstitutions to harness the collective intelligence embedded in their\ndistributed datasets without compromising patient privacy.\n","authors":["Paul K. Mandal"],"pdf_url":"https://arxiv.org/pdf/2312.10237v1.pdf","comment":"14 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2312.10217v1","updated":"2023-12-15T21:30:49Z","published":"2023-12-15T21:30:49Z","title":"T-MAE: Temporal Masked Autoencoders for Point Cloud Representation\n  Learning","summary":"  The scarcity of annotated data in outdoor point cloud segmentation poses a\nsignificant obstacle in harnessing the modeling capabilities of advanced\nnetworks like transformers. Consequently, scholars have been actively\ninvestigating efficacious self-supervised pre-training strategies, e.g.\ncontrasting learning and reconstruction-based pretext tasks. Nevertheless,\ntemporal information, which is inherent in the LiDAR point cloud sequence, is\nconsistently disregarded. To better utilize this property, we propose an\neffective pre-training strategy, namely Temporal Masked AutoEncoders (T-MAE),\nwhich takes as input temporally adjacent frames and learns temporal dependency.\nA SiamWCA backbone, containing a Siamese encoder and a window-based\ncross-attention (WCA) module, is established for the two-frame input. Taking\ninto account that the motion of an ego-vehicle alters the illumination angles\nof the same instance, temporal modeling also serves as a robust and natural\ndata augmentation, enhancing the comprehension of target objects. Moreover,\ninstead of utilizing consecutive frames, it is more cost-effective and powerful\nby using distant historical frames. SiamWCA is a powerful architecture but\nheavily relies on annotated data. With our T-MAE pre-training strategy, we\nachieve the best performance on the Waymo dataset among self-supervised\nlearning methods. Comprehensive experiments are conducted to validate all\ncomponents of our proposal. Upon acceptance, the source code will be made\naccessible.\n","authors":["Weijie Wei","Fatemeh Karimi Nejadasl","Theo Gevers","Martin R. Oswald"],"pdf_url":"https://arxiv.org/pdf/2312.10217v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2308.10997v3","updated":"2023-12-15T21:16:22Z","published":"2023-08-14T14:07:17Z","title":"MarkovGen: Structured Prediction for Efficient Text-to-Image Generation","summary":"  Modern text-to-image generation models produce high-quality images that are\nboth photorealistic and faithful to the text prompts. However, this quality\ncomes at significant computational cost: nearly all of these models are\niterative and require running sampling multiple times with large models. This\niterative process is needed to ensure that different regions of the image are\nnot only aligned with the text prompt, but also compatible with each other. In\nthis work, we propose a light-weight approach to achieving this compatibility\nbetween different regions of an image, using a Markov Random Field (MRF) model.\nWe demonstrate the effectiveness of this method on top of the latent\ntoken-based Muse text-to-image model. The MRF richly encodes the compatibility\namong image tokens at different spatial locations to improve quality and\nsignificantly reduce the required number of Muse sampling steps. Inference with\nthe MRF is significantly cheaper, and its parameters can be quickly learned\nthrough back-propagation by modeling MRF inference as a differentiable\nneural-network layer. Our full model, MarkovGen, uses this proposed MRF model\nto both speed up Muse by 1.5X and produce higher quality images by decreasing\nundesirable image artifacts.\n","authors":["Sadeep Jayasumana","Daniel Glasner","Srikumar Ramalingam","Andreas Veit","Ayan Chakrabarti","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2308.10997v3.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2312.09958v1","updated":"2023-12-15T17:11:07Z","published":"2023-12-15T17:11:07Z","title":"Distilling Large Language Models for Matching Patients to Clinical\n  Trials","summary":"  The recent success of large language models (LLMs) has paved the way for\ntheir adoption in the high-stakes domain of healthcare. Specifically, the\napplication of LLMs in patient-trial matching, which involves assessing patient\neligibility against clinical trial's nuanced inclusion and exclusion criteria,\nhas shown promise. Recent research has shown that GPT-3.5, a widely recognized\nLLM developed by OpenAI, can outperform existing methods with minimal 'variable\nengineering' by simply comparing clinical trial information against patient\nsummaries. However, there are significant challenges associated with using\nclosed-source proprietary LLMs like GPT-3.5 in practical healthcare\napplications, such as cost, privacy and reproducibility concerns. To address\nthese issues, this study presents the first systematic examination of the\nefficacy of both proprietary (GPT-3.5, and GPT-4) and open-source LLMs (LLAMA\n7B,13B, and 70B) for the task of patient-trial matching. Employing a\nmultifaceted evaluation framework, we conducted extensive automated and\nhuman-centric assessments coupled with a detailed error analysis for each\nmodel. To enhance the adaptability of open-source LLMs, we have created a\nspecialized synthetic dataset utilizing GPT-4, enabling effective fine-tuning\nunder constrained data conditions. Our findings reveal that open-source LLMs,\nwhen fine-tuned on this limited and synthetic dataset, demonstrate performance\nparity with their proprietary counterparts. This presents a massive opportunity\nfor their deployment in real-world healthcare applications. To foster further\nresearch and applications in this field, we release both the annotated\nevaluation dataset along with the fine-tuned LLM -- Trial-LLAMA -- for public\nuse.\n","authors":["Mauro Nievas","Aditya Basu","Yanshan Wang","Hrituraj Singh"],"pdf_url":"https://arxiv.org/pdf/2312.09958v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09948v1","updated":"2023-12-15T16:58:52Z","published":"2023-12-15T16:58:52Z","title":"GEAR-Up: Generative AI and External Knowledge-based Retrieval Upgrading\n  Scholarly Article Searches for Systematic Reviews","summary":"  Systematic reviews (SRs) - the librarian-assisted literature survey of\nscholarly articles takes time and requires significant human resources. Given\nthe ever-increasing volume of published studies, applying existing computing\nand informatics technology can decrease this time and resource burden. Due to\nthe revolutionary advances in (1) Generative AI such as ChatGPT, and (2)\nExternal knowledge-augmented information extraction efforts such as\nRetrieval-Augmented Generation, In this work, we explore the use of techniques\nfrom (1) and (2) for SR. We demonstrate a system that takes user queries,\nperforms query expansion to obtain enriched context (includes additional terms\nand definitions by querying language models and knowledge graphs), and uses\nthis context to search for articles on scholarly databases to retrieve\narticles. We perform qualitative evaluations of our system through comparison\nagainst sentinel (ground truth) articles provided by an in-house librarian. The\ndemo can be found at: https://youtu.be/zMdP56GJ9mU.\n","authors":["Kaushik Roy","Vedant Khandelwal","Harshul Surana","Valerie Vera","Amit Sheth","Heather Heckman"],"pdf_url":"https://arxiv.org/pdf/2312.09948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09901v1","updated":"2023-12-15T15:53:45Z","published":"2023-12-15T15:53:45Z","title":"Temporally and Distributionally Robust Optimization for Cold-start\n  Recommendation","summary":"  Collaborative Filtering (CF) recommender models highly depend on user-item\ninteractions to learn CF representations, thus falling short of recommending\ncold-start items. To address this issue, prior studies mainly introduce item\nfeatures (e.g., thumbnails) for cold-start item recommendation. They learn a\nfeature extractor on warm-start items to align feature representations with\ninteractions, and then leverage the feature extractor to extract the feature\nrepresentations of cold-start items for interaction prediction. Unfortunately,\nthe features of cold-start items, especially the popular ones, tend to diverge\nfrom those of warm-start ones due to temporal feature shifts, preventing the\nfeature extractor from accurately learning feature representations of\ncold-start items.\n  To alleviate the impact of temporal feature shifts, we consider using\nDistributionally Robust Optimization (DRO) to enhance the generation ability of\nthe feature extractor. Nonetheless, existing DRO methods face an inconsistency\nissue: the worse-case warm-start items emphasized during DRO training might not\nalign well with the cold-start item distribution. To capture the temporal\nfeature shifts and combat this inconsistency issue, we propose a novel temporal\nDRO with new optimization objectives, namely, 1) to integrate a worst-case\nfactor to improve the worst-case performance, and 2) to devise a shifting\nfactor to capture the shifting trend of item features and enhance the\noptimization of the potentially popular groups in cold-start items. Substantial\nexperiments on three real-world datasets validate the superiority of our\ntemporal DRO in enhancing the generalization ability of cold-start recommender\nmodels.\n","authors":["Xinyu Lin","Wenjie Wang","Jujia Zhao","Yongqi Li","Fuli Feng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2312.09901v1.pdf","comment":"Accepted by AAAI'24"},{"id":"http://arxiv.org/abs/2310.15950v2","updated":"2023-12-15T13:11:05Z","published":"2023-10-24T15:51:13Z","title":"Representation Learning with Large Language Models for Recommendation","summary":"  Recommender systems have seen significant advancements with the influence of\ndeep learning and graph neural networks, particularly in capturing complex\nuser-item relationships. However, these graph-based recommenders heavily depend\non ID-based data, potentially disregarding valuable textual information\nassociated with users and items, resulting in less informative learned\nrepresentations. Moreover, the utilization of implicit feedback data introduces\npotential noise and bias, posing challenges for the effectiveness of user\npreference learning. While the integration of large language models (LLMs) into\ntraditional ID-based recommenders has gained attention, challenges such as\nscalability issues, limitations in text-only reliance, and prompt input\nconstraints need to be addressed for effective implementation in practical\nrecommender systems. To address these challenges, we propose a model-agnostic\nframework RLMRec that aims to enhance existing recommenders with LLM-empowered\nrepresentation learning. It proposes a recommendation paradigm that integrates\nrepresentation learning with LLMs to capture intricate semantic aspects of user\nbehaviors and preferences. RLMRec incorporates auxiliary textual signals,\ndevelops a user/item profiling paradigm empowered by LLMs, and aligns the\nsemantic space of LLMs with the representation space of collaborative\nrelational signals through a cross-view alignment framework. This work further\nestablish a theoretical foundation demonstrating that incorporating textual\nsignals through mutual information maximization enhances the quality of\nrepresentations. In our evaluation, we integrate RLMRec with state-of-the-art\nrecommender models, while also analyzing its efficiency and robustness to noise\ndata. Our implementation codes are available at\nhttps://github.com/HKUDS/RLMRec.\n","authors":["Xubin Ren","Wei Wei","Lianghao Xia","Lixin Su","Suqi Cheng","Junfeng Wang","Dawei Yin","Chao Huang"],"pdf_url":"https://arxiv.org/pdf/2310.15950v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09715v1","updated":"2023-12-15T11:42:50Z","published":"2023-12-15T11:42:50Z","title":"CETN: Contrast-enhanced Through Network for CTR Prediction","summary":"  Click-through rate (CTR) Prediction is a crucial task in personalized\ninformation retrievals, such as industrial recommender systems, online\nadvertising, and web search. Most existing CTR Prediction models utilize\nexplicit feature interactions to overcome the performance bottleneck of\nimplicit feature interactions. Hence, deep CTR models based on parallel\nstructures (e.g., DCN, FinalMLP, xDeepFM) have been proposed to obtain joint\ninformation from different semantic spaces. However, these parallel\nsubcomponents lack effective supervisory signals, making it challenging to\nefficiently capture valuable multi-views feature interaction information in\ndifferent semantic spaces. To address this issue, we propose a simple yet\neffective novel CTR model: Contrast-enhanced Through Network for CTR (CETN), so\nas to ensure the diversity and homogeneity of feature interaction information.\nSpecifically, CETN employs product-based feature interactions and the\naugmentation (perturbation) concept from contrastive learning to segment\ndifferent semantic spaces, each with distinct activation functions. This\nimproves diversity in the feature interaction information captured by the\nmodel. Additionally, we introduce self-supervised signals and through\nconnection within each semantic space to ensure the homogeneity of the captured\nfeature interaction information. The experiments and research conducted on four\nreal datasets demonstrate that our model consistently outperforms twenty\nbaseline models in terms of AUC and Logloss.\n","authors":["Honghao Li","Lei Sang","Yi Zhang","Xuyun Zhang","Yiwen Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.09715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.12397v2","updated":"2023-12-15T11:06:16Z","published":"2022-08-26T01:49:02Z","title":"Causal Inference in Recommender Systems: A Survey and Future Directions","summary":"  Recommender systems have become crucial in information filtering nowadays.\nExisting recommender systems extract user preferences based on the correlation\nin data, such as behavioral correlation in collaborative filtering,\nfeature-feature, or feature-behavior correlation in click-through rate\nprediction. However, unfortunately, the real world is driven by causality, not\njust correlation, and correlation does not imply causation. For instance,\nrecommender systems might recommend a battery charger to a user after buying a\nphone, where the latter can serve as the cause of the former; such a causal\nrelation cannot be reversed. Recently, to address this, researchers in\nrecommender systems have begun utilizing causal inference to extract causality,\nthereby enhancing the recommender system. In this survey, we offer a\ncomprehensive review of the literature on causal inference-based\nrecommendation. Initially, we introduce the fundamental concepts of both\nrecommender system and causal inference as the foundation for subsequent\ncontent. We then highlight the typical issues faced by non-causality\nrecommender system. Following that, we thoroughly review the existing work on\ncausal inference-based recommender systems, based on a taxonomy of three-aspect\nchallenges that causal inference can address. Finally, we discuss the open\nproblems in this critical research area and suggest important potential future\nworks.\n","authors":["Chen Gao","Yu Zheng","Wenjie Wang","Fuli Feng","Xiangnan He","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2208.12397v2.pdf","comment":"Accepted by ACM Transactions on Information Systems (TOIS)"},{"id":"http://arxiv.org/abs/2312.09684v1","updated":"2023-12-15T10:58:45Z","published":"2023-12-15T10:58:45Z","title":"Context-Aware Sequential Model for Multi-Behaviour Recommendation","summary":"  Sequential recommendation models are crucial for next-item recommendations in\nonline platforms, capturing complex patterns in user interactions. However,\nmany focus on a single behavior, overlooking valuable implicit interactions\nlike clicks and favorites. Existing multi-behavioral models often fail to\nsimultaneously capture sequential patterns. We propose CASM, a Context-Aware\nSequential Model, leveraging sequential models to seamlessly handle multiple\nbehaviors. CASM employs context-aware multi-head self-attention for\nheterogeneous historical interactions and a weighted binary cross-entropy loss\nfor precise control over behavior contributions. Experimental results on four\ndatasets demonstrate CASM's superiority over state-of-the-art approaches.\n","authors":["Shereen Elsayed","Ahmed Rashed","Lars Schmidt-Thieme"],"pdf_url":"https://arxiv.org/pdf/2312.09684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09670v1","updated":"2023-12-15T10:31:36Z","published":"2023-12-15T10:31:36Z","title":"Probing Pretrained Language Models with Hierarchy Properties","summary":"  Since Pretrained Language Models (PLMs) are the cornerstone of the most\nrecent Information Retrieval (IR) models, the way they encode semantic\nknowledge is particularly important. However, little attention has been given\nto studying the PLMs' capability to capture hierarchical semantic knowledge.\nTraditionally, evaluating such knowledge encoded in PLMs relies on their\nperformance on a task-dependent evaluation approach based on proxy tasks, such\nas hypernymy detection. Unfortunately, this approach potentially ignores other\nimplicit and complex taxonomic relations. In this work, we propose a\ntask-agnostic evaluation method able to evaluate to what extent PLMs can\ncapture complex taxonomy relations, such as ancestors and siblings. The\nevaluation is based on intrinsic properties that capture the hierarchical\nnature of taxonomies. Our experimental evaluation shows that the\nlexico-semantic knowledge implicitly encoded in PLMs does not always capture\nhierarchical relations. We further demonstrate that the proposed properties can\nbe injected into PLMs to improve their understanding of hierarchy. Through\nevaluations on taxonomy reconstruction, hypernym discovery and reading\ncomprehension tasks, we show that the knowledge about hierarchy is moderately\nbut not systematically transferable across tasks.\n","authors":["Jesús Lovón-Melgarejo","Jose G. Moreno","Romaric Besançon","Olivier Ferret","Lynda Tamine"],"pdf_url":"https://arxiv.org/pdf/2312.09670v1.pdf","comment":"Accepted at ECIR 2024"},{"id":"http://arxiv.org/abs/2312.09631v1","updated":"2023-12-15T09:21:11Z","published":"2023-12-15T09:21:11Z","title":"Context-Driven Interactive Query Simulations Based on Generative Large\n  Language Models","summary":"  Simulating user interactions enables a more user-oriented evaluation of\ninformation retrieval (IR) systems. While user simulations are cost-efficient\nand reproducible, many approaches often lack fidelity regarding real user\nbehavior. Most notably, current user models neglect the user's context, which\nis the primary driver of perceived relevance and the interactions with the\nsearch results. To this end, this work introduces the simulation of\ncontext-driven query reformulations. The proposed query generation methods\nbuild upon recent Large Language Model (LLM) approaches and consider the user's\ncontext throughout the simulation of a search session. Compared to simple\ncontext-free query generation approaches, these methods show better\neffectiveness and allow the simulation of more efficient IR sessions.\nSimilarly, our evaluations consider more interaction context than current\nsession-based measures and reveal interesting complementary insights in\naddition to the established evaluation protocols. We conclude with directions\nfor future work and provide an entirely open experimental setup.\n","authors":["Björn Engelmann","Timo Breuer","Jana Isabelle Friese","Philipp Schaer","Norbert Fuhr"],"pdf_url":"https://arxiv.org/pdf/2312.09631v1.pdf","comment":"Accepted at ECIR 2024"},{"id":"http://arxiv.org/abs/2312.09627v1","updated":"2023-12-15T09:10:05Z","published":"2023-12-15T09:10:05Z","title":"TF-CLIP: Learning Text-free CLIP for Video-based Person\n  Re-Identification","summary":"  Large-scale language-image pre-trained models (e.g., CLIP) have shown\nsuperior performances on many cross-modal retrieval tasks. However, the problem\nof transferring the knowledge learned from such models to video-based person\nre-identification (ReID) has barely been explored. In addition, there is a lack\nof decent text descriptions in current ReID benchmarks. To address these\nissues, in this work, we propose a novel one-stage text-free CLIP-based\nlearning framework named TF-CLIP for video-based person ReID. More\nspecifically, we extract the identity-specific sequence feature as the\nCLIP-Memory to replace the text feature. Meanwhile, we design a\nSequence-Specific Prompt (SSP) module to update the CLIP-Memory online. To\ncapture temporal information, we further propose a Temporal Memory Diffusion\n(TMD) module, which consists of two key components: Temporal Memory\nConstruction (TMC) and Memory Diffusion (MD). Technically, TMC allows the\nframe-level memories in a sequence to communicate with each other, and to\nextract temporal information based on the relations within the sequence. MD\nfurther diffuses the temporal memories to each token in the original features\nto obtain more robust sequence features. Extensive experiments demonstrate that\nour proposed method shows much better results than other state-of-the-art\nmethods on MARS, LS-VID and iLIDS-VID. The code is available at\nhttps://github.com/AsuradaYuci/TF-CLIP.\n","authors":["Chenyang Yu","Xuehu Liu","Yingquan Wang","Pingping Zhang","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2312.09627v1.pdf","comment":"This work is accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2312.09602v1","updated":"2023-12-15T08:33:06Z","published":"2023-12-15T08:33:06Z","title":"Multi-Modality is All You Need for Transferable Recommender Systems","summary":"  ID-based Recommender Systems (RecSys), where each item is assigned a unique\nidentifier and subsequently converted into an embedding vector, have dominated\nthe designing of RecSys. Though prevalent, such ID-based paradigm is not\nsuitable for developing transferable RecSys and is also susceptible to the\ncold-start issue. In this paper, we unleash the boundaries of the ID-based\nparadigm and propose a Pure Multi-Modality based Recommender system (PMMRec),\nwhich relies solely on the multi-modal contents of the items (e.g., texts and\nimages) and learns transition patterns general enough to transfer across\ndomains and platforms. Specifically, we design a plug-and-play framework\narchitecture consisting of multi-modal item encoders, a fusion module, and a\nuser encoder. To align the cross-modal item representations, we propose a novel\nnext-item enhanced cross-modal contrastive learning objective, which is\nequipped with both inter- and intra-modality negative samples and explicitly\nincorporates the transition patterns of user behaviors into the item encoders.\nTo ensure the robustness of user representations, we propose a novel noised\nitem detection objective and a robustness-aware contrastive learning objective,\nwhich work together to denoise user sequences in a self-supervised manner.\nPMMRec is designed to be loosely coupled, so after being pre-trained on the\nsource data, each component can be transferred alone, or in conjunction with\nother components, allowing PMMRec to achieve versatility under both\nmulti-modality and single-modality transfer learning settings. Extensive\nexperiments on 4 sources and 10 target datasets demonstrate that PMMRec\nsurpasses the state-of-the-art recommenders in both recommendation performance\nand transferability. Our code and dataset is available at:\nhttps://github.com/ICDE24/PMMRec.\n","authors":["Youhua Li","Hanwen Du","Yongxin Ni","Pengpeng Zhao","Qi Guo","Fajie Yuan","Xiaofang Zhou"],"pdf_url":"https://arxiv.org/pdf/2312.09602v1.pdf","comment":"ICDE'24 Accepted"},{"id":"http://arxiv.org/abs/2312.09591v1","updated":"2023-12-15T08:01:55Z","published":"2023-12-15T08:01:55Z","title":"Incorporating Judgment Prediction into Legal Case Retrieval via\n  Law-aware Generative Retrieval","summary":"  Legal case retrieval and judgment prediction are crucial components in\nintelligent legal systems. In practice, determining whether two cases share the\nsame charges through legal judgment prediction is essential for establishing\ntheir relevance in case retrieval. However, current studies on legal case\nretrieval merely focus on the semantic similarity between paired cases,\nignoring their charge-level consistency. This separation leads to a lack of\ncontext and potential inaccuracies in the case retrieval that can undermine\ntrust in the system's decision-making process. Given the guidance role of laws\nto both tasks and inspired by the success of generative retrieval, in this\nwork, we propose to incorporate judgment prediction into legal case retrieval,\nachieving a novel law-aware Generative legal case retrieval method called Gear.\nSpecifically, Gear first extracts rationales (key circumstances and key\nelements) for legal cases according to the definition of charges in laws,\nensuring a shared and informative representation for both tasks. Then in\naccordance with the inherent hierarchy of laws, we construct a law structure\nconstraint tree and assign law-aware semantic identifier(s) to each case based\non this tree. These designs enable a unified traversal from the root, through\nintermediate charge nodes, to case-specific leaf nodes, which respectively\ncorrespond to two tasks. Additionally, in the training, we also introduce a\nrevision loss that jointly minimizes the discrepancy between the identifiers of\npredicted and labeled charges as well as retrieved cases, improving the\naccuracy and consistency for both tasks. Extensive experiments on two datasets\ndemonstrate that Gear consistently outperforms state-of-the-art methods in\nlegal case retrieval while maintaining competitive judgment prediction\nperformance.\n","authors":["Weicong Qin","Zelin Cao","Weijie Yu","Zihua Si","Sirui Chen","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2312.09591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07597v2","updated":"2023-12-15T05:18:11Z","published":"2023-09-14T10:57:50Z","title":"C-Pack: Packaged Resources To Advance General Chinese Embedding","summary":"  We introduce C-Pack, a package of resources that significantly advance the\nfield of general Chinese embeddings. C-Pack includes three critical resources.\n1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6\ntasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated\nfrom labeled and unlabeled Chinese corpora for training embedding models. 3)\nC-TEM is a family of embedding models covering multiple sizes. Our models\noutperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the\ntime of the release. We also integrate and optimize the entire suite of\ntraining methods for C-TEM. Along with our resources on general Chinese\nembedding, we release our data and models for English text embeddings. The\nEnglish models achieve state-of-the-art performance on MTEB benchmark;\nmeanwhile, our released English data is 2 times larger than the Chinese data.\nAll these resources are made publicly available at\nhttps://github.com/FlagOpen/FlagEmbedding.\n","authors":["Shitao Xiao","Zheng Liu","Peitian Zhang","Niklas Muennighoff"],"pdf_url":"https://arxiv.org/pdf/2309.07597v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09511v1","updated":"2023-12-15T03:28:19Z","published":"2023-12-15T03:28:19Z","title":"MONET: Modality-Embracing Graph Convolutional Network and Target-Aware\n  Attention for Multimedia Recommendation","summary":"  In this paper, we focus on multimedia recommender systems using graph\nconvolutional networks (GCNs) where the multimodal features as well as\nuser-item interactions are employed together. Our study aims to exploit\nmultimodal features more effectively in order to accurately capture users'\npreferences for items. To this end, we point out following two limitations of\nexisting GCN-based multimedia recommender systems: (L1) although multimodal\nfeatures of interacted items by a user can reveal her preferences on items,\nexisting methods utilize GCN designed to focus only on capturing collaborative\nsignals, resulting in insufficient reflection of the multimodal features in the\nfinal user/item embeddings; (L2) although a user decides whether to prefer the\ntarget item by considering its multimodal features, existing methods represent\nher as only a single embedding regardless of the target item's multimodal\nfeatures and then utilize her embedding to predict her preference for the\ntarget item. To address the above issues, we propose a novel multimedia\nrecommender system, named MONET, composed of following two core ideas:\nmodality-embracing GCN (MeGCN) and target-aware attention. Through extensive\nexperiments using four real-world datasets, we demonstrate i) the significant\nsuperiority of MONET over seven state-of-the-art competitors (up to 30.32%\nhigher accuracy in terms of recall@20, compared to the best competitor) and ii)\nthe effectiveness of the two core ideas in MONET. All MONET codes are available\nat https://github.com/Kimyungi/MONET.\n","authors":["Yungi Kim","Taeri Kim","Won-Yong Shin","Sang-Wook Kim"],"pdf_url":"https://arxiv.org/pdf/2312.09511v1.pdf","comment":"Accepted by WSDM 2024"},{"id":"http://arxiv.org/abs/2312.09508v1","updated":"2023-12-15T03:19:53Z","published":"2023-12-15T03:19:53Z","title":"IndicIRSuite: Multilingual Dataset and Neural Information Models for\n  Indian Languages","summary":"  In this paper, we introduce Neural Information Retrieval resources for 11\nwidely spoken Indian Languages (Assamese, Bengali, Gujarati, Hindi, Kannada,\nMalayalam, Marathi, Oriya, Punjabi, Tamil, and Telugu) from two major Indian\nlanguage families (Indo-Aryan and Dravidian). These resources include (a)\nINDIC-MARCO, a multilingual version of the MSMARCO dataset in 11 Indian\nLanguages created using Machine Translation, and (b) Indic-ColBERT, a\ncollection of 11 distinct Monolingual Neural Information Retrieval models, each\ntrained on one of the 11 languages in the INDIC-MARCO dataset. To the best of\nour knowledge, IndicIRSuite is the first attempt at building large-scale Neural\nInformation Retrieval resources for a large number of Indian languages, and we\nhope that it will help accelerate research in Neural IR for Indian Languages.\nExperiments demonstrate that Indic-ColBERT achieves 47.47% improvement in the\nMRR@10 score averaged over the INDIC-MARCO baselines for all 11 Indian\nlanguages except Oriya, 12.26% improvement in the NDCG@10 score averaged over\nthe MIRACL Bengali and Hindi Language baselines, and 20% improvement in the\nMRR@100 Score over the Mr.Tydi Bengali Language baseline. IndicIRSuite is\navailable at https://github.com/saifulhaq95/IndicIRSuite\n","authors":["Saiful Haq","Ashutosh Sharma","Pushpak Bhattacharyya"],"pdf_url":"https://arxiv.org/pdf/2312.09508v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2312.10029v1","updated":"2023-12-15T18:49:43Z","published":"2023-12-15T18:49:43Z","title":"Challenges with unsupervised LLM knowledge discovery","summary":"  We show that existing unsupervised methods on large language model (LLM)\nactivations do not discover knowledge -- instead they seem to discover whatever\nfeature of the activations is most prominent. The idea behind unsupervised\nknowledge elicitation is that knowledge satisfies a consistency structure,\nwhich can be used to discover knowledge. We first prove theoretically that\narbitrary features (not just knowledge) satisfy the consistency structure of a\nparticular leading unsupervised knowledge-elicitation method,\ncontrast-consistent search (Burns et al. - arXiv:2212.03827). We then present a\nseries of experiments showing settings in which unsupervised methods result in\nclassifiers that do not predict knowledge, but instead predict a different\nprominent feature. We conclude that existing unsupervised methods for\ndiscovering latent knowledge are insufficient, and we contribute sanity checks\nto apply to evaluating future knowledge elicitation methods. Conceptually, we\nhypothesise that the identification issues explored here, e.g. distinguishing a\nmodel's knowledge from that of a simulated character's, will persist for future\nunsupervised methods.\n","authors":["Sebastian Farquhar","Vikrant Varma","Zachary Kenton","Johannes Gasteiger","Vladimir Mikulik","Rohin Shah"],"pdf_url":"https://arxiv.org/pdf/2312.10029v1.pdf","comment":"12 pages (38 including references and appendices). First three\n  authors equal contribution, randomised order"},{"id":"http://arxiv.org/abs/2305.12066v2","updated":"2023-12-15T18:49:38Z","published":"2023-05-20T03:07:43Z","title":"Dynamic Gradient Balancing for Enhanced Adversarial Attacks on\n  Multi-Task Models","summary":"  Multi-task learning (MTL) creates a single machine learning model called\nmulti-task model to simultaneously perform multiple tasks. Although the\nsecurity of single task classifiers has been extensively studied, there are\nseveral critical security research questions for multi-task models including 1)\nHow secure are multi-task models to single task adversarial machine learning\nattacks, 2) Can adversarial attacks be designed to attack multiple tasks\nsimultaneously, and 3) Does task sharing and adversarial training increase\nmulti-task model robustness to adversarial attacks? In this paper, we answer\nthese questions through careful analysis and rigorous experimentation. First,\nwe develop na\\\"ive adaptation of single-task white-box attacks and analyze\ntheir inherent drawbacks. We then propose a novel attack framework, Dynamic\nGradient Balancing Attack (DGBA). Our framework poses the problem of attacking\na multi-task model as an optimization problem based on averaged relative loss\nchange, which can be solved by approximating the problem as an integer linear\nprogramming problem. Extensive evaluation on two popular MTL benchmarks, NYUv2\nand Tiny-Taxonomy, demonstrates the effectiveness of DGBA compared to na\\\"ive\nmulti-task attack baselines on both clean and adversarially trained multi-task\nmodels. The results also reveal a fundamental trade-off between improving task\naccuracy by sharing parameters across tasks and undermining model robustness\ndue to increased attack transferability from parameter sharing. DGBA is\nopen-sourced and available at https://github.com/zhanglijun95/MTLAttack-DGBA.\n","authors":["Lijun Zhang","Xiao Liu","Kaleel Mahmood","Caiwen Ding","Hui Guan"],"pdf_url":"https://arxiv.org/pdf/2305.12066v2.pdf","comment":"19 pages, 6 figures; AAAI24"},{"id":"http://arxiv.org/abs/2310.03725v2","updated":"2023-12-15T18:44:46Z","published":"2023-10-05T17:46:31Z","title":"Stochastic interpolants with data-dependent couplings","summary":"  Generative models inspired by dynamical transport of measure -- such as flows\nand diffusions -- construct a continuous-time map between two probability\ndensities. Conventionally, one of these is the target density, only accessible\nthrough samples, while the other is taken as a simple base density that is\ndata-agnostic. In this work, using the framework of stochastic interpolants, we\nformalize how to \\textit{couple} the base and the target densities, whereby\nsamples from the base are computed conditionally given samples from the target\nin a way that is different from (but does preclude) incorporating information\nabout class labels or continuous embeddings. This enables us to construct\ndynamical transport maps that serve as conditional generative models. We show\nthat these transport maps can be learned by solving a simple square loss\nregression problem analogous to the standard independent setting. We\ndemonstrate the usefulness of constructing dependent couplings in practice\nthrough experiments in super-resolution and in-painting.\n","authors":["Michael S. Albergo","Mark Goldstein","Nicholas M. Boffi","Rajesh Ranganath","Eric Vanden-Eijnden"],"pdf_url":"https://arxiv.org/pdf/2310.03725v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10024v1","updated":"2023-12-15T18:43:45Z","published":"2023-12-15T18:43:45Z","title":"Accelerating Neural Network Training: A Brief Review","summary":"  The process of training a deep neural network is characterized by significant\ntime requirements and associated costs. Although researchers have made\nconsiderable progress in this area, further work is still required due to\nresource constraints. This study examines innovative approaches to expedite the\ntraining process of deep neural networks (DNN), with specific emphasis on three\nstate-of-the-art models such as ResNet50, Vision Transformer (ViT), and\nEfficientNet. The research utilizes sophisticated methodologies, including\nGradient Accumulation (GA), Automatic Mixed Precision (AMP), and Pin Memory\n(PM), in order to optimize performance and accelerate the training procedure.\n  The study examines the effects of these methodologies on the DNN models\ndiscussed earlier, assessing their efficacy with regard to training rate and\ncomputational efficacy. The study showcases the efficacy of including GA as a\nstrategic approach, resulting in a noteworthy decrease in the duration required\nfor training. This enables the models to converge at a faster pace. The\nutilization of AMP enhances the speed of computations by taking advantage of\nthe advantages offered by lower precision arithmetic while maintaining the\ncorrectness of the model.\n  Furthermore, this study investigates the application of Pin Memory as a\nstrategy to enhance the efficiency of data transmission between the central\nprocessing unit and the graphics processing unit, thereby offering a promising\nopportunity for enhancing overall performance. The experimental findings\ndemonstrate that the combination of these sophisticated methodologies\nsignificantly accelerates the training of DNNs, offering vital insights for\nexperts seeking to improve the effectiveness of deep learning processes.\n","authors":["Sahil Nokhwal","Priyanka Chilakalapudi","Preeti Donekal","Manoj Chandrasekharan","Suman Nokhwal","Ram Swaroop","Raj Bala","Saurabh Pahune","Ankit Chaudhary"],"pdf_url":"https://arxiv.org/pdf/2312.10024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10019v1","updated":"2023-12-15T18:38:18Z","published":"2023-12-15T18:38:18Z","title":"Understanding Probe Behaviors through Variational Bounds of Mutual\n  Information","summary":"  With the success of self-supervised representations, researchers seek a\nbetter understanding of the information encapsulated within a representation.\nAmong various interpretability methods, we focus on classification-based linear\nprobing. We aim to foster a solid understanding and provide guidelines for\nlinear probing by constructing a novel mathematical framework leveraging\ninformation theory. First, we connect probing with the variational bounds of\nmutual information (MI) to relax the probe design, equating linear probing with\nfine-tuning. Then, we investigate empirical behaviors and practices of probing\nthrough our mathematical framework. We analyze the layer-wise performance curve\nbeing convex, which seemingly violates the data processing inequality. However,\nwe show that the intermediate representations can have the biggest MI estimate\nbecause of the tradeoff between better separability and decreasing MI. We\nfurther suggest that the margin of linearly separable representations can be a\ncriterion for measuring the \"goodness of representation.\" We also compare\naccuracy with MI as the measuring criteria. Finally, we empirically validate\nour claims by observing the self-supervised speech models on retaining word and\nphoneme information.\n","authors":["Kwanghee Choi","Jee-weon Jung","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2312.10019v1.pdf","comment":"Accepted to ICASSP 2024, implementation available at\n  https://github.com/juice500ml/information_probing"},{"id":"http://arxiv.org/abs/2312.08782v2","updated":"2023-12-15T18:25:48Z","published":"2023-12-14T10:02:55Z","title":"Toward General-Purpose Robots via Foundation Models: A Survey and\n  Meta-Analysis","summary":"  Building general-purpose robots that can operate seamlessly, in any\nenvironment, with any object, and utilizing various skills to complete diverse\ntasks has been a long-standing goal in Artificial Intelligence. Unfortunately,\nhowever, most existing robotic systems have been constrained - having been\ndesigned for specific tasks, trained on specific datasets, and deployed within\nspecific environments. These systems usually require extensively-labeled data,\nrely on task-specific models, have numerous generalization issues when deployed\nin real-world scenarios, and struggle to remain robust to distribution shifts.\nMotivated by the impressive open-set performance and content generation\ncapabilities of web-scale, large-capacity pre-trained models (i.e., foundation\nmodels) in research fields such as Natural Language Processing (NLP) and\nComputer Vision (CV), we devote this survey to exploring (i) how these existing\nfoundation models from NLP and CV can be applied to the field of robotics, and\nalso exploring (ii) what a robotics-specific foundation model would look like.\nWe begin by providing an overview of what constitutes a conventional robotic\nsystem and the fundamental barriers to making it universally applicable. Next,\nwe establish a taxonomy to discuss current work exploring ways to leverage\nexisting foundation models for robotics and develop ones catered to robotics.\nFinally, we discuss key challenges and promising future directions in using\nfoundation models for enabling general-purpose robotic systems. We encourage\nreaders to view our living GitHub repository of resources, including papers\nreviewed in this survey as well as related projects and repositories for\ndeveloping foundation models for robotics.\n","authors":["Yafei Hu","Quanting Xie","Vidhi Jain","Jonathan Francis","Jay Patrikar","Nikhil Keetha","Seungchan Kim","Yaqi Xie","Tianyi Zhang","Shibo Zhao","Yu Quan Chong","Chen Wang","Katia Sycara","Matthew Johnson-Roberson","Dhruv Batra","Xiaolong Wang","Sebastian Scherer","Zsolt Kira","Fei Xia","Yonatan Bisk"],"pdf_url":"https://arxiv.org/pdf/2312.08782v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10008v1","updated":"2023-12-15T18:24:28Z","published":"2023-12-15T18:24:28Z","title":"Movement Primitive Diffusion: Learning Gentle Robotic Manipulation of\n  Deformable Objects","summary":"  Policy learning in robot-assisted surgery (RAS) lacks data efficient and\nversatile methods that exhibit the desired motion quality for delicate surgical\ninterventions. To this end, we introduce Movement Primitive Diffusion (MPD), a\nnovel method for imitation learning (IL) in RAS that focuses on gentle\nmanipulation of deformable objects. The approach combines the versatility of\ndiffusion-based imitation learning (DIL) with the high-quality motion\ngeneration capabilities of Probabilistic Dynamic Movement Primitives (ProDMPs).\nThis combination enables MPD to achieve gentle manipulation of deformable\nobjects, while maintaining data efficiency critical for RAS applications where\ndemonstration data is scarce. We evaluate MPD across various simulated tasks\nand a real world robotic setup on both state and image observations. MPD\noutperforms state-of-the-art DIL methods in success rate, motion quality, and\ndata efficiency.\n","authors":["Paul Maria Scheikl","Nicolas Schreiber","Christoph Haas","Niklas Freymuth","Gerhard Neumann","Rudolf Lioutikov","Franziska Mathis-Ullrich"],"pdf_url":"https://arxiv.org/pdf/2312.10008v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10007v1","updated":"2023-12-15T18:23:50Z","published":"2023-12-15T18:23:50Z","title":"Faithful Persona-based Conversational Dataset Generation with Large\n  Language Models","summary":"  High-quality conversational datasets are essential for developing AI models\nthat can communicate with users. One way to foster deeper interactions between\na chatbot and its user is through personas, aspects of the user's character\nthat provide insights into their personality, motivations, and behaviors.\nTraining Natural Language Processing (NLP) models on a diverse and\ncomprehensive persona-based dataset can lead to conversational models that\ncreate a deeper connection with the user, and maintain their engagement. In\nthis paper, we leverage the power of Large Language Models (LLMs) to create a\nlarge, high-quality conversational dataset from a seed dataset. We propose a\nGenerator-Critic architecture framework to expand the initial dataset, while\nimproving the quality of its conversations. The Generator is an LLM prompted to\noutput conversations. The Critic consists of a mixture of expert LLMs that\ncontrol the quality of the generated conversations. These experts select the\nbest generated conversations, which we then use to improve the Generator. We\nrelease Synthetic-Persona-Chat, consisting of 20k conversations seeded from\nPersona-Chat. We evaluate the quality of Synthetic-Persona-Chat and our\ngeneration framework on different dimensions through extensive experiments, and\nobserve that the losing rate of Synthetic-Persona-Chat against Persona-Chat\nduring Turing test decreases from 17.2% to 8.8% over three iterations.\n","authors":["Pegah Jandaghi","XiangHai Sheng","Xinyi Bai","Jay Pujara","Hakim Sidahmed"],"pdf_url":"https://arxiv.org/pdf/2312.10007v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10004v1","updated":"2023-12-15T18:20:25Z","published":"2023-12-15T18:20:25Z","title":"Symplectic Autoencoders for Model Reduction of Hamiltonian Systems","summary":"  Many applications, such as optimization, uncertainty quantification and\ninverse problems, require repeatedly performing simulations of\nlarge-dimensional physical systems for different choices of parameters. This\ncan be prohibitively expensive.\n  In order to save computational cost, one can construct surrogate models by\nexpressing the system in a low-dimensional basis, obtained from training data.\nThis is referred to as model reduction.\n  Past investigations have shown that, when performing model reduction of\nHamiltonian systems, it is crucial to preserve the symplectic structure\nassociated with the system in order to ensure long-term numerical stability.\n  Up to this point structure-preserving reductions have largely been limited to\nlinear transformations. We propose a new neural network architecture in the\nspirit of autoencoders, which are established tools for dimension reduction and\nfeature extraction in data science, to obtain more general mappings.\n  In order to train the network, a non-standard gradient descent approach is\napplied that leverages the differential-geometric structure emerging from the\nnetwork design.\n  The new architecture is shown to significantly outperform existing designs in\naccuracy.\n","authors":["Benedikt Brantner","Michael Kraus"],"pdf_url":"https://arxiv.org/pdf/2312.10004v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10001v1","updated":"2023-12-15T18:19:22Z","published":"2023-12-15T18:19:22Z","title":"Modeling Unknown Stochastic Dynamical System via Autoencoder","summary":"  We present a numerical method to learn an accurate predictive model for an\nunknown stochastic dynamical system from its trajectory data. The method seeks\nto approximate the unknown flow map of the underlying system. It employs the\nidea of autoencoder to identify the unobserved latent random variables. In our\napproach, we design an encoding function to discover the latent variables,\nwhich are modeled as unit Gaussian, and a decoding function to reconstruct the\nfuture states of the system. Both the encoder and decoder are expressed as deep\nneural networks (DNNs). Once the DNNs are trained by the trajectory data, the\ndecoder serves as a predictive model for the unknown stochastic system. Through\nan extensive set of numerical examples, we demonstrate that the method is able\nto produce long-term system predictions by using short bursts of trajectory\ndata. It is also applicable to systems driven by non-Gaussian noises.\n","authors":["Zhongshu Xu","Yuan Chen","Qifan Chen","Dongbin Xiu"],"pdf_url":"https://arxiv.org/pdf/2312.10001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09997v1","updated":"2023-12-15T18:15:20Z","published":"2023-12-15T18:15:20Z","title":"One Self-Configurable Model to Solve Many Abstract Visual Reasoning\n  Problems","summary":"  Abstract Visual Reasoning (AVR) comprises a wide selection of various\nproblems similar to those used in human IQ tests. Recent years have brought\ndynamic progress in solving particular AVR tasks, however, in the contemporary\nliterature AVR problems are largely dealt with in isolation, leading to highly\nspecialized task-specific methods. With the aim of developing universal\nlearning systems in the AVR domain, we propose the unified model for solving\nSingle-Choice Abstract visual Reasoning tasks (SCAR), capable of solving\nvarious single-choice AVR tasks, without making any a priori assumptions about\nthe task structure, in particular the number and location of panels. The\nproposed model relies on a novel Structure-Aware dynamic Layer (SAL), which\nadapts its weights to the structure of the considered AVR problem. Experiments\nconducted on Raven's Progressive Matrices, Visual Analogy Problems, and Odd One\nOut problems show that SCAR (SAL-based models, in general) effectively solves\ndiverse AVR tasks, and its performance is on par with the state-of-the-art\ntask-specific baselines. What is more, SCAR demonstrates effective knowledge\nreuse in multi-task and transfer learning settings. To our knowledge, this work\nis the first successful attempt to construct a general single-choice AVR solver\nrelying on self-configurable architecture and unified solving method. With this\nwork we aim to stimulate and foster progress on task-independent research paths\nin the AVR domain, with the long-term goal of development of a general AVR\nsolver.\n","authors":["Mikołaj Małkiński","Jacek Mańdziuk"],"pdf_url":"https://arxiv.org/pdf/2312.09997v1.pdf","comment":"Accepted to The 38th Annual AAAI Conference on Artificial\n  Intelligence (AAAI 2024)"},{"id":"http://arxiv.org/abs/2312.03179v2","updated":"2023-12-15T18:07:38Z","published":"2023-12-05T23:05:36Z","title":"CaloQVAE : Simulating high-energy particle-calorimeter interactions\n  using hybrid quantum-classical generative models","summary":"  The Large Hadron Collider's high luminosity era presents major computational\nchallenges in the analysis of collision events. Large amounts of Monte Carlo\n(MC) simulation will be required to constrain the statistical uncertainties of\nthe simulated datasets below these of the experimental data. Modelling of\nhigh-energy particles propagating through the calorimeter section of the\ndetector is the most computationally intensive MC simulation task. We introduce\na technique combining recent advancements in generative models and quantum\nannealing for fast and efficient simulation of high-energy particle-calorimeter\ninteractions.\n","authors":["Sehmimul Hoque","Hao Jia","Abhishek Abhishek","Mojde Fadaie","J. Quetzalcoatl Toledo-Marín","Tiago Vale","Roger G. Melko","Maximilian Swiatlowski","Wojciech T. Fedorko"],"pdf_url":"https://arxiv.org/pdf/2312.03179v2.pdf","comment":"6 pages, 3 figures"},{"id":"http://arxiv.org/abs/2312.09983v1","updated":"2023-12-15T17:50:18Z","published":"2023-12-15T17:50:18Z","title":"Toward Computationally Efficient Inverse Reinforcement Learning via\n  Reward Shaping","summary":"  Inverse reinforcement learning (IRL) is computationally challenging, with\ncommon approaches requiring the solution of multiple reinforcement learning\n(RL) sub-problems. This work motivates the use of potential-based reward\nshaping to reduce the computational burden of each RL sub-problem. This work\nserves as a proof-of-concept and we hope will inspire future developments\ntowards computationally efficient IRL.\n","authors":["Lauren H. Cooke","Harvey Klyne","Edwin Zhang","Cassidy Laidlaw","Milind Tambe","Finale Doshi-Velez"],"pdf_url":"https://arxiv.org/pdf/2312.09983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09982v1","updated":"2023-12-15T17:49:24Z","published":"2023-12-15T17:49:24Z","title":"ACPO: AI-Enabled Compiler-Driven Program Optimization","summary":"  The key to performance optimization of a program is to decide correctly when\na certain transformation should be applied by a compiler. Traditionally, such\nprofitability decisions are made by hand-coded algorithms tuned for a very\nsmall number of benchmarks, usually requiring a great deal of effort to be\nretuned when the benchmark suite changes. This is an ideal opportunity to apply\nmachine-learning models to speed up the tuning process; while this realization\nhas been around since the late 90s, only recent advancements in ML enabled a\npractical application of ML to compilers as an end-to-end framework. Even so,\nseamless integration of ML into the compiler would require constant rebuilding\nof the compiler when models are updated.\n  This paper presents ACPO: \\textbf{\\underline{A}}I-Enabled\n\\textbf{\\underline{C}}ompiler-driven \\textbf{\\underline{P}}rogram\n\\textbf{\\underline{O}}ptimization; a novel framework to provide LLVM with\nsimple and comprehensive tools to benefit from employing ML models for\ndifferent optimization passes. We first showcase the high-level view, class\nhierarchy, and functionalities of ACPO and subsequently, demonstrate \\taco{a\ncouple of use cases of ACPO by ML-enabling the Loop Unroll and Function\nInlining passes and describe how ACPO can be leveraged to optimize other\npasses. Experimental results reveal that ACPO model for Loop Unroll is able to\ngain on average 4\\% and 3\\%, 5.4\\%, 0.2\\% compared to LLVM's O3 optimization\nwhen deployed on Polybench, Coral-2, CoreMark, and Graph-500, respectively.\nFurthermore, by adding the Inliner model as well, ACPO is able to provide up to\n4.5\\% and 2.4\\% on Polybench and Cbench compared with LLVM's O3 optimization,\nrespectively.\n","authors":["Amir H. Ashouri","Muhammad Asif Manzoor","Duc Minh Vu","Raymond Zhang","Ziwen Wang","Angel Zhang","Bryan Chan","Tomasz S. Czajkowski","Yaoqing Gao"],"pdf_url":"https://arxiv.org/pdf/2312.09982v1.pdf","comment":"Preprint version of ACPO work under revision at ACM TACO 2023"},{"id":"http://arxiv.org/abs/2212.07514v2","updated":"2023-12-15T17:45:51Z","published":"2022-12-14T21:39:15Z","title":"PulseImpute: A Novel Benchmark Task for Pulsative Physiological Signal\n  Imputation","summary":"  The promise of Mobile Health (mHealth) is the ability to use wearable sensors\nto monitor participant physiology at high frequencies during daily life to\nenable temporally-precise health interventions. However, a major challenge is\nfrequent missing data. Despite a rich imputation literature, existing\ntechniques are ineffective for the pulsative signals which comprise many\nmHealth applications, and a lack of available datasets has stymied progress. We\naddress this gap with PulseImpute, the first large-scale pulsative signal\nimputation challenge which includes realistic mHealth missingness models, an\nextensive set of baselines, and clinically-relevant downstream tasks. Our\nbaseline models include a novel transformer-based architecture designed to\nexploit the structure of pulsative signals. We hope that PulseImpute will\nenable the ML community to tackle this significant and challenging task.\n","authors":["Maxwell A. Xu","Alexander Moreno","Supriya Nagesh","V. Burak Aydemir","David W. Wetter","Santosh Kumar","James M. Rehg"],"pdf_url":"https://arxiv.org/pdf/2212.07514v2.pdf","comment":"NeurIPS 2022 | Code available at:\n  https://github.com/rehg-lab/pulseimpute | Data available at:\n  https://doi.org/10.5281/zenodo.7129964"},{"id":"http://arxiv.org/abs/2312.09978v1","updated":"2023-12-15T17:41:51Z","published":"2023-12-15T17:41:51Z","title":"Small jet engine reservoir computing digital twin","summary":"  Machine learning was applied to create a digital twin of a numerical\nsimulation of a single-scroll jet engine. A similar model based on the insights\ngained from this numerical study was used to create a digital twin of a JetCat\nP100-RX jet engine using only experimental data. Engine data was collected from\na custom sensor system measuring parameters such as thrust, exhaust gas\ntemperature, shaft speed, weather conditions, etc. Data was gathered while the\nengine was placed under different test conditions by controlling shaft speed.\nThe machine learning model was generated (trained) using a next-generation\nreservoir computer, a best-in-class machine learning algorithm for dynamical\nsystems. Once the model was trained, it was used to predict behavior it had\nnever seen with an accuracy of better than 1.8% when compared to the testing\ndata.\n","authors":["C. J. Wright","N. Biederman","B. Gyovai","D. J. Gauthier","J. P. Wilhelm"],"pdf_url":"https://arxiv.org/pdf/2312.09978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.08754v2","updated":"2023-12-15T17:34:36Z","published":"2023-04-18T06:25:11Z","title":"W-MAE: Pre-trained weather model with masked autoencoder for\n  multi-variable weather forecasting","summary":"  Weather forecasting is a long-standing computational challenge with direct\nsocietal and economic impacts. This task involves a large amount of continuous\ndata collection and exhibits rich spatiotemporal dependencies over long\nperiods, making it highly suitable for deep learning models. In this paper, we\napply pre-training techniques to weather forecasting and propose W-MAE, a\nWeather model with Masked AutoEncoder pre-training for weather forecasting.\nW-MAE is pre-trained in a self-supervised manner to reconstruct spatial\ncorrelations within meteorological variables. On the temporal scale, we\nfine-tune the pre-trained W-MAE to predict the future states of meteorological\nvariables, thereby modeling the temporal dependencies present in weather data.\nWe conduct our experiments using the fifth-generation ECMWF Reanalysis (ERA5)\ndata, with samples selected every six hours. Experimental results show that our\nW-MAE framework offers three key benefits: 1) when predicting the future state\nof meteorological variables, the utilization of our pre-trained W-MAE can\neffectively alleviate the problem of cumulative errors in prediction,\nmaintaining stable performance in the short-to-medium term; 2) when predicting\ndiagnostic variables (e.g., total precipitation), our model exhibits\nsignificant performance advantages over FourCastNet; 3) Our task-agnostic\npre-training schema can be easily integrated with various task-specific models.\nWhen our pre-training framework is applied to FourCastNet, it yields an average\n20% performance improvement in Anomaly Correlation Coefficient (ACC).\n","authors":["Xin Man","Chenghong Zhang","Jin Feng","Changyu Li","Jie Shao"],"pdf_url":"https://arxiv.org/pdf/2304.08754v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09971v1","updated":"2023-12-15T17:34:11Z","published":"2023-12-15T17:34:11Z","title":"GreenLightningAI: An Efficient AI System with Decoupled Structural and\n  Quantitative Knowledge","summary":"  The number and complexity of artificial intelligence (AI) applications is\ngrowing relentlessly. As a result, even with the many algorithmic and\nmathematical advances experienced over past decades as well as the impressive\nenergy efficiency and computational capacity of current hardware accelerators,\ntraining the most powerful and popular deep neural networks comes at very high\neconomic and environmental costs. Recognising that additional optimisations of\nconventional neural network training is very difficult, this work takes a\nradically different approach by proposing GreenLightningAI, a new AI system\ndesign consisting of a linear model that is capable of emulating the behaviour\nof deep neural networks by subsetting the model for each particular sample. The\nnew AI system stores the information required to select the system subset for a\ngiven sample (referred to as structural information) separately from the linear\nmodel parameters (referred to as quantitative knowledge). In this paper we\npresent a proof of concept, showing that the structural information stabilises\nfar earlier than the quantitative knowledge. Additionally, we show\nexperimentally that the structural information can be kept unmodified when\nre-training the AI system with new samples while still achieving a validation\naccuracy similar to that obtained when re-training a neural network with\nsimilar size. Since the proposed AI system is based on a linear model, multiple\ncopies of the model, trained with different datasets, can be easily combined.\nThis enables faster and greener (re)-training algorithms, including incremental\nre-training and federated incremental re-training.\n","authors":["Jose Duato","Jose I. Mestre","Manuel F. Dolz","Enrique S. Quintana-Ortí"],"pdf_url":"https://arxiv.org/pdf/2312.09971v1.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2308.04237v2","updated":"2023-12-15T17:30:52Z","published":"2023-08-08T13:03:36Z","title":"Federated Inference with Reliable Uncertainty Quantification over\n  Wireless Channels via Conformal Prediction","summary":"  In this paper, we consider a wireless federated inference scenario in which\ndevices and a server share a pre-trained machine learning model. The devices\ncommunicate statistical information about their local data to the server over a\ncommon wireless channel, aiming to enhance the quality of the inference\ndecision at the server. Recent work has introduced federated conformal\nprediction (CP), which leverages devices-to-server communication to improve the\nreliability of the server's decision. With federated CP, devices communicate to\nthe server information about the loss accrued by the shared pre-trained model\non the local data, and the server leverages this information to calibrate a\ndecision interval, or set, so that it is guaranteed to contain the correct\nanswer with a pre-defined target reliability level. Previous work assumed\nnoise-free communication, whereby devices can communicate a single real number\nto the server. In this paper, we study for the first time federated CP in a\nwireless setting. We introduce a novel protocol, termed wireless federated\nconformal prediction (WFCP), which builds on type-based multiple access (TBMA)\nand on a novel quantile correction strategy. WFCP is proved to provide formal\nreliability guarantees in terms of coverage of the predicted set produced by\nthe server. Using numerical results, we demonstrate the significant advantages\nof WFCP against digital implementations of existing federated CP schemes,\nespecially in regimes with limited communication resources and/or large number\nof devices.\n","authors":["Meiyi Zhu","Matteo Zecchin","Sangwoo Park","Caili Guo","Chunyan Feng","Osvaldo Simeone"],"pdf_url":"https://arxiv.org/pdf/2308.04237v2.pdf","comment":"16 pages, 7 figures"},{"id":"http://arxiv.org/abs/2312.09969v1","updated":"2023-12-15T17:28:09Z","published":"2023-12-15T17:28:09Z","title":"Scalable and hyper-parameter-free non-parametric covariate shift\n  adaptation with conditional sampling","summary":"  Many existing covariate shift adaptation methods estimate sample weights to\nbe used in the risk estimation in order to mitigate the gap between the source\nand the target distribution. However, non-parametrically estimating the optimal\nweights typically involves computationally expensive hyper-parameter tuning\nthat is crucial to the final performance. In this paper, we propose a new\nnon-parametric approach to covariate shift adaptation which avoids estimating\nweights and has no hyper-parameter to be tuned. Our basic idea is to label\nunlabeled target data according to the $k$-nearest neighbors in the source\ndataset. Our analysis indicates that setting $k = 1$ is an optimal choice.\nThanks to this property, there is no need to tune any hyper-parameters, unlike\nother non-parametric methods. Moreover, our method achieves a running time\nquasi-linear in the sample size with a theoretical guarantee, for the first\ntime in the literature to the best of our knowledge. Our results include sharp\nrates of convergence for estimating the joint probability distribution of the\ntarget data. In particular, the variance of our estimators has the same rate of\nconvergence as for standard parametric estimation despite their non-parametric\nnature. Our numerical experiments show that proposed method brings drastic\nreduction in the running time with accuracy comparable to that of the\nstate-of-the-art methods.\n","authors":["François Portier","Lionel Truquet","Ikko Yamane"],"pdf_url":"https://arxiv.org/pdf/2312.09969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09961v1","updated":"2023-12-15T17:16:04Z","published":"2023-12-15T17:16:04Z","title":"Risk-Aware Continuous Control with Neural Contextual Bandits","summary":"  Recent advances in learning techniques have garnered attention for their\napplicability to a diverse range of real-world sequential decision-making\nproblems. Yet, many practical applications have critical constraints for\noperation in real environments. Most learning solutions often neglect the risk\nof failing to meet these constraints, hindering their implementation in\nreal-world contexts. In this paper, we propose a risk-aware decision-making\nframework for contextual bandit problems, accommodating constraints and\ncontinuous action spaces. Our approach employs an actor multi-critic\narchitecture, with each critic characterizing the distribution of performance\nand constraint metrics. Our framework is designed to cater to various risk\nlevels, effectively balancing constraint satisfaction against performance. To\ndemonstrate the effectiveness of our approach, we first compare it against\nstate-of-the-art baseline methods in a synthetic environment, highlighting the\nimpact of intrinsic environmental noise across different risk configurations.\nFinally, we evaluate our framework in a real-world use case involving a 5G\nmobile network where only our approach consistently satisfies the system\nconstraint (a signal processing reliability target) with a small performance\ntoll (8.5% increase in power consumption).\n","authors":["Jose A. Ayala-Romero","Andres Garcia-Saavedra","Xavier Costa-Perez"],"pdf_url":"https://arxiv.org/pdf/2312.09961v1.pdf","comment":"12 pages, 13 figures"},{"id":"http://arxiv.org/abs/2312.00655v3","updated":"2023-12-15T17:10:35Z","published":"2023-12-01T15:30:43Z","title":"Machine Learning for Health symposium 2023 -- Findings track","summary":"  A collection of the accepted Findings papers that were presented at the 3rd\nMachine Learning for Health symposium (ML4H 2023), which was held on December\n10, 2023, in New Orleans, Louisiana, USA. ML4H 2023 invited high-quality\nsubmissions on relevant problems in a variety of health-related disciplines\nincluding healthcare, biomedicine, and public health. Two submission tracks\nwere offered: the archival Proceedings track, and the non-archival Findings\ntrack. Proceedings were targeted at mature work with strong technical\nsophistication and a high impact to health. The Findings track looked for new\nideas that could spark insightful discussion, serve as valuable resources for\nthe community, or could enable new collaborations. Submissions to the\nProceedings track, if not accepted, were automatically considered for the\nFindings track. All the manuscripts submitted to ML4H Symposium underwent a\ndouble-blind peer-review process.\n","authors":["Stefan Hegselmann","Antonio Parziale","Divya Shanmugam","Shengpu Tang","Mercy Nyamewaa Asiedu","Serina Chang","Thomas Hartvigsen","Harvineet Singh"],"pdf_url":"https://arxiv.org/pdf/2312.00655v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09950v1","updated":"2023-12-15T17:01:35Z","published":"2023-12-15T17:01:35Z","title":"Peer Learning: Learning Complex Policies in Groups from Scratch via\n  Action Recommendations","summary":"  Peer learning is a novel high-level reinforcement learning framework for\nagents learning in groups. While standard reinforcement learning trains an\nindividual agent in trial-and-error fashion, all on its own, peer learning\naddresses a related setting in which a group of agents, i.e., peers, learns to\nmaster a task simultaneously together from scratch. Peers are allowed to\ncommunicate only about their own states and actions recommended by others:\n\"What would you do in my situation?\". Our motivation is to study the learning\nbehavior of these agents. We formalize the teacher selection process in the\naction advice setting as a multi-armed bandit problem and therefore highlight\nthe need for exploration. Eventually, we analyze the learning behavior of the\npeers and observe their ability to rank the agents' performance within the\nstudy group and understand which agents give reliable advice. Further, we\ncompare peer learning with single agent learning and a state-of-the-art action\nadvice baseline. We show that peer learning is able to outperform single-agent\nlearning and the baseline in several challenging discrete and continuous OpenAI\nGym domains. Doing so, we also show that within such a framework complex\npolicies from action recommendations beyond discrete action spaces can evolve.\n","authors":["Cedric Derstroff","Mattia Cerrato","Jannis Brugger","Jan Peters","Stefan Kramer"],"pdf_url":"https://arxiv.org/pdf/2312.09950v1.pdf","comment":"9 pages, 7 figures, AAAI-24"},{"id":"http://arxiv.org/abs/2312.09940v1","updated":"2023-12-15T16:53:55Z","published":"2023-12-15T16:53:55Z","title":"Sketch and shift: a robust decoder for compressive clustering","summary":"  Compressive learning is an emerging approach to drastically reduce the memory\nfootprint of large-scale learning, by first summarizing a large dataset into a\nlow-dimensional sketch vector, and then decoding from this sketch the latent\ninformation needed for learning. In light of recent progress on information\npreservation guarantees for sketches based on random features, a major\nobjective is to design easy-to-tune algorithms (called decoders) to robustly\nand efficiently extract this information. To address the underlying non-convex\noptimization problems, various heuristics have been proposed. In the case of\ncompressive clustering, the standard heuristic is CL-OMPR, a variant of sliding\nFrank-Wolfe. Yet, CL-OMPR is hard to tune, and the examination of its\nrobustness was overlooked. In this work, we undertake a scrutinized examination\nof CL-OMPR to circumvent its limitations. In particular, we show how this\nalgorithm can fail to recover the clusters even in advantageous scenarios. To\ngain insight, we show how the deficiencies of this algorithm can be attributed\nto optimization difficulties related to the structure of a correlation function\nappearing at core steps of the algorithm. To address these limitations, we\npropose an alternative decoder offering substantial improvements over CL-OMPR.\nIts design is notably inspired from the mean shift algorithm, a classic\napproach to detect the local maxima of kernel density estimators. The proposed\nalgorithm can extract clustering information from a sketch of the MNIST dataset\nthat is 10 times smaller than previously.\n","authors":["Ayoub Belhadji","Rémi Gribonval"],"pdf_url":"https://arxiv.org/pdf/2312.09940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09939v1","updated":"2023-12-15T16:51:36Z","published":"2023-12-15T16:51:36Z","title":"Quantum Generative Adversarial Networks: Bridging Classical and Quantum\n  Realms","summary":"  In this pioneering research paper, we present a groundbreaking exploration\ninto the synergistic fusion of classical and quantum computing paradigms within\nthe realm of Generative Adversarial Networks (GANs). Our objective is to\nseamlessly integrate quantum computational elements into the conventional GAN\narchitecture, thereby unlocking novel pathways for enhanced training processes.\n  Drawing inspiration from the inherent capabilities of quantum bits (qubits),\nwe delve into the incorporation of quantum data representation methodologies\nwithin the GAN framework. By capitalizing on the unique quantum features, we\naim to accelerate the training process of GANs, offering a fresh perspective on\nthe optimization of generative models.\n  Our investigation deals with theoretical considerations and evaluates the\npotential quantum advantages that may manifest in terms of training efficiency\nand generative quality. We confront the challenges inherent in the\nquantum-classical amalgamation, addressing issues related to quantum hardware\nconstraints, error correction mechanisms, and scalability considerations. This\nresearch is positioned at the forefront of quantum-enhanced machine learning,\npresenting a critical stride towards harnessing the computational power of\nquantum systems to expedite the training of Generative Adversarial Networks.\nThrough our comprehensive examination of the interface between classical and\nquantum realms, we aim to uncover transformative insights that will propel the\nfield forward, fostering innovation and advancing the frontier of quantum\nmachine learning.\n","authors":["Sahil Nokhwal","Suman Nokhwal","Ram Swaroop","Raj Bala","Ankit Chaudhary"],"pdf_url":"https://arxiv.org/pdf/2312.09939v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09938v1","updated":"2023-12-15T16:49:57Z","published":"2023-12-15T16:49:57Z","title":"Assume-Guarantee Reinforcement Learning","summary":"  We present a modular approach to \\emph{reinforcement learning} (RL) in\nenvironments consisting of simpler components evolving in parallel. A\nmonolithic view of such modular environments may be prohibitively large to\nlearn, or may require unrealizable communication between the components in the\nform of a centralized controller. Our proposed approach is based on the\nassume-guarantee paradigm where the optimal control for the individual\ncomponents is synthesized in isolation by making \\emph{assumptions} about the\nbehaviors of neighboring components, and providing \\emph{guarantees} about\ntheir own behavior. We express these \\emph{assume-guarantee contracts} as\nregular languages and provide automatic translations to scalar rewards to be\nused in RL. By combining local probabilities of satisfaction for each\ncomponent, we provide a lower bound on the probability of satisfaction of the\ncomplete system. By solving a Markov game for each component, RL can produce a\ncontroller for each component that maximizes this lower bound. The controller\nutilizes the information it receives through communication, observations, and\nany knowledge of a coarse model of other agents. We experimentally demonstrate\nthe efficiency of the proposed approach on a variety of case studies.\n","authors":["Milad Kazemi","Mateo Perez","Fabio Somenzi","Sadegh Soudjani","Ashutosh Trivedi","Alvaro Velasquez"],"pdf_url":"https://arxiv.org/pdf/2312.09938v1.pdf","comment":"This is the extended version of the paper accepted in the SRRAI\n  Special Track at the Conference on Artificial Intelligence (AAAI-24)"},{"id":"http://arxiv.org/abs/2305.14160v3","updated":"2023-12-15T16:48:15Z","published":"2023-05-23T15:26:20Z","title":"Label Words are Anchors: An Information Flow Perspective for\n  Understanding In-Context Learning","summary":"  In-context learning (ICL) emerges as a promising capability of large language\nmodels (LLMs) by providing them with demonstration examples to perform diverse\ntasks. However, the underlying mechanism of how LLMs learn from the provided\ncontext remains under-explored. In this paper, we investigate the working\nmechanism of ICL through an information flow lens. Our findings reveal that\nlabel words in the demonstration examples function as anchors: (1) semantic\ninformation aggregates into label word representations during the shallow\ncomputation layers' processing; (2) the consolidated information in label words\nserves as a reference for LLMs' final predictions. Based on these insights, we\nintroduce an anchor re-weighting method to improve ICL performance, a\ndemonstration compression technique to expedite inference, and an analysis\nframework for diagnosing ICL errors in GPT2-XL. The promising applications of\nour findings again validate the uncovered ICL working mechanism and pave the\nway for future studies.\n","authors":["Lean Wang","Lei Li","Damai Dai","Deli Chen","Hao Zhou","Fandong Meng","Jie Zhou","Xu Sun"],"pdf_url":"https://arxiv.org/pdf/2305.14160v3.pdf","comment":"Accepted by EMNLP 2023"},{"id":"http://arxiv.org/abs/2312.09926v1","updated":"2023-12-15T16:31:44Z","published":"2023-12-15T16:31:44Z","title":"FuXi-S2S: An accurate machine learning model for global subseasonal\n  forecasts","summary":"  Skillful subseasonal forecasts beyond 2 weeks are crucial for a wide range of\napplications across various sectors of society. Recently, state-of-the-art\nmachine learning based weather forecasting models have made significant\nadvancements, outperforming the high-resolution forecast (HRES) from the\nEuropean Centre for Medium-Range Weather Forecasts (ECMWF). However, the full\npotential of machine learning models in subseasonal forecasts has yet to be\nfully explored. In this study, we introduce FuXi Subseasonal-to-Seasonal\n(FuXi-S2S), a machine learning based subseasonal forecasting model that\nprovides global daily mean forecasts up to 42 days, covering 5 upper-air\natmospheric variables at 13 pressure levels and 11 surface variables. FuXi-S2S\nintegrates an enhanced FuXi base model with a perturbation module for\nflow-dependent perturbations in hidden features, and incorporates Perlin noise\nto perturb initial conditions. The model is developed using 72 years of daily\nstatistics from ECMWF ERA5 reanalysis data. When compared to the ECMWF\nSubseasonal-to-Seasonal (S2S) reforecasts, the FuXi-S2S forecasts demonstrate\nsuperior deterministic and ensemble forecasts for total precipitation (TP),\noutgoing longwave radiation (OLR), and geopotential at 500 hPa (Z500). Although\nit shows slightly inferior performance in predicting 2-meter temperature (T2M),\nit has clear advantages over land area. Regarding the extreme forecasts,\nFuXi-S2S outperforms ECMWF S2S globally for TP. Furthermore, FuXi-S2S forecasts\nsurpass the ECMWF S2S reforecasts in predicting the Madden Julian Oscillation\n(MJO), a key source of subseasonal predictability. They extend the skillful\nprediction of MJO from 30 days to 36 days.\n","authors":["Lei Chen","Xiaohui Zhong","Jie Wu","Deliang Chen","Shangping Xie","Qingchen Chao","Chensen Lin","Zixin Hu","Bo Lu","Hao Li","Yuan Qi"],"pdf_url":"https://arxiv.org/pdf/2312.09926v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.01460v2","updated":"2023-12-15T16:29:24Z","published":"2021-07-03T16:23:31Z","title":"Mava: a research library for distributed multi-agent reinforcement\n  learning in JAX","summary":"  Multi-agent reinforcement learning (MARL) research is inherently\ncomputationally expensive and it is often difficult to obtain a sufficient\nnumber of experiment samples to test hypotheses and make robust statistical\nclaims. Furthermore, MARL algorithms are typically complex in their design and\ncan be tricky to implement correctly. These aspects of MARL present a difficult\nchallenge when it comes to creating useful software for advanced research. Our\ncriteria for such software is that it should be simple enough to use to\nimplement new ideas quickly, while at the same time be scalable and fast enough\nto test those ideas in a reasonable amount of time. In this preliminary\ntechnical report, we introduce Mava, a research library for MARL written purely\nin JAX, that aims to fulfill these criteria. We discuss the design and core\nfeatures of Mava, and demonstrate its use and performance across a variety of\nenvironments. In particular, we show Mava's substantial speed advantage, with\nimprovements of 10-100x compared to other popular MARL frameworks, while\nmaintaining strong performance. This allows for researchers to test ideas in a\nfew minutes instead of several hours. Finally, Mava forms part of an ecosystem\nof libraries that seamlessly integrate with each other to help facilitate\nadvanced research in MARL. We hope Mava will benefit the community and help\ndrive scientifically sound and statistically robust research in the field. The\nopen-source repository for Mava is available at\nhttps://github.com/instadeepai/Mava.\n","authors":["Ruan de Kock","Omayma Mahjoub","Sasha Abramowitz","Wiem Khlifi","Callum Rhys Tilbury","Claude Formanek","Andries Smit","Arnu Pretorius"],"pdf_url":"https://arxiv.org/pdf/2107.01460v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.07213v3","updated":"2023-12-15T16:28:21Z","published":"2023-04-14T15:52:57Z","title":"Very high resolution canopy height maps from RGB imagery using\n  self-supervised vision transformer and convolutional decoder trained on\n  Aerial Lidar","summary":"  Vegetation structure mapping is critical for understanding the global carbon\ncycle and monitoring nature-based approaches to climate adaptation and\nmitigation. Repeated measurements of these data allow for the observation of\ndeforestation or degradation of existing forests, natural forest regeneration,\nand the implementation of sustainable agricultural practices like agroforestry.\nAssessments of tree canopy height and crown projected area at a high spatial\nresolution are also important for monitoring carbon fluxes and assessing\ntree-based land uses, since forest structures can be highly spatially\nheterogeneous, especially in agroforestry systems. Very high resolution\nsatellite imagery (less than one meter (1m) Ground Sample Distance) makes it\npossible to extract information at the tree level while allowing monitoring at\na very large scale. This paper presents the first high-resolution canopy height\nmap concurrently produced for multiple sub-national jurisdictions.\nSpecifically, we produce very high resolution canopy height maps for the states\nof California and Sao Paulo, a significant improvement in resolution over the\nten meter (10m) resolution of previous Sentinel / GEDI based worldwide maps of\ncanopy height. The maps are generated by the extraction of features from a\nself-supervised model trained on Maxar imagery from 2017 to 2020, and the\ntraining of a dense prediction decoder against aerial lidar maps. We also\nintroduce a post-processing step using a convolutional network trained on GEDI\nobservations. We evaluate the proposed maps with set-aside validation lidar\ndata as well as by comparing with other remotely sensed maps and\nfield-collected data, and find our model produces an average Mean Absolute\nError (MAE) of 2.8 meters and Mean Error (ME) of 0.6 meters.\n","authors":["Jamie Tolan","Hung-I Yang","Ben Nosarzewski","Guillaume Couairon","Huy Vo","John Brandt","Justine Spore","Sayantan Majumdar","Daniel Haziza","Janaki Vamaraju","Theo Moutakanni","Piotr Bojanowski","Tracy Johns","Brian White","Tobias Tiecke","Camille Couprie"],"pdf_url":"https://arxiv.org/pdf/2304.07213v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09912v1","updated":"2023-12-15T16:23:25Z","published":"2023-12-15T16:23:25Z","title":"Reliable Probabilistic Classification with Neural Networks","summary":"  Venn Prediction (VP) is a new machine learning framework for producing\nwell-calibrated probabilistic predictions. In particular it provides\nwell-calibrated lower and upper bounds for the conditional probability of an\nexample belonging to each possible class of the problem at hand. This paper\nproposes five VP methods based on Neural Networks (NNs), which is one of the\nmost widely used machine learning techniques. The proposed methods are\nevaluated experimentally on four benchmark datasets and the obtained results\ndemonstrate the empirical well-calibratedness of their outputs and their\nsuperiority over the outputs of the traditional NN classifier.\n","authors":["Harris Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2312.09912v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.13764v3","updated":"2023-12-15T16:11:46Z","published":"2023-01-31T16:55:42Z","title":"Unsupervised Neighborhood Propagation Kernel Layers for Semi-supervised\n  Node Classification","summary":"  We present a deep Graph Convolutional Kernel Machine (GCKM) for\nsemi-supervised node classification in graphs. The method is built of two main\ntypes of blocks: (i) We introduce unsupervised kernel machine layers\npropagating the node features in a one-hop neighborhood, using implicit node\nfeature mappings. (ii) We specify a semi-supervised classification kernel\nmachine through the lens of the Fenchel-Young inequality. We derive an\neffective initialization scheme and efficient end-to-end training algorithm in\nthe dual variables for the full architecture. The main idea underlying GCKM is\nthat, because of the unsupervised core, the final model can achieve higher\nperformance in semi-supervised node classification when few labels are\navailable for training. Experimental results demonstrate the effectiveness of\nthe proposed framework.\n","authors":["Sonny Achten","Francesco Tonin","Panagiotis Patrinos","Johan A. K. Suykens"],"pdf_url":"https://arxiv.org/pdf/2301.13764v3.pdf","comment":"Accepted for publication in AAAI 2024"},{"id":"http://arxiv.org/abs/2310.20092v3","updated":"2023-12-15T16:09:51Z","published":"2023-10-31T00:12:14Z","title":"The Missing U for Efficient Diffusion Models","summary":"  Diffusion Probabilistic Models stand as a critical tool in generative\nmodelling, enabling the generation of complex data distributions. This family\nof generative models yields record-breaking performance in tasks such as image\nsynthesis, video generation, and molecule design. Despite their capabilities,\ntheir efficiency, especially in the reverse process, remains a challenge due to\nslow convergence rates and high computational costs. In this paper, we\nintroduce an approach that leverages continuous dynamical systems to design a\nnovel denoising network for diffusion models that is more parameter-efficient,\nexhibits faster convergence, and demonstrates increased noise robustness.\nExperimenting with Denoising Diffusion Probabilistic Models (DDPMs), our\nframework operates with approximately a quarter of the parameters, and $\\sim$\n30\\% of the Floating Point Operations (FLOPs) compared to standard U-Nets in\nDDPMs. Furthermore, our model is notably faster in inference than the baseline\nwhen measured in fair and equal conditions. We also provide a mathematical\nintuition as to why our proposed reverse process is faster as well as a\nmathematical discussion of the empirical tradeoffs in the denoising downstream\ntask. Finally, we argue that our method is compatible with existing performance\nenhancement techniques, enabling further improvements in efficiency, quality,\nand speed.\n","authors":["Sergio Calvo-Ordonez","Chun-Wun Cheng","Jiahao Huang","Lipei Zhang","Guang Yang","Carola-Bibiane Schonlieb","Angelica I Aviles-Rivero"],"pdf_url":"https://arxiv.org/pdf/2310.20092v3.pdf","comment":"20 pages, 14 figures, Under review at Transactions of Machine\n  Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2312.06957v2","updated":"2023-12-15T16:04:24Z","published":"2023-12-12T03:34:09Z","title":"Online Saddle Point Problem and Online Convex-Concave Optimization","summary":"  Centered around solving the Online Saddle Point problem, this paper\nintroduces the Online Convex-Concave Optimization (OCCO) framework, which\ninvolves a sequence of two-player time-varying convex-concave games. We propose\nthe generalized duality gap (Dual-Gap) as the performance metric and establish\nthe parallel relationship between OCCO with Dual-Gap and Online Convex\nOptimization (OCO) with regret. To demonstrate the natural extension of OCCO\nfrom OCO, we develop two algorithms, the implicit online mirror descent-ascent\nand its optimistic variant. Analysis reveals that their duality gaps share\nsimilar expression forms with the corresponding dynamic regrets arising from\nimplicit updates in OCO. Empirical results further substantiate the\neffectiveness of our algorithms. Simultaneously, we unveil that the dynamic\nNash equilibrium regret, which was initially introduced in a recent paper, has\ninherent defects.\n","authors":["Qing-xin Meng","Jian-wei Liu"],"pdf_url":"https://arxiv.org/pdf/2312.06957v2.pdf","comment":"Add Remark 8 and Section 6"},{"id":"http://arxiv.org/abs/2309.01013v2","updated":"2023-12-15T16:01:41Z","published":"2023-09-02T20:24:24Z","title":"Streaming Active Learning for Regression Problems Using Regression via\n  Classification","summary":"  One of the challenges in deploying a machine learning model is that the\nmodel's performance degrades as the operating environment changes. To maintain\nthe performance, streaming active learning is used, in which the model is\nretrained by adding a newly annotated sample to the training dataset if the\nprediction of the sample is not certain enough. Although many streaming active\nlearning methods have been proposed for classification, few efforts have been\nmade for regression problems, which are often handled in the industrial field.\nIn this paper, we propose to use the regression-via-classification framework\nfor streaming active learning for regression. Regression-via-classification\ntransforms regression problems into classification problems so that streaming\nactive learning methods proposed for classification problems can be applied\ndirectly to regression problems. Experimental validation on four real data sets\nshows that the proposed method can perform regression with higher accuracy at\nthe same annotation cost.\n","authors":["Shota Horiguchi","Kota Dohi","Yohei Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2309.01013v2.pdf","comment":"Accepted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2212.07786v2","updated":"2023-12-15T15:59:44Z","published":"2022-12-14T17:34:03Z","title":"Convergent Data-driven Regularizations for CT Reconstruction","summary":"  The reconstruction of images from their corresponding noisy Radon transform\nis a typical example of an ill-posed linear inverse problem as arising in the\napplication of computerized tomography (CT). As the (naive) solution does not\ndepend on the measured data continuously, regularization is needed to\nre-establish a continuous dependence. In this work, we investigate simple, but\nyet still provably convergent approaches to learning linear regularization\nmethods from data. More specifically, we analyze two approaches: One generic\nlinear regularization that learns how to manipulate the singular values of the\nlinear operator in an extension of our previous work, and one tailored approach\nin the Fourier domain that is specific to CT-reconstruction. We prove that such\napproaches become convergent regularization methods as well as the fact that\nthe reconstructions they provide are typically much smoother than the training\ndata they were trained on. Finally, we compare the spectral as well as the\nFourier-based approaches for CT-reconstruction numerically, discuss their\nadvantages and disadvantages and investigate the effect of discretization\nerrors at different resolutions.\n","authors":["Samira Kabri","Alexander Auras","Danilo Riccio","Hartmut Bauermeister","Martin Benning","Michael Moeller","Martin Burger"],"pdf_url":"https://arxiv.org/pdf/2212.07786v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09899v1","updated":"2023-12-15T15:49:53Z","published":"2023-12-15T15:49:53Z","title":"SQA-SAM: Segmentation Quality Assessment for Medical Images Utilizing\n  the Segment Anything Model","summary":"  Segmentation quality assessment (SQA) plays a critical role in the deployment\nof a medical image based AI system. Users need to be informed/alerted whenever\nan AI system generates unreliable/incorrect predictions. With the introduction\nof the Segment Anything Model (SAM), a general foundation segmentation model,\nnew research opportunities emerged in how one can utilize SAM for medical image\nsegmentation. In this paper, we propose a novel SQA method, called SQA-SAM,\nwhich exploits SAM to enhance the accuracy of quality assessment for medical\nimage segmentation. When a medical image segmentation model (MedSeg) produces\npredictions for a test image, we generate visual prompts based on the\npredictions, and SAM is utilized to generate segmentation maps corresponding to\nthe visual prompts. How well MedSeg's segmentation aligns with SAM's\nsegmentation indicates how well MedSeg's segmentation aligns with the general\nperception of objectness and image region partition. We develop a score measure\nfor such alignment. In experiments, we find that the generated scores exhibit\nmoderate to strong positive correlation (in Pearson correlation and Spearman\ncorrelation) with Dice coefficient scores reflecting the true segmentation\nquality.\n","authors":["Yizhe Zhang","Shuo Wang","Tao Zhou","Qi Dou","Danny Z. Chen"],"pdf_url":"https://arxiv.org/pdf/2312.09899v1.pdf","comment":"Work in progress;"},{"id":"http://arxiv.org/abs/2312.09887v1","updated":"2023-12-15T15:34:29Z","published":"2023-12-15T15:34:29Z","title":"Probabilistic learning of the Purkinje network from the\n  electrocardiogram","summary":"  The identification of the Purkinje conduction system in the heart is a\nchallenging task, yet essential for a correct definition of cardiac digital\ntwins for precision cardiology. Here, we propose a probabilistic approach for\nidentifying the Purkinje network from non-invasive clinical data such as the\nstandard electrocardiogram (ECG). We use cardiac imaging to build an\nanatomically accurate model of the ventricles; we algorithmically generate a\nrule-based Purkinje network tailored to the anatomy; we simulate physiological\nelectrocardiograms with a fast model; we identify the geometrical and\nelectrical parameters of the Purkinje-ECG model with Bayesian optimization and\napproximate Bayesian computation. The proposed approach is inherently\nprobabilistic and generates a population of plausible Purkinje networks, all\nfitting the ECG within a given tolerance. In this way, we can estimate the\nuncertainty of the parameters, thus providing reliable predictions. We test our\nmethodology in physiological and pathological scenarios, showing that we are\nable to accurately recover the ECG with our model. We propagate the uncertainty\nin the Purkinje network parameters in a simulation of conduction system pacing\ntherapy. Our methodology is a step forward in creation of digital twins from\nnon-invasive data in precision medicine. An open source implementation can be\nfound at http://github.com/fsahli/purkinje-learning\n","authors":["Felipe Álvarez-Barrientos","Mariana Salinas-Camus","Simone Pezzuto","Francisco Sahli Costabal"],"pdf_url":"https://arxiv.org/pdf/2312.09887v1.pdf","comment":"18 pages, 9 figures"},{"id":"http://arxiv.org/abs/2312.09885v1","updated":"2023-12-15T15:32:25Z","published":"2023-12-15T15:32:25Z","title":"Simple Weak Coresets for Non-Decomposable Classification Measures","summary":"  While coresets have been growing in terms of their application, barring few\nexceptions, they have mostly been limited to unsupervised settings. We consider\nsupervised classification problems, and non-decomposable evaluation measures in\nsuch settings. We show that stratified uniform sampling based coresets have\nexcellent empirical performance that are backed by theoretical guarantees too.\nWe focus on the F1 score and Matthews Correlation Coefficient, two widely used\nnon-decomposable objective functions that are nontrivial to optimize for and\nshow that uniform coresets attain a lower bound for coreset size, and have good\nempirical performance, comparable with ``smarter'' coreset construction\nstrategies.\n","authors":["Jayesh Malaviya","Anirban Dasgupta","Rachit Chhaya"],"pdf_url":"https://arxiv.org/pdf/2312.09885v1.pdf","comment":"Accepted at AAAI 2024"},{"id":"http://arxiv.org/abs/2312.09881v1","updated":"2023-12-15T15:28:25Z","published":"2023-12-15T15:28:25Z","title":"Dynamic Heterogeneous Federated Learning with Multi-Level Prototypes","summary":"  Federated learning shows promise as a privacy-preserving collaborative\nlearning technique. Existing heterogeneous federated learning mainly focuses on\nskewing the label distribution across clients. However, most approaches suffer\nfrom catastrophic forgetting and concept drift, mainly when the global\ndistribution of all classes is extremely unbalanced and the data distribution\nof the client dynamically evolves over time. In this paper, we study the new\ntask, i.e., Dynamic Heterogeneous Federated Learning (DHFL), which addresses\nthe practical scenario where heterogeneous data distributions exist among\ndifferent clients and dynamic tasks within the client. Accordingly, we propose\na novel federated learning framework named Federated Multi-Level Prototypes\n(FedMLP) and design federated multi-level regularizations. To mitigate concept\ndrift, we construct prototypes and semantic prototypes to provide fruitful\ngeneralization knowledge and ensure the continuity of prototype spaces. To\nmaintain the model stability and consistency of convergence, three\nregularizations are introduced as training losses, i.e., prototype-based\nregularization, semantic prototype-based regularization, and federated\ninter-task regularization. Extensive experiments show that the proposed method\nachieves state-of-the-art performance in various settings.\n","authors":["Shunxin Guo","Hongsong Wang","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2312.09881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09877v1","updated":"2023-12-15T15:26:13Z","published":"2023-12-15T15:26:13Z","title":"Distributed Learning of Mixtures of Experts","summary":"  In modern machine learning problems we deal with datasets that are either\ndistributed by nature or potentially large for which distributing the\ncomputations is usually a standard way to proceed, since centralized algorithms\nare in general ineffective. We propose a distributed learning approach for\nmixtures of experts (MoE) models with an aggregation strategy to construct a\nreduction estimator from local estimators fitted parallelly to distributed\nsubsets of the data. The aggregation is based on an optimal minimization of an\nexpected transportation divergence between the large MoE composed of local\nestimators and the unknown desired MoE model. We show that the provided\nreduction estimator is consistent as soon as the local estimators to be\naggregated are consistent, and its construction is performed by a proposed\nmajorization-minimization (MM) algorithm that is computationally effective. We\nstudy the statistical and numerical properties for the proposed reduction\nestimator on experiments that demonstrate its performance compared to namely\nthe global estimator constructed in a centralized way from the full dataset.\nFor some situations, the computation time is more than ten times faster, for a\ncomparable performance. Our source codes are publicly available on Github.\n","authors":["Faïcel Chamroukhi","Nhat Thien Pham"],"pdf_url":"https://arxiv.org/pdf/2312.09877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09871v1","updated":"2023-12-15T15:18:33Z","published":"2023-12-15T15:18:33Z","title":"ChemTime: Rapid and Early Classification for Multivariate Time Series\n  Classification of Chemical Sensors","summary":"  Multivariate time series data are ubiquitous in the application of machine\nlearning to problems in the physical sciences. Chemiresistive sensor arrays are\nhighly promising in chemical detection tasks relevant to industrial, safety,\nand military applications. Sensor arrays are an inherently multivariate time\nseries data collection tool which demand rapid and accurate classification of\narbitrary chemical analytes. Previous research has benchmarked data-agnostic\nmultivariate time series classifiers across diverse multivariate time series\nsupervised tasks in order to find general-purpose classification algorithms. To\nour knowledge, there has yet to be an effort to survey machine learning and\ntime series classification approaches to chemiresistive hardware sensor arrays\nfor the detection of chemical analytes. In addition to benchmarking existing\napproaches to multivariate time series classifiers, we incorporate findings\nfrom a model survey to propose the novel \\textit{ChemTime} approach to sensor\narray classification for chemical sensing. We design experiments addressing the\nunique challenges of hardware sensor arrays classification including the rapid\nclassification ability of classifiers and minimization of inference time while\nmaintaining performance for deployed lightweight hardware sensing devices. We\nfind that \\textit{ChemTime} is uniquely positioned for the chemical sensing\ntask by combining rapid and early classification of time series with beneficial\ninference and high accuracy.\n","authors":["Alexander M. Moore","Randy C. Paffenroth","Kenneth T. Ngo","Joshua R. Uzarski"],"pdf_url":"https://arxiv.org/pdf/2312.09871v1.pdf","comment":"14 pages, 12 figures"},{"id":"http://arxiv.org/abs/2312.09869v1","updated":"2023-12-15T15:14:55Z","published":"2023-12-15T15:14:55Z","title":"Learning in Online Principle-Agent Interactions: The Power of Menus","summary":"  We study a ubiquitous learning challenge in online principal-agent problems\nduring which the principal learns the agent's private information from the\nagent's revealed preferences in historical interactions. This paradigm includes\nimportant special cases such as pricing and contract design, which have been\nwidely studied in recent literature. However, existing work considers the case\nwhere the principal can only choose a single strategy at every round to\ninteract with the agent and then observe the agent's revealed preference\nthrough their actions. In this paper, we extend this line of study to allow the\nprincipal to offer a menu of strategies to the agent and learn additionally\nfrom observing the agent's selection from the menu. We provide a thorough\ninvestigation of several online principal-agent problem settings and\ncharacterize their sample complexities, accompanied by the corresponding\nalgorithms we have developed. We instantiate this paradigm to several important\ndesign problems $-$ including Stackelberg (security) games, contract design,\nand information design. Finally, we also explore the connection between our\nfindings and existing results about online learning in Stackelberg games, and\nwe offer a solution that can overcome a key hard instance of Peng et al.\n(2019).\n","authors":["Minbiao Han","Michael Albert","Haifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2312.09869v1.pdf","comment":"AAAI Conference on Artificial Intelligence (AAAI 2024)"},{"id":"http://arxiv.org/abs/2312.09865v1","updated":"2023-12-15T15:09:16Z","published":"2023-12-15T15:09:16Z","title":"Automating reward function configuration for drug design","summary":"  Designing reward functions that guide generative molecular design (GMD)\nalgorithms to desirable areas of chemical space is of critical importance in\nAI-driven drug discovery. Traditionally, this has been a manual and error-prone\ntask; the selection of appropriate computational methods to approximate\nbiological assays is challenging and the aggregation of computed values into a\nsingle score even more so, leading to potential reliance on trial-and-error\napproaches. We propose a novel approach for automated reward configuration that\nrelies solely on experimental data, mitigating the challenges of manual reward\nadjustment on drug discovery projects. Our method achieves this by constructing\na ranking over experimental data based on Pareto dominance over the\nmulti-objective space, then training a neural network to approximate the reward\nfunction such that rankings determined by the predicted reward correlate with\nthose determined by the Pareto dominance relation. We validate our method using\ntwo case studies. In the first study we simulate Design-Make-Test-Analyse\n(DMTA) cycles by alternating reward function updates and generative runs guided\nby that function. We show that the learned function adapts over time to yield\ncompounds that score highly with respect to evaluation functions taken from the\nliterature. In the second study we apply our algorithm to historical data from\nfour real drug discovery projects. We show that our algorithm yields reward\nfunctions that outperform the predictive accuracy of human-defined functions,\nachieving an improvement of up to 0.4 in Spearman's correlation against a\nground truth evaluation function that encodes the target drug profile for that\nproject. Our method provides an efficient data-driven way to configure reward\nfunctions for GMD, and serves as a strong baseline for future research into\ntransformative approaches for the automation of drug discovery.\n","authors":["Marius Urbonas","Temitope Ajileye","Paul Gainer","Douglas Pires"],"pdf_url":"https://arxiv.org/pdf/2312.09865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12367v2","updated":"2023-12-15T15:05:30Z","published":"2023-08-23T18:12:11Z","title":"SafeAR: Towards Safer Algorithmic Recourse by Risk-Aware Policies","summary":"  With the growing use of machine learning (ML) models in critical domains such\nas finance and healthcare, the need to offer recourse for those adversely\naffected by the decisions of ML models has become more important; individuals\nought to be provided with recommendations on actions to take for improving\ntheir situation and thus receiving a favorable decision. Prior work on\nsequential algorithmic recourse -- which recommends a series of changes --\nfocuses on action feasibility and uses the proximity of feature changes to\ndetermine action costs. However, the uncertainties of feature changes and the\nrisk of higher than average costs in recourse have not been considered. It is\nundesirable if a recourse could (with some probability) result in a worse\nsituation from which recovery requires an extremely high cost. It is essential\nto incorporate risks when computing and evaluating recourse. We call the\nrecourse computed with such risk considerations as Safer Algorithmic Recourse\n(SafeAR). The objective is to empower people to choose a recourse based on\ntheir risk tolerance. In this work, we discuss and show how existing recourse\ndesiderata can fail to capture the risk of higher costs. We present a method to\ncompute recourse policies that consider variability in cost and connect\nalgorithmic recourse literature with risk-sensitive reinforcement learning. We\nalso adopt measures \"Value at Risk\" and \"Conditional Value at Risk\" from the\nfinancial literature to summarize risk concisely. We apply our method to two\nreal-world datasets and compare policies with different risk-aversion levels\nusing risk measures and recourse desiderata (sparsity and proximity).\n","authors":["Haochen Wu","Shubham Sharma","Sunandita Patra","Sriram Gopalakrishnan"],"pdf_url":"https://arxiv.org/pdf/2308.12367v2.pdf","comment":"Supplemental material appended to main paper"},{"id":"http://arxiv.org/abs/2312.09860v1","updated":"2023-12-15T15:05:25Z","published":"2023-12-15T15:05:25Z","title":"Automatic Rao-Blackwellization for Sequential Monte Carlo with Belief\n  Propagation","summary":"  Exact Bayesian inference on state-space models~(SSM) is in general\nuntractable, and unfortunately, basic Sequential Monte Carlo~(SMC) methods do\nnot yield correct approximations for complex models. In this paper, we propose\na mixed inference algorithm that computes closed-form solutions using belief\npropagation as much as possible, and falls back to sampling-based SMC methods\nwhen exact computations fail. This algorithm thus implements automatic\nRao-Blackwellization and is even exact for Gaussian tree models.\n","authors":["Waïss Azizian","Guillaume Baudart","Marc Lelarge"],"pdf_url":"https://arxiv.org/pdf/2312.09860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09857v1","updated":"2023-12-15T15:03:55Z","published":"2023-12-15T15:03:55Z","title":"Deep Unsupervised Domain Adaptation for Time Series Classification: a\n  Benchmark","summary":"  Unsupervised Domain Adaptation (UDA) aims to harness labeled source data to\ntrain models for unlabeled target data. Despite extensive research in domains\nlike computer vision and natural language processing, UDA remains underexplored\nfor time series data, which has widespread real-world applications ranging from\nmedicine and manufacturing to earth observation and human activity recognition.\nOur paper addresses this gap by introducing a comprehensive benchmark for\nevaluating UDA techniques for time series classification, with a focus on deep\nlearning methods. We provide seven new benchmark datasets covering various\ndomain shifts and temporal dynamics, facilitating fair and standardized UDA\nmethod assessments with state of the art neural network backbones (e.g.\nInception) for time series data. This benchmark offers insights into the\nstrengths and limitations of the evaluated approaches while preserving the\nunsupervised nature of domain adaptation, making it directly applicable to\npractical problems. Our paper serves as a vital resource for researchers and\npractitioners, advancing domain adaptation solutions for time series data and\nfostering innovation in this critical field. The implementation code of this\nbenchmark is available at https://github.com/EricssonResearch/UDA-4-TSC.\n","authors":["Hassan Ismail Fawaz","Ganesh Del Grosso","Tanguy Kerdoncuff","Aurelie Boisbunon","Illyyne Saffar"],"pdf_url":"https://arxiv.org/pdf/2312.09857v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.15364v5","updated":"2023-12-15T15:02:34Z","published":"2022-05-24T14:25:28Z","title":"Associative Learning Mechanism for Drug-Target Interaction Prediction","summary":"  As a necessary process in drug development, finding a drug compound that can\nselectively bind to a specific protein is highly challenging and costly.\nDrug-target affinity (DTA), which represents the strength of drug-target\ninteraction (DTI), has played an important role in the DTI prediction task over\nthe past decade. Although deep learning has been applied to DTA-related\nresearch, existing solutions ignore fundamental correlations between molecular\nsubstructures in molecular representation learning of drug compound\nmolecules/protein targets. Moreover, traditional methods lack the\ninterpretability of the DTA prediction process. This results in missing feature\ninformation of intermolecular interactions, thereby affecting prediction\nperformance. Therefore, this paper proposes a DTA prediction method with\ninteractive learning and an autoencoder mechanism. The proposed model enhances\nthe corresponding ability to capture the feature information of a single\nmolecular sequence by the drug/protein molecular representation learning module\nand supplements the information interaction between molecular sequence pairs by\nthe interactive information learning module. The DTA value prediction module\nfuses the drug-target pair interaction information to output the predicted\nvalue of DTA. Additionally, this paper theoretically proves that the proposed\nmethod maximizes evidence lower bound (ELBO) for the joint distribution of the\nDTA prediction model, which enhances the consistency of the probability\ndistribution between the actual value and the predicted value. The experimental\nresults confirm mutual transformer-drug target affinity (MT-DTA) achieves\nbetter performance than other comparative methods.\n","authors":["Zhiqin Zhu","Zheng Yao","Guanqiu Qi","Neal Mazur","Baisen Cong"],"pdf_url":"https://arxiv.org/pdf/2205.15364v5.pdf","comment":"The extended and final version of this paper has been published with\n  open access modality in the CAAI Transactions on Intelligence Technology and\n  can be found at link LINK HERE. Please refer to the TRIT published version in\n  your scientific papers"},{"id":"http://arxiv.org/abs/2312.09852v1","updated":"2023-12-15T14:58:34Z","published":"2023-12-15T14:58:34Z","title":"Learning Distributions on Manifolds with Free-form Flows","summary":"  Many real world data, particularly in the natural sciences and computer\nvision, lie on known Riemannian manifolds such as spheres, tori or the group of\nrotation matrices. The predominant approaches to learning a distribution on\nsuch a manifold require solving a differential equation in order to sample from\nthe model and evaluate densities. The resulting sampling times are slowed down\nby a high number of function evaluations. In this work, we propose an\nalternative approach which only requires a single function evaluation followed\nby a projection to the manifold. Training is achieved by an adaptation of the\nrecently proposed free-form flow framework to Riemannian manifolds. The central\nidea is to estimate the gradient of the negative log-likelihood via a trace\nevaluated in the tangent space. We evaluate our method on various manifolds,\nand find significantly faster inference at competitive performance compared to\nprevious work. We make our code public at https://github.com/vislearn/FFF.\n","authors":["Peter Sorrenson","Felix Draxler","Armand Rousselot","Sander Hummerich","Ullrich Köthe"],"pdf_url":"https://arxiv.org/pdf/2312.09852v1.pdf","comment":"Preprint, under review"},{"id":"http://arxiv.org/abs/2312.09845v1","updated":"2023-12-15T14:50:14Z","published":"2023-12-15T14:50:14Z","title":"Learned Regularization for Inverse Problems: Insights from a Spectral\n  Model","summary":"  The aim of this paper is to provide a theoretically founded investigation of\nstate-of-the-art learning approaches for inverse problems. We give an extended\ndefinition of regularization methods and their convergence in terms of the\nunderlying data distributions, which paves the way for future theoretical\nstudies. Based on a simple spectral learning model previously introduced for\nsupervised learning, we investigate some key properties of different learning\nparadigms for inverse problems, which can be formulated independently of\nspecific architectures. In particular we investigate the regularization\nproperties, bias, and critical dependence on training data distributions.\nMoreover, our framework allows to highlight and compare the specific behavior\nof the different paradigms in the infinite-dimensional limit.\n","authors":["Martin Burger","Samira Kabri"],"pdf_url":"https://arxiv.org/pdf/2312.09845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09844v1","updated":"2023-12-15T14:49:41Z","published":"2023-12-15T14:49:41Z","title":"Small Dataset, Big Gains: Enhancing Reinforcement Learning by Offline\n  Pre-Training with Model Based Augmentation","summary":"  Offline reinforcement learning leverages pre-collected datasets of\ntransitions to train policies. It can serve as effective initialization for\nonline algorithms, enhancing sample efficiency and speeding up convergence.\nHowever, when such datasets are limited in size and quality, offline\npre-training can produce sub-optimal policies and lead to degraded online\nreinforcement learning performance. In this paper we propose a model-based data\naugmentation strategy to maximize the benefits of offline reinforcement\nlearning pre-training and reduce the scale of data needed to be effective. Our\napproach leverages a world model of the environment trained on the offline\ndataset to augment states during offline pre-training. We evaluate our approach\non a variety of MuJoCo robotic tasks and our results show it can jump-start\nonline fine-tuning and substantially reduce - in some cases by an order of\nmagnitude - the required number of environment interactions.\n","authors":["Girolamo Macaluso","Alessandro Sestini","Andrew D. Bagdanov"],"pdf_url":"https://arxiv.org/pdf/2312.09844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15316v3","updated":"2023-12-15T14:40:00Z","published":"2023-08-29T14:02:27Z","title":"3D-MuPPET: 3D Multi-Pigeon Pose Estimation and Tracking","summary":"  Markerless methods for animal posture tracking have been rapidly developing\nrecently, but frameworks and benchmarks for tracking large animal groups in 3D\nare still lacking. To overcome this gap in the literature, we present\n3D-MuPPET, a framework to estimate and track 3D poses of up to 10 pigeons at\ninteractive speed using multiple camera views. We train a pose estimator to\ninfer 2D keypoints and bounding boxes of multiple pigeons, then triangulate the\nkeypoints to 3D. For identity matching of individuals in all views, we first\ndynamically match 2D detections to global identities in the first frame, then\nuse a 2D tracker to maintain IDs across views in subsequent frames. We achieve\ncomparable accuracy to a state of the art 3D pose estimator in terms of median\nerror and Percentage of Correct Keypoints. Additionally, we benchmark the\ninference speed of 3D-MuPPET, with up to 9.45 fps in 2D and 1.89 fps in 3D, and\nperform quantitative tracking evaluation, which yields encouraging results.\nFinally, we showcase two novel applications for 3D-MuPPET. First, we train a\nmodel with data of single pigeons and achieve comparable results in 2D and 3D\nposture estimation for up to 5 pigeons. Second, we show that 3D-MuPPET also\nworks in outdoors without additional annotations from natural environments.\nBoth use cases simplify the domain shift to new species and environments,\nlargely reducing annotation effort needed for 3D posture tracking. To the best\nof our knowledge we are the first to present a framework for 2D/3D animal\nposture and trajectory tracking that works in both indoor and outdoor\nenvironments for up to 10 individuals. We hope that the framework can open up\nnew opportunities in studying animal collective behaviour and encourages\nfurther developments in 3D multi-animal posture tracking.\n","authors":["Urs Waldmann","Alex Hoi Hang Chan","Hemal Naik","Máté Nagy","Iain D. Couzin","Oliver Deussen","Bastian Goldluecke","Fumihiro Kano"],"pdf_url":"https://arxiv.org/pdf/2308.15316v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09832v1","updated":"2023-12-15T14:38:28Z","published":"2023-12-15T14:38:28Z","title":"Disentangling Linear Mode-Connectivity","summary":"  Linear mode-connectivity (LMC) (or lack thereof) is one of the intriguing\ncharacteristics of neural network loss landscapes. While empirically well\nestablished, it unfortunately still lacks a proper theoretical understanding.\nEven worse, although empirical data points are abound, a systematic study of\nwhen networks exhibit LMC is largely missing in the literature. In this work we\naim to close this gap. We explore how LMC is affected by three factors: (1)\narchitecture (sparsity, weight-sharing), (2) training strategy (optimization\nsetup) as well as (3) the underlying dataset. We place particular emphasis on\nminimal but non-trivial settings, removing as much unnecessary complexity as\npossible. We believe that our insights can guide future theoretical works on\nuncovering the inner workings of LMC.\n","authors":["Gul Sena Altintas","Gregor Bachmann","Lorenzo Noci","Thomas Hofmann"],"pdf_url":"https://arxiv.org/pdf/2312.09832v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2312.09830v1","updated":"2023-12-15T14:34:14Z","published":"2023-12-15T14:34:14Z","title":"Socio-Economic Deprivation Analysis: Diffusion Maps","summary":"  This report proposes a model to predict the location of the most deprived\nareas in a city using data from the census. A census data is very high\ndimensional and needs to be simplified. We use a novel algorithm to reduce\ndimensionality and find patterns: The diffusion map. Features are defined by\neigenvectors of the Laplacian matrix that defines the diffusion map.\nEigenvectors corresponding to the smallest eigenvalues indicate specific\npopulation features. Previous work has found qualitatively that the second most\nimportant dimension for describing the census data in Bristol is linked to\ndeprivation. In this report, we analyse how good this dimension is as a model\nfor predicting deprivation by comparing with the recognised measures. The\nPearson correlation coefficient was found to be over 0.7. The top 10 per cent\nof deprived areas in the UK which also locate in Bristol are extracted to test\nthe accuracy of the model. There are 52 most deprived areas, and 38 areas are\ncorrectly identified by comparing to the model. The influence of scores of IMD\ndomains that do not correlate with the models, Eigenvector 2 entries of\nnon-deprived OAs and orthogonality of Eigenvectors cause the model to fail the\nprediction of 14 deprived areas.\n  However, overall, the model shows a high performance to predict the future\ndeprivation of overall areas where the project considers. This project is\nexpected to support the government to allocate resources and funding.\n","authors":["June Moh Goo"],"pdf_url":"https://arxiv.org/pdf/2312.09830v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09821v1","updated":"2023-12-15T14:20:16Z","published":"2023-12-15T14:20:16Z","title":"Fragility, Robustness and Antifragility in Deep Learning","summary":"  We propose a systematic analysis of deep neural networks (DNNs) based on a\nsignal processing technique for network parameter removal, in the form of\nsynaptic filters that identifies the fragility, robustness and antifragility\ncharacteristics of DNN parameters. Our proposed analysis investigates if the\nDNN performance is impacted negatively, invariantly, or positively on both\nclean and adversarially perturbed test datasets when the DNN undergoes synaptic\nfiltering. We define three \\textit{filtering scores} for quantifying the\nfragility, robustness and antifragility characteristics of DNN parameters based\non the performances for (i) clean dataset, (ii) adversarial dataset, and (iii)\nthe difference in performances of clean and adversarial datasets. We validate\nthe proposed systematic analysis on ResNet-18, ResNet-50, SqueezeNet-v1.1 and\nShuffleNet V2 x1.0 network architectures for MNIST, CIFAR10 and Tiny ImageNet\ndatasets. The filtering scores, for a given network architecture, identify\nnetwork parameters that are invariant in characteristics across different\ndatasets over learning epochs. Vice-versa, for a given dataset, the filtering\nscores identify the parameters that are invariant in characteristics across\ndifferent network architectures. We show that our synaptic filtering method\nimproves the test accuracy of ResNet and ShuffleNet models on adversarial\ndatasets when only the robust and antifragile parameters are selectively\nretrained at any given epoch, thus demonstrating applications of the proposed\nstrategy in improving model robustness.\n","authors":["Chandresh Pravin","Ivan Martino","Giuseppe Nicosia","Varun Ojha"],"pdf_url":"https://arxiv.org/pdf/2312.09821v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09820v1","updated":"2023-12-15T14:18:54Z","published":"2023-12-15T14:18:54Z","title":"On the locality of local neural operator in learning fluid dynamics","summary":"  This paper launches a thorough discussion on the locality of local neural\noperator (LNO), which is the core that enables LNO great flexibility on varied\ncomputational domains in solving transient partial differential equations\n(PDEs). We investigate the locality of LNO by looking into its receptive field\nand receptive range, carrying a main concern about how the locality acts in LNO\ntraining and applications. In a large group of LNO training experiments for\nlearning fluid dynamics, it is found that an initial receptive range compatible\nwith the learning task is crucial for LNO to perform well. On the one hand, an\nover-small receptive range is fatal and usually leads LNO to numerical\noscillation; on the other hand, an over-large receptive range hinders LNO from\nachieving the best accuracy. We deem rules found in this paper general when\napplying LNO to learn and solve transient PDEs in diverse fields. Practical\nexamples of applying the pre-trained LNOs in flow prediction are presented to\nconfirm the findings further. Overall, with the architecture properly designed\nwith a compatible receptive range, the pre-trained LNO shows commendable\naccuracy and efficiency in solving practical cases.\n","authors":["Ximeng Ye","Hongyu Li","Jingjie Huang","Guoliang Qin"],"pdf_url":"https://arxiv.org/pdf/2312.09820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09817v1","updated":"2023-12-15T14:17:16Z","published":"2023-12-15T14:17:16Z","title":"Calibrated One Round Federated Learning with Bayesian Inference in the\n  Predictive Space","summary":"  Federated Learning (FL) involves training a model over a dataset distributed\namong clients, with the constraint that each client's dataset is localized and\npossibly heterogeneous. In FL, small and noisy datasets are common,\nhighlighting the need for well-calibrated models that represent the uncertainty\nof predictions. The closest FL techniques to achieving such goals are the\nBayesian FL methods which collect parameter samples from local posteriors, and\naggregate them to approximate the global posterior. To improve scalability for\nlarger models, one common Bayesian approach is to approximate the global\npredictive posterior by multiplying local predictive posteriors. In this work,\nwe demonstrate that this method gives systematically overconfident predictions,\nand we remedy this by proposing $\\beta$-Predictive Bayes, a Bayesian FL\nalgorithm that interpolates between a mixture and product of the predictive\nposteriors, using a tunable parameter $\\beta$. This parameter is tuned to\nimprove the global ensemble's calibration, before it is distilled to a single\nmodel. Our method is evaluated on a variety of regression and classification\ndatasets to demonstrate its superiority in calibration to other baselines, even\nas data heterogeneity increases. Code available at\nhttps://github.com/hasanmohsin/betaPredBayes_FL\n","authors":["Mohsin Hasan","Guojun Zhang","Kaiyang Guo","Xi Chen","Pascal Poupart"],"pdf_url":"https://arxiv.org/pdf/2312.09817v1.pdf","comment":"7 pages, 2 figures. To appear at AAAI 2024"},{"id":"http://arxiv.org/abs/2312.09806v1","updated":"2023-12-15T14:04:23Z","published":"2023-12-15T14:04:23Z","title":"Improving Biomedical Entity Linking with Retrieval-enhanced Learning","summary":"  Biomedical entity linking (BioEL) has achieved remarkable progress with the\nhelp of pre-trained language models. However, existing BioEL methods usually\nstruggle to handle rare and difficult entities due to long-tailed distribution.\nTo address this limitation, we introduce a new scheme $k$NN-BioEL, which\nprovides a BioEL model with the ability to reference similar instances from the\nentire training corpus as clues for prediction, thus improving the\ngeneralization capabilities. Moreover, we design a contrastive learning\nobjective with dynamic hard negative sampling (DHNS) that improves the quality\nof the retrieved neighbors during inference. Extensive experimental results\nshow that $k$NN-BioEL outperforms state-of-the-art baselines on several\ndatasets.\n","authors":["Zhenxi Lin","Ziheng Zhang","Xian Wu","Yefeng Zheng"],"pdf_url":"https://arxiv.org/pdf/2312.09806v1.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.09802v1","updated":"2023-12-15T14:01:56Z","published":"2023-12-15T14:01:56Z","title":"Concept Prerequisite Relation Prediction by Using\n  Permutation-Equivariant Directed Graph Neural Networks","summary":"  This paper studies the problem of CPRP, concept prerequisite relation\nprediction, which is a fundamental task in using AI for education. CPRP is\nusually formulated into a link-prediction task on a relationship graph of\nconcepts and solved by training the graph neural network (GNN) model. However,\ncurrent directed GNNs fail to manage graph isomorphism which refers to the\ninvariance of non-isomorphic graphs, reducing the expressivity of resulting\nrepresentations. We present a permutation-equivariant directed GNN model by\nintroducing the Weisfeiler-Lehman test into directed GNN learning. Our method\nis then used for CPRP and evaluated on three public datasets. The experimental\nresults show that our model delivers better prediction performance than the\nstate-of-the-art methods.\n","authors":["Xiran Qu","Xuequn Shang","Yupei Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.09802v1.pdf","comment":"9 pages, 1figure, 1 Table, Accepted by AAAI Workshop for AI for\n  Education"},{"id":"http://arxiv.org/abs/2312.09797v1","updated":"2023-12-15T13:54:48Z","published":"2023-12-15T13:54:48Z","title":"Part Representation Learning with Teacher-Student Decoder for Occluded\n  Person Re-identification","summary":"  Occluded person re-identification (ReID) is a very challenging task due to\nthe occlusion disturbance and incomplete target information. Leveraging\nexternal cues such as human pose or parsing to locate and align part features\nhas been proven to be very effective in occluded person ReID. Meanwhile, recent\nTransformer structures have a strong ability of long-range modeling.\nConsidering the above facts, we propose a Teacher-Student Decoder (TSD)\nframework for occluded person ReID, which utilizes the Transformer decoder with\nthe help of human parsing. More specifically, our proposed TSD consists of a\nParsing-aware Teacher Decoder (PTD) and a Standard Student Decoder (SSD). PTD\nemploys human parsing cues to restrict Transformer's attention and imparts this\ninformation to SSD through feature distillation. Thereby, SSD can learn from\nPTD to aggregate information of body parts automatically. Moreover, a mask\ngenerator is designed to provide discriminative regions for better ReID. In\naddition, existing occluded person ReID benchmarks utilize occluded samples as\nqueries, which will amplify the role of alleviating occlusion interference and\nunderestimate the impact of the feature absence issue. Contrastively, we\npropose a new benchmark with non-occluded queries, serving as a complement to\nthe existing benchmark. Extensive experiments demonstrate that our proposed\nmethod is superior and the new benchmark is essential. The source codes are\navailable at https://github.com/hh23333/TSD.\n","authors":["Shang Gao","Chenyang Yu","Pingping Zhang","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2312.09797v1.pdf","comment":"Accepted by ICASSP2024"},{"id":"http://arxiv.org/abs/2211.11727v4","updated":"2023-12-15T13:53:14Z","published":"2022-11-21T18:47:11Z","title":"Parametric Classification for Generalized Category Discovery: A Baseline\n  Study","summary":"  Generalized Category Discovery (GCD) aims to discover novel categories in\nunlabelled datasets using knowledge learned from labelled samples. Previous\nstudies argued that parametric classifiers are prone to overfitting to seen\ncategories, and endorsed using a non-parametric classifier formed with\nsemi-supervised k-means. However, in this study, we investigate the failure of\nparametric classifiers, verify the effectiveness of previous design choices\nwhen high-quality supervision is available, and identify unreliable\npseudo-labels as a key problem. We demonstrate that two prediction biases\nexist: the classifier tends to predict seen classes more often, and produces an\nimbalanced distribution across seen and novel categories. Based on these\nfindings, we propose a simple yet effective parametric classification method\nthat benefits from entropy regularisation, achieves state-of-the-art\nperformance on multiple GCD benchmarks and shows strong robustness to unknown\nclass numbers. We hope the investigation and proposed simple framework can\nserve as a strong baseline to facilitate future studies in this field. Our code\nis available at: https://github.com/CVMI-Lab/SimGCD.\n","authors":["Xin Wen","Bingchen Zhao","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2211.11727v4.pdf","comment":"v3: ICCV'23 version; v4: updated the dataset table"},{"id":"http://arxiv.org/abs/2312.09793v1","updated":"2023-12-15T13:49:29Z","published":"2023-12-15T13:49:29Z","title":"PAC-Bayes Generalisation Bounds for Dynamical Systems Including Stable\n  RNNs","summary":"  In this paper, we derive a PAC-Bayes bound on the generalisation gap, in a\nsupervised time-series setting for a special class of discrete-time non-linear\ndynamical systems. This class includes stable recurrent neural networks (RNN),\nand the motivation for this work was its application to RNNs. In order to\nachieve the results, we impose some stability constraints, on the allowed\nmodels. Here, stability is understood in the sense of dynamical systems. For\nRNNs, these stability conditions can be expressed in terms of conditions on the\nweights. We assume the processes involved are essentially bounded and the loss\nfunctions are Lipschitz. The proposed bound on the generalisation gap depends\non the mixing coefficient of the data distribution, and the essential supremum\nof the data. Furthermore, the bound converges to zero as the dataset size\nincreases. In this paper, we 1) formalize the learning problem, 2) derive a\nPAC-Bayesian error bound for such systems, 3) discuss various consequences of\nthis error bound, and 4) show an illustrative example, with discussions on\ncomputing the proposed bound. Unlike other available bounds the derived bound\nholds for non i.i.d. data (time-series) and it does not grow with the number of\nsteps of the RNN.\n","authors":["Deividas Eringis","John Leth","Zheng-Hua Tan","Rafal Wisniewski","Mihaly Petreczky"],"pdf_url":"https://arxiv.org/pdf/2312.09793v1.pdf","comment":"Accepted to AAAI2024 conference"},{"id":"http://arxiv.org/abs/2312.09792v1","updated":"2023-12-15T13:48:55Z","published":"2023-12-15T13:48:55Z","title":"Latent Diffusion Models with Image-Derived Annotations for Enhanced\n  AI-Assisted Cancer Diagnosis in Histopathology","summary":"  Artificial Intelligence (AI) based image analysis has an immense potential to\nsupport diagnostic histopathology, including cancer diagnostics. However,\ndeveloping supervised AI methods requires large-scale annotated datasets. A\npotentially powerful solution is to augment training data with synthetic data.\nLatent diffusion models, which can generate high-quality, diverse synthetic\nimages, are promising. However, the most common implementations rely on\ndetailed textual descriptions, which are not generally available in this\ndomain. This work proposes a method that constructs structured textual prompts\nfrom automatically extracted image features. We experiment with the PCam\ndataset, composed of tissue patches only loosely annotated as healthy or\ncancerous. We show that including image-derived features in the prompt, as\nopposed to only healthy and cancerous labels, improves the Fr\\'echet Inception\nDistance (FID) from 178.8 to 90.2. We also show that pathologists find it\nchallenging to detect synthetic images, with a median sensitivity/specificity\nof 0.55/0.55. Finally, we show that synthetic data effectively trains AI\nmodels.\n","authors":["Pedro Osorio","Guillermo Jimenez-Perez","Javier Montalt-Tordera","Jens Hooge","Guillem Duran-Ballester","Shivam Singh","Moritz Radbruch","Ute Bach","Sabrina Schroeder","Krystyna Siudak","Julia Vienenkoetter","Bettina Lawrenz","Sadegh Mohammadi"],"pdf_url":"https://arxiv.org/pdf/2312.09792v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09790v1","updated":"2023-12-15T13:47:16Z","published":"2023-12-15T13:47:16Z","title":"End-to-End Training of Neural Networks for Automotive Radar Interference\n  Mitigation","summary":"  In this paper we propose a new method for training neural networks (NNs) for\nfrequency modulated continuous wave (FMCW) radar mutual interference\nmitigation. Instead of training NNs to regress from interfered to clean radar\nsignals as in previous work, we train NNs directly on object detection maps. We\ndo so by performing a continuous relaxation of the cell-averaging constant\nfalse alarm rate (CA-CFAR) peak detector, which is a well-established algorithm\nfor object detection using radar. With this new training objective we are able\nto increase object detection performance by a large margin. Furthermore, we\nintroduce separable convolution kernels to strongly reduce the number of\nparameters and computational complexity of convolutional NN architectures for\nradar applications. We validate our contributions with experiments on\nreal-world measurement data and compare them against signal processing\ninterference mitigation methods.\n","authors":["Christian Oswald","Mate Toth","Paul Meissner","Franz Pernkopf"],"pdf_url":"https://arxiv.org/pdf/2312.09790v1.pdf","comment":"2023 IEEE International Radar Conference (RADAR), 6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2312.09789v1","updated":"2023-12-15T13:44:54Z","published":"2023-12-15T13:44:54Z","title":"Optimization meets Machine Learning: An Exact Algorithm for\n  Semi-Supervised Support Vector Machines","summary":"  Support vector machines (SVMs) are well-studied supervised learning models\nfor binary classification. In many applications, large amounts of samples can\nbe cheaply and easily obtained. What is often a costly and error-prone process\nis to manually label these instances. Semi-supervised support vector machines\n(S3VMs) extend the well-known SVM classifiers to the semi-supervised approach,\naiming at maximizing the margin between samples in the presence of unlabeled\ndata. By leveraging both labeled and unlabeled data, S3VMs attempt to achieve\nbetter accuracy and robustness compared to traditional SVMs. Unfortunately, the\nresulting optimization problem is non-convex and hence difficult to solve\nexactly. In this paper, we present a new branch-and-cut approach for S3VMs\nusing semidefinite programming (SDP) relaxations. We apply optimality-based\nbound tightening to bound the feasible set. Box constraints allow us to include\nvalid inequalities, strengthening the lower bound. The resulting SDP relaxation\nprovides bounds significantly stronger than the ones available in the\nliterature. For the upper bound, instead, we define a local search exploiting\nthe solution of the SDP relaxation. Computational results highlight the\nefficiency of the algorithm, showing its capability to solve instances with a\nnumber of data points 10 times larger than the ones solved in the literature.\n","authors":["Veronica Piccialli","Jan Schwiddessen","Antonio M. Sudoso"],"pdf_url":"https://arxiv.org/pdf/2312.09789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09788v1","updated":"2023-12-15T13:43:24Z","published":"2023-12-15T13:43:24Z","title":"Collaborating Foundation models for Domain Generalized Semantic\n  Segmentation","summary":"  Domain Generalized Semantic Segmentation (DGSS) deals with training a model\non a labeled source domain with the aim of generalizing to unseen domains\nduring inference. Existing DGSS methods typically effectuate robust features by\nmeans of Domain Randomization (DR). Such an approach is often limited as it can\nonly account for style diversification and not content. In this work, we take\nan orthogonal approach to DGSS and propose to use an assembly of CoLlaborative\nFOUndation models for Domain Generalized Semantic Segmentation (CLOUDS). In\ndetail, CLOUDS is a framework that integrates FMs of various kinds: (i) CLIP\nbackbone for its robust feature representation, (ii) generative models to\ndiversify the content, thereby covering various modes of the possible target\ndistribution, and (iii) Segment Anything Model (SAM) for iteratively refining\nthe predictions of the segmentation model. Extensive experiments show that our\nCLOUDS excels in adapting from synthetic to real DGSS benchmarks and under\nvarying weather conditions, notably outperforming prior methods by 5.6% and\n6.7% on averaged miou, respectively. The code is available at :\nhttps://github.com/yasserben/CLOUDS\n","authors":["Yasser Benigmim","Subhankar Roy","Slim Essid","Vicky Kalogeiton","Stéphane Lathuilière"],"pdf_url":"https://arxiv.org/pdf/2312.09788v1.pdf","comment":"https://github.com/yasserben/CLOUDS"},{"id":"http://arxiv.org/abs/2312.09787v1","updated":"2023-12-15T13:41:20Z","published":"2023-12-15T13:41:20Z","title":"Physics-informed Neural Network Estimation of Material Properties in\n  Soft Tissue Nonlinear Biomechanical Models","summary":"  The development of biophysical models for clinical applications is rapidly\nadvancing in the research community, thanks to their predictive nature and\ntheir ability to assist the interpretation of clinical data. However,\nhigh-resolution and accurate multi-physics computational models are\ncomputationally expensive and their personalisation involves fine calibration\nof a large number of parameters, which may be space-dependent, challenging\ntheir clinical translation. In this work, we propose a new approach which\nrelies on the combination of physics-informed neural networks (PINNs) with\nthree-dimensional soft tissue nonlinear biomechanical models, capable of\nreconstructing displacement fields and estimating heterogeneous\npatient-specific biophysical properties. The proposed learning algorithm\nencodes information from a limited amount of displacement and, in some cases,\nstrain data, that can be routinely acquired in the clinical setting, and\ncombines it with the physics of the problem, represented by a mathematical\nmodel based on partial differential equations, to regularise the problem and\nimprove its convergence properties. Several benchmarks are presented to show\nthe accuracy and robustness of the proposed method and its great potential to\nenable the robust and effective identification of patient-specific,\nheterogeneous physical properties, s.a. tissue stiffness properties. In\nparticular, we demonstrate the capability of the PINN to detect the presence,\nlocation and severity of scar tissue, which is beneficial to develop\npersonalised simulation models for disease diagnosis, especially for cardiac\napplications.\n","authors":["Federica Caforio","Francesco Regazzoni","Stefano Pagani","Elias Karabelas","Christoph Augustin","Gundolf Haase","Gernot Plank","Alfio Quarteroni"],"pdf_url":"https://arxiv.org/pdf/2312.09787v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.16599v2","updated":"2023-12-15T13:37:42Z","published":"2023-08-31T09:57:52Z","title":"Using machine learning to understand causal relationships between urban\n  form and travel CO2 emissions across continents","summary":"  Climate change mitigation in urban mobility requires policies reconfiguring\nurban form to increase accessibility and facilitate low-carbon modes of\ntransport. However, current policy research has insufficiently assessed urban\nform effects on car travel at three levels: (1) Causality -- Can causality be\nestablished beyond theoretical and correlation-based analyses? (2)\nGeneralizability -- Do relationships hold across different cities and world\nregions? (3) Context specificity -- How do relationships vary across\nneighborhoods of a city? Here, we address all three gaps via causal graph\ndiscovery and explainable machine learning to detect urban form effects on\nintra-city car travel, based on mobility data of six cities across three\ncontinents. We find significant causal effects of urban form on trip emissions\nand inter-feature effects, which had been neglected in previous work. Our\nresults demonstrate that destination accessibility matters most overall, while\nlow density and low connectivity also sharply increase CO$_2$ emissions. These\ngeneral trends are similar across cities but we find idiosyncratic effects that\ncan lead to substantially different recommendations. In more monocentric\ncities, we identify spatial corridors -- about 10--50 km from the city center\n-- where subcenter-oriented development is more relevant than increased access\nto the main center. Our work demonstrates a novel application of machine\nlearning that enables new research addressing the needs of causality,\ngeneralizability, and contextual specificity for scaling evidence-based urban\nclimate solutions.\n","authors":["Felix Wagner","Florian Nachtigall","Lukas Franken","Nikola Milojevic-Dupont","Rafael H. M. Pereira","Nicolas Koch","Jakob Runge","Marta Gonzalez","Felix Creutzig"],"pdf_url":"https://arxiv.org/pdf/2308.16599v2.pdf","comment":"32 pages, 24 figures, 6 tables"},{"id":"http://arxiv.org/abs/2303.01125v2","updated":"2023-12-15T13:37:19Z","published":"2023-03-02T10:09:11Z","title":"Distilling Multi-Level X-vector Knowledge for Small-footprint Speaker\n  Verification","summary":"  Even though deep speaker models have demonstrated impressive accuracy in\nspeaker verification tasks, this often comes at the expense of increased model\nsize and computation time, presenting challenges for deployment in\nresource-constrained environments. Our research focuses on addressing this\nlimitation through the development of small footprint deep speaker embedding\nextraction using knowledge distillation. While previous work in this domain has\nconcentrated on speaker embedding extraction at the utterance level, our\napproach involves amalgamating embeddings from different levels of the x-vector\nmodel (teacher network) to train a compact student network. The results\nhighlight the significance of frame-level information, with the student models\nexhibiting a remarkable size reduction of 85%-91% compared to their teacher\ncounterparts, depending on the size of the teacher embeddings. Notably, by\nconcatenating teacher embeddings, we achieve student networks that maintain\ncomparable performance to the teacher while enjoying a substantial 75%\nreduction in model size. These findings and insights extend to other x-vector\nvariants, underscoring the broad applicability of our approach.\n","authors":["Xuechen Liu","Md Sahidullah","Tomi Kinnunen"],"pdf_url":"https://arxiv.org/pdf/2303.01125v2.pdf","comment":"Submitted to IEEE Transactions on Consumer Electronics at Dec. 2023.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2312.09783v1","updated":"2023-12-15T13:36:54Z","published":"2023-12-15T13:36:54Z","title":"Keep the Faith: Faithful Explanations in Convolutional Neural Networks\n  for Case-Based Reasoning","summary":"  Explaining predictions of black-box neural networks is crucial when applied\nto decision-critical tasks. Thus, attribution maps are commonly used to\nidentify important image regions, despite prior work showing that humans prefer\nexplanations based on similar examples. To this end, ProtoPNet learns a set of\nclass-representative feature vectors (prototypes) for case-based reasoning.\nDuring inference, similarities of latent features to prototypes are linearly\nclassified to form predictions and attribution maps are provided to explain the\nsimilarity. In this work, we evaluate whether architectures for case-based\nreasoning fulfill established axioms required for faithful explanations using\nthe example of ProtoPNet. We show that such architectures allow the extraction\nof faithful explanations. However, we prove that the attribution maps used to\nexplain the similarities violate the axioms. We propose a new procedure to\nextract explanations for trained ProtoPNets, named ProtoPFaith. Conceptually,\nthese explanations are Shapley values, calculated on the similarity scores of\neach prototype. They allow to faithfully answer which prototypes are present in\nan unseen image and quantify each pixel's contribution to that presence,\nthereby complying with all axioms. The theoretical violations of ProtoPNet\nmanifest in our experiments on three datasets (CUB-200-2011, Stanford Dogs,\nRSNA) and five architectures (ConvNet, ResNet, ResNet50, WideResNet50,\nResNeXt50). Our experiments show a qualitative difference between the\nexplanations given by ProtoPNet and ProtoPFaith. Additionally, we quantify the\nexplanations with the Area Over the Perturbation Curve, on which ProtoPFaith\noutperforms ProtoPNet on all experiments by a factor $>10^3$.\n","authors":["Tom Nuno Wolf","Fabian Bongratz","Anne-Marie Rickmann","Sebastian Pölsterl","Christian Wachinger"],"pdf_url":"https://arxiv.org/pdf/2312.09783v1.pdf","comment":"To be published in proceedings of AAAI Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2312.09778v1","updated":"2023-12-15T13:30:04Z","published":"2023-12-15T13:30:04Z","title":"Hypergraph-MLP: Learning on Hypergraphs without Message Passing","summary":"  Hypergraphs are vital in modelling data with higher-order relations\ncontaining more than two entities, gaining prominence in machine learning and\nsignal processing. Many hypergraph neural networks leverage message passing\nover hypergraph structures to enhance node representation learning, yielding\nimpressive performances in tasks like hypergraph node classification. However,\nthese message-passing-based models face several challenges, including\noversmoothing as well as high latency and sensitivity to structural\nperturbations at inference time. To tackle those challenges, we propose an\nalternative approach where we integrate the information about hypergraph\nstructures into training supervision without explicit message passing, thus\nalso removing the reliance on it at inference. Specifically, we introduce\nHypergraph-MLP, a novel learning framework for hypergraph-structured data,\nwhere the learning model is a straightforward multilayer perceptron (MLP)\nsupervised by a loss function based on a notion of signal smoothness on\nhypergraphs. Experiments on hypergraph node classification tasks demonstrate\nthat Hypergraph-MLP achieves competitive performance compared to existing\nbaselines, and is considerably faster and more robust against structural\nperturbations at inference.\n","authors":["Bohan Tang","Siheng Chen","Xiaowen Dong"],"pdf_url":"https://arxiv.org/pdf/2312.09778v1.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.09775v1","updated":"2023-12-15T13:28:42Z","published":"2023-12-15T13:28:42Z","title":"A Comparative Evaluation of Additive Separability Tests for\n  Physics-Informed Machine Learning","summary":"  Many functions characterising physical systems are additively separable. This\nis the case, for instance, of mechanical Hamiltonian functions in physics,\npopulation growth equations in biology, and consumer preference and utility\nfunctions in economics. We consider the scenario in which a surrogate of a\nfunction is to be tested for additive separability. The detection that the\nsurrogate is additively separable can be leveraged to improve further learning.\nHence, it is beneficial to have the ability to test for such separability in\nsurrogates. The mathematical approach is to test if the mixed partial\nderivative of the surrogate is zero; or empirically, lower than a threshold. We\npresent and comparatively and empirically evaluate the eight methods to compute\nthe mixed partial derivative of a surrogate function.\n","authors":["Zi-Yu Khoo","Jonathan Sze Choong Low","Stéphane Bressan"],"pdf_url":"https://arxiv.org/pdf/2312.09775v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.05836v2","updated":"2023-12-15T13:23:43Z","published":"2023-04-11T14:20:31Z","title":"A Game-theoretic Framework for Privacy-preserving Federated Learning","summary":"  In federated learning, benign participants aim to optimize a global model\ncollaboratively. However, the risk of \\textit{privacy leakage} cannot be\nignored in the presence of \\textit{semi-honest} adversaries. Existing research\nhas focused either on designing protection mechanisms or on inventing attacking\nmechanisms. While the battle between defenders and attackers seems\nnever-ending, we are concerned with one critical question: is it possible to\nprevent potential attacks in advance? To address this, we propose the first\ngame-theoretic framework that considers both FL defenders and attackers in\nterms of their respective payoffs, which include computational costs, FL model\nutilities, and privacy leakage risks. We name this game the federated learning\nprivacy game (FLPG), in which neither defenders nor attackers are aware of all\nparticipants' payoffs.\n  To handle the \\textit{incomplete information} inherent in this situation, we\npropose associating the FLPG with an \\textit{oracle} that has two primary\nresponsibilities. First, the oracle provides lower and upper bounds of the\npayoffs for the players. Second, the oracle acts as a correlation device,\nprivately providing suggested actions to each player. With this novel\nframework, we analyze the optimal strategies of defenders and attackers.\nFurthermore, we derive and demonstrate conditions under which the attacker, as\na rational decision-maker, should always follow the oracle's suggestion\n\\textit{not to attack}.\n","authors":["Xiaojin Zhang","Lixin Fan","Siwei Wang","Wenjie Li","Kai Chen","Qiang Yang"],"pdf_url":"https://arxiv.org/pdf/2304.05836v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09766v1","updated":"2023-12-15T13:12:49Z","published":"2023-12-15T13:12:49Z","title":"Celestial Machine Learning: From Data to Mars and Beyond with AI Feynman","summary":"  Can a machine or algorithm discover or learn Kepler's first law from\nastronomical sightings alone? We emulate Johannes Kepler's discovery of the\nequation of the orbit of Mars with the Rudolphine tables using AI Feynman, a\nphysics-inspired tool for symbolic regression.\n","authors":["Zi-Yu Khoo","Abel Yang","Jonathan Sze Choong Low","Stéphane Bressan"],"pdf_url":"https://arxiv.org/pdf/2312.09766v1.pdf","comment":"v1: long version v2: accepted as a short paper"},{"id":"http://arxiv.org/abs/2307.02405v3","updated":"2023-12-15T13:10:19Z","published":"2023-07-05T16:27:33Z","title":"$ν^2$-Flows: Fast and improved neutrino reconstruction in\n  multi-neutrino final states with conditional normalizing flows","summary":"  In this work we introduce $\\nu^2$-Flows, an extension of the $\\nu$-Flows\nmethod to final states containing multiple neutrinos. The architecture can\nnatively scale for all combinations of object types and multiplicities in the\nfinal state for any desired neutrino multiplicities. In $t\\bar{t}$ dilepton\nevents, the momenta of both neutrinos and correlations between them are\nreconstructed more accurately than when using the most popular standard\nanalytical techniques, and solutions are found for all events. Inference time\nis significantly faster than competing methods, and can be reduced further by\nevaluating in parallel on graphics processing units. We apply $\\nu^2$-Flows to\n$t\\bar{t}$ dilepton events and show that the per-bin uncertainties in unfolded\ndistributions is much closer to the limit of performance set by perfect\nneutrino reconstruction than standard techniques. For the chosen double\ndifferential observables $\\nu^2$-Flows results in improved statistical\nprecision for each bin by a factor of 1.5 to 2 in comparison to the Neutrino\nWeighting method and up to a factor of four in comparison to the Ellipse\napproach.\n","authors":["John Andrew Raine","Matthew Leigh","Knut Zoch","Tobias Golling"],"pdf_url":"https://arxiv.org/pdf/2307.02405v3.pdf","comment":"24 pages, 19 figures, 6 tables"},{"id":"http://arxiv.org/abs/2310.03985v2","updated":"2023-12-15T13:07:44Z","published":"2023-10-06T03:04:11Z","title":"Dementia Assessment Using Mandarin Speech with an Attention-based Speech\n  Recognition Encoder","summary":"  Dementia diagnosis requires a series of different testing methods, which is\ncomplex and time-consuming. Early detection of dementia is crucial as it can\nprevent further deterioration of the condition. This paper utilizes a speech\nrecognition model to construct a dementia assessment system tailored for\nMandarin speakers during the picture description task. By training an\nattention-based speech recognition model on voice data closely resembling\nreal-world scenarios, we have significantly enhanced the model's recognition\ncapabilities. Subsequently, we extracted the encoder from the speech\nrecognition model and added a linear layer for dementia assessment. We\ncollected Mandarin speech data from 99 subjects and acquired their clinical\nassessments from a local hospital. We achieved an accuracy of 92.04% in\nAlzheimer's disease detection and a mean absolute error of 9% in clinical\ndementia rating score prediction.\n","authors":["Zih-Jyun Lin","Yi-Ju Chen","Po-Chih Kuo","Likai Huang","Chaur-Jong Hu","Cheng-Yu Chen"],"pdf_url":"https://arxiv.org/pdf/2310.03985v2.pdf","comment":"Accepted to IEEE ICASSP 2024"},{"id":"http://arxiv.org/abs/2206.14284v5","updated":"2023-12-15T13:03:15Z","published":"2022-06-28T20:50:14Z","title":"Optimal Estimation of Generic Dynamics by Path-Dependent Neural Jump\n  ODEs","summary":"  This paper studies the problem of forecasting general stochastic processes\nusing a path-dependent extension of the Neural Jump ODE (NJ-ODE) framework\n\\citep{herrera2021neural}. While NJ-ODE was the first framework to establish\nconvergence guarantees for the prediction of irregularly observed time series,\nthese results were limited to data stemming from It\\^o-diffusions with complete\nobservations, in particular Markov processes, where all coordinates are\nobserved simultaneously. In this work, we generalise these results to generic,\npossibly non-Markovian or discontinuous, stochastic processes with incomplete\nobservations, by utilising the reconstruction properties of the signature\ntransform. These theoretical results are supported by empirical studies, where\nit is shown that the path-dependent NJ-ODE outperforms the original NJ-ODE\nframework in the case of non-Markovian data. Moreover, we show that PD-NJ-ODE\ncan be applied successfully to classical stochastic filtering problems and to\nlimit order book (LOB) data.\n","authors":["Florian Krach","Marc Nübel","Josef Teichmann"],"pdf_url":"https://arxiv.org/pdf/2206.14284v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09758v1","updated":"2023-12-15T12:58:05Z","published":"2023-12-15T12:58:05Z","title":"Diagnosing and Rectifying Fake OOD Invariance: A Restructured Causal\n  Approach","summary":"  Invariant representation learning (IRL) encourages the prediction from\ninvariant causal features to labels de-confounded from the environments,\nadvancing the technical roadmap of out-of-distribution (OOD) generalization.\nDespite spotlights around, recent theoretical results verified that some causal\nfeatures recovered by IRLs merely pretend domain-invariantly in the training\nenvironments but fail in unseen domains. The \\emph{fake invariance} severely\nendangers OOD generalization since the trustful objective can not be diagnosed\nand existing causal surgeries are invalid to rectify. In this paper, we review\na IRL family (InvRat) under the Partially and Fully Informative Invariant\nFeature Structural Causal Models (PIIF SCM /FIIF SCM) respectively, to certify\ntheir weaknesses in representing fake invariant features, then, unify their\ncausal diagrams to propose ReStructured SCM (RS-SCM). RS-SCM can ideally\nrebuild the spurious and the fake invariant features simultaneously. Given\nthis, we further develop an approach based on conditional mutual information\nwith respect to RS-SCM, then rigorously rectify the spurious and fake invariant\neffects. It can be easily implemented by a small feature selection subnet\nintroduced in the IRL family, which is alternatively optimized to achieve our\ngoal. Experiments verified the superiority of our approach to fight against the\nfake invariant issue across a variety of OOD generalization benchmarks.\n","authors":["Ziliang Chen","Yongsen Zheng","Zhao-Rong Lai","Quanlong Guan","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2312.09758v1.pdf","comment":"AAAI-2024"},{"id":"http://arxiv.org/abs/2312.08944v2","updated":"2023-12-15T12:48:25Z","published":"2023-12-14T13:53:56Z","title":"What's Next? Predicting Hamiltonian Dynamics from Discrete Observations\n  of a Vector Field","summary":"  We present several methods for predicting the dynamics of Hamiltonian systems\nfrom discrete observations of their vector field. Each method is either\ninformed or uninformed of the Hamiltonian property. We empirically and\ncomparatively evaluate the methods and observe that information that the system\nis Hamiltonian can be effectively informed, and that different methods strike\ndifferent trade-offs between efficiency and effectiveness for different\ndynamical systems.\n","authors":["Zi-Yu Khoo","Delong Zhang","Stéphane Bressan"],"pdf_url":"https://arxiv.org/pdf/2312.08944v2.pdf","comment":"v1: long paper v2: accepted paper (short paper)"},{"id":"http://arxiv.org/abs/2312.09748v1","updated":"2023-12-15T12:39:27Z","published":"2023-12-15T12:39:27Z","title":"Verification-Friendly Deep Neural Networks","summary":"  Machine learning techniques often lack formal correctness guarantees. This is\nevidenced by the widespread adversarial examples that plague most deep-learning\napplications. This resulted in several research efforts that aim at verifying\ndeep neural networks, with a particular focus on safety-critical applications.\nHowever, formal verification techniques still face major scalability and\nprecision challenges when dealing with the complexity of such networks. The\nover-approximation introduced during the formal verification process to tackle\nthe scalability challenge often results in inconclusive analysis. To address\nthis challenge, we propose a novel framework to generate Verification-friendly\nNeural Networks (VNNs). We present a post-training optimization framework to\nachieve a balance between preserving prediction performance and robustness in\nthe resulting networks. Our proposed framework proves to result in networks\nthat are comparable to the original ones in terms of prediction performance,\nwhile amenable to verification. This essentially enables us to establish\nrobustness for more VNNs than their deep neural network counterparts, in a more\ntime-efficient manner.\n","authors":["Anahita Baninajjar","Ahmed Rezine","Amir Aminifar"],"pdf_url":"https://arxiv.org/pdf/2312.09748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09744v1","updated":"2023-12-15T12:31:35Z","published":"2023-12-15T12:31:35Z","title":"Bridging the Semantic-Numerical Gap: A Numerical Reasoning Method of\n  Cross-modal Knowledge Graph for Material Property Prediction","summary":"  Using machine learning (ML) techniques to predict material properties is a\ncrucial research topic. These properties depend on numerical data and semantic\nfactors. Due to the limitations of small-sample datasets, existing methods\ntypically adopt ML algorithms to regress numerical properties or transfer other\npre-trained knowledge graphs (KGs) to the material. However, these methods\ncannot simultaneously handle semantic and numerical information. In this paper,\nwe propose a numerical reasoning method for material KGs (NR-KG), which\nconstructs a cross-modal KG using semantic nodes and numerical proxy nodes. It\ncaptures both types of information by projecting KG into a canonical KG and\nutilizes a graph neural network to predict material properties. In this\nprocess, a novel projection prediction loss is proposed to extract semantic\nfeatures from numerical information. NR-KG facilitates end-to-end processing of\ncross-modal data, mining relationships and cross-modal information in\nsmall-sample datasets, and fully utilizes valuable experimental data to enhance\nmaterial prediction. We further propose two new High-Entropy Alloys (HEA)\nproperty datasets with semantic descriptions. NR-KG outperforms\nstate-of-the-art (SOTA) methods, achieving relative improvements of 25.9% and\n16.1% on two material datasets. Besides, NR-KG surpasses SOTA methods on two\npublic physical chemistry molecular datasets, showing improvements of 22.2% and\n54.3%, highlighting its potential application and generalizability. We hope the\nproposed datasets, algorithms, and pre-trained models can facilitate the\ncommunities of KG and AI for materials.\n","authors":["Guangxuan Song","Dongmei Fu","Zhongwei Qiu","Zijiang Yang","Jiaxin Dai","Lingwei Ma","Dawei Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.09744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09741v1","updated":"2023-12-15T12:30:30Z","published":"2023-12-15T12:30:30Z","title":"PELP: Pioneer Event Log Prediction Using Sequence-to-Sequence Neural\n  Networks","summary":"  Process mining, a data-driven approach for analyzing, visualizing, and\nimproving business processes using event logs, has emerged as a powerful\ntechnique in the field of business process management. Process forecasting is a\nsub-field of process mining that studies how to predict future processes and\nprocess models. In this paper, we introduce and motivate the problem of event\nlog prediction and present our approach to solving the event log prediction\nproblem, in particular, using the sequence-to-sequence deep learning approach.\nWe evaluate and analyze the prediction outcomes on a variety of synthetic logs\nand seven real-life logs and show that our approach can generate perfect\npredictions on synthetic logs and that deep learning techniques have the\npotential to be applied in real-world event log prediction tasks. We further\nprovide practical recommendations for event log predictions grounded in the\noutcomes of the conducted experiments.\n","authors":["Wenjun Zhou","Artem Polyvyanyy","James Bailey"],"pdf_url":"https://arxiv.org/pdf/2312.09741v1.pdf","comment":"CAiSE 2024 submission"},{"id":"http://arxiv.org/abs/2312.09734v1","updated":"2023-12-15T12:19:48Z","published":"2023-12-15T12:19:48Z","title":"Learning of Hamiltonian Dynamics with Reproducing Kernel Hilbert Spaces","summary":"  This paper presents a method for learning Hamiltonian dynamics from a limited\nset of data points. The Hamiltonian vector field is found by regularized\noptimization over a reproducing kernel Hilbert space of vector fields that are\ninherently Hamiltonian, and where the vector field is required to be odd or\neven. This is done with a symplectic kernel, and it is shown how this\nsymplectic kernel can be modified to be odd or even. The performance of the\nmethod is validated in simulations for two Hamiltonian systems. It is shown\nthat the learned dynamics are Hamiltonian, and that the learned Hamiltonian\nvector field can be prescribed to be odd or even.\n","authors":["Torbjørn Smith","Olav Egeland"],"pdf_url":"https://arxiv.org/pdf/2312.09734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2003.02214v3","updated":"2023-12-15T12:01:20Z","published":"2020-03-04T17:36:00Z","title":"Generic Unsupervised Optimization for a Latent Variable Model With\n  Exponential Family Observables","summary":"  Latent variable models (LVMs) represent observed variables by parameterized\nfunctions of latent variables. Prominent examples of LVMs for unsupervised\nlearning are probabilistic PCA or probabilistic SC which both assume a weighted\nlinear summation of the latents to determine the mean of a Gaussian\ndistribution for the observables. In many cases, however, observables do not\nfollow a Gaussian distribution. For unsupervised learning, LVMs which assume\nspecific non-Gaussian observables have therefore been considered. Already for\nspecific choices of distributions, parameter optimization is challenging and\nonly a few previous contributions considered LVMs with more generally defined\nobservable distributions. Here, we consider LVMs that are defined for a range\nof different distributions, i.e., observables can follow any (regular)\ndistribution of the exponential family. The novel class of LVMs presented is\ndefined for binary latents, and it uses maximization in place of summation to\nlink the latents to observables. To derive an optimization procedure, we follow\nan EM approach for maximum likelihood parameter estimation. We show that a set\nof very concise parameter update equations can be derived which feature the\nsame functional form for all exponential family distributions. The derived\ngeneric optimization can consequently be applied to different types of metric\ndata as well as to different types of discrete data. Also, the derived\noptimization equations can be combined with a recently suggested variational\nacceleration which is likewise generically applicable to the LVMs considered\nhere. So, the combination maintains generic and direct applicability of the\nderived optimization procedure, but, crucially, enables efficient scalability.\nWe numerically verify our analytical results and discuss some potential\napplications such as learning of variance structure, noise type estimation and\ndenoising.\n","authors":["Hamid Mousavi","Jakob Drefs","Florian Hirschberger","Jörg Lücke"],"pdf_url":"https://arxiv.org/pdf/2003.02214v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09108v2","updated":"2023-12-15T11:51:43Z","published":"2023-12-14T16:44:38Z","title":"Greedy Shapley Client Selection for Communication-Efficient Federated\n  Learning","summary":"  The standard client selection algorithms for Federated Learning (FL) are\noften unbiased and involve uniform random sampling of clients. This has been\nproven sub-optimal for fast convergence under practical settings characterized\nby significant heterogeneity in data distribution, computing, and communication\nresources across clients. For applications having timing constraints due to\nlimited communication opportunities with the parameter server (PS), the client\nselection strategy is critical to complete model training within the fixed\nbudget of communication rounds. To address this, we develop a biased client\nselection strategy, GreedyFed, that identifies and greedily selects the most\ncontributing clients in each communication round. This method builds on a fast\napproximation algorithm for the Shapley Value at the PS, making the computation\ntractable for real-world applications with many clients. Compared to various\nclient selection strategies on several real-world datasets, GreedyFed\ndemonstrates fast and stable convergence with high accuracy under timing\nconstraints and when imposing a higher degree of heterogeneity in data\ndistribution, systems constraints, and privacy requirements.\n","authors":["Pranava Singhal","Shashi Raj Pandey","Petar Popovski"],"pdf_url":"https://arxiv.org/pdf/2312.09108v2.pdf","comment":"submitted to IEEE Communication Letters"},{"id":"http://arxiv.org/abs/2211.04806v2","updated":"2023-12-15T11:40:58Z","published":"2022-11-09T11:04:50Z","title":"Machine-Learned Exclusion Limits without Binning","summary":"  Machine-Learned Likelihoods (MLL) combines machine-learning classification\ntechniques with likelihood-based inference tests to estimate the experimental\nsensitivity of high-dimensional data sets. We extend the MLL method by\nincluding Kernel Density Estimators (KDE) to avoid binning the classifier\noutput to extract the resulting one-dimensional signal and background\nprobability density functions. We first test our method on toy models generated\nwith multivariate Gaussian distributions, where the true probability\ndistribution functions are known. Later, we apply the method to two cases of\ninterest at the LHC: a search for exotic Higgs bosons, and a $Z'$ boson\ndecaying into lepton pairs. In contrast to physical-based quantities, the\ntypical fluctuations of the ML outputs give non-smooth probability\ndistributions for pure-signal and pure-background samples. The non-smoothness\nis propagated into the density estimation due to the good performance and\nflexibility of the KDE method. We study its impact on the final significance\ncomputation, and we compare the results using the average of several\nindependent ML output realizations, which allows us to obtain smoother\ndistributions. We conclude that the significance estimation turns out to be not\nsensible to this issue.\n","authors":["Ernesto Arganda","Andres D. Perez","Martin de los Rios","Rosa María Sandá Seoane"],"pdf_url":"https://arxiv.org/pdf/2211.04806v2.pdf","comment":"24 pages, 8 figures, 3 tables, 1 appendix (version published in\n  EPJC). MLL+KDE code available from\n  https://github.com/AndresDanielPerez/2211.04806-ML-Likelihood-with-KDE"},{"id":"http://arxiv.org/abs/2312.09708v1","updated":"2023-12-15T11:30:18Z","published":"2023-12-15T11:30:18Z","title":"GraphRARE: Reinforcement Learning Enhanced Graph Neural Network with\n  Relative Entropy","summary":"  Graph neural networks (GNNs) have shown advantages in graph-based analysis\ntasks. However, most existing methods have the homogeneity assumption and show\npoor performance on heterophilic graphs, where the linked nodes have dissimilar\nfeatures and different class labels, and the semantically related nodes might\nbe multi-hop away. To address this limitation, this paper presents GraphRARE, a\ngeneral framework built upon node relative entropy and deep reinforcement\nlearning, to strengthen the expressive capability of GNNs. An innovative node\nrelative entropy, which considers node features and structural similarity, is\nused to measure mutual information between node pairs. In addition, to avoid\nthe sub-optimal solutions caused by mixing useful information and noises of\nremote nodes, a deep reinforcement learning-based algorithm is developed to\noptimize the graph topology. This algorithm selects informative nodes and\ndiscards noisy nodes based on the defined node relative entropy. Extensive\nexperiments are conducted on seven real-world datasets. The experimental\nresults demonstrate the superiority of GraphRARE in node classification and its\ncapability to optimize the original graph topology.\n","authors":["Tianhao Peng","Wenjun Wu","Haitao Yuan","Zhifeng Bao","Zhao Pengrui","Xin Yu","Xuetao Lin","Yu Liang","Yanjun Pu"],"pdf_url":"https://arxiv.org/pdf/2312.09708v1.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2312.09691v1","updated":"2023-12-15T11:10:34Z","published":"2023-12-15T11:10:34Z","title":"Quilt: Robust Data Segment Selection against Concept Drifts","summary":"  Continuous machine learning pipelines are common in industrial settings where\nmodels are periodically trained on data streams. Unfortunately, concept drifts\nmay occur in data streams where the joint distribution of the data X and label\ny, P(X, y), changes over time and possibly degrade model accuracy. Existing\nconcept drift adaptation approaches mostly focus on updating the model to the\nnew data possibly using ensemble techniques of previous models and tend to\ndiscard the drifted historical data. However, we contend that explicitly\nutilizing the drifted data together leads to much better model accuracy and\npropose Quilt, a data-centric framework for identifying and selecting data\nsegments that maximize model accuracy. To address the potential downside of\nefficiency, Quilt extends existing data subset selection techniques, which can\nbe used to reduce the training data without compromising model accuracy. These\ntechniques cannot be used as is because they only assume virtual drifts where\nthe posterior probabilities P(y|X) are assumed not to change. In contrast, a\nkey challenge in our setup is to also discard undesirable data segments with\nconcept drifts. Quilt thus discards drifted data segments and selects data\nsegment subsets holistically for accurate and efficient model training. The two\noperations use gradient-based scores, which have little computation overhead.\nIn our experiments, we show that Quilt outperforms state-of-the-art drift\nadaptation and data selection baselines on synthetic and real datasets.\n","authors":["Minsu Kim","Seong-Hyeon Hwang","Steven Euijong Whang"],"pdf_url":"https://arxiv.org/pdf/2312.09691v1.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2312.09681v1","updated":"2023-12-15T10:53:09Z","published":"2023-12-15T10:53:09Z","title":"Urban Region Embedding via Multi-View Contrastive Prediction","summary":"  Recently, learning urban region representations utilizing multi-modal data\n(information views) has become increasingly popular, for deep understanding of\nthe distributions of various socioeconomic features in cities. However,\nprevious methods usually blend multi-view information in a posteriors stage,\nfalling short in learning coherent and consistent representations across\ndifferent views. In this paper, we form a new pipeline to learn consistent\nrepresentations across varying views, and propose the multi-view Contrastive\nPrediction model for urban Region embedding (ReCP), which leverages the\nmultiple information views from point-of-interest (POI) and human mobility\ndata. Specifically, ReCP comprises two major modules, namely an intra-view\nlearning module utilizing contrastive learning and feature reconstruction to\ncapture the unique information from each single view, and inter-view learning\nmodule that perceives the consistency between the two views using a contrastive\nprediction learning scheme. We conduct thorough experiments on two downstream\ntasks to assess the proposed model, i.e., land use clustering and region\npopularity prediction. The experimental results demonstrate that our model\noutperforms state-of-the-art baseline methods significantly in urban region\nrepresentation learning.\n","authors":["Zechen Li","Weiming Huang","Kai Zhao","Min Yang","Yongshun Gong","Meng Chen"],"pdf_url":"https://arxiv.org/pdf/2312.09681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02909v2","updated":"2023-12-15T10:52:34Z","published":"2023-11-06T06:40:43Z","title":"Distributed Matrix-Based Sampling for Graph Neural Network Training","summary":"  The primary contribution of this paper is new methods for reducing\ncommunication in the sampling step for distributed GNN training. Here, we\npropose a matrix-based bulk sampling approach that expresses sampling as a\nsparse matrix multiplication (SpGEMM) and samples multiple minibatches at once.\nWhen the input graph topology does not fit on a single device, our method\ndistributes the graph and use communication-avoiding SpGEMM algorithms to scale\nGNN minibatch sampling, enabling GNN training on much larger graphs than those\nthat can fit into a single device memory. When the input graph topology (but\nnot the embeddings) fits in the memory of one GPU, our approach (1) performs\nsampling without communication, (2) amortizes the overheads of sampling a\nminibatch, and (3) can represent multiple sampling algorithms by simply using\ndifferent matrix constructions. In addition to new methods for sampling, we\nshow that judiciously replicating feature data with a simple all-to-all\nexchange can outperform current methods for the feature extraction step in\ndistributed GNN training. We provide experimental results on the largest Open\nGraph Benchmark (OGB) datasets on $128$ GPUs, and show that our pipeline is\n$2.5\\times$ faster Quiver (a distributed extension to PyTorch-Geometric) on a\n$3$-layer GraphSAGE network. On datasets outside of OGB, we show a $8.46\\times$\nspeedup on $128$ GPUs in-per epoch time. Finally, we show scaling when the\ngraph is distributed across GPUs and scaling for both node-wise and layer-wise\nsampling algorithms\n","authors":["Alok Tripathy","Katherine Yelick","Aydin Buluc"],"pdf_url":"https://arxiv.org/pdf/2311.02909v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09674v1","updated":"2023-12-15T10:36:13Z","published":"2023-12-15T10:36:13Z","title":"Optimal Regret Bounds for Collaborative Learning in Bandits","summary":"  We consider regret minimization in a general collaborative multi-agent\nmulti-armed bandit model, in which each agent faces a finite set of arms and\nmay communicate with other agents through a central controller. The optimal arm\nfor each agent in this model is the arm with the largest expected mixed reward,\nwhere the mixed reward of each arm is a weighted average of its rewards across\nall agents, making communication among agents crucial. While near-optimal\nsample complexities for best arm identification are known under this\ncollaborative model, the question of optimal regret remains open. In this work,\nwe address this problem and propose the first algorithm with order optimal\nregret bounds under this collaborative bandit model. Furthermore, we show that\nonly a small constant number of expected communication rounds is needed.\n","authors":["Amitis Shidani","Sattar Vakili"],"pdf_url":"https://arxiv.org/pdf/2312.09674v1.pdf","comment":"Algorithmic Learning Theory (ALT) 2024"},{"id":"http://arxiv.org/abs/2312.09673v1","updated":"2023-12-15T10:35:30Z","published":"2023-12-15T10:35:30Z","title":"Style Generation in Robot Calligraphy with Deep Generative Adversarial\n  Networks","summary":"  Robot calligraphy is an emerging exploration of artificial intelligence in\nthe fields of art and education. Traditional calligraphy generation researches\nmainly focus on methods such as tool-based image processing, generative models,\nand style transfer. Unlike the English alphabet, the number of Chinese\ncharacters is tens of thousands, which leads to difficulties in the generation\nof a style consistent Chinese calligraphic font with over 6000 characters. Due\nto the lack of high-quality data sets, formal definitions of calligraphy\nknowledge, and scientific art evaluation methods, The results generated are\nfrequently of low quality and falls short of professional-level requirements.\nTo address the above problem, this paper proposes an automatic calligraphy\ngeneration model based on deep generative adversarial networks (deepGAN) that\ncan generate style calligraphy fonts with professional standards. The key\nhighlights of the proposed method include: (1) The datasets use a\nhigh-precision calligraphy synthesis method to ensure its high quality and\nsufficient quantity; (2) Professional calligraphers are invited to conduct a\nseries of Turing tests to evaluate the gap between model generation results and\nhuman artistic level; (3) Experimental results indicate that the proposed model\nis the state-of-the-art among current calligraphy generation methods. The\nTuring tests and similarity evaluations validate the effectiveness of the\nproposed method.\n","authors":["Xiaoming Wang","Zhiguo Gong"],"pdf_url":"https://arxiv.org/pdf/2312.09673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12550v3","updated":"2023-12-15T10:29:34Z","published":"2023-11-21T11:59:16Z","title":"Explainable Time Series Anomaly Detection using Masked Latent Generative\n  Modeling","summary":"  We present a novel time series anomaly detection method that achieves\nexcellent detection accuracy while offering a superior level of explainability.\nOur proposed method, TimeVQVAE-AD, leverages masked generative modeling adapted\nfrom the cutting-edge time series generation method known as TimeVQVAE. The\nprior model is trained on the discrete latent space of a time-frequency domain.\nNotably, the dimensional semantics of the time-frequency domain are preserved\nin the latent space, enabling us to compute anomaly scores across different\nfrequency bands, which provides a better insight into the detected anomalies.\nAdditionally, the generative nature of the prior model allows for sampling\nlikely normal states for detected anomalies, enhancing the explainability of\nthe detected anomalies through counterfactuals. Our experimental evaluation on\nthe UCR Time Series Anomaly archive demonstrates that TimeVQVAE-AD\nsignificantly surpasses the existing methods in terms of detection accuracy and\nexplainability. We provide our implementation on GitHub:\n\\url{https://github.com/ML4ITS/TimeVQVAE-AnomalyDetection}.\n","authors":["Daesoo Lee","Sara Malacarne","Erlend Aune"],"pdf_url":"https://arxiv.org/pdf/2311.12550v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09663v1","updated":"2023-12-15T10:23:07Z","published":"2023-12-15T10:23:07Z","title":"Toward Deep Drum Source Separation","summary":"  In the past, the field of drum source separation faced significant challenges\ndue to limited data availability, hindering the adoption of cutting-edge deep\nlearning methods that have found success in other related audio applications.\nIn this manuscript, we introduce StemGMD, a large-scale audio dataset of\nisolated single-instrument drum stems. Each audio clip is synthesized from MIDI\nrecordings of expressive drums performances using ten real-sounding acoustic\ndrum kits. Totaling 1224 hours, StemGMD is the largest audio dataset of drums\nto date and the first to comprise isolated audio clips for every instrument in\na canonical nine-piece drum kit. We leverage StemGMD to develop LarsNet, a\nnovel deep drum source separation model. Through a bank of dedicated U-Nets,\nLarsNet can separate five stems from a stereo drum mixture faster than\nreal-time and is shown to significantly outperform state-of-the-art nonnegative\nspectro-temporal factorization methods.\n","authors":["Alessandro Ilic Mezza","Riccardo Giampiccolo","Alberto Bernardini","Augusto Sarti"],"pdf_url":"https://arxiv.org/pdf/2312.09663v1.pdf","comment":"9 pages, 2 figures. Submitted to Pattern Recognition Letters"},{"id":"http://arxiv.org/abs/2310.03708v3","updated":"2023-12-15T09:58:18Z","published":"2023-10-05T17:35:26Z","title":"Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct\n  Preference Optimization","summary":"  A single language model (LM), despite aligning well with an average labeler\nthrough reinforcement learning from human feedback (RLHF), may not universally\nsuit diverse human preferences. Recent approaches therefore opt for\ncustomization by collecting multi-dimensional feedback and creating distinct\nreward models (RMs) for each dimension (e.g., helpfulness, harmlessness, or\nhonesty). Different LMs can then be optimized for different preferences using\nmulti-objective RLHF (MORLHF) with different reward weightings. Yet, RL\nfine-tuning is unstable and resource-heavy, especially for MORLHF with diverse\nand usually conflicting objectives. In this paper, we present Multi-Objective\nDirect Preference Optimization (MODPO), an RL-free algorithm that extends\nDirect Preference Optimization (DPO) for multiple alignment objectives with\nminimal overheads. Essentially, MODPO folds language modeling directly into\nreward modeling, training LMs as implicit collective reward models (cRMs) that\ncombine all objectives with specific weightings. While theoretically guaranteed\nto produce the same optimal solutions as MORLHF, MODPO is practically more\nstable and computationally efficient. Empirical results from safety alignment\nand long-form question answering confirm that MODPO matches or outperforms\nexisting methods, consistently producing a Pareto front of LMs that cater to\ndiverse preferences with 3 times less computational resources compared to\nMORLHF.\n","authors":["Zhanhui Zhou","Jie Liu","Chao Yang","Jing Shao","Yu Liu","Xiangyu Yue","Wanli Ouyang","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2310.03708v3.pdf","comment":"Multi-Objective Direct Preference Optimization for LLMs"},{"id":"http://arxiv.org/abs/2312.08820v2","updated":"2023-12-15T09:52:51Z","published":"2023-12-14T11:09:50Z","title":"How to Raise a Robot -- A Case for Neuro-Symbolic AI in Constrained Task\n  Planning for Humanoid Assistive Robots","summary":"  Humanoid robots will be able to assist humans in their daily life, in\nparticular due to their versatile action capabilities. However, while these\nrobots need a certain degree of autonomy to learn and explore, they also should\nrespect various constraints, for access control and beyond. We explore the\nnovel field of incorporating privacy, security, and access control constraints\nwith robot task planning approaches. We report preliminary results on the\nclassical symbolic approach, deep-learned neural networks, and modern ideas\nusing large language models as knowledge base. From analyzing their trade-offs,\nwe conclude that a hybrid approach is necessary, and thereby present a new use\ncase for the emerging field of neuro-symbolic artificial intelligence.\n","authors":["Niklas Hemken","Florian Jacob","Fabian Peller-Konrad","Rainer Kartmann","Tamim Asfour","Hannes Hartenstein"],"pdf_url":"https://arxiv.org/pdf/2312.08820v2.pdf","comment":"8 pages, follow-up extended version of our SACMAT 2023 poster\n  abstract: \"Poster: How to Raise a Robot - Beyond Access Control Constraints\n  in Assistive Humanoid Robots\"\n  https://dl.acm.org/doi/abs/10.1145/3589608.3595078"},{"id":"http://arxiv.org/abs/2312.09651v1","updated":"2023-12-15T09:52:17Z","published":"2023-12-15T09:52:17Z","title":"What to Remember: Self-Adaptive Continual Learning for Audio Deepfake\n  Detection","summary":"  The rapid evolution of speech synthesis and voice conversion has raised\nsubstantial concerns due to the potential misuse of such technology, prompting\na pressing need for effective audio deepfake detection mechanisms. Existing\ndetection models have shown remarkable success in discriminating known deepfake\naudio, but struggle when encountering new attack types. To address this\nchallenge, one of the emergent effective approaches is continual learning. In\nthis paper, we propose a continual learning approach called Radian Weight\nModification (RWM) for audio deepfake detection. The fundamental concept\nunderlying RWM involves categorizing all classes into two groups: those with\ncompact feature distributions across tasks, such as genuine audio, and those\nwith more spread-out distributions, like various types of fake audio. These\ndistinctions are quantified by means of the in-class cosine distance, which\nsubsequently serves as the basis for RWM to introduce a trainable gradient\nmodification direction for distinct data types. Experimental evaluations\nagainst mainstream continual learning methods reveal the superiority of RWM in\nterms of knowledge acquisition and mitigating forgetting in audio deepfake\ndetection. Furthermore, RWM's applicability extends beyond audio deepfake\ndetection, demonstrating its potential significance in diverse machine learning\ndomains such as image recognition.\n","authors":["Xiaohui Zhang","Jiangyan Yi","Chenglong Wang","Chuyuan Zhang","Siding Zeng","Jianhua Tao"],"pdf_url":"https://arxiv.org/pdf/2312.09651v1.pdf","comment":"Accepted by the main track The 38th Annual AAAI Conference on\n  Artificial Intelligence (AAAI 2024)"},{"id":"http://arxiv.org/abs/2308.10822v2","updated":"2023-12-15T09:30:43Z","published":"2023-08-14T03:20:28Z","title":"A Novel Ehanced Move Recognition Algorithm Based on Pre-trained Models\n  with Positional Embeddings","summary":"  The recognition of abstracts is crucial for effectively locating the content\nand clarifying the article. Existing move recognition algorithms lack the\nability to learn word position information to obtain contextual semantics. This\npaper proposes a novel enhanced move recognition algorithm with an improved\npre-trained model and a gated network with attention mechanism for unstructured\nabstracts of Chinese scientific and technological papers. The proposed\nalgorithm first performs summary data segmentation and vocabulary training. The\nEP-ERNIE$\\_$AT-GRU framework is leveraged to incorporate word positional\ninformation, facilitating deep semantic learning and targeted feature\nextraction. Experimental results demonstrate that the proposed algorithm\nachieves 13.37$\\%$ higher accuracy on the split dataset than on the original\ndataset and a 7.55$\\%$ improvement in accuracy over the basic comparison model.\n","authors":["Hao Wen","Jie Wang","Xiaodong Qiao"],"pdf_url":"https://arxiv.org/pdf/2308.10822v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09639v1","updated":"2023-12-15T09:28:40Z","published":"2023-12-15T09:28:40Z","title":"Multiple Instance Learning for Uplift Modeling","summary":"  Uplift modeling is widely used in performance marketing to estimate effects\nof promotion campaigns (e.g., increase of customer retention rate). Since it is\nimpossible to observe outcomes of a recipient in treatment (e.g., receiving a\ncertain promotion) and control (e.g., without promotion) groups simultaneously\n(i.e., counter-factual), uplift models are mainly trained on instances of\ntreatment and control groups separately to form two models respectively, and\nuplifts are predicted by the difference of predictions from these two models\n(i.e., two-model method). When responses are noisy and the treatment effect is\nfractional, induced individual uplift predictions will be inaccurate, resulting\nin targeting undesirable customers. Though it is impossible to obtain the ideal\nground-truth individual uplifts, known as Individual Treatment Effects (ITEs),\nalternatively, an average uplift of a group of users, called Average Treatment\nEffect (ATE), can be observed from experimental deliveries. Upon this, similar\nto Multiple Instance Learning (MIL) in which each training sample is a bag of\ninstances, our framework sums up individual user uplift predictions for each\nbag of users as its bag-wise ATE prediction, and regularizes it to its ATE\nlabel, thus learning more accurate individual uplifts. Additionally, to amplify\nthe fractional treatment effect, bags are composed of instances with adjacent\nindividual uplift predictions, instead of random instances. Experiments\nconducted on two datasets show the effectiveness and universality of the\nproposed framework.\n","authors":["Yao Zhao","Haipeng Zhang","Shiwei Lyu","Ruiying Jiang","Jinjie Gu","Guannan Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.09639v1.pdf","comment":"short paper of CIKM22(full version)"},{"id":"http://arxiv.org/abs/2312.09636v1","updated":"2023-12-15T09:25:48Z","published":"2023-12-15T09:25:48Z","title":"A Malware Classification Survey on Adversarial Attacks and Defences","summary":"  As the number and complexity of malware attacks continue to increase, there\nis an urgent need for effective malware detection systems. While deep learning\nmodels are effective at detecting malware, they are vulnerable to adversarial\nattacks. Attacks like this can create malicious files that are resistant to\ndetection, creating a significant cybersecurity risk. Recent research has seen\nthe development of several adversarial attack and response approaches aiming at\nstrengthening deep learning models' resilience to such attacks. This survey\nstudy offers an in-depth look at current research in adversarial attack and\ndefensive strategies for malware classification in cybersecurity. The methods\nare classified into four categories: generative models, feature-based\napproaches, ensemble methods, and hybrid tactics. The article outlines\ncutting-edge procedures within each area, assessing their benefits and\ndrawbacks. Each topic presents cutting-edge approaches and explores their\nadvantages and disadvantages. In addition, the study discusses the datasets and\nassessment criteria that are often utilized on this subject. Finally, it\nidentifies open research difficulties and suggests future study options. This\ndocument is a significant resource for malware categorization and cyber\nsecurity researchers and practitioners.\n","authors":["Mahesh Datta Sai Ponnuru","Likhitha Amasala","Tanu Sree Bhimavarapu","Guna Chaitanya Garikipati"],"pdf_url":"https://arxiv.org/pdf/2312.09636v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09634v1","updated":"2023-12-15T09:23:56Z","published":"2023-12-15T09:23:56Z","title":"Vectorizing string entries for data processing on tables: when are\n  larger language models better?","summary":"  There are increasingly efficient data processing pipelines that work on\nvectors of numbers, for instance most machine learning models, or vector\ndatabases for fast similarity search. These require converting the data to\nnumbers. While this conversion is easy for simple numerical and categorical\nentries, databases are strife with text entries, such as names or descriptions.\nIn the age of large language models, what's the best strategies to vectorize\ntables entries, baring in mind that larger models entail more operational\ncomplexity? We study the benefits of language models in 14 analytical tasks on\ntables while varying the training size, as well as for a fuzzy join benchmark.\nWe introduce a simple characterization of a column that reveals two settings:\n1) a dirty categories setting, where strings share much similarities across\nentries, and conversely 2) a diverse entries setting. For dirty categories,\npretrained language models bring little-to-no benefit compared to simpler\nstring models. For diverse entries, we show that larger language models improve\ndata processing. For these we investigate the complexity-performance tradeoffs\nand show that they reflect those of classic text embedding: larger models tend\nto perform better, but it is useful to fine tune them for embedding purposes.\n","authors":["Léo Grinsztajn","Edouard Oyallon","Myung Jun Kim","Gaël Varoquaux"],"pdf_url":"https://arxiv.org/pdf/2312.09634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13030v3","updated":"2023-12-15T09:20:57Z","published":"2023-05-22T13:33:37Z","title":"Adaptive action supervision in reinforcement learning from real-world\n  multi-agent demonstrations","summary":"  Modeling of real-world biological multi-agents is a fundamental problem in\nvarious scientific and engineering fields. Reinforcement learning (RL) is a\npowerful framework to generate flexible and diverse behaviors in cyberspace;\nhowever, when modeling real-world biological multi-agents, there is a domain\ngap between behaviors in the source (i.e., real-world data) and the target\n(i.e., cyberspace for RL), and the source environment parameters are usually\nunknown. In this paper, we propose a method for adaptive action supervision in\nRL from real-world demonstrations in multi-agent scenarios. We adopt an\napproach that combines RL and supervised learning by selecting actions of\ndemonstrations in RL based on the minimum distance of dynamic time warping for\nutilizing the information of the unknown source dynamics. This approach can be\neasily applied to many existing neural network architectures and provide us\nwith an RL model balanced between reproducibility as imitation and\ngeneralization ability to obtain rewards in cyberspace. In the experiments,\nusing chase-and-escape and football tasks with the different dynamics between\nthe unknown source and target environments, we show that our approach achieved\na balance between the reproducibility and the generalization ability compared\nwith the baselines. In particular, we used the tracking data of professional\nfootball players as expert demonstrations in football and show successful\nperformances despite the larger gap between behaviors in the source and target\nenvironments than the chase-and-escape task.\n","authors":["Keisuke Fujii","Kazushi Tsutsui","Atom Scott","Hiroshi Nakahara","Naoya Takeishi","Yoshinobu Kawahara"],"pdf_url":"https://arxiv.org/pdf/2305.13030v3.pdf","comment":"14 pages, 5 figures, accepted in ICAART 2024 Oral"},{"id":"http://arxiv.org/abs/2312.09627v1","updated":"2023-12-15T09:10:05Z","published":"2023-12-15T09:10:05Z","title":"TF-CLIP: Learning Text-free CLIP for Video-based Person\n  Re-Identification","summary":"  Large-scale language-image pre-trained models (e.g., CLIP) have shown\nsuperior performances on many cross-modal retrieval tasks. However, the problem\nof transferring the knowledge learned from such models to video-based person\nre-identification (ReID) has barely been explored. In addition, there is a lack\nof decent text descriptions in current ReID benchmarks. To address these\nissues, in this work, we propose a novel one-stage text-free CLIP-based\nlearning framework named TF-CLIP for video-based person ReID. More\nspecifically, we extract the identity-specific sequence feature as the\nCLIP-Memory to replace the text feature. Meanwhile, we design a\nSequence-Specific Prompt (SSP) module to update the CLIP-Memory online. To\ncapture temporal information, we further propose a Temporal Memory Diffusion\n(TMD) module, which consists of two key components: Temporal Memory\nConstruction (TMC) and Memory Diffusion (MD). Technically, TMC allows the\nframe-level memories in a sequence to communicate with each other, and to\nextract temporal information based on the relations within the sequence. MD\nfurther diffuses the temporal memories to each token in the original features\nto obtain more robust sequence features. Extensive experiments demonstrate that\nour proposed method shows much better results than other state-of-the-art\nmethods on MARS, LS-VID and iLIDS-VID. The code is available at\nhttps://github.com/AsuradaYuci/TF-CLIP.\n","authors":["Chenyang Yu","Xuehu Liu","Yingquan Wang","Pingping Zhang","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2312.09627v1.pdf","comment":"This work is accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2312.09623v1","updated":"2023-12-15T09:05:06Z","published":"2023-12-15T09:05:06Z","title":"A novel dual-stream time-frequency contrastive pretext tasks framework\n  for sleep stage classification","summary":"  Self-supervised learning addresses the challenge encountered by many\nsupervised methods, i.e. the requirement of large amounts of annotated data.\nThis challenge is particularly pronounced in fields such as the\nelectroencephalography (EEG) research domain. Self-supervised learning operates\ninstead by utilizing pseudo-labels, which are generated by pretext tasks, to\nobtain a rich and meaningful data representation. In this study, we aim at\nintroducing a dual-stream pretext task architecture that operates both in the\ntime and frequency domains. In particular, we have examined the incorporation\nof the novel Frequency Similarity (FS) pretext task into two existing pretext\ntasks, Relative Positioning (RP) and Temporal Shuffling (TS). We assess the\naccuracy of these models using the Physionet Challenge 2018 (PC18) dataset in\nthe context of the downstream task sleep stage classification. The inclusion of\nFS resulted in a notable improvement in downstream task accuracy, with a 1.28\npercent improvement on RP and a 2.02 percent improvement on TS. Furthermore,\nwhen visualizing the learned embeddings using Uniform Manifold Approximation\nand Projection (UMAP), distinct clusters emerge, indicating that the learned\nrepresentations carry meaningful information.\n","authors":["Sergio Kazatzidis","Siamak Mehrkanoon"],"pdf_url":"https://arxiv.org/pdf/2312.09623v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2308.15059v3","updated":"2023-12-15T09:04:01Z","published":"2023-08-29T06:43:29Z","title":"OEBench: Investigating Open Environment Challenges in Real-World\n  Relational Data Streams","summary":"  How to get insights from relational data streams in a timely manner is a hot\nresearch topic. Data streams can present unique challenges, such as\ndistribution drifts, outliers, emerging classes, and changing features, which\nhave recently been described as open environment challenges for machine\nlearning. While existing studies have been done on incremental learning for\ndata streams, their evaluations are mostly conducted with synthetic datasets.\nThus, a natural question is how those open environment challenges look like and\nhow existing incremental learning algorithms perform on real-world relational\ndata streams. To fill this gap, we develop an Open Environment Benchmark named\nOEBench to evaluate open environment challenges in real-world relational data\nstreams. Specifically, we investigate 55 real-world relational data streams and\nestablish that open environment scenarios are indeed widespread, which presents\nsignificant challenges for stream learning algorithms. Through benchmarks with\nexisting incremental learning algorithms, we find that increased data quantity\nmay not consistently enhance the model accuracy when applied in open\nenvironment scenarios, where machine learning models can be significantly\ncompromised by missing values, distribution drifts, or anomalies in real-world\ndata streams. The current techniques are insufficient in effectively mitigating\nthese challenges brought by open environments. More researches are needed to\naddress real-world open environment challenges. All datasets and code are\nopen-sourced in https://github.com/sjtudyq/OEBench.\n","authors":["Yiqun Diao","Yutong Yang","Qinbin Li","Bingsheng He","Mian Lu"],"pdf_url":"https://arxiv.org/pdf/2308.15059v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09613v1","updated":"2023-12-15T08:54:32Z","published":"2023-12-15T08:54:32Z","title":"Rethinking Causal Relationships Learning in Graph Neural Networks","summary":"  Graph Neural Networks (GNNs) demonstrate their significance by effectively\nmodeling complex interrelationships within graph-structured data. To enhance\nthe credibility and robustness of GNNs, it becomes exceptionally crucial to\nbolster their ability to capture causal relationships. However, despite recent\nadvancements that have indeed strengthened GNNs from a causal learning\nperspective, conducting an in-depth analysis specifically targeting the causal\nmodeling prowess of GNNs remains an unresolved issue. In order to\ncomprehensively analyze various GNN models from a causal learning perspective,\nwe constructed an artificially synthesized dataset with known and controllable\ncausal relationships between data and labels. The rationality of the generated\ndata is further ensured through theoretical foundations. Drawing insights from\nanalyses conducted using our dataset, we introduce a lightweight and highly\nadaptable GNN module designed to strengthen GNNs' causal learning capabilities\nacross a diverse range of tasks. Through a series of experiments conducted on\nboth synthetic datasets and other real-world datasets, we empirically validate\nthe effectiveness of the proposed module.\n","authors":["Hang Gao","Chengyu Yao","Jiangmeng Li","Lingyu Si","Yifan Jin","Fengge Wu","Changwen Zheng","Huaping Liu"],"pdf_url":"https://arxiv.org/pdf/2312.09613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09610v1","updated":"2023-12-15T08:53:45Z","published":"2023-12-15T08:53:45Z","title":"A Synthesis of Green Architectural Tactics for ML-Enabled Systems","summary":"  The rapid adoption of artificial intelligence (AI) and machine learning (ML)\nhas generated growing interest in understanding their environmental impact and\nthe challenges associated with designing environmentally friendly ML-enabled\nsystems. While Green AI research, i.e., research that tries to minimize the\nenergy footprint of AI, is receiving increasing attention, very few concrete\nguidelines are available on how ML-enabled systems can be designed to be more\nenvironmentally sustainable. In this paper, we provide a catalog of 30 green\narchitectural tactics for ML-enabled systems to fill this gap. An architectural\ntactic is a high-level design technique to improve software quality, in our\ncase environmental sustainability. We derived the tactics from the analysis of\n51 peer-reviewed publications that primarily explore Green AI, and validated\nthem using a focus group approach with three experts. The 30 tactics we\nidentified are aimed to serve as an initial reference guide for further\nexploration into Green AI from a software engineering perspective, and assist\nin designing sustainable ML-enabled systems. To enhance transparency and\nfacilitate their widespread use and extension, we make the tactics available\nonline in easily consumable formats. Wide-spread adoption of these tactics has\nthe potential to substantially reduce the societal impact of ML-enabled systems\nregarding their energy and carbon footprint.\n","authors":["Heli Järvenpää","Patricia Lago","Justus Bogner","Grace Lewis","Henry Muccini","Ipek Ozkaya"],"pdf_url":"https://arxiv.org/pdf/2312.09610v1.pdf","comment":"Accepted for publication at the 2024 International Conference on\n  Software Engineering - Software Engineering in Society (ICSE-SEIS'2024)"},{"id":"http://arxiv.org/abs/2312.09606v1","updated":"2023-12-15T08:39:02Z","published":"2023-12-15T08:39:02Z","title":"Reliable Prediction Intervals with Regression Neural Networks","summary":"  This paper proposes an extension to conventional regression Neural Networks\n(NNs) for replacing the point predictions they produce with prediction\nintervals that satisfy a required level of confidence. Our approach follows a\nnovel machine learning framework, called Conformal Prediction (CP), for\nassigning reliable confidence measures to predictions without assuming anything\nmore than that the data are independent and identically distributed (i.i.d.).\nWe evaluate the proposed method on four benchmark datasets and on the problem\nof predicting Total Electron Content (TEC), which is an important parameter in\ntrans-ionospheric links; for the latter we use a dataset of more than 60000 TEC\nmeasurements collected over a period of 11 years. Our experimental results show\nthat the prediction intervals produced by our method are both well-calibrated\nand tight enough to be useful in practice.\n","authors":["Harris Papadopoulos","Haris Haralambous"],"pdf_url":"https://arxiv.org/pdf/2312.09606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09603v1","updated":"2023-12-15T08:34:31Z","published":"2023-12-15T08:34:31Z","title":"Stethoscope-guided Supervised Contrastive Learning for Cross-domain\n  Adaptation on Respiratory Sound Classification","summary":"  Despite the remarkable advances in deep learning technology, achieving\nsatisfactory performance in lung sound classification remains a challenge due\nto the scarcity of available data. Moreover, the respiratory sound samples are\ncollected from a variety of electronic stethoscopes, which could potentially\nintroduce biases into the trained models. When a significant distribution shift\noccurs within the test dataset or in a practical scenario, it can substantially\ndecrease the performance. To tackle this issue, we introduce cross-domain\nadaptation techniques, which transfer the knowledge from a source domain to a\ndistinct target domain. In particular, by considering different stethoscope\ntypes as individual domains, we propose a novel stethoscope-guided supervised\ncontrastive learning approach. This method can mitigate any domain-related\ndisparities and thus enables the model to distinguish respiratory sounds of the\nrecording variation of the stethoscope. The experimental results on the ICBHI\ndataset demonstrate that the proposed methods are effective in reducing the\ndomain dependency and achieving the ICBHI Score of 61.71%, which is a\nsignificant improvement of 2.16% over the baseline.\n","authors":["June-Woo Kim","Sangmin Bae","Won-Yang Cho","Byungjo Lee","Ho-Young Jung"],"pdf_url":"https://arxiv.org/pdf/2312.09603v1.pdf","comment":"accepted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.09601v1","updated":"2023-12-15T08:32:28Z","published":"2023-12-15T08:32:28Z","title":"Binary Code Summarization: Benchmarking ChatGPT/GPT-4 and Other Large\n  Language Models","summary":"  Binary code summarization, while invaluable for understanding code semantics,\nis challenging due to its labor-intensive nature. This study delves into the\npotential of large language models (LLMs) for binary code comprehension. To\nthis end, we present BinSum, a comprehensive benchmark and dataset of over 557K\nbinary functions and introduce a novel method for prompt synthesis and\noptimization. To more accurately gauge LLM performance, we also propose a new\nsemantic similarity metric that surpasses traditional exact-match approaches.\nOur extensive evaluation of prominent LLMs, including ChatGPT, GPT-4, Llama 2,\nand Code Llama, reveals 10 pivotal insights. This evaluation generates 4\nbillion inference tokens, incurred a total expense of 11,418 US dollars and 873\nNVIDIA A100 GPU hours. Our findings highlight both the transformative potential\nof LLMs in this field and the challenges yet to be overcome.\n","authors":["Xin Jin","Jonathan Larson","Weiwei Yang","Zhiqiang Lin"],"pdf_url":"https://arxiv.org/pdf/2312.09601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09597v1","updated":"2023-12-15T08:27:39Z","published":"2023-12-15T08:27:39Z","title":"Deep Generative Models for Detector Signature Simulation: An Analytical\n  Taxonomy","summary":"  In modern collider experiments, the quest to explore fundamental interactions\nbetween elementary particles has reached unparalleled levels of precision.\nSignatures from particle physics detectors are low-level objects encoding the\nphysics of collisions. The complete simulation of them in a detector is a\nmemory and storage-intensive task. To address this computational bottleneck in\nparticle physics, \"Fast Simulation\" has been introduced and refined over the\nyears. The field has seen a surge in interest in surrogate modeling the\ndetector simulation, fueled by the advancements in deep generative models.\nThese models aim to generate responses that are statistically identical to the\nobserved data. In this paper, we conduct a comprehensive and exhaustive\ntaxonomic review of the existing literature on the simulation of detector\nsignatures from both methodological and application-wise perspectives.\nInitially, we formulate the problem of detector signature simulation and\ndiscuss its different variations that can be unified. Next, we classify the\nstate-of-the-art methods into four distinct categories based on their\nunderlying model architectures, summarizing their respective generation\nstrategies. We then identify and discuss three key application areas. Finally,\nwe shed light on the challenges and opportunities that lie ahead in detector\nsignature simulation, setting the stage for future research and development.\n","authors":["Hosein Hashemi","Claudius Krause"],"pdf_url":"https://arxiv.org/pdf/2312.09597v1.pdf","comment":"Submitted to Reviews in Physics"},{"id":"http://arxiv.org/abs/2310.14360v4","updated":"2023-12-15T08:19:59Z","published":"2023-10-22T17:03:56Z","title":"Is ChatGPT a game changer for geocoding -- a benchmark for geocoding\n  address parsing techniques","summary":"  The remarkable success of GPT models across various tasks, including toponymy\nrecognition motivates us to assess the performance of the GPT-3 model in the\ngeocoding address parsing task. To ensure that the evaluation more accurately\nmirrors performance in real-world scenarios with diverse user input qualities\nand resolve the pressing need for a 'gold standard' evaluation dataset for\ngeocoding systems, we introduce a benchmark dataset of low-quality address\ndescriptions synthesized based on human input patterns mining from actual input\nlogs of a geocoding system in production. This dataset has 21 different input\nerrors and variations; contains over 239,000 address records that are uniquely\nselected from streets across all U.S. 50 states and D.C.; and consists of three\nsubsets to be used as training, validation, and testing sets. Building on this,\nwe train and gauge the performance of the GPT-3 model in extracting address\ncomponents, contrasting its performance with transformer-based and LSTM-based\nmodels. The evaluation results indicate that Bidirectional LSTM-CRF model has\nachieved the best performance over these transformer-based models and GPT-3\nmodel. Transformer-based models demonstrate very comparable results compared to\nthe Bidirectional LSTM-CRF model. The GPT-3 model, though trailing in\nperformance, showcases potential in the address parsing task with few-shot\nexamples, exhibiting room for improvement with additional fine-tuning. We open\nsource the code and data of this presented benchmark so that researchers can\nutilize it for future model development or extend it to evaluate similar tasks,\nsuch as document geocoding.\n","authors":["Zhengcong Yin","Diya Li","Daniel W. Goldberg"],"pdf_url":"https://arxiv.org/pdf/2310.14360v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.11476v2","updated":"2023-12-15T08:06:38Z","published":"2023-05-19T06:56:02Z","title":"Learning Diverse Risk Preferences in Population-based Self-play","summary":"  Among the great successes of Reinforcement Learning (RL), self-play\nalgorithms play an essential role in solving competitive games. Current\nself-play algorithms optimize the agent to maximize expected win-rates against\nits current or historical copies, making it often stuck in the local optimum\nand its strategy style simple and homogeneous. A possible solution is to\nimprove the diversity of policies, which helps the agent break the stalemate\nand enhances its robustness when facing different opponents. However, enhancing\ndiversity in the self-play algorithms is not trivial. In this paper, we aim to\nintroduce diversity from the perspective that agents could have diverse risk\npreferences in the face of uncertainty. Specifically, we design a novel\nreinforcement learning algorithm called Risk-sensitive Proximal Policy\nOptimization (RPPO), which smoothly interpolates between worst-case and\nbest-case policy learning and allows for policy learning with desired risk\npreferences. Seamlessly integrating RPPO with population-based self-play,\nagents in the population optimize dynamic risk-sensitive objectives with\nexperiences from playing against diverse opponents. Empirical results show that\nour method achieves comparable or superior performance in competitive games and\nthat diverse modes of behaviors emerge. Our code is public online at\n\\url{https://github.com/Jackory/RPBT}.\n","authors":["Yuhua Jiang","Qihan Liu","Xiaoteng Ma","Chenghao Li","Yiqin Yang","Jun Yang","Bin Liang","Qianchuan Zhao"],"pdf_url":"https://arxiv.org/pdf/2305.11476v2.pdf","comment":"AAAI2024"},{"id":"http://arxiv.org/abs/2312.09585v1","updated":"2023-12-15T07:47:03Z","published":"2023-12-15T07:47:03Z","title":"Joint State Estimation and Noise Identification Based on Variational\n  Optimization","summary":"  In this article, the state estimation problems with unknown process noise and\nmeasurement noise covariances for both linear and nonlinear systems are\nconsidered. By formulating the joint estimation of system state and noise\nparameters into an optimization problem, a novel adaptive Kalman filter method\nbased on conjugate-computation variational inference, referred to as CVIAKF, is\nproposed to approximate the joint posterior probability density function of the\nlatent variables. Unlike the existing adaptive Kalman filter methods utilizing\nvariational inference in natural-parameter space, CVIAKF performs optimization\nin expectation-parameter space, resulting in a faster and simpler solution.\nMeanwhile, CVIAKF divides optimization objectives into conjugate and\nnon-conjugate parts of nonlinear dynamical models, whereas conjugate\ncomputations and stochastic mirror-descent are applied, respectively.\nRemarkably, the reparameterization trick is used to reduce the variance of\nstochastic gradients of the non-conjugate parts. The effectiveness of CVIAKF is\nvalidated through synthetic and real-world datasets of maneuvering target\ntracking.\n","authors":["Hua Lan","Shijie Zhao","Jinjie Hu","Zengfu Wang","Jing Fu"],"pdf_url":"https://arxiv.org/pdf/2312.09585v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2312.09578v1","updated":"2023-12-15T07:16:12Z","published":"2023-12-15T07:16:12Z","title":"Self-Supervised Learning for Anomalous Sound Detection","summary":"  State-of-the-art anomalous sound detection (ASD) systems are often trained by\nusing an auxiliary classification task to learn an embedding space. Doing so\nenables the system to learn embeddings that are robust to noise and are\nignoring non-target sound events but requires manually annotated meta\ninformation to be used as class labels. However, the less difficult the\nclassification task becomes, the less informative are the embeddings and the\nworse is the resulting ASD performance. A solution to this problem is to\nutilize self-supervised learning (SSL). In this work, feature exchange\n(FeatEx), a simple yet effective SSL approach for ASD, is proposed. In\naddition, FeatEx is compared to and combined with existing SSL approaches. As\nthe main result, a new state-of-the-art performance for the DCASE2023 ASD\ndataset is obtained that outperforms all other published results on this\ndataset by a large margin.\n","authors":["Kevin Wilkinghoff"],"pdf_url":"https://arxiv.org/pdf/2312.09578v1.pdf","comment":"Accepted for presentation at IEEE ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.09559v1","updated":"2023-12-15T06:34:35Z","published":"2023-12-15T06:34:35Z","title":"STEAM & MoSAFE: SOTIF Error-and-Failure Model & Analysis for AI-Enabled\n  Driving Automation","summary":"  Driving Automation Systems (DAS) are subject to complex road environments and\nvehicle behaviors and increasingly rely on sophisticated sensors and Artificial\nIntelligence (AI). These properties give rise to unique safety faults stemming\nfrom specification insufficiencies and technological performance limitations,\nwhere sensors and AI introduce errors that vary in magnitude and temporal\npatterns, posing potential safety risks. The Safety of the Intended\nFunctionality (SOTIF) standard emerges as a promising framework for addressing\nthese concerns, focusing on scenario-based analysis to identify hazardous\nbehaviors and their causes. Although the current standard provides a basic\ncause-and-effect model and high-level process guidance, it lacks concepts\nrequired to identify and evaluate hazardous errors, especially within the\ncontext of AI.\n  This paper introduces two key contributions to bridge this gap. First, it\ndefines the SOTIF Temporal Error and Failure Model (STEAM) as a refinement of\nthe SOTIF cause-and-effect model, offering a comprehensive system-design\nperspective. STEAM refines error definitions, introduces error sequences, and\nclassifies them as error sequence patterns, providing particular relevance to\nsystems employing advanced sensors and AI. Second, this paper proposes the\nModel-based SOTIF Analysis of Failures and Errors (MoSAFE) method, which allows\ninstantiating STEAM based on system-design models by deriving hazardous error\nsequence patterns at module level from hazardous behaviors at vehicle level via\nweakest precondition reasoning. Finally, the paper presents a case study\ncentered on an automated speed-control feature, illustrating the practical\napplicability of the refined model and the MoSAFE method in addressing complex\nsafety challenges in DAS.\n","authors":["Krzysztof Czarnecki","Hiroshi Kuwajima"],"pdf_url":"https://arxiv.org/pdf/2312.09559v1.pdf","comment":"19 pages, 8 figures, 5 tables"},{"id":"http://arxiv.org/abs/2306.10395v2","updated":"2023-12-15T06:03:42Z","published":"2023-06-17T17:30:43Z","title":"Distributed Semi-Supervised Sparse Statistical Inference","summary":"  The debiased estimator is a crucial tool in statistical inference for\nhigh-dimensional model parameters. However, constructing such an estimator\ninvolves estimating the high-dimensional inverse Hessian matrix, incurring\nsignificant computational costs. This challenge becomes particularly acute in\ndistributed setups, where traditional methods necessitate computing a debiased\nestimator on every machine. This becomes unwieldy, especially with a large\nnumber of machines. In this paper, we delve into semi-supervised sparse\nstatistical inference in a distributed setup. An efficient multi-round\ndistributed debiased estimator, which integrates both labeled and unlabelled\ndata, is developed. We will show that the additional unlabeled data helps to\nimprove the statistical rate of each round of iteration. Our approach offers\ntailored debiasing methods for $M$-estimation and generalized linear models\naccording to the specific form of the loss function. Our method also applies to\na non-smooth loss like absolute deviation loss. Furthermore, our algorithm is\ncomputationally efficient since it requires only one estimation of a\nhigh-dimensional inverse covariance matrix. We demonstrate the effectiveness of\nour method by presenting simulation studies and real data applications that\nhighlight the benefits of incorporating unlabeled data.\n","authors":["Jiyuan Tu","Weidong Liu","Xiaojun Mao","Mingyue Xu"],"pdf_url":"https://arxiv.org/pdf/2306.10395v2.pdf","comment":"IEEE Transactions on Information Theory, 2023"},{"id":"http://arxiv.org/abs/2312.08672v2","updated":"2023-12-15T05:56:36Z","published":"2023-12-14T06:08:59Z","title":"CAT: A Causally Graph Attention Network for Trimming Heterophilic Graph","summary":"  Local Attention-guided Message Passing Mechanism (LAMP) adopted in Graph\nAttention Networks (GATs) is designed to adaptively learn the importance of\nneighboring nodes for better local aggregation on the graph, which can bring\nthe representations of similar neighbors closer effectively, thus showing\nstronger discrimination ability. However, existing GATs suffer from a\nsignificant discrimination ability decline in heterophilic graphs because the\nhigh proportion of dissimilar neighbors can weaken the self-attention of the\ncentral node, jointly resulting in the deviation of the central node from\nsimilar nodes in the representation space. This kind of effect generated by\nneighboring nodes is called the Distraction Effect (DE) in this paper. To\nestimate and weaken the DE of neighboring nodes, we propose a Causally graph\nAttention network for Trimming heterophilic graph (CAT). To estimate the DE,\nsince the DE are generated through two paths (grab the attention assigned to\nneighbors and reduce the self-attention of the central node), we use Total\nEffect to model DE, which is a kind of causal estimand and can be estimated\nfrom intervened data; To weaken the DE, we identify the neighbors with the\nhighest DE (we call them Distraction Neighbors) and remove them. We adopt three\nrepresentative GATs as the base model within the proposed CAT framework and\nconduct experiments on seven heterophilic datasets in three different sizes.\nComparative experiments show that CAT can improve the node classification\naccuracy of all base GAT models. Ablation experiments and visualization further\nvalidate the enhancement of discrimination ability brought by CAT. The source\ncode is available at https://github.com/GeoX-Lab/CAT.\n","authors":["Silu He","Qinyao Luo","Xinsha Fu","Ling Zhao","Ronghua Du","Haifeng Li"],"pdf_url":"https://arxiv.org/pdf/2312.08672v2.pdf","comment":"24 pages, 17 figures, 4 tables"},{"id":"http://arxiv.org/abs/2312.08887v2","updated":"2023-12-15T05:52:44Z","published":"2023-12-13T09:42:04Z","title":"SpeedUpNet: A Plug-and-Play Hyper-Network for Accelerating Text-to-Image\n  Diffusion Models","summary":"  Text-to-image diffusion models (SD) exhibit significant advancements while\nrequiring extensive computational resources. Though many acceleration methods\nhave been proposed, they suffer from generation quality degradation or extra\ntraining cost generalizing to new fine-tuned models. To address these\nlimitations, we propose a novel and universal Stable-Diffusion (SD)\nacceleration module called SpeedUpNet(SUN). SUN can be directly plugged into\nvarious fine-tuned SD models without extra training. This technique utilizes\ncross-attention layers to learn the relative offsets in the generated image\nresults between negative and positive prompts achieving classifier-free\nguidance distillation with negative prompts controllable, and introduces a\nMulti-Step Consistency (MSC) loss to ensure a harmonious balance between\nreducing inference steps and maintaining consistency in the generated output.\nConsequently, SUN significantly reduces the number of inference steps to just 4\nsteps and eliminates the need for classifier-free guidance. It leads to an\noverall speedup of more than 10 times for SD models compared to the\nstate-of-the-art 25-step DPM-solver++, and offers two extra advantages: (1)\nclassifier-free guidance distillation with controllable negative prompts and\n(2) seamless integration into various fine-tuned Stable-Diffusion models\nwithout training. The effectiveness of the SUN has been verified through\nextensive experimentation. Project Page:\nhttps://williechai.github.io/speedup-plugin-for-stable-diffusions.github.io\n","authors":["Weilong Chai","DanDan Zheng","Jiajiong Cao","Zhiquan Chen","Changbao Wang","Chenguang Ma"],"pdf_url":"https://arxiv.org/pdf/2312.08887v2.pdf","comment":"Table 1. shows the comparison with existing methods, but the lack of\n  experimental data of the LCM method under 12-step makes the table incomplete.\n  We need to temporarily withdraw the manuscript and conduct corresponding\n  experiments before resubmitting it"},{"id":"http://arxiv.org/abs/2305.15835v2","updated":"2023-12-15T05:46:52Z","published":"2023-05-25T08:23:26Z","title":"PDE+: Enhancing Generalization via PDE with Adaptive Distributional\n  Diffusion","summary":"  The generalization of neural networks is a central challenge in machine\nlearning, especially concerning the performance under distributions that differ\nfrom training ones. Current methods, mainly based on the data-driven paradigm\nsuch as data augmentation, adversarial training, and noise injection, may\nencounter limited generalization due to model non-smoothness. In this paper, we\npropose to investigate generalization from a Partial Differential Equation\n(PDE) perspective, aiming to enhance it directly through the underlying\nfunction of neural networks, rather than focusing on adjusting input data.\nSpecifically, we first establish the connection between neural network\ngeneralization and the smoothness of the solution to a specific PDE, namely\n\"transport equation\". Building upon this, we propose a general framework that\nintroduces adaptive distributional diffusion into transport equation to enhance\nthe smoothness of its solution, thereby improving generalization. In the\ncontext of neural networks, we put this theoretical framework into practice as\n$\\textbf{PDE+}$ ($\\textbf{PDE}$ with $\\textbf{A}$daptive\n$\\textbf{D}$istributional $\\textbf{D}$iffusion) which diffuses each sample into\na distribution covering semantically similar inputs. This enables better\ncoverage of potentially unobserved distributions in training, thus improving\ngeneralization beyond merely data-driven methods. The effectiveness of PDE+ is\nvalidated through extensive experimental settings, demonstrating its superior\nperformance compared to SOTA methods.\n","authors":["Yige Yuan","Bingbing Xu","Bo Lin","Liang Hou","Fei Sun","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2305.15835v2.pdf","comment":"Accepted by Annual AAAI Conference on Artificial Intelligence (AAAI)\n  2024. Code is available at https://github.com/yuanyige/pde-add"},{"id":"http://arxiv.org/abs/2310.07958v4","updated":"2023-12-15T05:42:08Z","published":"2023-10-12T00:51:06Z","title":"Towards Causal Deep Learning for Vulnerability Detection","summary":"  Deep learning vulnerability detection has shown promising results in recent\nyears. However, an important challenge that still blocks it from being very\nuseful in practice is that the model is not robust under perturbation and it\ncannot generalize well over the out-of-distribution (OOD) data, e.g., applying\na trained model to unseen projects in real world. We hypothesize that this is\nbecause the model learned non-robust features, e.g., variable names, that have\nspurious correlations with labels. When the perturbed and OOD datasets no\nlonger have the same spurious features, the model prediction fails. To address\nthe challenge, in this paper, we introduced causality into deep learning\nvulnerability detection. Our approach CausalVul consists of two phases. First,\nwe designed novel perturbations to discover spurious features that the model\nmay use to make predictions. Second, we applied the causal learning algorithms,\nspecifically, do-calculus, on top of existing deep learning models to\nsystematically remove the use of spurious features and thus promote causal\nbased prediction. Our results show that CausalVul consistently improved the\nmodel accuracy, robustness and OOD performance for all the state-of-the-art\nmodels and datasets we experimented. To the best of our knowledge, this is the\nfirst work that introduces do calculus based causal learning to software\nengineering models and shows it's indeed useful for improving the model\naccuracy, robustness and generalization. Our replication package is located at\nhttps://figshare.com/s/0ffda320dcb96c249ef2.\n","authors":["Md Mahbubur Rahman","Ira Ceka","Chengzhi Mao","Saikat Chakraborty","Baishakhi Ray","Wei Le"],"pdf_url":"https://arxiv.org/pdf/2310.07958v4.pdf","comment":"Accepted at ICSE 2024 (not camera-ready version)"},{"id":"http://arxiv.org/abs/2311.17104v2","updated":"2023-12-15T05:27:30Z","published":"2023-11-28T09:14:55Z","title":"Single-Cell Deep Clustering Method Assisted by Exogenous Gene\n  Information: A Novel Approach to Identifying Cell Types","summary":"  In recent years, the field of single-cell data analysis has seen a marked\nadvancement in the development of clustering methods. Despite advancements,\nmost of these algorithms still concentrate on analyzing the provided\nsingle-cell matrix data. However, in medical applications, single-cell data\noften involves a wealth of exogenous information, including gene networks.\nOverlooking this aspect could lead to information loss and clustering results\ndevoid of significant clinical relevance. An innovative single-cell deep\nclustering method, incorporating exogenous gene information, has been proposed\nto overcome this limitation. This model leverages exogenous gene network\ninformation to facilitate the clustering process, generating discriminative\nrepresentations. Specifically, we have developed an attention-enhanced graph\nautoencoder, which is designed to efficiently capture the topological features\nbetween cells. Concurrently, we conducted a random walk on an exogenous\nProtein-Protein Interaction (PPI) network, thereby acquiring the gene's\ntopological features. Ultimately, during the clustering process, we integrated\nboth sets of information and reconstructed the features of both cells and genes\nto generate a discriminative representation. Extensive experiments have\nvalidated the effectiveness of our proposed method. This research offers\nenhanced insights into the characteristics and distribution of cells, thereby\nlaying the groundwork for early diagnosis and treatment of diseases.\n","authors":["Dayu Hu","Ke Liang","Hao Yu","Xinwang Liu"],"pdf_url":"https://arxiv.org/pdf/2311.17104v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09540v1","updated":"2023-12-15T05:22:39Z","published":"2023-12-15T05:22:39Z","title":"A Novel Hybrid Ordinal Learning Model with Health Care Application","summary":"  Ordinal learning (OL) is a type of machine learning models with broad utility\nin health care applications such as diagnosis of different grades of a disease\n(e.g., mild, modest, severe) and prediction of the speed of disease progression\n(e.g., very fast, fast, moderate, slow). This paper aims to tackle a situation\nwhen precisely labeled samples are limited in the training set due to cost or\navailability constraints, whereas there could be an abundance of samples with\nimprecise labels. We focus on imprecise labels that are intervals, i.e., one\ncan know that a sample belongs to an interval of labels but cannot know which\nunique label it has. This situation is quite common in health care datasets due\nto limitations of the diagnostic instrument, sparse clinical visits, or/and\npatient dropout. Limited research has been done to develop OL models with\nimprecise/interval labels. We propose a new Hybrid Ordinal Learner (HOL) to\nintegrate samples with both precise and interval labels to train a robust OL\nmodel. We also develop a tractable and efficient optimization algorithm to\nsolve the HOL formulation. We compare HOL with several recently developed OL\nmethods on four benchmarking datasets, which demonstrate the superior\nperformance of HOL. Finally, we apply HOL to a real-world dataset for\npredicting the speed of progressing to Alzheimer's Disease (AD) for individuals\nwith Mild Cognitive Impairment (MCI) based on a combination of multi-modality\nneuroimaging and demographic/clinical datasets. HOL achieves high accuracy in\nthe prediction and outperforms existing methods. The capability of accurately\npredicting the speed of progression to AD for each individual with MCI has the\npotential for helping facilitate more individually-optimized interventional\nstrategies.\n","authors":["Lujia Wang","Hairong Wang","Yi Su","Fleming Lure","Jing Li"],"pdf_url":"https://arxiv.org/pdf/2312.09540v1.pdf","comment":"16 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2210.01272v2","updated":"2023-12-15T04:58:36Z","published":"2022-10-03T23:44:38Z","title":"A systematic review of the use of Deep Learning in Satellite Imagery for\n  Agriculture","summary":"  Agricultural research is essential for increasing food production to meet the\nrequirements of an increasing population in the coming decades. Recently,\nsatellite technology has been improving rapidly and deep learning has seen much\nsuccess in generic computer vision tasks and many application areas which\npresents an important opportunity to improve analysis of agricultural land.\nHere we present a systematic review of 150 studies to find the current uses of\ndeep learning on satellite imagery for agricultural research. Although we\nidentify 5 categories of agricultural monitoring tasks, the majority of the\nresearch interest is in crop segmentation and yield prediction. We found that,\nwhen used, modern deep learning methods consistently outperformed traditional\nmachine learning across most tasks; the only exception was that Long Short-Term\nMemory (LSTM) Recurrent Neural Networks did not consistently outperform Random\nForests (RF) for yield prediction. The reviewed studies have largely adopted\nmethodologies from generic computer vision, except for one major omission:\nbenchmark datasets are not utilised to evaluate models across studies, making\nit difficult to compare results. Additionally, some studies have specifically\nutilised the extra spectral resolution available in satellite imagery, but\nother divergent properties of satellite images - such as the hugely different\nscales of spatial patterns - are not being taken advantage of in the reviewed\nstudies.\n","authors":["Brandon Victor","Zhen He","Aiden Nibali"],"pdf_url":"https://arxiv.org/pdf/2210.01272v2.pdf","comment":"23 pages, 5 figures and 10 tables in main paper. Supplementary\n  materials section also included in main pdf. Update: All tables with specific\n  references have been moved to supplementary. Main text now uses only\n  aggregated information"},{"id":"http://arxiv.org/abs/2312.09533v1","updated":"2023-12-15T04:51:43Z","published":"2023-12-15T04:51:43Z","title":"Adversarial Robustness on Image Classification with $k$-means","summary":"  In this paper we explore the challenges and strategies for enhancing the\nrobustness of $k$-means clustering algorithms against adversarial\nmanipulations. We evaluate the vulnerability of clustering algorithms to\nadversarial attacks, emphasising the associated security risks. Our study\ninvestigates the impact of incremental attack strength on training, introduces\nthe concept of transferability between supervised and unsupervised models, and\nhighlights the sensitivity of unsupervised models to sample distributions. We\nadditionally introduce and evaluate an adversarial training method that\nimproves testing performance in adversarial scenarios, and we highlight the\nimportance of various parameters in the proposed training method, such as\ncontinuous learning, centroid initialisation, and adversarial step-count.\n","authors":["Rollin Omari","Junae Kim","Paul Montague"],"pdf_url":"https://arxiv.org/pdf/2312.09533v1.pdf","comment":"6 pages, 3 figures, 2 equations, 1 algorithm"},{"id":"http://arxiv.org/abs/2310.06622v2","updated":"2023-12-15T04:46:00Z","published":"2023-10-10T13:39:18Z","title":"Robustness May be More Brittle than We Think under Different Degrees of\n  Distribution Shifts","summary":"  Out-of-distribution (OOD) generalization is a complicated problem due to the\nidiosyncrasies of possible distribution shifts between training and test\ndomains. Most benchmarks employ diverse datasets to address this issue;\nhowever, the degree of the distribution shift between the training domains and\nthe test domains of each dataset remains largely fixed. This may lead to biased\nconclusions that either underestimate or overestimate the actual OOD\nperformance of a model. Our study delves into a more nuanced evaluation setting\nthat covers a broad range of shift degrees. We show that the robustness of\nmodels can be quite brittle and inconsistent under different degrees of\ndistribution shifts, and therefore one should be more cautious when drawing\nconclusions from evaluations under a limited range of degrees. In addition, we\nobserve that large-scale pre-trained models, such as CLIP, are sensitive to\neven minute distribution shifts of novel downstream tasks. This indicates that\nwhile pre-trained representations may help improve downstream in-distribution\nperformance, they could have minimal or even adverse effects on generalization\nin certain OOD scenarios of the downstream task if not used properly. In light\nof these findings, we encourage future research to conduct evaluations across a\nbroader range of shift degrees whenever possible.\n","authors":["Kaican Li","Yifan Zhang","Lanqing Hong","Zhenguo Li","Nevin L. Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.06622v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01878v4","updated":"2023-12-15T04:09:44Z","published":"2023-12-04T13:20:15Z","title":"HGPROMPT: Bridging Homogeneous and Heterogeneous Graphs for Few-shot\n  Prompt Learning","summary":"  Graph neural networks (GNNs) and heterogeneous graph neural networks (HGNNs)\nare prominent techniques for homogeneous and heterogeneous graph representation\nlearning, yet their performance in an end-to-end supervised framework greatly\ndepends on the availability of task-specific supervision. To reduce the\nlabeling cost, pre-training on self-supervised pretext tasks has become a\npopular paradigm,but there is often a gap between the pre-trained model and\ndownstream tasks, stemming from the divergence in their objectives. To bridge\nthe gap, prompt learning has risen as a promising direction especially in\nfew-shot settings, without the need to fully fine-tune the pre-trained model.\nWhile there has been some early exploration of prompt-based learning on graphs,\nthey primarily deal with homogeneous graphs, ignoring the heterogeneous graphs\nthat are prevalent in downstream applications. In this paper, we propose\nHGPROMPT, a novel pre-training and prompting framework to unify not only\npre-training and downstream tasks but also homogeneous and heterogeneous graphs\nvia a dual-template design. Moreover, we propose dual-prompt in HGPROMPT to\nassist a downstream task in locating the most relevant prior to bridge the gaps\ncaused by not only feature variations but also heterogeneity differences across\ntasks. Finally, we thoroughly evaluate and analyze HGPROMPT through extensive\nexperiments on three public datasets.\n","authors":["Xingtong Yu","Yuan Fang","Zemin Liu","Xinming Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.01878v4.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2304.01731v4","updated":"2023-12-15T03:21:09Z","published":"2023-04-04T12:04:19Z","title":"Selective Knowledge Sharing for Privacy-Preserving Federated\n  Distillation without A Good Teacher","summary":"  While federated learning is promising for privacy-preserving collaborative\nlearning without revealing local data, it remains vulnerable to white-box\nattacks and struggles to adapt to heterogeneous clients. Federated distillation\n(FD), built upon knowledge distillation--an effective technique for\ntransferring knowledge from a teacher model to student models--emerges as an\nalternative paradigm, which provides enhanced privacy guarantees and addresses\nmodel heterogeneity. Nevertheless, challenges arise due to variations in local\ndata distributions and the absence of a well-trained teacher model, which leads\nto misleading and ambiguous knowledge sharing that significantly degrades model\nperformance. To address these issues, this paper proposes a selective knowledge\nsharing mechanism for FD, termed Selective-FD. It includes client-side\nselectors and a server-side selector to accurately and precisely identify\nknowledge from local and ensemble predictions, respectively. Empirical studies,\nbacked by theoretical insights, demonstrate that our approach enhances the\ngeneralization capabilities of the FD framework and consistently outperforms\nbaseline methods.\n","authors":["Jiawei Shao","Fangzhao Wu","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.01731v4.pdf","comment":"This paper was accepted to Nature Communications"},{"id":"http://arxiv.org/abs/2311.17401v2","updated":"2023-12-15T03:19:23Z","published":"2023-11-29T07:09:25Z","title":"Gene-MOE: A Sparsely-gated Framework for Pan-Cancer Genomic Analysis","summary":"  Benefiting from the advancements in deep learning, various genomic analytical\ntechniques, such as survival analysis, classification of tumors and their\nsubtypes, and exploration of specific pathways, have significantly enhanced our\nunderstanding of the biological mechanisms driving cancer. However, the\noverfitting issue, arising from the limited number of patient samples, poses a\nchallenge in improving the accuracy of genome analysis by deepening the neural\nnetwork. Furthermore, it remains uncertain whether novel approaches such as the\nsparsely gated mixture of expert (MOE) and self-attention mechanisms can\nimprove the accuracy of genomic analysis. In this paper, we introduce a novel\nsparsely gated RNA-seq analysis framework called Gene-MOE. This framework\nexploits the potential of the MOE layers and the proposed mixture of attention\nexpert (MOAE) layers to enhance the analysis accuracy. Additionally, it\naddresses overfitting challenges by integrating pan-cancer information from 33\ndistinct cancer types through pre-training.We pre-trained Gene-MOE on TCGA\npan-cancer RNA-seq dataset with 33 cancer types. Subsequently, we conducted\nexperiments involving cancer classification and survival analysis based on the\npre-trained Gene-MOE. According to the survival analysis results on 14 cancer\ntypes, Gene-MOE outperformed state-of-the-art models on 12 cancer types.\nThrough detailed feature analysis, we found that the Gene-MOE model could learn\nrich feature representations of high-dimensional genes. According to the\nclassification results, the total accuracy of the classification model for 33\ncancer classifications reached 95.8%, representing the best performance\ncompared to state-of-the-art models. These results indicate that Gene-MOE holds\nstrong potential for use in cancer classification and survival analysis.\n","authors":["Xiangyu Meng","Xue Li","Qing Yang","Huanhuan Dai","Lian Qiao","Hongzhen Ding","Long Hao","Xun Wang"],"pdf_url":"https://arxiv.org/pdf/2311.17401v2.pdf","comment":"submit to bioinformatics"},{"id":"http://arxiv.org/abs/2312.09505v1","updated":"2023-12-15T03:06:19Z","published":"2023-12-15T03:06:19Z","title":"Adaptive Integration of Partial Label Learning and Negative Learning for\n  Enhanced Noisy Label Learning","summary":"  There has been significant attention devoted to the effectiveness of various\ndomains, such as semi-supervised learning, contrastive learning, and\nmeta-learning, in enhancing the performance of methods for noisy label learning\n(NLL) tasks. However, most existing methods still depend on prior assumptions\nregarding clean samples amidst different sources of noise (\\eg, a pre-defined\ndrop rate or a small subset of clean samples). In this paper, we propose a\nsimple yet powerful idea called \\textbf{NPN}, which revolutionizes\n\\textbf{N}oisy label learning by integrating \\textbf{P}artial label learning\n(PLL) and \\textbf{N}egative learning (NL). Toward this goal, we initially\ndecompose the given label space adaptively into the candidate and complementary\nlabels, thereby establishing the conditions for PLL and NL. We propose two\nadaptive data-driven paradigms of label disambiguation for PLL: hard\ndisambiguation and soft disambiguation. Furthermore, we generate reliable\ncomplementary labels using all non-candidate labels for NL to enhance model\nrobustness through indirect supervision. To maintain label reliability during\nthe later stage of model training, we introduce a consistency regularization\nterm that encourages agreement between the outputs of multiple augmentations.\nExperiments conducted on both synthetically corrupted and real-world noisy\ndatasets demonstrate the superiority of NPN compared to other state-of-the-art\n(SOTA) methods. The source code has been made available at\n{\\color{purple}{\\url{https://github.com/NUST-Machine-Intelligence-Laboratory/NPN}}}.\n","authors":["Mengmeng Sheng","Zeren Sun","Zhenhuang Cai","Tao Chen","Yichao Zhou","Yazhou Yao"],"pdf_url":"https://arxiv.org/pdf/2312.09505v1.pdf","comment":"accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.09504v1","updated":"2023-12-15T03:04:28Z","published":"2023-12-15T03:04:28Z","title":"Combinatorial Complexes: Bridging the Gap Between Cell Complexes and\n  Hypergraphs","summary":"  Graph-based signal processing techniques have become essential for handling\ndata in non-Euclidean spaces. However, there is a growing awareness that these\ngraph models might need to be expanded into `higher-order' domains to\neffectively represent the complex relations found in high-dimensional data.\nSuch higher-order domains are typically modeled either as hypergraphs, or as\nsimplicial, cubical or other cell complexes. In this context, cell complexes\nare often seen as a subclass of hypergraphs with additional algebraic structure\nthat can be exploited, e.g., to develop a spectral theory. In this article, we\npromote an alternative perspective. We argue that hypergraphs and cell\ncomplexes emphasize \\emph{different} types of relations, which may have\ndifferent utility depending on the application context. Whereas hypergraphs are\neffective in modeling set-type, multi-body relations between entities, cell\ncomplexes provide an effective means to model hierarchical,\ninterior-to-boundary type relations. We discuss the relative advantages of\nthese two choices and elaborate on the previously introduced concept of a\ncombinatorial complex that enables co-existing set-type and hierarchical\nrelations. Finally, we provide a brief numerical experiment to demonstrate that\nthis modelling flexibility can be advantageous in learning tasks.\n","authors":["Mustafa Hajij","Ghada Zamzmi","Theodore Papamarkou","Aldo Guzmán-Sáenz","Tolga Birdal","Michael T. Schaub"],"pdf_url":"https://arxiv.org/pdf/2312.09504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04103v2","updated":"2023-12-15T03:04:04Z","published":"2023-12-07T07:37:15Z","title":"Enhancing the Rationale-Input Alignment for Self-explaining\n  Rationalization","summary":"  Rationalization empowers deep learning models with self-explaining\ncapabilities through a cooperative game, where a generator selects a\nsemantically consistent subset of the input as a rationale, and a subsequent\npredictor makes predictions based on the selected rationale. In this paper, we\ndiscover that rationalization is prone to a problem named \\emph{rationale\nshift}, which arises from the algorithmic bias of the cooperative game.\nRationale shift refers to a situation where the semantics of the selected\nrationale may deviate from the original input, but the predictor still produces\naccurate predictions based on the deviation, resulting in a compromised\ngenerator with misleading feedback.\n  To address this issue, we first demonstrate the importance of the alignment\nbetween the rationale and the full input through both empirical observations\nand theoretical analysis. Subsequently, we introduce a novel approach called\nDAR (\\textbf{D}iscriminatively \\textbf{A}ligned \\textbf{R}ationalization),\nwhich utilizes an auxiliary module pretrained on the full input to\ndiscriminatively align the selected rationale and the original input. We\ntheoretically illustrate how DAR accomplishes the desired alignment, thereby\novercoming the rationale shift problem. The experiments on two widely used\nreal-world benchmarks show that the proposed method significantly improves the\nexplanation quality (measured by the overlap between the model-selected\nexplanation and the human-annotated rationale) as compared to state-of-the-art\ntechniques. Additionally, results on two synthetic settings further validate\nthe effectiveness of DAR in addressing the rationale shift problem.\n","authors":["Wei Liu","Haozhao Wang","Jun Wang","Zhiying Deng","YuanKai Zhang","Cheng Wang","Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2312.04103v2.pdf","comment":"Accept at ICDE 2024"},{"id":"http://arxiv.org/abs/2312.06578v2","updated":"2023-12-15T02:50:34Z","published":"2023-12-11T18:09:55Z","title":"Multi-class Support Vector Machine with Maximizing Minimum Margin","summary":"  Support Vector Machine (SVM) stands out as a prominent machine learning\ntechnique widely applied in practical pattern recognition tasks. It achieves\nbinary classification by maximizing the \"margin\", which represents the minimum\ndistance between instances and the decision boundary. Although many efforts\nhave been dedicated to expanding SVM for multi-class case through strategies\nsuch as one versus one and one versus the rest, satisfactory solutions remain\nto be developed. In this paper, we propose a novel method for multi-class SVM\nthat incorporates pairwise class loss considerations and maximizes the minimum\nmargin. Adhering to this concept, we embrace a new formulation that imparts\nheightened flexibility to multi-class SVM. Furthermore, the correlations\nbetween the proposed method and multiple forms of multi-class SVM are analyzed.\nThe proposed regularizer, akin to the concept of \"margin\", can serve as a\nseamless enhancement over the softmax in deep learning, providing guidance for\nnetwork parameter learning. Empirical evaluations demonstrate the effectiveness\nand superiority of our proposed method over existing multi-classification\nmethods.Code is available at https://github.com/zz-haooo/M3SVM.\n","authors":["Feiping Nie","Zhezheng Hao","Rong Wang"],"pdf_url":"https://arxiv.org/pdf/2312.06578v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09498v1","updated":"2023-12-15T02:45:33Z","published":"2023-12-15T02:45:33Z","title":"Neural Gaussian Similarity Modeling for Differential Graph Structure\n  Learning","summary":"  Graph Structure Learning (GSL) has demonstrated considerable potential in the\nanalysis of graph-unknown non-Euclidean data across a wide range of domains.\nHowever, constructing an end-to-end graph structure learning model poses a\nchallenge due to the impediment of gradient flow caused by the nearest neighbor\nsampling strategy. In this paper, we construct a differential graph structure\nlearning model by replacing the non-differentiable nearest neighbor sampling\nwith a differentiable sampling using the reparameterization trick. Under this\nframework, we argue that the act of sampling \\mbox{nearest} neighbors may not\ninvariably be essential, particularly in instances where node features exhibit\na significant degree of similarity. To alleviate this issue, the bell-shaped\nGaussian Similarity (GauSim) modeling is proposed to sample non-nearest\nneighbors. To adaptively model the similarity, we further propose Neural\nGaussian Similarity (NeuralGauSim) with learnable parameters featuring flexible\nsampling behaviors. In addition, we develop a scalable method by transferring\nthe large-scale graph to the transition graph to significantly reduce the\ncomplexity. Experimental results demonstrate the effectiveness of the proposed\nmethods.\n","authors":["Xiaolong Fan","Maoguo Gong","Yue Wu","Zedong Tang","Jieyi Liu"],"pdf_url":"https://arxiv.org/pdf/2312.09498v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2201.10547v3","updated":"2023-12-15T02:43:04Z","published":"2022-01-25T18:56:16Z","title":"Optimal Data Selection: An Online Distributed View","summary":"  The blessing of ubiquitous data also comes with a curse: the communication,\nstorage, and labeling of massive, mostly redundant datasets. We seek to solve\nthis problem at its core, collecting only valuable data and throwing out the\nrest via submodular maximization. Specifically, we develop algorithms for the\nonline and distributed version of the problem, where data selection occurs in\nan uncoordinated fashion across multiple data streams. We design a general and\nflexible core selection routine for our algorithms which, given any stream of\ndata, any assessment of its value, and any formulation of its selection cost,\nextracts the most valuable subset of the stream up to a constant factor while\nusing minimal memory. Notably, our methods have the same theoretical guarantees\nas their offline counterparts, and, as far as we know, provide the first\nguarantees for online distributed submodular optimization in the literature.\nFinally, in learning tasks on ImageNet and MNIST, we show that our selection\nmethods outperform random selection by $5-20\\%$.\n","authors":["Mariel Werner","Anastasios Angelopoulos","Stephen Bates","Michael I. Jordan"],"pdf_url":"https://arxiv.org/pdf/2201.10547v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16973v2","updated":"2023-12-15T02:15:29Z","published":"2023-11-24T00:16:00Z","title":"DemoFusion: Democratising High-Resolution Image Generation With No $$$","summary":"  High-resolution image generation with Generative Artificial Intelligence\n(GenAI) has immense potential but, due to the enormous capital investment\nrequired for training, it is increasingly centralised to a few large\ncorporations, and hidden behind paywalls. This paper aims to democratise\nhigh-resolution GenAI by advancing the frontier of high-resolution generation\nwhile remaining accessible to a broad audience. We demonstrate that existing\nLatent Diffusion Models (LDMs) possess untapped potential for higher-resolution\nimage generation. Our novel DemoFusion framework seamlessly extends open-source\nGenAI models, employing Progressive Upscaling, Skip Residual, and Dilated\nSampling mechanisms to achieve higher-resolution image generation. The\nprogressive nature of DemoFusion requires more passes, but the intermediate\nresults can serve as \"previews\", facilitating rapid prompt iteration.\n","authors":["Ruoyi Du","Dongliang Chang","Timothy Hospedales","Yi-Zhe Song","Zhanyu Ma"],"pdf_url":"https://arxiv.org/pdf/2311.16973v2.pdf","comment":"Project Page: https://ruoyidu.github.io/demofusion/demofusion.html"},{"id":"http://arxiv.org/abs/2303.05754v2","updated":"2023-12-15T02:12:55Z","published":"2023-03-10T07:42:49Z","title":"Decomposed Diffusion Sampler for Accelerating Large-Scale Inverse\n  Problems","summary":"  Krylov subspace, which is generated by multiplying a given vector by the\nmatrix of a linear transformation and its successive powers, has been\nextensively studied in classical optimization literature to design algorithms\nthat converge quickly for large linear inverse problems. For example, the\nconjugate gradient method (CG), one of the most popular Krylov subspace\nmethods, is based on the idea of minimizing the residual error in the Krylov\nsubspace. However, with the recent advancement of high-performance diffusion\nsolvers for inverse problems, it is not clear how classical wisdom can be\nsynergistically combined with modern diffusion models. In this study, we\npropose a novel and efficient diffusion sampling strategy that synergistically\ncombine the diffusion sampling and Krylov subspace methods. Specifically, we\nprove that if the tangent space at a denoised sample by Tweedie's formula forms\na Krylov subspace, then the CG initialized with the denoised data ensures the\ndata consistency update to remain in the tangent space. This negates the need\nto compute the manifold-constrained gradient (MCG), leading to a more efficient\ndiffusion sampling method. Our method is applicable regardless of the\nparametrization and setting (i.e., VE, VP). Notably, we achieve\nstate-of-the-art reconstruction quality on challenging real-world medical\ninverse imaging problems, including multi-coil MRI reconstruction and 3D CT\nreconstruction. Moreover, our proposed method achieves more than 80 times\nfaster inference time than the previous state-of-the-art method.\n","authors":["Hyungjin Chung","Suhyeon Lee","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2303.05754v2.pdf","comment":"28 pages, 9 figures"},{"id":"http://arxiv.org/abs/2307.13332v2","updated":"2023-12-15T02:11:42Z","published":"2023-07-25T08:44:58Z","title":"The Optimal Approximation Factors in Misspecified Off-Policy Value\n  Function Estimation","summary":"  Theoretical guarantees in reinforcement learning (RL) are known to suffer\nmultiplicative blow-up factors with respect to the misspecification error of\nfunction approximation. Yet, the nature of such \\emph{approximation factors} --\nespecially their optimal form in a given learning problem -- is poorly\nunderstood. In this paper we study this question in linear off-policy value\nfunction estimation, where many open questions remain. We study the\napproximation factor in a broad spectrum of settings, such as with the weighted\n$L_2$-norm (where the weighting is the offline state distribution), the\n$L_\\infty$ norm, the presence vs. absence of state aliasing, and full vs.\npartial coverage of the state space. We establish the optimal asymptotic\napproximation factors (up to constants) for all of these settings. In\nparticular, our bounds identify two instance-dependent factors for the\n$L_2(\\mu)$ norm and only one for the $L_\\infty$ norm, which are shown to\ndictate the hardness of off-policy evaluation under misspecification.\n","authors":["Philip Amortila","Nan Jiang","Csaba Szepesvári"],"pdf_url":"https://arxiv.org/pdf/2307.13332v2.pdf","comment":"Accepted to ICML 2023. The arXiv version contains improved results"},{"id":"http://arxiv.org/abs/2312.08948v2","updated":"2023-12-15T02:06:53Z","published":"2023-12-14T13:59:12Z","title":"LSTM Network Analysis of Vehicle-Type Fatalities on Great Britain's\n  Roads","summary":"  This study harnesses the predictive capabilities of Long Short-Term Memory\n(LSTM) networks to analyse and predict road traffic accidents in Great Britain.\nIt addresses the challenge of traffic accident forecasting, which is paramount\nfor devising effective preventive measures. We utilised an extensive dataset\nencompassing reported collisions, casualties, and vehicles involvements from\n1926 to 2022, provided by the Department for Transport (DfT). The data\nunderwent stringent processing to rectify missing values and normalise\nfeatures, ensuring robust LSTM network input.\n","authors":["Abiodun Finbarrs Oketunji","James Hanify","Salter Heffron-Smith"],"pdf_url":"https://arxiv.org/pdf/2312.08948v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18396v3","updated":"2023-12-15T02:03:10Z","published":"2023-05-28T13:08:13Z","title":"LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly\n  Transformers","summary":"  The community explored to build private inference frameworks for\ntransformer-based large language models (LLMs) in a server-client setting,\nwhere the server holds the model parameters and the client inputs its private\ndata (or prompt) for inference. However, these frameworks impose significant\noverhead when the private inputs are forward propagated through the original\nLLMs. In this paper, we show that substituting the computation- and\ncommunication-heavy operators in the transformer architecture with\nprivacy-computing friendly approximations can greatly reduce the private\ninference costs while incurring very minor impact on model performance.\nCompared to state-of-the-art Iron (NeurIPS 2022), our privacy-computing\nfriendly model inference pipeline achieves a $5\\times$ acceleration in\ncomputation and an 80% reduction in communication overhead, while retaining\nnearly identical accuracy.\n","authors":["Xuanqi Liu","Zhuotao Liu"],"pdf_url":"https://arxiv.org/pdf/2305.18396v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05836v4","updated":"2023-12-15T01:58:38Z","published":"2023-11-10T02:47:15Z","title":"UMedNeRF: Uncertainty-aware Single View Volumetric Rendering for Medical\n  Neural Radiance Fields","summary":"  In the field of clinical medicine, computed tomography (CT) is an effective\nmedical imaging modality for the diagnosis of various pathologies. Compared\nwith X-ray images, CT images can provide more information, including\nmulti-planar slices and three-dimensional structures for clinical diagnosis.\nHowever, CT imaging requires patients to be exposed to large doses of ionizing\nradiation for a long time, which may cause irreversible physical harm. In this\npaper, we propose an Uncertainty-aware MedNeRF (UMedNeRF) network based on\ngenerated radiation fields. The network can learn a continuous representation\nof CT projections from 2D X-ray images by obtaining the internal structure and\ndepth information and using adaptive loss weights to ensure the quality of the\ngenerated images. Our model is trained on publicly available knee and chest\ndatasets, and we show the results of CT projection rendering with a single\nX-ray and compare our method with other methods based on generated radiation\nfields.\n","authors":["Jing Hu","Qinrui Fan","Shu Hu","Siwei Lyu","Xi Wu","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2311.05836v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09489v1","updated":"2023-12-15T01:56:27Z","published":"2023-12-15T01:56:27Z","title":"Multi-stage Learning for Radar Pulse Activity Segmentation","summary":"  Radio signal recognition is a crucial function in electronic warfare. Precise\nidentification and localisation of radar pulse activities are required by\nelectronic warfare systems to produce effective countermeasures. Despite the\nimportance of these tasks, deep learning-based radar pulse activity recognition\nmethods have remained largely underexplored. While deep learning for radar\nmodulation recognition has been explored previously, classification tasks are\ngenerally limited to short and non-interleaved IQ signals, limiting their\napplicability to military applications. To address this gap, we introduce an\nend-to-end multi-stage learning approach to detect and localise pulse\nactivities of interleaved radar signals across an extended time horizon. We\npropose a simple, yet highly effective multi-stage architecture for\nincrementally predicting fine-grained segmentation masks that localise radar\npulse activities across multiple channels. We demonstrate the performance of\nour approach against several reference models on a novel radar dataset, while\nalso providing a first-of-its-kind benchmark for radar pulse activity\nsegmentation.\n","authors":["Zi Huang","Akila Pemasiri","Simon Denman","Clinton Fookes","Terrence Martin"],"pdf_url":"https://arxiv.org/pdf/2312.09489v1.pdf","comment":"5 pages, 8 figures"},{"id":"http://arxiv.org/abs/2312.09488v1","updated":"2023-12-15T01:54:20Z","published":"2023-12-15T01:54:20Z","title":"Sequence adaptive field-imperfection estimation (SAFE): retrospective\n  estimation and correction of $B_1^+$ and $B_0$ inhomogeneities for enhanced\n  MRF quantification","summary":"  $B_1^+$ and $B_0$ field-inhomogeneities can significantly reduce accuracy and\nrobustness of MRF's quantitative parameter estimates. Additional $B_1^+$ and\n$B_0$ calibration scans can mitigate this but add scan time and cannot be\napplied retrospectively to previously collected data. Here, we proposed a\ncalibration-free sequence-adaptive deep-learning framework, to estimate and\ncorrect for $B_1^+$ and $B_0$ effects of any MRF sequence. We demonstrate its\ncapability on arbitrary MRF sequences at 3T, where no training data were\npreviously obtained. Such approach can be applied to any previously-acquired\nand future MRF-scans. The flexibility in directly applying this framework to\nother quantitative sequences is also highlighted.\n","authors":["Mengze Gao","Xiaozhi Cao","Daniel Abraham","Zihan Zhou","Kawin Setsompop"],"pdf_url":"https://arxiv.org/pdf/2312.09488v1.pdf","comment":"12 pages, 5 figures, submitted to International Society for Magnetic\n  Resonance in Medicine 31th Scientific Meeting, 2024"},{"id":"http://arxiv.org/abs/2312.09486v1","updated":"2023-12-15T01:52:35Z","published":"2023-12-15T01:52:35Z","title":"Unraveling Batch Normalization for Realistic Test-Time Adaptation","summary":"  While recent test-time adaptations exhibit efficacy by adjusting batch\nnormalization to narrow domain disparities, their effectiveness diminishes with\nrealistic mini-batches due to inaccurate target estimation. As previous\nattempts merely introduce source statistics to mitigate this issue, the\nfundamental problem of inaccurate target estimation still persists, leaving the\nintrinsic test-time domain shifts unresolved. This paper delves into the\nproblem of mini-batch degradation. By unraveling batch normalization, we\ndiscover that the inexact target statistics largely stem from the substantially\nreduced class diversity in batch. Drawing upon this insight, we introduce a\nstraightforward tool, Test-time Exponential Moving Average (TEMA), to bridge\nthe class diversity gap between training and testing batches. Importantly, our\nTEMA adaptively extends the scope of typical methods beyond the current batch\nto incorporate a diverse set of class information, which in turn boosts an\naccurate target estimation. Built upon this foundation, we further design a\nnovel layer-wise rectification strategy to consistently promote test-time\nperformance. Our proposed method enjoys a unique advantage as it requires\nneither training nor tuning parameters, offering a truly hassle-free solution.\nIt significantly enhances model robustness against shifted domains and\nmaintains resilience in diverse real-world scenarios with various batch sizes,\nachieving state-of-the-art performance on several major benchmarks. Code is\navailable at \\url{https://github.com/kiwi12138/RealisticTTA}.\n","authors":["Zixian Su","Jingwei Guo","Kai Yao","Xi Yang","Qiufeng Wang","Kaizhu Huang"],"pdf_url":"https://arxiv.org/pdf/2312.09486v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.09481v1","updated":"2023-12-15T01:38:26Z","published":"2023-12-15T01:38:26Z","title":"Continual Adversarial Defense","summary":"  In response to the rapidly evolving nature of adversarial attacks on a\nmonthly basis, numerous defenses have been proposed to generalize against as\nmany known attacks as possible. However, designing a defense method that can\ngeneralize to all types of attacks, including unseen ones, is not realistic\nbecause the environment in which defense systems operate is dynamic and\ncomprises various unique attacks used by many attackers. The defense system\nneeds to upgrade itself by utilizing few-shot defense feedback and efficient\nmemory. Therefore, we propose the first continual adversarial defense (CAD)\nframework that adapts to any attacks in a dynamic scenario, where various\nattacks emerge stage by stage. In practice, CAD is modeled under four\nprinciples: (1) continual adaptation to new attacks without catastrophic\nforgetting, (2) few-shot adaptation, (3) memory-efficient adaptation, and (4)\nhigh accuracy on both clean and adversarial images. We leverage cutting-edge\ncontinual learning, few-shot learning, and ensemble learning techniques to\nqualify the principles. Experiments conducted on CIFAR-10 and ImageNet-100\nvalidate the effectiveness of our approach against multiple stages of 10 modern\nadversarial attacks and significant improvements over 10 baseline methods. In\nparticular, CAD is capable of quickly adapting with minimal feedback and a low\ncost of defense failure, while maintaining good performance against old\nattacks. Our research sheds light on a brand-new paradigm for continual defense\nadaptation against dynamic and evolving attacks.\n","authors":["Qian Wang","Yaoyao Liu","Hefei Ling","Yingwei Li","Qihao Liu","Ping Li","Jiazhong Chen","Alan Yuille","Ning Yu"],"pdf_url":"https://arxiv.org/pdf/2312.09481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09478v1","updated":"2023-12-15T01:35:00Z","published":"2023-12-15T01:35:00Z","title":"Entropy Causal Graphs for Multivariate Time Series Anomaly Detection","summary":"  Many multivariate time series anomaly detection frameworks have been proposed\nand widely applied. However, most of these frameworks do not consider intrinsic\nrelationships between variables in multivariate time series data, thus ignoring\nthe causal relationship among variables and degrading anomaly detection\nperformance. This work proposes a novel framework called CGAD, an entropy\nCausal Graph for multivariate time series Anomaly Detection. CGAD utilizes\ntransfer entropy to construct graph structures that unveil the underlying\ncausal relationships among time series data. Weighted graph convolutional\nnetworks combined with causal convolutions are employed to model both the\ncausal graph structures and the temporal patterns within multivariate time\nseries data. Furthermore, CGAD applies anomaly scoring, leveraging median\nabsolute deviation-based normalization to improve the robustness of the anomaly\nidentification process. Extensive experiments demonstrate that CGAD outperforms\nstate-of-the-art methods on real-world datasets with a 15% average improvement\nbased on three different multivariate time series anomaly detection metrics.\n","authors":["Falih Gozi Febrinanto","Kristen Moore","Chandra Thapa","Mujie Liu","Vidya Saikrishna","Jiangang Ma","Feng Xia"],"pdf_url":"https://arxiv.org/pdf/2312.09478v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06403v2","updated":"2023-12-15T01:29:21Z","published":"2023-12-11T14:24:24Z","title":"Debiased Machine Learning and Network Cohesion for Doubly-Robust\n  Differential Reward Models in Contextual Bandits","summary":"  A common approach to learning mobile health (mHealth) intervention policies\nis linear Thompson sampling. Two desirable mHealth policy features are (1)\npooling information across individuals and time and (2) incorporating a\ntime-varying baseline reward. Previous approaches pooled information across\nindividuals but not time, failing to capture trends in treatment effects over\ntime. In addition, these approaches did not explicitly model the baseline\nreward, which limited the ability to precisely estimate the parameters in the\ndifferential reward model. In this paper, we propose a novel Thompson sampling\nalgorithm, termed ''DML-TS-NNR'' that leverages (1) nearest-neighbors to\nefficiently pool information on the differential reward function across users\nand time and (2) the Double Machine Learning (DML) framework to explicitly\nmodel baseline rewards and stay agnostic to the supervised learning algorithms\nused. By explicitly modeling baseline rewards, we obtain smaller confidence\nsets for the differential reward parameters. We offer theoretical guarantees on\nthe pseudo-regret, which are supported by empirical results. Importantly, the\nDML-TS-NNR algorithm demonstrates robustness to potential misspecifications in\nthe baseline reward model.\n","authors":["Easton K. Huch","Jieru Shi","Madeline R. Abbott","Jessica R. Golbus","Alexander Moreno","Walter H. Dempsey"],"pdf_url":"https://arxiv.org/pdf/2312.06403v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11983v3","updated":"2023-12-15T00:35:30Z","published":"2023-09-21T11:39:33Z","title":"Variational Connectionist Temporal Classification for Order-Preserving\n  Sequence Modeling","summary":"  Connectionist temporal classification (CTC) is commonly adopted for sequence\nmodeling tasks like speech recognition, where it is necessary to preserve order\nbetween the input and target sequences. However, CTC is only applied to\ndeterministic sequence models, where the latent space is discontinuous and\nsparse, which in turn makes them less capable of handling data variability when\ncompared to variational models. In this paper, we integrate CTC with a\nvariational model and derive loss functions that can be used to train more\ngeneralizable sequence models that preserve order. Specifically, we derive two\nversions of the novel variational CTC based on two reasonable assumptions, the\nfirst being that the variational latent variables at each time step are\nconditionally independent; and the second being that these latent variables are\nMarkovian. We show that both loss functions allow direct optimization of the\nvariational lower bound for the model log-likelihood, and present\ncomputationally tractable forms for implementing them.\n","authors":["Zheng Nan","Ting Dang","Vidhyasaharan Sethu","Beena Ahmed"],"pdf_url":"https://arxiv.org/pdf/2309.11983v3.pdf","comment":"5 pages, 3 figures, conference"},{"id":"http://arxiv.org/abs/2312.09411v1","updated":"2023-12-15T00:22:55Z","published":"2023-12-15T00:22:55Z","title":"OTOv3: Automatic Architecture-Agnostic Neural Network Training and\n  Compression from Structured Pruning to Erasing Operators","summary":"  Compressing a predefined deep neural network (DNN) into a compact sub-network\nwith competitive performance is crucial in the efficient machine learning\nrealm. This topic spans various techniques, from structured pruning to neural\narchitecture search, encompassing both pruning and erasing operators\nperspectives. Despite advancements, existing methods suffers from complex,\nmulti-stage processes that demand substantial engineering and domain knowledge,\nlimiting their broader applications. We introduce the third-generation\nOnly-Train-Once (OTOv3), which first automatically trains and compresses a\ngeneral DNN through pruning and erasing operations, creating a compact and\ncompetitive sub-network without the need of fine-tuning. OTOv3 simplifies and\nautomates the training and compression process, minimizes the engineering\nefforts required from users. It offers key technological advancements: (i)\nautomatic search space construction for general DNNs based on dependency graph\nanalysis; (ii) Dual Half-Space Projected Gradient (DHSPG) and its enhanced\nversion with hierarchical search (H2SPG) to reliably solve (hierarchical)\nstructured sparsity problems and ensure sub-network validity; and (iii)\nautomated sub-network construction using solutions from DHSPG/H2SPG and\ndependency graphs. Our empirical results demonstrate the efficacy of OTOv3\nacross various benchmarks in structured pruning and neural architecture search.\nOTOv3 produces sub-networks that match or exceed the state-of-the-arts. The\nsource code will be available at https://github.com/tianyic/only_train_once.\n","authors":["Tianyi Chen","Tianyu Ding","Zhihui Zhu","Zeyu Chen","HsiangTao Wu","Ilya Zharkov","Luming Liang"],"pdf_url":"https://arxiv.org/pdf/2312.09411v1.pdf","comment":"39 pages. Due to the page dim limitation, the full appendix is\n  attached here https://tinyurl.com/otov3appendix. Recommend to zoom-in for\n  finer details. arXiv admin note: text overlap with arXiv:2305.18030"},{"id":"http://arxiv.org/abs/2312.09410v1","updated":"2023-12-15T00:21:00Z","published":"2023-12-15T00:21:00Z","title":"Prediction of rare events in the operation of household equipment using\n  co-evolving time series","summary":"  In this study, we propose an approach for predicting rare events by\nexploiting time series in coevolution. Our approach involves a weighted\nautologistic regression model, where we leverage the temporal behavior of the\ndata to enhance predictive capabilities. By addressing the issue of imbalanced\ndatasets, we establish constraints leading to weight estimation and to improved\nperformance. Evaluation on synthetic and real-world datasets confirms that our\napproach outperform state-of-the-art of predicting home equipment failure\nmethods.\n","authors":["Hadia Mecheri","Islam Benamirouche","Feriel Fass","Djemel Ziou","Nassima Kadri"],"pdf_url":"https://arxiv.org/pdf/2312.09410v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2312.09797v1","updated":"2023-12-15T13:54:48Z","published":"2023-12-15T13:54:48Z","title":"Part Representation Learning with Teacher-Student Decoder for Occluded\n  Person Re-identification","summary":"  Occluded person re-identification (ReID) is a very challenging task due to\nthe occlusion disturbance and incomplete target information. Leveraging\nexternal cues such as human pose or parsing to locate and align part features\nhas been proven to be very effective in occluded person ReID. Meanwhile, recent\nTransformer structures have a strong ability of long-range modeling.\nConsidering the above facts, we propose a Teacher-Student Decoder (TSD)\nframework for occluded person ReID, which utilizes the Transformer decoder with\nthe help of human parsing. More specifically, our proposed TSD consists of a\nParsing-aware Teacher Decoder (PTD) and a Standard Student Decoder (SSD). PTD\nemploys human parsing cues to restrict Transformer's attention and imparts this\ninformation to SSD through feature distillation. Thereby, SSD can learn from\nPTD to aggregate information of body parts automatically. Moreover, a mask\ngenerator is designed to provide discriminative regions for better ReID. In\naddition, existing occluded person ReID benchmarks utilize occluded samples as\nqueries, which will amplify the role of alleviating occlusion interference and\nunderestimate the impact of the feature absence issue. Contrastively, we\npropose a new benchmark with non-occluded queries, serving as a complement to\nthe existing benchmark. Extensive experiments demonstrate that our proposed\nmethod is superior and the new benchmark is essential. The source codes are\navailable at https://github.com/hh23333/TSD.\n","authors":["Shang Gao","Chenyang Yu","Pingping Zhang","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2312.09797v1.pdf","comment":"Accepted by ICASSP2024"},{"id":"http://arxiv.org/abs/2312.09753v1","updated":"2023-12-15T12:47:00Z","published":"2023-12-15T12:47:00Z","title":"MORE: A Multimodal Object-Entity Relation Extraction Dataset with a\n  Benchmark Evaluation","summary":"  Extracting relational facts from multimodal data is a crucial task in the\nfield of multimedia and knowledge graphs that feeds into widespread real-world\napplications. The emphasis of recent studies centers on recognizing relational\nfacts in which both entities are present in one modality and supplementary\ninformation is used from other modalities. However, such works disregard a\nsubstantial amount of multimodal relational facts that arise across different\nmodalities, such as one entity seen in a text and another in an image. In this\npaper, we propose a new task, namely Multimodal Object-Entity Relation\nExtraction, which aims to extract \"object-entity\" relational facts from image\nand text data. To facilitate research on this task, we introduce MORE, a new\ndataset comprising 21 relation types and 20,264 multimodal relational facts\nannotated on 3,559 pairs of textual news titles and corresponding images. To\nshow the challenges of Multimodal Object-Entity Relation Extraction, we\nevaluated recent state-of-the-art methods for multimodal relation extraction\nand conducted a comprehensive experimentation analysis on MORE. Our results\ndemonstrate significant challenges for existing methods, underlining the need\nfor further research on this task. Based on our experiments, we identify\nseveral promising directions for future research. The MORE dataset and code are\navailable at https://github.com/NJUNLP/MORE.\n","authors":["Liang He","Hongke Wang","Yongchang Cao","Zhen Wu","Jianbing Zhang","Xinyu Dai"],"pdf_url":"https://arxiv.org/pdf/2312.09753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.11681v3","updated":"2023-12-15T09:42:25Z","published":"2023-08-22T14:58:36Z","title":"VadCLIP: Adapting Vision-Language Models for Weakly Supervised Video\n  Anomaly Detection","summary":"  The recent contrastive language-image pre-training (CLIP) model has shown\ngreat success in a wide range of image-level tasks, revealing remarkable\nability for learning powerful visual representations with rich semantics. An\nopen and worthwhile problem is efficiently adapting such a strong model to the\nvideo domain and designing a robust video anomaly detector. In this work, we\npropose VadCLIP, a new paradigm for weakly supervised video anomaly detection\n(WSVAD) by leveraging the frozen CLIP model directly without any pre-training\nand fine-tuning process. Unlike current works that directly feed extracted\nfeatures into the weakly supervised classifier for frame-level binary\nclassification, VadCLIP makes full use of fine-grained associations between\nvision and language on the strength of CLIP and involves dual branch. One\nbranch simply utilizes visual features for coarse-grained binary\nclassification, while the other fully leverages the fine-grained language-image\nalignment. With the benefit of dual branch, VadCLIP achieves both\ncoarse-grained and fine-grained video anomaly detection by transferring\npre-trained knowledge from CLIP to WSVAD task. We conduct extensive experiments\non two commonly-used benchmarks, demonstrating that VadCLIP achieves the best\nperformance on both coarse-grained and fine-grained WSVAD, surpassing the\nstate-of-the-art methods by a large margin. Specifically, VadCLIP achieves\n84.51% AP and 88.02% AUC on XD-Violence and UCF-Crime, respectively. Code and\nfeatures are released at https://github.com/nwpu-zxr/VadCLIP.\n","authors":["Peng Wu","Xuerong Zhou","Guansong Pang","Lingru Zhou","Qingsen Yan","Peng Wang","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2308.11681v3.pdf","comment":"Accept to AAAI2024"},{"id":"http://arxiv.org/abs/2312.09627v1","updated":"2023-12-15T09:10:05Z","published":"2023-12-15T09:10:05Z","title":"TF-CLIP: Learning Text-free CLIP for Video-based Person\n  Re-Identification","summary":"  Large-scale language-image pre-trained models (e.g., CLIP) have shown\nsuperior performances on many cross-modal retrieval tasks. However, the problem\nof transferring the knowledge learned from such models to video-based person\nre-identification (ReID) has barely been explored. In addition, there is a lack\nof decent text descriptions in current ReID benchmarks. To address these\nissues, in this work, we propose a novel one-stage text-free CLIP-based\nlearning framework named TF-CLIP for video-based person ReID. More\nspecifically, we extract the identity-specific sequence feature as the\nCLIP-Memory to replace the text feature. Meanwhile, we design a\nSequence-Specific Prompt (SSP) module to update the CLIP-Memory online. To\ncapture temporal information, we further propose a Temporal Memory Diffusion\n(TMD) module, which consists of two key components: Temporal Memory\nConstruction (TMC) and Memory Diffusion (MD). Technically, TMC allows the\nframe-level memories in a sequence to communicate with each other, and to\nextract temporal information based on the relations within the sequence. MD\nfurther diffuses the temporal memories to each token in the original features\nto obtain more robust sequence features. Extensive experiments demonstrate that\nour proposed method shows much better results than other state-of-the-art\nmethods on MARS, LS-VID and iLIDS-VID. The code is available at\nhttps://github.com/AsuradaYuci/TF-CLIP.\n","authors":["Chenyang Yu","Xuehu Liu","Yingquan Wang","Pingping Zhang","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2312.09627v1.pdf","comment":"This work is accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2312.09612v1","updated":"2023-12-15T08:54:15Z","published":"2023-12-15T08:54:15Z","title":"TOP-ReID: Multi-spectral Object Re-Identification with Token Permutation","summary":"  Multi-spectral object Re-identification (ReID) aims to retrieve specific\nobjects by leveraging complementary information from different image spectra.\nIt delivers great advantages over traditional single-spectral ReID in complex\nvisual environment. However, the significant distribution gap among different\nimage spectra poses great challenges for effective multi-spectral feature\nrepresentations. In addition, most of current Transformer-based ReID methods\nonly utilize the global feature of class tokens to achieve the holistic\nretrieval, ignoring the local discriminative ones. To address the above issues,\nwe step further to utilize all the tokens of Transformers and propose a cyclic\ntoken permutation framework for multi-spectral object ReID, dubbled TOP-ReID.\nMore specifically, we first deploy a multi-stream deep network based on vision\nTransformers to preserve distinct information from different image spectra.\nThen, we propose a Token Permutation Module (TPM) for cyclic multi-spectral\nfeature aggregation. It not only facilitates the spatial feature alignment\nacross different image spectra, but also allows the class token of each\nspectrum to perceive the local details of other spectra. Meanwhile, we propose\na Complementary Reconstruction Module (CRM), which introduces dense token-level\nreconstruction constraints to reduce the distribution gap across different\nimage spectra. With the above modules, our proposed framework can generate more\ndiscriminative multi-spectral features for robust object ReID. Extensive\nexperiments on three ReID benchmarks (i.e., RGBNT201, RGBNT100 and MSVR310)\nverify the effectiveness of our methods. The code is available at\nhttps://github.com/924973292/TOP-ReID.\n","authors":["Yuhao Wang","Xuehu Liu","Pingping Zhang","Hu Lu","Zhengzheng Tu","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2312.09612v1.pdf","comment":"This work is accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2312.08931v2","updated":"2023-12-15T03:27:30Z","published":"2023-12-13T06:08:37Z","title":"N-Gram Unsupervised Compoundation and Feature Injection for Better\n  Symbolic Music Understanding","summary":"  The first step to apply deep learning techniques for symbolic music\nunderstanding is to transform musical pieces (mainly in MIDI format) into\nsequences of predefined tokens like note pitch, note velocity, and chords.\nSubsequently, the sequences are fed into a neural sequence model to accomplish\nspecific tasks. Music sequences exhibit strong correlations between adjacent\nelements, making them prime candidates for N-gram techniques from Natural\nLanguage Processing (NLP). Consider classical piano music: specific melodies\nmight recur throughout a piece, with subtle variations each time. In this\npaper, we propose a novel method, NG-Midiformer, for understanding symbolic\nmusic sequences that leverages the N-gram approach. Our method involves first\nprocessing music pieces into word-like sequences with our proposed unsupervised\ncompoundation, followed by using our N-gram Transformer encoder, which can\neffectively incorporate N-gram information to enhance the primary encoder part\nfor better understanding of music sequences. The pre-training process on\nlarge-scale music datasets enables the model to thoroughly learn the N-gram\ninformation contained within music sequences, and subsequently apply this\ninformation for making inferences during the fine-tuning stage. Experiment on\nvarious datasets demonstrate the effectiveness of our method and achieved\nstate-of-the-art performance on a series of music understanding downstream\ntasks. The code and model weights will be released at\nhttps://github.com/CinqueOrigin/NG-Midiformer.\n","authors":["Jinhao Tian","Zuchao Li","Jiajia Li","Ping Wang"],"pdf_url":"https://arxiv.org/pdf/2312.08931v2.pdf","comment":"8 pages, 2 figures, aaai2024"},{"id":"http://arxiv.org/abs/2312.09505v1","updated":"2023-12-15T03:06:19Z","published":"2023-12-15T03:06:19Z","title":"Adaptive Integration of Partial Label Learning and Negative Learning for\n  Enhanced Noisy Label Learning","summary":"  There has been significant attention devoted to the effectiveness of various\ndomains, such as semi-supervised learning, contrastive learning, and\nmeta-learning, in enhancing the performance of methods for noisy label learning\n(NLL) tasks. However, most existing methods still depend on prior assumptions\nregarding clean samples amidst different sources of noise (\\eg, a pre-defined\ndrop rate or a small subset of clean samples). In this paper, we propose a\nsimple yet powerful idea called \\textbf{NPN}, which revolutionizes\n\\textbf{N}oisy label learning by integrating \\textbf{P}artial label learning\n(PLL) and \\textbf{N}egative learning (NL). Toward this goal, we initially\ndecompose the given label space adaptively into the candidate and complementary\nlabels, thereby establishing the conditions for PLL and NL. We propose two\nadaptive data-driven paradigms of label disambiguation for PLL: hard\ndisambiguation and soft disambiguation. Furthermore, we generate reliable\ncomplementary labels using all non-candidate labels for NL to enhance model\nrobustness through indirect supervision. To maintain label reliability during\nthe later stage of model training, we introduce a consistency regularization\nterm that encourages agreement between the outputs of multiple augmentations.\nExperiments conducted on both synthetically corrupted and real-world noisy\ndatasets demonstrate the superiority of NPN compared to other state-of-the-art\n(SOTA) methods. The source code has been made available at\n{\\color{purple}{\\url{https://github.com/NUST-Machine-Intelligence-Laboratory/NPN}}}.\n","authors":["Mengmeng Sheng","Zeren Sun","Zhenhuang Cai","Tao Chen","Yichao Zhou","Yazhou Yao"],"pdf_url":"https://arxiv.org/pdf/2312.09505v1.pdf","comment":"accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.10201v1","updated":"2023-12-15T20:58:05Z","published":"2023-12-15T20:58:05Z","title":"CARAT: Contrastive Feature Reconstruction and Aggregation for\n  Multi-modal Multi-label Emotion Recognition","summary":"  Multi-modal multi-label emotion recognition (MMER) aims to identify relevant\nemotions from multiple modalities. The challenge of MMER is how to effectively\ncapture discriminative features for multiple labels from heterogeneous data.\nRecent studies are mainly devoted to exploring various fusion strategies to\nintegrate multi-modal information into a unified representation for all labels.\nHowever, such a learning scheme not only overlooks the specificity of each\nmodality but also fails to capture individual discriminative features for\ndifferent labels. Moreover, dependencies of labels and modalities cannot be\neffectively modeled. To address these issues, this paper presents ContrAstive\nfeature Reconstruction and AggregaTion (CARAT) for the MMER task. Specifically,\nwe devise a reconstruction-based fusion mechanism to better model fine-grained\nmodality-to-label dependencies by contrastively learning modal-separated and\nlabel-specific features. To further exploit the modality complementarity, we\nintroduce a shuffle-based aggregation strategy to enrich co-occurrence\ncollaboration among labels. Experiments on two benchmark datasets CMU-MOSEI and\nM3ED demonstrate the effectiveness of CARAT over state-of-the-art methods. Code\nis available at https://github.com/chengzju/CARAT.\n","authors":["Cheng Peng","Ke Chen","Lidan Shou","Gang Chen"],"pdf_url":"https://arxiv.org/pdf/2312.10201v1.pdf","comment":null}]},"2023-12-18T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2312.09494v2","updated":"2023-12-18T02:50:02Z","published":"2023-12-15T02:42:05Z","title":"No-Skim: Towards Efficiency Robustness Evaluation on Skimming-based\n  Language Models","summary":"  To reduce the computation cost and the energy consumption in large language\nmodels (LLM), skimming-based acceleration dynamically drops unimportant tokens\nof the input sequence progressively along layers of the LLM while preserving\nthe tokens of semantic importance. However, our work for the first time reveals\nthe acceleration may be vulnerable to Denial-of-Service (DoS) attacks. In this\npaper, we propose No-Skim, a general framework to help the owners of\nskimming-based LLM to understand and measure the robustness of their\nacceleration scheme. Specifically, our framework searches minimal and\nunnoticeable perturbations at character-level and token-level to generate\nadversarial inputs that sufficiently increase the remaining token ratio, thus\nincreasing the computation cost and energy consumption. We systematically\nevaluate the vulnerability of the skimming acceleration in various LLM\narchitectures including BERT and RoBERTa on the GLUE benchmark. In the worst\ncase, the perturbation found by No-Skim substantially increases the running\ncost of LLM by over 145% on average. Moreover, No-Skim extends the evaluation\nframework to various scenarios, making the evaluation conductible with\ndifferent level of knowledge.\n","authors":["Shengyao Zhang","Mi Zhang","Xudong Pan","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2312.09494v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03360v2","updated":"2023-12-18T01:43:56Z","published":"2023-12-06T08:55:55Z","title":"Teaching Specific Scientific Knowledge into Large Language Models\n  through Additional Training","summary":"  Through additional training, we explore embedding specialized scientific\nknowledge into the Llama 2 Large Language Model (LLM). Key findings reveal that\neffective knowledge integration requires reading texts from multiple\nperspectives, especially in instructional formats. We utilize text augmentation\nto tackle the scarcity of specialized texts, including style conversions and\ntranslations. Hyperparameter optimization proves crucial, with different size\nmodels (7b, 13b, and 70b) reasonably undergoing additional training. Validating\nour methods, we construct a dataset of 65,000 scientific papers. Although we\nhave succeeded in partially embedding knowledge, the study highlights the\ncomplexities and limitations of incorporating specialized information into\nLLMs, suggesting areas for further improvement.\n","authors":["Kan Hatakeyama-Sato","Yasuhiko Igarashi","Shun Katakami","Yuta Nabae","Teruaki Hayakawa"],"pdf_url":"https://arxiv.org/pdf/2312.03360v2.pdf","comment":"added token information for some texts, and fixed typo"},{"id":"http://arxiv.org/abs/2312.09979v2","updated":"2023-12-18T16:46:13Z","published":"2023-12-15T17:45:06Z","title":"LoRAMoE: Revolutionizing Mixture of Experts for Maintaining World\n  Knowledge in Language Model Alignment","summary":"  Supervised fine-tuning (SFT) is a crucial step for large language models\n(LLMs), enabling them to align with human instructions and enhance their\ncapabilities in downstream tasks. When the models are required to align with a\nbroader range of downstream tasks, or there is a desire to notably improve the\nperformance on a specific task, a substantial increase in fine-tuning data\noften emerges as the solution. However, we find that large-scale increases in\ninstruction data can disrupt the world knowledge previously stored in the LLMs,\ni.e., world knowledge forgetting. In this paper, we introduce LoRAMoE to\naddress the above challenge. The LoRAMoE is a plugin version of Mixture of\nExperts (MoE). The plugin form ensures the integrity of world knowledge by\nfreezing the backbone model during the training phase. We then propose the use\nof localized balancing constraints to coordinate parts of experts for task\nutilization, meanwhile enabling other experts to fully leverage the world\nknowledge stored in the models. Experimental results demonstrate that LoRAMoE\ncan reasonably coordinate experts based on data type during inference, and even\ndramatically increasing instruction data does not result in knowledge\nforgetting. Moreover, LoRAMoE provides additional benefits for the performance\nof downstream tasks, indicating the potential of our approach for multi-task\nlearning.\n","authors":["Shihan Dou","Enyu Zhou","Yan Liu","Songyang Gao","Jun Zhao","Wei Shen","Yuhao Zhou","Zhiheng Xi","Xiao Wang","Xiaoran Fan","Shiliang Pu","Jiang Zhu","Rui Zheng","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2312.09979v2.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2311.15786v4","updated":"2023-12-18T08:08:32Z","published":"2023-11-27T13:01:59Z","title":"YUAN 2.0: A Large Language Model with Localized Filtering-based\n  Attention","summary":"  In this work, we develop and release Yuan 2.0, a series of large language\nmodels with parameters ranging from 2.1 billion to 102.6 billion. The Localized\nFiltering-based Attention (LFA) is introduced to incorporate prior knowledge of\nlocal dependencies of natural language into Attention. A data filtering and\ngenerating system is presented to build pre-training and fine-tuning dataset in\nhigh quality. A distributed training method with non-uniform pipeline parallel,\ndata parallel, and optimizer parallel is proposed, which greatly reduces the\nbandwidth requirements of intra-node communication, and achieves good\nperformance in large-scale distributed training. Yuan 2.0 models display\nimpressive ability in code generation, math problem-solving, and chatting\ncompared with existing models. The latest version of YUAN 2.0, including model\nweights and source code, is accessible at Github.\n","authors":["Shaohua Wu","Xudong Zhao","Shenling Wang","Jiangang Luo","Lingjun Li","Xi Chen","Bing Zhao","Wei Wang","Tong Yu","Rongguo Zhang","Jiahua Zhang","Chao Wang"],"pdf_url":"https://arxiv.org/pdf/2311.15786v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02775v3","updated":"2023-12-18T23:23:06Z","published":"2023-11-05T21:43:02Z","title":"AI-TA: Towards an Intelligent Question-Answer Teaching Assistant using\n  Open-Source LLMs","summary":"  Responding to the thousands of student questions on online QA platforms each\nsemester has a considerable human cost, particularly in computing courses with\nrapidly growing enrollments. To address the challenges of scalable and\nintelligent question-answering (QA), we introduce an innovative solution that\nleverages open-source Large Language Models (LLMs) from the LLaMA-2 family to\nensure data privacy. Our approach combines augmentation techniques such as\nretrieval augmented generation (RAG), supervised fine-tuning (SFT), and\nlearning from human preferences data using Direct Preference Optimization\n(DPO). Through extensive experimentation on a Piazza dataset from an\nintroductory CS course, comprising 10,000 QA pairs and 1,500 pairs of\npreference data, we demonstrate a significant 30% improvement in the quality of\nanswers, with RAG being a particularly impactful addition. Our contributions\ninclude the development of a novel architecture for educational QA, extensive\nevaluations of LLM performance utilizing both human assessments and LLM-based\nmetrics, and insights into the challenges and future directions of educational\ndata processing. This work paves the way for the development of AI-TA, an\nintelligent QA assistant customizable for courses with an online QA platform\n","authors":["Yann Hicke","Anmol Agarwal","Qianou Ma","Paul Denny"],"pdf_url":"https://arxiv.org/pdf/2311.02775v3.pdf","comment":"Updates for camera-ready submission"},{"id":"http://arxiv.org/abs/2309.15016v2","updated":"2023-12-18T21:43:01Z","published":"2023-09-26T15:36:29Z","title":"Question-Answering Approach to Evaluating Legal Summaries","summary":"  Traditional evaluation metrics like ROUGE compare lexical overlap between the\nreference and generated summaries without taking argumentative structure into\naccount, which is important for legal summaries. In this paper, we propose a\nnovel legal summarization evaluation framework that utilizes GPT-4 to generate\na set of question-answer pairs that cover main points and information in the\nreference summary. GPT-4 is then used to generate answers based on the\ngenerated summary for the questions from the reference summary. Finally, GPT-4\ngrades the answers from the reference summary and the generated summary. We\nexamined the correlation between GPT-4 grading with human grading. The results\nsuggest that this question-answering approach with GPT-4 can be a useful tool\nfor gauging the quality of the summary.\n","authors":["Huihui Xu","Kevin Ashley"],"pdf_url":"https://arxiv.org/pdf/2309.15016v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11720v1","updated":"2023-12-18T21:42:34Z","published":"2023-12-18T21:42:34Z","title":"Assessing Logical Reasoning Capabilities of Encoder-Only Transformer\n  Models","summary":"  Logical reasoning is central to complex human activities, such as thinking,\ndebating, and planning; it is also a central component of many AI systems as\nwell. In this paper, we investigate the extent to which encoder-only\ntransformer language models (LMs) can reason according to logical rules. We ask\nwhether those LMs can deduce theorems in propositional calculus and first-order\nlogic; if their relative success in these problems reflects general logical\ncapabilities; and which layers contribute the most to the task. First, we show\nfor several encoder-only LMs that they can be trained, to a reasonable degree,\nto determine logical validity on various datasets. Next, by cross-probing\nfine-tuned models on these datasets, we show that LMs have difficulty in\ntransferring their putative logical reasoning ability, which suggests that they\nmay have learned dataset-specific features, instead of a general capability.\nFinally, we conduct a layerwise probing experiment, which shows that the\nhypothesis classification task is mostly solved through higher layers.\n","authors":["Paulo Pirozelli","Marcos M. José","Paulo de Tarso P. Filho","Anarosa A. F. Brandão","Fabio G. Cozman"],"pdf_url":"https://arxiv.org/pdf/2312.11720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.03498v2","updated":"2023-12-18T21:09:51Z","published":"2023-11-06T20:13:29Z","title":"In-Context Exemplars as Clues to Retrieving from Large Associative\n  Memory","summary":"  Recently, large language models (LLMs) have made remarkable progress in\nnatural language processing. The most representative ability of LLMs is\nin-context learning (ICL), which enables LLMs to learn patterns from in-context\nexemplars without training. The performance of ICL greatly depends on the\nexemplars used. However, how to choose exemplars remains unclear due to the\nlack of understanding of how in-context learning works. In this paper, we\npresent a novel perspective on ICL by conceptualizing it as contextual\nretrieval from a model of associative memory. We establish a theoretical\nframework of ICL based on Hopfield Networks. Based on our framework, we look\ninto how in-context exemplars influence the performance of ICL and propose more\nefficient active exemplar selection. Our study sheds new light on the mechanism\nof ICL by connecting it to memory retrieval, with potential implications for\nadvancing the understanding of LLMs.\n","authors":["Jiachen Zhao"],"pdf_url":"https://arxiv.org/pdf/2311.03498v2.pdf","comment":"Presented at Neural Conversational AI @ ICML 2023 and Associative\n  Memory & Hopfield Networks @ NeurIPS 2023"},{"id":"http://arxiv.org/abs/2312.11703v1","updated":"2023-12-18T21:03:46Z","published":"2023-12-18T21:03:46Z","title":"Shaping Political Discourse using multi-source News Summarization","summary":"  Multi-document summarization is the process of automatically generating a\nconcise summary of multiple documents related to the same topic. This summary\ncan help users quickly understand the key information from a large collection\nof documents. Multi-document summarization systems are more complex than\nsingle-document summarization systems due to the need to identify and combine\ninformation from multiple sources. In this paper, we have developed a machine\nlearning model that generates a concise summary of a topic from multiple news\ndocuments. The model is designed to be unbiased by sampling its input equally\nfrom all the different aspects of the topic, even if the majority of the news\nsources lean one way.\n","authors":["Charles Rajan","Nishit Asnani","Shreya Singh"],"pdf_url":"https://arxiv.org/pdf/2312.11703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11701v1","updated":"2023-12-18T20:58:58Z","published":"2023-12-18T20:58:58Z","title":"Opportunities and Challenges of Applying Large Language Models in\n  Building Energy Efficiency and Decarbonization Studies: An Exploratory\n  Overview","summary":"  In recent years, the rapid advancement and impressive capabilities of Large\nLanguage Models (LLMs) have been evident across various domains. This paper\nexplores the application, implications, and potential of LLMs in building\nenergy efficiency and decarbonization studies. The wide-ranging capabilities of\nLLMs are examined in the context of the building energy field, including\nintelligent control systems, code generation, data infrastructure, knowledge\nextraction, and education. Despite the promising potential of LLMs, challenges\nincluding complex and expensive computation, data privacy, security and\ncopyright, complexity in fine-tuned LLMs, and self-consistency are discussed.\nThe paper concludes with a call for future research focused on the enhancement\nof LLMs for domain-specific tasks, multi-modal LLMs, and collaborative research\nbetween AI and energy experts.\n","authors":["Liang Zhang","Zhelun Chen"],"pdf_url":"https://arxiv.org/pdf/2312.11701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11681v1","updated":"2023-12-18T20:01:58Z","published":"2023-12-18T20:01:58Z","title":"Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows","summary":"  LLM chains enable complex tasks by decomposing work into a sequence of\nsub-tasks. Crowdsourcing workflows similarly decompose complex tasks into\nsmaller tasks for human crowdworkers. Chains address LLM errors analogously to\nthe way crowdsourcing workflows address human error. To characterize\nopportunities for LLM chaining, we survey 107 papers across the crowdsourcing\nand chaining literature to construct a design space for chain development. The\ndesign space connects an LLM designer's objectives to strategies they can use\nto achieve those objectives, and tactics to implement each strategy. To explore\nhow techniques from crowdsourcing may apply to chaining, we adapt crowdsourcing\nworkflows to implement LLM chains across three case studies: creating a\ntaxonomy, shortening text, and writing a short story. From the design space and\nour case studies, we identify which techniques transfer from crowdsourcing to\nLLM chaining and raise implications for future research and development.\n","authors":["Madeleine Grunde-McLaughlin","Michelle S. Lam","Ranjay Krishna","Daniel S. Weld","Jeffrey Heer"],"pdf_url":"https://arxiv.org/pdf/2312.11681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11671v1","updated":"2023-12-18T19:27:09Z","published":"2023-12-18T19:27:09Z","title":"Evaluating Language-Model Agents on Realistic Autonomous Tasks","summary":"  In this report, we explore the ability of language model agents to acquire\nresources, create copies of themselves, and adapt to novel challenges they\nencounter in the wild. We refer to this cluster of capabilities as \"autonomous\nreplication and adaptation\" or ARA. We believe that systems capable of ARA\ncould have wide-reaching and hard-to-anticipate consequences, and that\nmeasuring and forecasting ARA may be useful for informing measures around\nsecurity, monitoring, and alignment. Additionally, once a system is capable of\nARA, placing bounds on a system's capabilities may become significantly more\ndifficult.\n  We construct four simple example agents that combine language models with\ntools that allow them to take actions in the world. We then evaluate these\nagents on 12 tasks relevant to ARA. We find that these language model agents\ncan only complete the easiest tasks from this list, although they make some\nprogress on the more challenging tasks. Unfortunately, these evaluations are\nnot adequate to rule out the possibility that near-future agents will be\ncapable of ARA. In particular, we do not think that these evaluations provide\ngood assurance that the ``next generation'' of language models (e.g. 100x\neffective compute scaleup on existing models) will not yield agents capable of\nARA, unless intermediate evaluations are performed during pretraining.\nRelatedly, we expect that fine-tuning of the existing models could produce\nsubstantially more competent agents, even if the fine-tuning is not directly\ntargeted at ARA.\n","authors":["Megan Kinniment","Lucas Jun Koba Sato","Haoxing Du","Brian Goodrich","Max Hasin","Lawrence Chan","Luke Harold Miles","Tao R. Lin","Hjalmar Wijk","Joel Burget","Aaron Ho","Elizabeth Barnes","Paul Christiano"],"pdf_url":"https://arxiv.org/pdf/2312.11671v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2312.11462v1","updated":"2023-12-18T18:59:46Z","published":"2023-12-18T18:59:46Z","title":"Cascade Speculative Drafting for Even Faster LLM Inference","summary":"  Speculative decoding enhances the efficiency of large language models (LLMs)\nby leveraging a draft model to draft for a larger target model to review.\nHowever, drafting in speculative decoding involves slow autoregressive\ngeneration and generating tokens of different importance with the same time\nallocation. These two inefficiencies lead to its suboptimal performance. To\naddress this issue, we introduce Cascade Speculative Drafting (CS. Drafting), a\nnovel approach that employs two types of cascades. The Vertical Cascade\neliminates autoregressive generation from neural models. The Horizontal Cascade\nconstitutes efficient time allocation in drafting with its optimality supported\nby our theoretical analysis. Combining both cascades, our CS. Drafting\nalgorithm has achieved up to 72 percent additional speedup over speculative\ndecoding in our experiments while keeping the same output distribution.\n","authors":["Ziyi Chen","Xiaocong Yang","Jiacheng Lin","Chenkai Sun","Jie Huang","Kevin Chen-Chuan Chang"],"pdf_url":"https://arxiv.org/pdf/2312.11462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17333v2","updated":"2023-12-18T18:55:49Z","published":"2023-10-26T11:59:45Z","title":"Arabic Fine-Grained Entity Recognition","summary":"  Traditional NER systems are typically trained to recognize coarse-grained\nentities, and less attention is given to classifying entities into a hierarchy\nof fine-grained lower-level subtypes. This article aims to advance Arabic NER\nwith fine-grained entities. We chose to extend Wojood (an open-source Nested\nArabic Named Entity Corpus) with subtypes. In particular, four main entity\ntypes in Wojood, geopolitical entity (GPE), location (LOC), organization (ORG),\nand facility (FAC), are extended with 31 subtypes. To do this, we first revised\nWojood's annotations of GPE, LOC, ORG, and FAC to be compatible with the LDC's\nACE guidelines, which yielded 5, 614 changes. Second, all mentions of GPE, LOC,\nORG, and FAC (~44K) in Wojood are manually annotated with the LDC's ACE\nsub-types. We refer to this extended version of Wojood as WojoodF ine. To\nevaluate our annotations, we measured the inter-annotator agreement (IAA) using\nboth Cohen's Kappa and F1 score, resulting in 0.9861 and 0.9889, respectively.\nTo compute the baselines of WojoodF ine, we fine-tune three pre-trained Arabic\nBERT encoders in three settings: flat NER, nested NER and nested NER with\nsubtypes and achieved F1 score of 0.920, 0.866, and 0.885, respectively. Our\ncorpus and models are open-source and available at\nhttps://sina.birzeit.edu/wojood/.\n","authors":["Haneen Liqreina","Mustafa Jarrar","Mohammed Khalilia","Ahmed Oumar El-Shangiti","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2310.17333v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11444v1","updated":"2023-12-18T18:47:42Z","published":"2023-12-18T18:47:42Z","title":"An In-depth Look at Gemini's Language Abilities","summary":"  The recently released Google Gemini class of models are the first to\ncomprehensively report results that rival the OpenAI GPT series across a wide\nvariety of tasks. In this paper, we do an in-depth exploration of Gemini's\nlanguage abilities, making two contributions. First, we provide a third-party,\nobjective comparison of the abilities of the OpenAI GPT and Google Gemini\nmodels with reproducible code and fully transparent results. Second, we take a\ncloser look at the results, identifying areas where one of the two model\nclasses excels. We perform this analysis over 10 datasets testing a variety of\nlanguage abilities, including reasoning, answering knowledge-based questions,\nsolving math problems, translating between languages, generating code, and\nacting as instruction-following agents. From this analysis, we find that Gemini\nPro achieves accuracy that is close but slightly inferior to the corresponding\nGPT 3.5 Turbo on all tasks that we benchmarked. We further provide explanations\nfor some of this under-performance, including failures in mathematical\nreasoning with many digits, sensitivity to multiple-choice answer ordering,\naggressive content filtering, and others. We also identify areas where Gemini\ndemonstrates comparably high performance, including generation into non-English\nlanguages, and handling longer and more complex reasoning chains. Code and data\nfor reproduction can be found at https://github.com/neulab/gemini-benchmark\n","authors":["Syeda Nahida Akter","Zichun Yu","Aashiq Muhamed","Tianyue Ou","Alex Bäuerle","Ángel Alexander Cabrera","Krish Dholakia","Chenyan Xiong","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2312.11444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11441v1","updated":"2023-12-18T18:44:10Z","published":"2023-12-18T18:44:10Z","title":"Social Learning: Towards Collaborative Learning with Large Language\n  Models","summary":"  We introduce the framework of \"social learning\" in the context of large\nlanguage models (LLMs), whereby models share knowledge with each other in a\nprivacy-aware manner using natural language. We present and evaluate two\napproaches for knowledge transfer between LLMs. In the first scenario, we allow\nthe model to generate abstract prompts aiming to teach the task. In our second\napproach, models transfer knowledge by generating synthetic examples. We\nevaluate these methods across diverse datasets and quantify memorization as a\nproxy for privacy loss. These techniques inspired by social learning yield\npromising results with low memorization of the original data. In particular, we\nshow that performance using these methods is comparable to results with the use\nof original labels and prompts. Our work demonstrates the viability of social\nlearning for LLMs, establishes baseline approaches and highlights several\nunexplored areas for future work.\n","authors":["Amirkeivan Mohtashami","Florian Hartmann","Sian Gooding","Lukas Zilka","Matt Sharifi","Blaise Aguera y Arcas"],"pdf_url":"https://arxiv.org/pdf/2312.11441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11420v1","updated":"2023-12-18T18:21:43Z","published":"2023-12-18T18:21:43Z","title":"Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM\n  Finetuning","summary":"  This paper introduces an efficient strategy to transform Large Language\nModels (LLMs) into Multi-Modal Large Language Models (MLLMs). By\nconceptualizing this transformation as a domain adaptation process, i.e.,\ntransitioning from text understanding to embracing multiple modalities, we\nintriguingly note that, within each attention block, tuning LayerNorm suffices\nto yield strong performance. Moreover, when benchmarked against other tuning\napproaches like full parameter finetuning or LoRA, its benefits on efficiency\nare substantial. For example, when compared to LoRA on a 13B model scale,\nperformance can be enhanced by an average of over 20% across five multi-modal\ntasks, and meanwhile, results in a significant reduction of trainable\nparameters by 41.9% and a decrease in GPU memory usage by 17.6%. On top of this\nLayerNorm strategy, we showcase that selectively tuning only with\nconversational data can improve efficiency further. Beyond these empirical\noutcomes, we provide a comprehensive analysis to explore the role of LayerNorm\nin adapting LLMs to the multi-modal domain and improving the expressive power\nof the model.\n","authors":["Bingchen Zhao","Haoqin Tu","Chen Wei","Jieru Mei","Cihang Xie"],"pdf_url":"https://arxiv.org/pdf/2312.11420v1.pdf","comment":"The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2312.11399v1","updated":"2023-12-18T18:02:41Z","published":"2023-12-18T18:02:41Z","title":"News Signals: An NLP Library for Text and Time Series","summary":"  We present an open-source Python library for building and using datasets\nwhere inputs are clusters of textual data, and outputs are sequences of real\nvalues representing one or more time series signals. The news-signals library\nsupports diverse data science and NLP problem settings related to the\nprediction of time series behaviour using textual data feeds. For example, in\nthe news domain, inputs are document clusters corresponding to daily news\narticles about a particular entity, and targets are explicitly associated\nreal-valued time series: the volume of news about a particular person or\ncompany, or the number of pageviews of specific Wikimedia pages. Despite many\nindustry and research use cases for this class of problem settings, to the best\nof our knowledge, News Signals is the only open-source library designed\nspecifically to facilitate data science and research settings with natural\nlanguage inputs and time series targets. In addition to the core codebase for\nbuilding and interacting with datasets, we also conduct a suite of experiments\nusing several popular Machine Learning libraries, which are used to establish\nbaselines for time series anomaly prediction using textual inputs.\n","authors":["Chris Hokamp","Demian Gholipour Ghalandari","Parsa Ghaffari"],"pdf_url":"https://arxiv.org/pdf/2312.11399v1.pdf","comment":"EMNLP NLP-OSS Workshop, December 2023"},{"id":"http://arxiv.org/abs/2312.11395v1","updated":"2023-12-18T17:55:05Z","published":"2023-12-18T17:55:05Z","title":"Verb Categorisation for Hindi Word Problem Solving","summary":"  Word problem Solving is a challenging NLP task that deals with solving\nmathematical problems described in natural language. Recently, there has been\nrenewed interest in developing word problem solvers for Indian languages. As\npart of this paper, we have built a Hindi arithmetic word problem solver which\nmakes use of verbs. Additionally, we have created verb categorization data for\nHindi. Verbs are very important for solving word problems with\naddition/subtraction operations as they help us identify the set of operations\nrequired to solve the word problems. We propose a rule-based solver that uses\nverb categorisation to identify operations in a word problem and generate\nanswers for it. To perform verb categorisation, we explore several approaches\nand present a comparative study.\n","authors":["Harshita Sharma","Pruthwik Mishra","Dipti Misra Sharma"],"pdf_url":"https://arxiv.org/pdf/2312.11395v1.pdf","comment":"16 pages, 17 figures, ICON 2023 Conference"},{"id":"http://arxiv.org/abs/2312.11370v1","updated":"2023-12-18T17:36:20Z","published":"2023-12-18T17:36:20Z","title":"G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model","summary":"  Large language models (LLMs) have shown remarkable proficiency in human-level\nreasoning and generation capabilities, which encourages extensive research on\ntheir application in mathematical problem solving. However, current work has\nbeen largely focused on text-based mathematical problems, with limited\ninvestigation in problems involving geometric information. Addressing this gap,\nwe aim to enable LLMs to solve geometric problems by understanding image input.\nWe first analyze the limitations of current Multimodal Large Language Models\n(MLLMs) in this area: they struggle to accurately comprehending basic geometric\nelements and their relationships. To overcome these challenges, we take\nadvantage of the unique characteristics of geometric problems (such as unique\ngeometric logical form, and geometric scalability) and the capacity of the\ntextual LLMs to build an enriched multimodal geometry dataset based on existing\ndata. The augmented dataset, Geo170K, contains more than 170K geometric\nimage-caption and question-answer pairs. Utilizing our constructed Geo170K\ndataset, we develop G-LLaVA, which demonstrates exceptional performance in\nsolving geometric problems, significantly outperforming GPT-4-V on the\nMathVista benchmark with only 7B parameters.\n","authors":["Jiahui Gao","Renjie Pi","Jipeng Zhang","Jiacheng Ye","Wanjun Zhong","Yufei Wang","Lanqing Hong","Jianhua Han","Hang Xu","Zhenguo Li","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2312.11370v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2312.11361v1","updated":"2023-12-18T17:18:04Z","published":"2023-12-18T17:18:04Z","title":"NoMIRACL: Knowing When You Don't Know for Robust Multilingual\n  Retrieval-Augmented Generation","summary":"  Retrieval-augmented generation (RAG) grounds large language model (LLM)\noutput by leveraging external knowledge sources to reduce factual\nhallucinations. However, prior works lack a comprehensive evaluation of\ndifferent language families, making it challenging to evaluate LLM robustness\nagainst errors in external retrieved knowledge. To overcome this, we establish\nNoMIRACL, a human-annotated dataset for evaluating LLM robustness in RAG across\n18 typologically diverse languages. NoMIRACL includes both a non-relevant and a\nrelevant subset. Queries in the non-relevant subset contain passages manually\njudged as non-relevant or noisy, whereas queries in the relevant subset include\nat least a single judged relevant passage. We measure LLM robustness using two\nmetrics: (i) hallucination rate, measuring model tendency to hallucinate an\nanswer, when the answer is not present in passages in the non-relevant subset,\nand (ii) error rate, measuring model inaccuracy to recognize relevant passages\nin the relevant subset. We build a GPT-4 baseline which achieves a 33.2%\nhallucination rate on the non-relevant and a 14.9% error rate on the relevant\nsubset on average. Our evaluation reveals that GPT-4 hallucinates frequently in\nhigh-resource languages, such as French or English. This work highlights an\nimportant avenue for future research to improve LLM robustness to learn how to\nbetter reject non-relevant information in RAG.\n","authors":["Nandan Thakur","Luiz Bonifacio","Xinyu Zhang","Odunayo Ogundepo","Ehsan Kamalloo","David Alfonso-Hermelo","Xiaoguang Li","Qun Liu","Boxing Chen","Mehdi Rezagholizadeh","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2312.11361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11356v1","updated":"2023-12-18T17:12:35Z","published":"2023-12-18T17:12:35Z","title":"The Problem of Coherence in Natural Language Explanations of\n  Recommendations","summary":"  Providing natural language explanations for recommendations is particularly\nuseful from the perspective of a non-expert user. Although several methods for\nproviding such explanations have recently been proposed, we argue that an\nimportant aspect of explanation quality has been overlooked in their\nexperimental evaluation. Specifically, the coherence between generated text and\npredicted rating, which is a necessary condition for an explanation to be\nuseful, is not properly captured by currently used evaluation measures. In this\npaper, we highlight the issue of explanation and prediction coherence by 1)\npresenting results from a manual verification of explanations generated by one\nof the state-of-the-art approaches 2) proposing a method of automatic coherence\nevaluation 3) introducing a new transformer-based method that aims to produce\nmore coherent explanations than the state-of-the-art approaches 4) performing\nan experimental evaluation which demonstrates that this method significantly\nimproves the explanation coherence without affecting the other aspects of\nrecommendation performance.\n","authors":["Jakub Raczyński","Mateusz Lango","Jerzy Stefanowski"],"pdf_url":"https://arxiv.org/pdf/2312.11356v1.pdf","comment":"ECAI 2023"},{"id":"http://arxiv.org/abs/2312.11345v1","updated":"2023-12-18T16:51:26Z","published":"2023-12-18T16:51:26Z","title":"Implicit Affordance Acquisition via Causal Action-Effect Modeling in the\n  Video Domain","summary":"  Affordance knowledge is a fundamental aspect of commonsense knowledge. Recent\nfindings indicate that world knowledge emerges through large-scale\nself-supervised pretraining, motivating our exploration of acquiring affordance\nknowledge from the visual domain. To this end, we augment an existing\ninstructional video resource to create the new Causal Action-Effect (CAE)\ndataset and design two novel pretraining tasks -- Masked Action Modeling (MAM)\nand Masked Effect Modeling (MEM) -- promoting the acquisition of two affordance\nproperties in models: behavior and entity equivalence, respectively. We\nempirically demonstrate the effectiveness of our proposed methods in learning\naffordance properties. Furthermore, we show that a model pretrained on both\ntasks outperforms a strong image-based visual-linguistic foundation model\n(FLAVA) as well as pure linguistic models on a zero-shot physical reasoning\nprobing task.\n","authors":["Hsiu-Yu Yang","Carina Silberer"],"pdf_url":"https://arxiv.org/pdf/2312.11345v1.pdf","comment":"Accepted at IJCNLP-AACL 2023"},{"id":"http://arxiv.org/abs/2312.11344v1","updated":"2023-12-18T16:50:27Z","published":"2023-12-18T16:50:27Z","title":"Muted: Multilingual Targeted Offensive Speech Identification and\n  Visualization","summary":"  Offensive language such as hate, abuse, and profanity (HAP) occurs in various\ncontent on the web. While previous work has mostly dealt with sentence level\nannotations, there have been a few recent attempts to identify offensive spans\nas well. We build upon this work and introduce Muted, a system to identify\nmultilingual HAP content by displaying offensive arguments and their targets\nusing heat maps to indicate their intensity. Muted can leverage any\ntransformer-based HAP-classification model and its attention mechanism\nout-of-the-box to identify toxic spans, without further fine-tuning. In\naddition, we use the spaCy library to identify the specific targets and\narguments for the words predicted by the attention heatmaps. We present the\nmodel's performance on identifying offensive spans and their targets in\nexisting datasets and present new annotations on German text. Finally, we\ndemonstrate our proposed visualization tool on multilingual inputs.\n","authors":["Christoph Tillmann","Aashka Trivedi","Sara Rosenthal","Santosh Borse","Rong Zhang","Avirup Sil","Bishwaranjan Bhattacharjee"],"pdf_url":"https://arxiv.org/pdf/2312.11344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.16456v2","updated":"2023-12-18T16:27:03Z","published":"2023-07-31T07:31:48Z","title":"Camoscio: an Italian Instruction-tuned LLaMA","summary":"  In recent years Large Language Models (LLMs) have increased the state of the\nart on several natural language processing tasks. However, their accessibility\nis often limited to paid API services, posing challenges for researchers in\nconducting extensive investigations. On the other hand, while some open-source\nmodels have been proposed by the community, they are typically English-centric\nor multilingual without a specific adaptation for the Italian language. In an\neffort to democratize the available and open resources for the Italian\nlanguage, in this paper we introduce Camoscio: a language model specifically\ntuned to follow users' prompts in Italian. Specifically, we finetuned the\nsmallest variant of LLaMA (7b) with LoRA on a corpus of instruction prompts\ntranslated to Italian via ChatGPT. Results indicate that the model's zero-shot\nperformance on various downstream tasks in Italian competes favorably with\nexisting models specifically finetuned for those tasks. All the artifacts\n(code, dataset, model) are released to the community at the following url:\nhttps://github.com/teelinsan/camoscio\n","authors":["Andrea Santilli","Emanuele Rodolà"],"pdf_url":"https://arxiv.org/pdf/2307.16456v2.pdf","comment":"Published at CLiC-it 2023"},{"id":"http://arxiv.org/abs/2308.09720v2","updated":"2023-12-18T16:17:36Z","published":"2023-08-09T09:15:07Z","title":"On the Unexpected Abilities of Large Language Models","summary":"  Large Language Models (LLMs) are capable of displaying a wide range of\nabilities that are not directly connected with the task for which they are\ntrained: predicting the next words of human-written texts. In this article, I\nreview recent research investigating the cognitive abilities developed by LLMs\nand their relation to human cognition. I discuss the nature of the indirect\nprocess that leads to the acquisition of these cognitive abilities, their\nrelation to other indirect processes, and the implications for the acquisition\nof integrated abilities. Moreover, I propose the factors that enable the\ndevelopment of abilities that are related only very indirectly to the proximal\nobjective of the training task. Finally, I discuss whether the full set of\ncapabilities that LLMs could possibly develop is predictable.\n","authors":["Stefano Nolfi"],"pdf_url":"https://arxiv.org/pdf/2308.09720v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2308.15452v6","updated":"2023-12-18T16:15:33Z","published":"2023-08-29T17:22:39Z","title":"When Do Program-of-Thoughts Work for Reasoning?","summary":"  In the realm of embodied artificial intelligence, the reasoning capabilities\nof Large Language Models (LLMs) play a pivotal role. Although there are\neffective methods like program-of-thought prompting for LLMs which uses\nprogramming language to tackle complex reasoning tasks, the specific impact of\ncode data on the improvement of reasoning capabilities remains under-explored.\nTo address this gap, we propose complexity-impacted reasoning score (CIRS),\nwhich combines structural and logical attributes, to measure the correlation\nbetween code and reasoning abilities. Specifically, we use the abstract syntax\ntree to encode the structural information and calculate logical complexity by\nconsidering the difficulty and the cyclomatic complexity. Through an empirical\nanalysis, we find not all code data of complexity can be learned or understood\nby LLMs. Optimal level of complexity is critical to the improvement of\nreasoning abilities by program-aided prompting. Then we design an\nauto-synthesizing and stratifying algorithm, and apply it to instruction\ngeneration for mathematical reasoning and code data filtering for code\ngeneration tasks. Extensive results demonstrates the effectiveness of our\nproposed approach. Code will be integrated into the EasyInstruct framework at\nhttps://github.com/zjunlp/EasyInstruct.\n","authors":["Zhen Bi","Ningyu Zhang","Yinuo Jiang","Shumin Deng","Guozhou Zheng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2308.15452v6.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2301.10405v7","updated":"2023-12-18T16:09:49Z","published":"2023-01-25T04:45:06Z","title":"Editing Language Model-based Knowledge Graph Embeddings","summary":"  Recently decades have witnessed the empirical success of framing Knowledge\nGraph (KG) embeddings via language models. However, language model-based KG\nembeddings are usually deployed as static artifacts, making them difficult to\nmodify post-deployment without re-training after deployment. To address this\nissue, we propose a new task of editing language model-based KG embeddings in\nthis paper. This task is designed to facilitate rapid, data-efficient updates\nto KG embeddings without compromising the performance of other aspects. We\nbuild four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and\nevaluate several knowledge editing baselines demonstrating the limited ability\nof previous models to handle the proposed challenging task. We further propose\na simple yet strong baseline dubbed KGEditor, which utilizes additional\nparametric layers of the hypernetwork to edit/add facts. Our comprehensive\nexperimental results reveal that KGEditor excels in updating specific facts\nwithout impacting the overall performance, even when faced with limited\ntraining resources. Code and datasets are available in\nhttps://github.com/zjunlp/PromptKG/tree/main/deltaKG.\n","authors":["Siyuan Cheng","Bozhong Tian","Xi Chen","Ningyu Zhang","Qingbing Liu","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2301.10405v7.pdf","comment":"AAAI 2024. The project website is\n  https://zjunlp.github.io/project/KGE_Editing/"},{"id":"http://arxiv.org/abs/2312.11312v1","updated":"2023-12-18T16:06:18Z","published":"2023-12-18T16:06:18Z","title":"APE-then-QE: Correcting then Filtering Pseudo Parallel Corpora for MT\n  Training Data Creation","summary":"  Automatic Post-Editing (APE) is the task of automatically identifying and\ncorrecting errors in the Machine Translation (MT) outputs. We propose a\nrepair-filter-use methodology that uses an APE system to correct errors on the\ntarget side of the MT training data. We select the sentence pairs from the\noriginal and corrected sentence pairs based on the quality scores computed\nusing a Quality Estimation (QE) model. To the best of our knowledge, this is a\nnovel adaptation of APE and QE to extract quality parallel corpus from the\npseudo-parallel corpus. By training with this filtered corpus, we observe an\nimprovement in the Machine Translation system's performance by 5.64 and 9.91\nBLEU points, for English-Marathi and Marathi-English, over the baseline model.\nThe baseline model is the one that is trained on the whole pseudo-parallel\ncorpus. Our work is not limited by the characteristics of English or Marathi\nlanguages; and is language pair-agnostic, given the necessary QE and APE data.\n","authors":["Akshay Batheja","Sourabh Deoghare","Diptesh Kanojia","Pushpak Bhattacharyya"],"pdf_url":"https://arxiv.org/pdf/2312.11312v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2306.03507"},{"id":"http://arxiv.org/abs/2308.08796v2","updated":"2023-12-18T15:51:43Z","published":"2023-08-17T06:04:28Z","title":"Chinese Spelling Correction as Rephrasing Language Model","summary":"  This paper studies Chinese Spelling Correction (CSC), which aims to detect\nand correct the potential spelling errors in a given sentence. Current\nstate-of-the-art methods regard CSC as a sequence tagging task and fine-tune\nBERT-based models on sentence pairs. However, we note a critical flaw in the\nprocess of tagging one character to another, that the correction is excessively\nconditioned on the error. This is opposite from human mindset, where\nindividuals rephrase the complete sentence based on its semantics, rather than\nsolely on the error patterns memorized before. Such a counter-intuitive\nlearning process results in the bottleneck of generalizability and\ntransferability of machine spelling correction. To address this, we propose\nRephrasing Language Model (ReLM), where the model is trained to rephrase the\nentire sentence by infilling additional slots, instead of\ncharacter-to-character tagging. This novel training paradigm achieves the new\nstate-of-the-art results across fine-tuned and zero-shot CSC benchmarks,\noutperforming previous counterparts by a large margin. Our method also learns\ntransferable language representation when CSC is jointly trained with other\ntasks.\n","authors":["Linfeng Liu","Hongqiu Wu","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2308.08796v2.pdf","comment":"Accepted by AAAI'2024"},{"id":"http://arxiv.org/abs/2312.11296v1","updated":"2023-12-18T15:45:39Z","published":"2023-12-18T15:45:39Z","title":"From Generalized Laughter to Personalized Chuckles: Unleashing the Power\n  of Data Fusion in Subjective Humor Detection","summary":"  The vast area of subjectivity in Natural Language Processing (NLP) poses a\nchallenge to the solutions typically used in generalized tasks. As exploration\nin the scope of generalized NLP is much more advanced, it implies the\ntremendous gap that is still to be addressed amongst all feasible tasks where\nan opinion, taste, or feelings are inherent, thus creating a need for a\nsolution, where a data fusion could take place. We have chosen the task of\nfunniness, as it heavily relies on the sense of humor, which is fundamentally\nsubjective. Our experiments across five personalized and four generalized\ndatasets involving several personalized deep neural architectures have shown\nthat the task of humor detection greatly benefits from the inclusion of\npersonalized data in the training process. We tested five scenarios of training\ndata fusion that focused on either generalized (majority voting) or\npersonalized approaches to humor detection. The best results were obtained for\nthe setup, in which all available personalized datasets were joined to train\nthe personalized reasoning model. It boosted the prediction performance by up\nto approximately 35% of the macro F1 score. Such a significant gain was\nobserved for all five personalized test sets. At the same time, the impact of\nthe model's architecture was much less than the personalization itself. It\nseems that concatenating personalized datasets, even with the cost of\nnormalizing the range of annotations across all datasets, if combined with the\npersonalized models, results in an enormous increase in the performance of\nhumor detection.\n","authors":["Julita Bielaniewicz","Przemysław Kazienko"],"pdf_url":"https://arxiv.org/pdf/2312.11296v1.pdf","comment":"10 pages, 13 figures, 2 tables"},{"id":"http://arxiv.org/abs/2312.11282v1","updated":"2023-12-18T15:23:06Z","published":"2023-12-18T15:23:06Z","title":"LLM-ARK: Knowledge Graph Reasoning Using Large Language Models via Deep\n  Reinforcement Learning","summary":"  With the evolution of pre-training methods, large language models (LLMs) have\nexhibited exemplary reasoning capabilities via prompt engineering. However, the\nabsence of Knowledge Graph (KG) environment awareness and the challenge of\nengineering viable optimization mechanisms for intermediary reasoning\nprocesses, constrict the performance of LLMs on KG reasoning tasks compared to\nsmaller models. We introduce LLM-ARK, a LLM grounded KG reasoning agent\ndesigned to deliver precise and adaptable predictions on KG paths. LLM-ARK\nutilizes Full Textual Environment (FTE) prompts to assimilate state information\nfor each step-sized intelligence. Leveraging LLMs to richly encode and\nrepresent various types of inputs and integrate the knowledge graph further\nwith path environment data, before making the final decision. Reframing the\nKnowledge Graph (KG) multi-hop inference problem as a sequential\ndecision-making issue, we optimize our model using the Proximal Policy\nOptimization (PPO) online policy gradient reinforcement learning algorithm\nwhich allows the model to learn from a vast array of reward signals across\ndiverse tasks and environments. We evaluate state-of-the-art LLM(GPT-4) and our\nmethod which using open-source models of varying sizes on OpenDialKG dataset.\nOur experiment shows that LLaMA7B-ARK provides excellent results with a\nperformance rate of 48.75% for the target@1 evaluation metric, far exceeding\nthe current state-of-the-art model by 17.64 percentage points. Meanwhile, GPT-4\naccomplished a score of only 14.91%, further highlighting the efficacy and\ncomplexity of our methodology. Our code is available on GitHub for further\naccess.\n","authors":["Yuxuan Huang"],"pdf_url":"https://arxiv.org/pdf/2312.11282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11272v1","updated":"2023-12-18T15:16:54Z","published":"2023-12-18T15:16:54Z","title":"Disentangling continuous and discrete linguistic signals in\n  transformer-based sentence embeddings","summary":"  Sentence and word embeddings encode structural and semantic information in a\ndistributed manner. Part of the information encoded -- particularly lexical\ninformation -- can be seen as continuous, whereas other -- like structural\ninformation -- is most often discrete. We explore whether we can compress\ntransformer-based sentence embeddings into a representation that separates\ndifferent linguistic signals -- in particular, information relevant to\nsubject-verb agreement and verb alternations. We show that by compressing an\ninput sequence that shares a targeted phenomenon into the latent layer of a\nvariational autoencoder-like system, the targeted linguistic information\nbecomes more explicit. A latent layer with both discrete and continuous\ncomponents captures better the targeted phenomena than a latent layer with only\ndiscrete or only continuous components. These experiments are a step towards\nseparating linguistic signals from distributed text embeddings and linking them\nto more symbolic representations.\n","authors":["Vivi Nastase","Paola Merlo"],"pdf_url":"https://arxiv.org/pdf/2312.11272v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11242v1","updated":"2023-12-18T14:40:20Z","published":"2023-12-18T14:40:20Z","title":"MAC-SQL: Multi-Agent Collaboration for Text-to-SQL","summary":"  Recent advancements in Text-to-SQL methods employing Large Language Models\n(LLMs) have demonstrated remarkable performance. Nonetheless, these approaches\ncontinue to encounter difficulties when handling extensive databases, intricate\nuser queries, and erroneous SQL results. To tackle these challenges, we present\n\\textbf{MAC-SQL}, a LLM-based multi-agent collaborative Text- to-SQL framework\nbased on LLMs. This framework comprises three agents: the \\textit{Selector},\naccountable for condensing voluminous databases and preserving relevant table\nschemas for user questions; the \\textit{Decomposer}, which disassembles complex\nuser questions into more straightforward sub-problems and resolves them\nprogressively; and the \\textit{Refiner}, tasked with validating and refining\ndefective SQL queries. We perform thorough experiments on two Text-to-SQL\ndatasets, BIRD and Spider, attaining a state-of-the-art execution accuracy of\n59.59\\% on the BIRD test set. Moreover, we have open-sourced an instruction\nfine-tuning model, \\textbf{SQL-Llama}, based on Code Llama 7B, in addition to\nan agent instruction dataset derived from training data based on BIRD and\nSpider. The SQL-Llama model has demonstrated encouraging outcomes on the\ndevelopment sets of both BIRD and Spider. However, when compared to the GPT-4\nmodel, there remains a notable potential for enhancement. Our code and data can\nbe accessed publicly at\n\\href{https://github.com/wbbeyourself/MAC-SQL}{https://github.com/wbbeyourself/MAC-SQL}.\n","authors":["Bing Wang","Changyu Ren","Jian Yang","Xinnian Liang","Jiaqi Bai","Qian-Wen Zhang","Zhao Yan","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2312.11242v1.pdf","comment":"Working in progress"},{"id":"http://arxiv.org/abs/2310.12439v2","updated":"2023-12-18T13:20:46Z","published":"2023-10-19T03:25:28Z","title":"PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models","summary":"  Prompts have significantly improved the performance of pretrained Large\nLanguage Models (LLMs) on various downstream tasks recently, making them\nincreasingly indispensable for a diverse range of LLM application scenarios.\nHowever, the backdoor vulnerability, a serious security threat that can\nmaliciously alter the victim model's normal predictions, has not been\nsufficiently explored for prompt-based LLMs. In this paper, we present\nPOISONPROMPT, a novel backdoor attack capable of successfully compromising both\nhard and soft prompt-based LLMs. We evaluate the effectiveness, fidelity, and\nrobustness of POISONPROMPT through extensive experiments on three popular\nprompt methods, using six datasets and three widely used LLMs. Our findings\nhighlight the potential security threats posed by backdoor attacks on\nprompt-based LLMs and emphasize the need for further research in this area.\n","authors":["Hongwei Yao","Jian Lou","Zhan Qin"],"pdf_url":"https://arxiv.org/pdf/2310.12439v2.pdf","comment":"To Appear in IEEE ICASSP 2024, code is available at:\n  https://github.com/grasses/PoisonPrompt"},{"id":"http://arxiv.org/abs/2309.15512v2","updated":"2023-12-18T12:52:08Z","published":"2023-09-27T09:27:03Z","title":"High-Fidelity Speech Synthesis with Minimal Supervision: All Using\n  Diffusion Models","summary":"  Text-to-speech (TTS) methods have shown promising results in voice cloning,\nbut they require a large number of labeled text-speech pairs.\nMinimally-supervised speech synthesis decouples TTS by combining two types of\ndiscrete speech representations(semantic \\& acoustic) and using two\nsequence-to-sequence tasks to enable training with minimal supervision.\nHowever, existing methods suffer from information redundancy and dimension\nexplosion in semantic representation, and high-frequency waveform distortion in\ndiscrete acoustic representation. Autoregressive frameworks exhibit typical\ninstability and uncontrollability issues. And non-autoregressive frameworks\nsuffer from prosodic averaging caused by duration prediction models. To address\nthese issues, we propose a minimally-supervised high-fidelity speech synthesis\nmethod, where all modules are constructed based on the diffusion models. The\nnon-autoregressive framework enhances controllability, and the duration\ndiffusion model enables diversified prosodic expression. Contrastive\nToken-Acoustic Pretraining (CTAP) is used as an intermediate semantic\nrepresentation to solve the problems of information redundancy and dimension\nexplosion in existing semantic coding methods. Mel-spectrogram is used as the\nacoustic representation. Both semantic and acoustic representations are\npredicted by continuous variable regression tasks to solve the problem of\nhigh-frequency fine-grained waveform distortion. Experimental results show that\nour proposed method outperforms the baseline method. We provide audio samples\non our website.\n","authors":["Chunyu Qiang","Hao Li","Yixin Tian","Yi Zhao","Ying Zhang","Longbiao Wang","Jianwu Dang"],"pdf_url":"https://arxiv.org/pdf/2309.15512v2.pdf","comment":"Accepted by ICASSP 2024. arXiv admin note: substantial text overlap\n  with arXiv:2307.15484; text overlap with arXiv:2309.00424"},{"id":"http://arxiv.org/abs/2309.00424v5","updated":"2023-12-18T12:49:49Z","published":"2023-09-01T12:35:43Z","title":"Learning Speech Representation From Contrastive Token-Acoustic\n  Pretraining","summary":"  For fine-grained generation and recognition tasks such as\nminimally-supervised text-to-speech (TTS), voice conversion (VC), and automatic\nspeech recognition (ASR), the intermediate representations extracted from\nspeech should serve as a \"bridge\" between text and acoustic information,\ncontaining information from both modalities. The semantic content is\nemphasized, while the paralinguistic information such as speaker identity and\nacoustic details should be de-emphasized. However, existing methods for\nextracting fine-grained intermediate representations from speech suffer from\nissues of excessive redundancy and dimension explosion. Contrastive learning is\na good method for modeling intermediate representations from two modalities.\nHowever, existing contrastive learning methods in the audio field focus on\nextracting global descriptive information for downstream audio classification\ntasks, making them unsuitable for TTS, VC, and ASR tasks. To address these\nissues, we propose a method named \"Contrastive Token-Acoustic Pretraining\n(CTAP)\", which uses two encoders to bring phoneme and speech into a joint\nmultimodal space, learning how to connect phoneme and speech at the frame\nlevel. The CTAP model is trained on 210k speech and phoneme pairs, achieving\nminimally-supervised TTS, VC, and ASR. The proposed CTAP method offers a\npromising solution for fine-grained generation and recognition downstream tasks\nin speech processing. We provide a website with audio samples.\n","authors":["Chunyu Qiang","Hao Li","Yixin Tian","Ruibo Fu","Tao Wang","Longbiao Wang","Jianwu Dang"],"pdf_url":"https://arxiv.org/pdf/2309.00424v5.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2307.15484v3","updated":"2023-12-18T12:48:01Z","published":"2023-07-28T11:20:23Z","title":"Minimally-Supervised Speech Synthesis with Conditional Diffusion Model\n  and Language Model: A Comparative Study of Semantic Coding","summary":"  Recently, there has been a growing interest in text-to-speech (TTS) methods\nthat can be trained with minimal supervision by combining two types of discrete\nspeech representations and using two sequence-to-sequence tasks to decouple\nTTS. However, existing methods suffer from three problems: the high\ndimensionality and waveform distortion of discrete speech representations, the\nprosodic averaging problem caused by the duration prediction model in\nnon-autoregressive frameworks, and the information redundancy and dimension\nexplosion problems of existing semantic encoding methods. To address these\nproblems, three progressive methods are proposed. First, we propose\nDiff-LM-Speech, an autoregressive structure consisting of a language model and\ndiffusion models, which models the semantic embedding into the mel-spectrogram\nbased on a diffusion model to achieve higher audio quality. We also introduce a\nprompt encoder structure based on a variational autoencoder and a prosody\nbottleneck to improve prompt representation ability. Second, we propose\nTetra-Diff-Speech, a non-autoregressive structure consisting of four diffusion\nmodel-based modules that design a duration diffusion model to achieve diverse\nprosodic expressions. Finally, we propose Tri-Diff-Speech, a non-autoregressive\nstructure consisting of three diffusion model-based modules that verify the\nnon-necessity of existing semantic encoding models and achieve the best\nresults. Experimental results show that our proposed methods outperform\nbaseline methods. We provide a website with audio samples.\n","authors":["Chunyu Qiang","Hao Li","Hao Ni","He Qu","Ruibo Fu","Tao Wang","Longbiao Wang","Jianwu Dang"],"pdf_url":"https://arxiv.org/pdf/2307.15484v3.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.11152v1","updated":"2023-12-18T12:46:09Z","published":"2023-12-18T12:46:09Z","title":"Prompt Based Tri-Channel Graph Convolution Neural Network for Aspect\n  Sentiment Triplet Extraction","summary":"  Aspect Sentiment Triplet Extraction (ASTE) is an emerging task to extract a\ngiven sentence's triplets, which consist of aspects, opinions, and sentiments.\nRecent studies tend to address this task with a table-filling paradigm, wherein\nword relations are encoded in a two-dimensional table, and the process involves\nclarifying all the individual cells to extract triples. However, these studies\nignore the deep interaction between neighbor cells, which we find quite helpful\nfor accurate extraction. To this end, we propose a novel model for the ASTE\ntask, called Prompt-based Tri-Channel Graph Convolution Neural Network\n(PT-GCN), which converts the relation table into a graph to explore more\ncomprehensive relational information. Specifically, we treat the original table\ncells as nodes and utilize a prompt attention score computation module to\ndetermine the edges' weights. This enables us to construct a target-aware\ngrid-like graph to enhance the overall extraction process. After that, a\ntriple-channel convolution module is conducted to extract precise sentiment\nknowledge. Extensive experiments on the benchmark datasets show that our model\nachieves state-of-the-art performance. The code is available at\nhttps://github.com/KunPunCN/PT-GCN.\n","authors":["Kun Peng","Lei Jiang","Hao Peng","Rui Liu","Zhengtao Yu","Jiaqian Ren","Zhifeng Hao","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2312.11152v1.pdf","comment":"Accepted in SIAM International Conference on Data Mining (SDM24)"},{"id":"http://arxiv.org/abs/2312.11142v1","updated":"2023-12-18T12:32:42Z","published":"2023-12-18T12:32:42Z","title":"Efficiency-oriented approaches for self-supervised speech representation\n  learning","summary":"  Self-supervised learning enables the training of large neural models without\nthe need for large, labeled datasets. It has been generating breakthroughs in\nseveral fields, including computer vision, natural language processing,\nbiology, and speech. In particular, the state-of-the-art in several speech\nprocessing applications, such as automatic speech recognition or speaker\nidentification, are models where the latent representation is learned using\nself-supervised approaches. Several configurations exist in self-supervised\nlearning for speech, including contrastive, predictive, and multilingual\napproaches. There is, however, a crucial limitation in most existing\napproaches: their high computational costs. These costs limit the deployment of\nmodels, the size of the training dataset, and the number of research groups\nthat can afford research with large self-supervised models. Likewise, we should\nconsider the environmental costs that high energy consumption implies. Efforts\nin this direction comprise optimization of existing models, neural architecture\nefficiency, improvements in finetuning for speech processing tasks, and data\nefficiency. But despite current efforts, more work could be done to address\nhigh computational costs in self-supervised representation learning.\n","authors":["Luis Lugo","Valentin Vielzeuf"],"pdf_url":"https://arxiv.org/pdf/2312.11142v1.pdf","comment":"16 pages, 3 figures"},{"id":"http://arxiv.org/abs/2305.19972v2","updated":"2023-12-18T12:29:00Z","published":"2023-05-31T16:01:20Z","title":"VILAS: Exploring the Effects of Vision and Language Context in Automatic\n  Speech Recognition","summary":"  Enhancing automatic speech recognition (ASR) performance by leveraging\nadditional multimodal information has shown promising results in previous\nstudies. However, most of these works have primarily focused on utilizing\nvisual cues derived from human lip motions. In fact, context-dependent visual\nand linguistic cues can also benefit in many scenarios. In this paper, we first\npropose ViLaS (Vision and Language into Automatic Speech Recognition), a novel\nmultimodal ASR model based on the continuous integrate-and-fire (CIF)\nmechanism, which can integrate visual and textual context simultaneously or\nseparately, to facilitate speech recognition. Next, we introduce an effective\ntraining strategy that improves performance in modal-incomplete test scenarios.\nThen, to explore the effects of integrating vision and language, we create\nVSDial, a multimodal ASR dataset with multimodal context cues in both Chinese\nand English versions. Finally, empirical results are reported on the public\nFlickr8K and self-constructed VSDial datasets. We explore various cross-modal\nfusion schemes, analyze fine-grained crossmodal alignment on VSDial, and\nprovide insights into the effects of integrating multimodal information on\nspeech recognition.\n","authors":["Ziyi Ni","Minglun Han","Feilong Chen","Linghui Meng","Jing Shi","Pin Lv","Bo Xu"],"pdf_url":"https://arxiv.org/pdf/2305.19972v2.pdf","comment":"Accepted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.11135v1","updated":"2023-12-18T12:26:27Z","published":"2023-12-18T12:26:27Z","title":"Linear Attention via Orthogonal Memory","summary":"  Efficient attentions have greatly improved the computational efficiency of\nTransformers. However, most existing linear attention mechanisms suffer from an\n\\emph{efficiency degradation} problem, leading to inefficiencies in causal\nlanguage modeling and hindering their application in long-range language\nmodels. This problem is more pronounced under language modeling with unbounded\ncontexts. In this paper, we propose \\textbf{L}inear \\textbf{A}ttention\n\\textbf{V}ia \\textbf{O}rthogonal memory~(\\shortname) to address these\nlimitations, achieving strong performance while maintaining linear complexity.\n\\shortname employs orthogonal decomposition to compress a context into a\nfixed-size orthogonal memory while effectively minimizing redundancy within the\ncontext. Given that orthogonal memory compresses global information, we further\ndissect the context to amplify fine-grained local information. Additionally, we\nembed the relative position encoding into \\shortname to improve the\nextrapolation ability. Experimental results show that \\shortname greatly\nimproves the efficiency of the causal language model with the best\nextrapolation performance and outperforms other efficient baselines. Further,\nwe endeavor to employ \\shortname for unbounded language modeling and\nsuccessfully scale the context length to 128K.\n","authors":["Jun Zhang","Shuyang Jiang","Jiangtao Feng","Lin Zheng","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2312.11135v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2309.05173v3","updated":"2023-12-18T12:17:54Z","published":"2023-09-11T00:02:05Z","title":"DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning","summary":"  Prompt tuning (PT), where a small amount of trainable soft (continuous)\nprompt vectors is affixed to the input of language models (LM), has shown\npromising results across various tasks and models for parameter-efficient\nfine-tuning (PEFT). PT stands out from other PEFT approaches because it\nmaintains competitive performance with fewer trainable parameters and does not\ndrastically scale up its parameters as the model size expands. However, PT\nintroduces additional soft prompt tokens, leading to longer input sequences,\nwhich significantly impacts training and inference time and memory usage due to\nthe Transformer's quadratic complexity. Particularly concerning for Large\nLanguage Models (LLMs) that face heavy daily querying. To address this issue,\nwe propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt\ninto a shorter soft prompt and a pair of low-rank matrices that are then\noptimised with two different learning rates. This allows DePT to achieve better\nperformance while saving over 20% memory and time costs compared to vanilla PT\nand its variants, without changing trainable parameter sizes. Through extensive\nexperiments on 23 natural language processing (NLP) and vision-language (VL)\ntasks, we demonstrate that DePT outperforms state-of-the-art PEFT approaches,\nincluding the full fine-tuning baseline in some scenarios. Additionally, we\nempirically show that DEPT grows more efficient as the model size increases.\nOur further study reveals that DePT integrates seamlessly with\nparameter-efficient transfer learning in the few-shot learning setting and\nhighlights its adaptability to various model architectures and sizes.\n","authors":["Zhengxiang Shi","Aldo Lipani"],"pdf_url":"https://arxiv.org/pdf/2309.05173v3.pdf","comment":"Code is available at https://github.com/ZhengxiangShi/DePT"},{"id":"http://arxiv.org/abs/2311.04498v4","updated":"2023-12-18T12:15:26Z","published":"2023-11-08T07:15:05Z","title":"NExT-Chat: An LMM for Chat, Detection and Segmentation","summary":"  The development of large language models (LLMs) has greatly advanced the\nfield of multimodal understanding, leading to the emergence of large multimodal\nmodels (LMMs). In order to enhance the level of visual comprehension, recent\nstudies have equipped LMMs with region-level understanding capabilities by\nrepresenting object bounding box coordinates as a series of text sequences\n(pix2seq). In this paper, we introduce a novel paradigm for object location\nmodeling called pix2emb method, where we ask the LMM to output the location\nembeddings and then decode them with different decoders. This paradigm allows\nus to use different location formats (such as bounding boxes and masks) in\nmultimodal conversations. Leveraging the proposed pix2emb method, we train an\nLMM named NExT-Chat and demonstrate its capability of handling multiple tasks\nlike visual grounding, region captioning, and grounded reasoning. Comprehensive\nexperiments show the effectiveness of our NExT-Chat on various tasks, e.g.,\nNExT-Chat (87.7) vs. Shikra (86.9) on POPE-Random, NExT-Chat (68.9) vs. LISA\n(67.9) on referring expression segmentation task, and NExT-Chat (79.6) vs.\nKosmos-2 (62.3) on region caption task. The code and model are released at\nhttps://github.com/NExT-ChatV/NExT-Chat.\n","authors":["Ao Zhang","Yuan Yao","Wei Ji","Zhiyuan Liu","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2311.04498v4.pdf","comment":"Technical Report (https://next-chatv.github.io/)"},{"id":"http://arxiv.org/abs/2310.17589v2","updated":"2023-12-18T11:34:31Z","published":"2023-10-26T17:11:42Z","title":"An Open Source Data Contamination Report for Large Language Models","summary":"  Data contamination in language model evaluation is increasingly prevalent as\nthe popularity of large language models. It allows models to \"cheat\" via\nmemorisation instead of displaying true capabilities. Therefore, contamination\nanalysis has became an crucial part of reliable model evaluation to validate\nresults. However, existing contamination analysis is usually conducted\ninternally by LLM developers and often lacks transparency and completeness.\nThis paper present an open source data contamination reports for the Llama\nseries models. We analyse six popular multi-choice QA benchmarks and quantify\ntheir overlapping with the training set of Llama. Various levels of\ncontamination ranging from 1\\% to 8.7\\% are found across benchmarks. Our\ncomparison also reveals that Llama models can gain over 5\\% higher accuracy on\ncontaminated subsets versus clean subsets. Data and code are available at:\nhttps://github.com/liyucheng09/Contamination_Detector.\n","authors":["Yucheng Li"],"pdf_url":"https://arxiv.org/pdf/2310.17589v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11069v1","updated":"2023-12-18T10:06:50Z","published":"2023-12-18T10:06:50Z","title":"Patterns of Closeness and Abstractness in Colexifications: The Case of\n  Indigenous Languages in the Americas","summary":"  Colexification refers to linguistic phenomena where multiple concepts\n(meanings) are expressed by the same lexical form, such as polysemy or\nhomophony. Colexifications have been found to be pervasive across languages and\ncultures. The problem of concreteness/abstractness of concepts is\ninterdisciplinary, studied from a cognitive standpoint in linguistics,\npsychology, psycholinguistics, neurophysiology, etc. In this paper, we\nhypothesize that concepts that are closer in concreteness/abstractness are more\nlikey to colexify, and we test the hypothesis across indigenous languages in\nAmericas.\n","authors":["Yiyi Chen","Johannes Bjerva"],"pdf_url":"https://arxiv.org/pdf/2312.11069v1.pdf","comment":"3 pages, 2 figures, 1 table, AmericasNLP 2023"},{"id":"http://arxiv.org/abs/2312.11062v1","updated":"2023-12-18T09:58:19Z","published":"2023-12-18T09:58:19Z","title":"Entity or Relation Embeddings? An Analysis of Encoding Strategies for\n  Relation Extraction","summary":"  Relation extraction is essentially a text classification problem, which can\nbe tackled by fine-tuning a pre-trained language model (LM). However, a key\nchallenge arises from the fact that relation extraction cannot\nstraightforwardly be reduced to sequence or token classification. Existing\napproaches therefore solve the problem in an indirect way: they fine-tune an LM\nto learn embeddings of the head and tail entities, and then predict the\nrelationship from these entity embeddings. Our hypothesis in this paper is that\nrelation extraction models can be improved by capturing relationships in a more\ndirect way. In particular, we experiment with appending a prompt with a [MASK]\ntoken, whose contextualised representation is treated as a relation embedding.\nWhile, on its own, this strategy significantly underperforms the aforementioned\napproach, we find that the resulting relation embeddings are highly\ncomplementary to what is captured by embeddings of the head and tail entity. By\njointly considering both types of representations, we end up with a simple\nmodel that outperforms the state-of-the-art across several relation extraction\nbenchmarks.\n","authors":["Frank Mtumbuka","Steven Schockaert"],"pdf_url":"https://arxiv.org/pdf/2312.11062v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11043v1","updated":"2023-12-18T09:18:43Z","published":"2023-12-18T09:18:43Z","title":"TDeLTA: A Light-weight and Robust Table Detection Method based on\n  Learning Text Arrangement","summary":"  The diversity of tables makes table detection a great challenge, leading to\nexisting models becoming more tedious and complex. Despite achieving high\nperformance, they often overfit to the table style in training set, and suffer\nfrom significant performance degradation when encountering out-of-distribution\ntables in other domains. To tackle this problem, we start from the essence of\nthe table, which is a set of text arranged in rows and columns. Based on this,\nwe propose a novel, light-weighted and robust Table Detection method based on\nLearning Text Arrangement, namely TDeLTA. TDeLTA takes the text blocks as\ninput, and then models the arrangement of them with a sequential encoder and an\nattention module. To locate the tables precisely, we design a\ntext-classification task, classifying the text blocks into 4 categories\naccording to their semantic roles in the tables. Experiments are conducted on\nboth the text blocks parsed from PDF and extracted by open-source OCR tools,\nrespectively. Compared to several state-of-the-art methods, TDeLTA achieves\ncompetitive results with only 3.1M model parameters on the large-scale public\ndatasets. Moreover, when faced with the cross-domain data under the 0-shot\nsetting, TDeLTA outperforms baselines by a large margin of nearly 7%, which\nshows the strong robustness and transferability of the proposed model.\n","authors":["Yang Fan","Xiangping Wu","Qingcai Chen","Heng Li","Yan Huang","Zhixiang Cai","Qitian Wu"],"pdf_url":"https://arxiv.org/pdf/2312.11043v1.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2312.11036v1","updated":"2023-12-18T09:13:41Z","published":"2023-12-18T09:13:41Z","title":"UniGen: A Unified Generative Framework for Retrieval and Question\n  Answering with Large Language Models","summary":"  Generative information retrieval, encompassing two major tasks of Generative\nDocument Retrieval (GDR) and Grounded Answer Generation (GAR), has gained\nsignificant attention in the area of information retrieval and natural language\nprocessing. Existing methods for GDR and GAR rely on separate retrieval and\nreader modules, which hinder simultaneous optimization. To overcome this, we\npresent \\textbf{UniGen}, a \\textbf{Uni}fied \\textbf{Gen}erative framework for\nretrieval and question answering that integrates both tasks into a single\ngenerative model leveraging the capabilities of large language models. UniGen\nemploys a shared encoder and two distinct decoders for generative retrieval and\nquestion answering. To facilitate the learning of both tasks, we introduce\nconnectors, generated by large language models, to bridge the gaps between\nquery inputs and generation targets, as well as between document identifiers\nand answers. Furthermore, we propose an iterative enhancement strategy that\nleverages generated answers and retrieved documents to iteratively improve both\ntasks. Through extensive experiments on the MS MARCO and NQ datasets, we\ndemonstrate the effectiveness of UniGen, showcasing its superior performance in\nboth the retrieval and the question answering tasks.\n","authors":["Xiaoxi Li","Yujia Zhou","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2312.11036v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11374v3","updated":"2023-12-18T08:58:51Z","published":"2023-10-17T16:15:34Z","title":"DialogueLLM: Context and Emotion Knowledge-Tuned Large Language Models\n  for Emotion Recognition in Conversations","summary":"  Large language models (LLMs) and their variants have shown extraordinary\nefficacy across numerous downstream natural language processing (NLP) tasks,\nwhich has presented a new vision for the development of NLP. Despite their\nremarkable performance in natural language generating (NLG), LLMs lack a\ndistinct focus on the emotion understanding domain. As a result, using LLMs for\nemotion recognition may lead to suboptimal and inadequate precision. Another\nlimitation of LLMs is that they are typical trained without leveraging\nmulti-modal information. To overcome these limitations, we propose DialogueLLM,\na context and emotion knowledge tuned LLM that is obtained by fine-tuning LLaMA\nmodels with 13,638 multi-modal (i.e., texts and videos) emotional dialogues.\nThe visual information is considered as the supplementary knowledge to\nconstruct high-quality instructions. We offer a comprehensive evaluation of our\nproposed model on three benchmarking emotion recognition in conversations (ERC)\ndatasets and compare the results against the SOTA baselines and other SOTA\nLLMs. Additionally, DialogueLLM-7B can be easily trained using LoRA on a 40GB\nA100 GPU in 5 hours, facilitating reproducibility for other researchers.\n","authors":["Yazhou Zhang","Mengyao Wang","Youxi Wu","Prayag Tiwari","Qiuchi Li","Benyou Wang","Jing Qin"],"pdf_url":"https://arxiv.org/pdf/2310.11374v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11020v1","updated":"2023-12-18T08:45:39Z","published":"2023-12-18T08:45:39Z","title":"Information Type Classification with Contrastive Task-Specialized\n  Sentence Encoders","summary":"  User-generated information content has become an important information source\nin crisis situations. However, classification models suffer from noise and\nevent-related biases which still poses a challenging task and requires\nsophisticated task-adaptation. To address these challenges, we propose the use\nof contrastive task-specialized sentence encoders for downstream\nclassification. We apply the task-specialization on the CrisisLex, HumAID, and\nTrecIS information type classification tasks and show performance gains w.r.t.\nF1-score. Furthermore, we analyse the cross-corpus and cross-lingual\ncapabilities for two German event relevancy classification datasets.\n","authors":["Philipp Seeberger","Tobias Bocklet","Korbinian Riedhammer"],"pdf_url":"https://arxiv.org/pdf/2312.11020v1.pdf","comment":"Accepted at KONVENS 2023"},{"id":"http://arxiv.org/abs/2312.11011v1","updated":"2023-12-18T08:27:33Z","published":"2023-12-18T08:27:33Z","title":"VinaLLaMA: LLaMA-based Vietnamese Foundation Model","summary":"  In this technical report, we present VinaLLaMA, an open-weight,\nstate-of-the-art (SOTA) Large Language Model for the Vietnamese language, built\nupon LLaMA-2 with an additional 800 billion trained tokens. VinaLLaMA not only\ndemonstrates fluency in Vietnamese but also exhibits a profound understanding\nof Vietnamese culture, making it a truly indigenous model. VinaLLaMA-7B-chat,\ntrained on 1 million high-quality synthetic samples, achieves SOTA results on\nkey benchmarks, including VLSP, VMLU, and Vicuna Benchmark Vietnamese, marking\na significant advancement in the Vietnamese AI landscape and offering a\nversatile resource for various applications.\n","authors":["Quan Nguyen","Huy Pham","Dung Dao"],"pdf_url":"https://arxiv.org/pdf/2312.11011v1.pdf","comment":"VinaLLaMA Technical Report - 13 pages"},{"id":"http://arxiv.org/abs/2308.06077v3","updated":"2023-12-18T08:26:48Z","published":"2023-08-11T11:29:51Z","title":"Fly-Swat or Cannon? Cost-Effective Language Model Choice via\n  Meta-Modeling","summary":"  Generative language models (LMs) have become omnipresent across data science.\nFor a wide variety of tasks, inputs can be phrased as natural language prompts\nfor an LM, from whose output the solution can then be extracted. LM performance\nhas consistently been increasing with model size - but so has the monetary cost\nof querying the ever larger models. Importantly, however, not all inputs are\nequally hard: some require larger LMs for obtaining a satisfactory solution,\nwhereas for others smaller LMs suffice. Based on this fact, we design a\nframework for cost-effective language model choice, called \"Fly-swat or cannon\"\n(FORC). Given a set of inputs and a set of candidate LMs, FORC judiciously\nassigns each input to an LM predicted to do well on the input according to a\nso-called meta-model, aiming to achieve high overall performance at low cost.\nThe cost-performance tradeoff can be flexibly tuned by the user. Options\ninclude, among others, maximizing total expected performance (or the number of\nprocessed inputs) while staying within a given cost budget, or minimizing total\ncost while processing all inputs. We evaluate FORC on 14 datasets covering five\nnatural language tasks, using four candidate LMs of vastly different size and\ncost. With FORC, we match the performance of the largest available LM while\nachieving a cost reduction of 63%. Via our publicly available library,\nresearchers as well as practitioners can thus save large amounts of money\nwithout sacrificing performance.\n","authors":["Marija Šakota","Maxime Peyrard","Robert West"],"pdf_url":"https://arxiv.org/pdf/2308.06077v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10997v1","updated":"2023-12-18T07:47:33Z","published":"2023-12-18T07:47:33Z","title":"Retrieval-Augmented Generation for Large Language Models: A Survey","summary":"  Large language models (LLMs) demonstrate powerful capabilities, but they\nstill face challenges in practical applications, such as hallucinations, slow\nknowledge updates, and lack of transparency in answers. Retrieval-Augmented\nGeneration (RAG) refers to the retrieval of relevant information from external\nknowledge bases before answering questions with LLMs. RAG has been demonstrated\nto significantly enhance answer accuracy, reduce model hallucination,\nparticularly for knowledge-intensive tasks. By citing sources, users can verify\nthe accuracy of answers and increase trust in model outputs. It also\nfacilitates knowledge updates and the introduction of domain-specific\nknowledge. RAG effectively combines the parameterized knowledge of LLMs with\nnon-parameterized external knowledge bases, making it one of the most important\nmethods for implementing large language models. This paper outlines the\ndevelopment paradigms of RAG in the era of LLMs, summarizing three paradigms:\nNaive RAG, Advanced RAG, and Modular RAG. It then provides a summary and\norganization of the three main components of RAG: retriever, generator, and\naugmentation methods, along with key technologies in each component.\nFurthermore, it discusses how to evaluate the effectiveness of RAG models,\nintroducing two evaluation methods for RAG, emphasizing key metrics and\nabilities for evaluation, and presenting the latest automatic evaluation\nframework. Finally, potential future research directions are introduced from\nthree aspects: vertical optimization, horizontal scalability, and the technical\nstack and ecosystem of RAG.\n","authors":["Yunfan Gao","Yun Xiong","Xinyu Gao","Kangxiang Jia","Jinliu Pan","Yuxi Bi","Yi Dai","Jiawei Sun","Haofen Wang"],"pdf_url":"https://arxiv.org/pdf/2312.10997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01090v2","updated":"2023-12-18T07:30:48Z","published":"2023-12-02T09:45:45Z","title":"Self Generated Wargame AI: Double Layer Agent Task Planning Based on\n  Large Language Model","summary":"  The large language models represented by ChatGPT have a disruptive impact on\nthe field of artificial intelligence. But it mainly focuses on natural language\nprocessing, speech recognition, machine learning and natural language\nunderstanding. This paper innovatively applies the large language model to the\nfield of intelligent decision-making, places the large language model in the\ndecision-making center, and constructs an agent architecture with the large\nlanguage model as the core. Based on this, it further proposes a two-layer\nagent task planning, issues and executes decision commands through the\ninteraction of natural language, and carries out simulation verification\nthrough the wargame simulation environment. Through the game confrontation\nsimulation experiment, it is found that the intelligent decision-making ability\nof the large language model is significantly stronger than the commonly used\nreinforcement learning AI and rule AI, and the intelligence, understandability\nand generalization are all better. And through experiments, it was found that\nthe intelligence of the large language model is closely related to prompt. This\nwork also extends the large language model from previous human-computer\ninteraction to the field of intelligent decision-making, which has important\nreference value and significance for the development of intelligent\ndecision-making.\n","authors":["Y. Sun","J. Zhao","C. Yu","W. Wang","X. Zhou"],"pdf_url":"https://arxiv.org/pdf/2312.01090v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.13177v2","updated":"2023-12-18T07:29:55Z","published":"2023-08-25T04:54:32Z","title":"How to Evaluate the Generalization of Detection? A Benchmark for\n  Comprehensive Open-Vocabulary Detection","summary":"  Object detection (OD) in computer vision has made significant progress in\nrecent years, transitioning from closed-set labels to open-vocabulary detection\n(OVD) based on large-scale vision-language pre-training (VLP). However, current\nevaluation methods and datasets are limited to testing generalization over\nobject types and referral expressions, which do not provide a systematic,\nfine-grained, and accurate benchmark of OVD models' abilities. In this paper,\nwe propose a new benchmark named OVDEval, which includes 9 sub-tasks and\nintroduces evaluations on commonsense knowledge, attribute understanding,\nposition understanding, object relation comprehension, and more. The dataset is\nmeticulously created to provide hard negatives that challenge models' true\nunderstanding of visual and linguistic input. Additionally, we identify a\nproblem with the popular Average Precision (AP) metric when benchmarking models\non these fine-grained label datasets and propose a new metric called\nNon-Maximum Suppression Average Precision (NMS-AP) to address this issue.\nExtensive experimental results show that existing top OVD models all fail on\nthe new tasks except for simple object types, demonstrating the value of the\nproposed dataset in pinpointing the weakness of current OVD models and guiding\nfuture research. Furthermore, the proposed NMS-AP metric is verified by\nexperiments to provide a much more truthful evaluation of OVD models, whereas\ntraditional AP metrics yield deceptive results. Data is available at\n\\url{https://github.com/om-ai-lab/OVDEval}\n","authors":["Yiyang Yao","Peng Liu","Tiancheng Zhao","Qianqian Zhang","Jiajia Liao","Chunxin Fang","Kyusong Lee","Qing Wang"],"pdf_url":"https://arxiv.org/pdf/2308.13177v2.pdf","comment":"Long paper accepted at AAAI 2024"},{"id":"http://arxiv.org/abs/2312.10987v1","updated":"2023-12-18T07:22:39Z","published":"2023-12-18T07:22:39Z","title":"Data Contamination Issues in Brain-to-Text Decoding","summary":"  Decoding non-invasive cognitive signals to natural language has long been the\ngoal of building practical brain-computer interfaces (BCIs). Recent major\nmilestones have successfully decoded cognitive signals like functional Magnetic\nResonance Imaging (fMRI) and electroencephalogram (EEG) into text under open\nvocabulary setting. However, how to split the datasets for training,\nvalidating, and testing in cognitive signal decoding task still remains\ncontroversial. In this paper, we conduct systematic analysis on current dataset\nsplitting methods and find the existence of data contamination largely\nexaggerates model performance. Specifically, first we find the leakage of test\nsubjects' cognitive signals corrupts the training of a robust encoder. Second,\nwe prove the leakage of text stimuli causes the auto-regressive decoder to\nmemorize information in test set. The decoder generates highly accurate text\nnot because it truly understands cognitive signals. To eliminate the influence\nof data contamination and fairly evaluate different models' generalization\nability, we propose a new splitting method for different types of cognitive\ndatasets (e.g. fMRI, EEG). We also test the performance of SOTA Brain-to-Text\ndecoding models under the proposed dataset splitting paradigm as baselines for\nfurther research.\n","authors":["Congchi Yin","Qian Yu","Zhiwei Fang","Jie He","Changping Peng","Zhangang Lin","Jingping Shao","Piji Li"],"pdf_url":"https://arxiv.org/pdf/2312.10987v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2309.11082v2","updated":"2023-12-18T06:47:29Z","published":"2023-09-20T06:08:11Z","title":"Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial\n  Margin Contrastive Learning","summary":"  In recent years, the explosion of web videos makes text-video retrieval\nincreasingly essential and popular for video filtering, recommendation, and\nsearch. Text-video retrieval aims to rank relevant text/video higher than\nirrelevant ones. The core of this task is to precisely measure the cross-modal\nsimilarity between texts and videos. Recently, contrastive learning methods\nhave shown promising results for text-video retrieval, most of which focus on\nthe construction of positive and negative pairs to learn text and video\nrepresentations. Nevertheless, they do not pay enough attention to hard\nnegative pairs and lack the ability to model different levels of semantic\nsimilarity. To address these two issues, this paper improves contrastive\nlearning using two novel techniques. First, to exploit hard examples for robust\ndiscriminative power, we propose a novel Dual-Modal Attention-Enhanced Module\n(DMAE) to mine hard negative pairs from textual and visual clues. By further\nintroducing a Negative-aware InfoNCE (NegNCE) loss, we are able to adaptively\nidentify all these hard negatives and explicitly highlight their impacts in the\ntraining loss. Second, our work argues that triplet samples can better model\nfine-grained semantic similarity compared to pairwise samples. We thereby\npresent a new Triplet Partial Margin Contrastive Learning (TPM-CL) module to\nconstruct partial order triplet samples by automatically generating\nfine-grained hard negatives for matched text-video pairs. The proposed TPM-CL\ndesigns an adaptive token masking strategy with cross-modal interaction to\nmodel subtle semantic differences. Extensive experiments demonstrate that the\nproposed approach outperforms existing methods on four widely-used text-video\nretrieval datasets, including MSR-VTT, MSVD, DiDeMo and ActivityNet.\n","authors":["Chen Jiang","Hong Liu","Xuzheng Yu","Qing Wang","Yuan Cheng","Jia Xu","Zhongyi Liu","Qingpei Guo","Wei Chu","Ming Yang","Yuan Qi"],"pdf_url":"https://arxiv.org/pdf/2309.11082v2.pdf","comment":"Accepted by ACM MM 2023"},{"id":"http://arxiv.org/abs/2312.10967v1","updated":"2023-12-18T06:41:23Z","published":"2023-12-18T06:41:23Z","title":"Knowledge Graphs and Pre-trained Language Models enhanced Representation\n  Learning for Conversational Recommender Systems","summary":"  Conversational recommender systems (CRS) utilize natural language\ninteractions and dialogue history to infer user preferences and provide\naccurate recommendations. Due to the limited conversation context and\nbackground knowledge, existing CRSs rely on external sources such as knowledge\ngraphs to enrich the context and model entities based on their inter-relations.\nHowever, these methods ignore the rich intrinsic information within entities.\nTo address this, we introduce the Knowledge-Enhanced Entity Representation\nLearning (KERL) framework, which leverages both the knowledge graph and a\npre-trained language model to improve the semantic understanding of entities\nfor CRS. In our KERL framework, entity textual descriptions are encoded via a\npre-trained language model, while a knowledge graph helps reinforce the\nrepresentation of these entities. We also employ positional encoding to\neffectively capture the temporal information of entities in a conversation. The\nenhanced entity representation is then used to develop a recommender component\nthat fuses both entity and contextual representations for more informed\nrecommendations, as well as a dialogue component that generates informative\nentity-related information in the response text. A high-quality knowledge graph\nwith aligned entity descriptions is constructed to facilitate our study, namely\nthe Wiki Movie Knowledge Graph (WikiMKG). The experimental results show that\nKERL achieves state-of-the-art results in both recommendation and response\ngeneration tasks.\n","authors":["Zhangchi Qiu","Ye Tao","Shirui Pan","Alan Wee-Chung Liew"],"pdf_url":"https://arxiv.org/pdf/2312.10967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10964v1","updated":"2023-12-18T06:40:24Z","published":"2023-12-18T06:40:24Z","title":"Generative linguistic representation for spoken language identification","summary":"  Effective extraction and application of linguistic features are central to\nthe enhancement of spoken Language IDentification (LID) performance. With the\nsuccess of recent large models, such as GPT and Whisper, the potential to\nleverage such pre-trained models for extracting linguistic features for LID\ntasks has become a promising area of research. In this paper, we explore the\nutilization of the decoder-based network from the Whisper model to extract\nlinguistic features through its generative mechanism for improving the\nclassification accuracy in LID tasks. We devised two strategies - one based on\nthe language embedding method and the other focusing on direct optimization of\nLID outputs while simultaneously enhancing the speech recognition tasks. We\nconducted experiments on the large-scale multilingual datasets MLS,\nVoxLingua107, and CommonVoice to test our approach. The experimental results\ndemonstrated the effectiveness of the proposed method on both in-domain and\nout-of-domain datasets for LID tasks.\n","authors":["Peng Shen","Xuguang Lu","Hisashi Kawai"],"pdf_url":"https://arxiv.org/pdf/2312.10964v1.pdf","comment":"Accepted by IEEE ASRU2023"},{"id":"http://arxiv.org/abs/2312.10961v1","updated":"2023-12-18T06:31:13Z","published":"2023-12-18T06:31:13Z","title":"Aspect-Based Sentiment Analysis with Explicit Sentiment Augmentations","summary":"  Aspect-based sentiment analysis (ABSA), a fine-grained sentiment\nclassification task, has received much attention recently. Many works\ninvestigate sentiment information through opinion words, such as ''good'' and\n''bad''. However, implicit sentiment widely exists in the ABSA dataset, which\nrefers to the sentence containing no distinct opinion words but still expresses\nsentiment to the aspect term. To deal with implicit sentiment, this paper\nproposes an ABSA method that integrates explicit sentiment augmentations. And\nwe propose an ABSA-specific augmentation method to create such augmentations.\nSpecifically, we post-trains T5 by rule-based data. We employ Syntax Distance\nWeighting and Unlikelihood Contrastive Regularization in the training procedure\nto guide the model to generate an explicit sentiment. Meanwhile, we utilize the\nConstrained Beam Search to ensure the augmentation sentence contains the aspect\nterms. We test ABSA-ESA on two of the most popular benchmarks of ABSA. The\nresults show that ABSA-ESA outperforms the SOTA baselines on implicit and\nexplicit sentiment accuracy.\n","authors":["Jihong Ouyang","Zhiyao Yang","Silong Liang","Bing Wang","Yimeng Wang","Ximing Li"],"pdf_url":"https://arxiv.org/pdf/2312.10961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10959v1","updated":"2023-12-18T06:29:53Z","published":"2023-12-18T06:29:53Z","title":"Speaker Mask Transformer for Multi-talker Overlapped Speech Recognition","summary":"  Multi-talker overlapped speech recognition remains a significant challenge,\nrequiring not only speech recognition but also speaker diarization tasks to be\naddressed. In this paper, to better address these tasks, we first introduce\nspeaker labels into an autoregressive transformer-based speech recognition\nmodel to support multi-speaker overlapped speech recognition. Then, to improve\nspeaker diarization, we propose a novel speaker mask branch to detection the\nspeech segments of individual speakers. With the proposed model, we can perform\nboth speech recognition and speaker diarization tasks simultaneously using a\nsingle model. Experimental results on the LibriSpeech-based overlapped dataset\ndemonstrate the effectiveness of the proposed method in both speech recognition\nand speaker diarization tasks, particularly enhancing the accuracy of speaker\ndiarization in relatively complex multi-talker scenarios.\n","authors":["Peng Shen","Xugang Lu","Hisashi Kawai"],"pdf_url":"https://arxiv.org/pdf/2312.10959v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10952v1","updated":"2023-12-18T06:08:51Z","published":"2023-12-18T06:08:51Z","title":"Soft Alignment of Modality Space for End-to-end Speech Translation","summary":"  End-to-end Speech Translation (ST) aims to convert speech into target text\nwithin a unified model. The inherent differences between speech and text\nmodalities often impede effective cross-modal and cross-lingual transfer.\nExisting methods typically employ hard alignment (H-Align) of individual speech\nand text segments, which can degrade textual representations. To address this,\nwe introduce Soft Alignment (S-Align), using adversarial training to align the\nrepresentation spaces of both modalities. S-Align creates a modality-invariant\nspace while preserving individual modality quality. Experiments on three\nlanguages from the MuST-C dataset show S-Align outperforms H-Align across\nmultiple tasks and offers translation capabilities on par with specialized\ntranslation models.\n","authors":["Yuhao Zhang","Kaiqi Kou","Bei Li","Chen Xu","Chunliang Zhang","Tong Xiao","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2312.10952v1.pdf","comment":"Accepted to ICASSP2024"},{"id":"http://arxiv.org/abs/2312.11572v1","updated":"2023-12-18T05:52:05Z","published":"2023-12-18T05:52:05Z","title":"Regularized Conditional Alignment for Multi-Domain Text Classification","summary":"  The most successful multi-domain text classification (MDTC) approaches employ\nthe shared-private paradigm to facilitate the enhancement of domain-invariant\nfeatures through domain-specific attributes. Additionally, they employ\nadversarial training to align marginal feature distributions. Nevertheless,\nthese methodologies encounter two primary challenges: (1) Neglecting\nclass-aware information during adversarial alignment poses a risk of\nmisalignment; (2) The limited availability of labeled data across multiple\ndomains fails to ensure adequate discriminative capacity for the model. To\ntackle these issues, we propose a method called Regularized Conditional\nAlignment (RCA) to align the joint distributions of domains and classes, thus\nmatching features within the same category and amplifying the discriminative\nqualities of acquired features. Moreover, we employ entropy minimization and\nvirtual adversarial training to constrain the uncertainty of predictions\npertaining to unlabeled data and enhance the model's robustness. Empirical\nresults on two benchmark datasets demonstrate that our RCA approach outperforms\nstate-of-the-art MDTC techniques.\n","authors":["Juntao Hu","Yuan Wu"],"pdf_url":"https://arxiv.org/pdf/2312.11572v1.pdf","comment":"This paper has been accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.10945v1","updated":"2023-12-18T05:50:10Z","published":"2023-12-18T05:50:10Z","title":"LaViP:Language-Grounded Visual Prompts","summary":"  We introduce a language-grounded visual prompting method to adapt the visual\nencoder of vision-language models for downstream tasks. By capitalizing on\nlanguage integration, we devise a parameter-efficient strategy to adjust the\ninput of the visual encoder, eliminating the need to modify or add to the\nmodel's parameters. Due to this design choice, our algorithm can operate even\nin black-box scenarios, showcasing adaptability in situations where access to\nthe model's parameters is constrained. We will empirically demonstrate that,\ncompared to prior art, grounding visual prompts with language enhances both the\naccuracy and speed of adaptation. Moreover, our algorithm excels in\nbase-to-novel class generalization, overcoming limitations of visual prompting\nand exhibiting the capacity to generalize beyond seen classes. We thoroughly\nassess and evaluate our method across a variety of image recognition datasets,\nsuch as EuroSAT, UCF101, DTD, and CLEVR, spanning different learning\nsituations, including few-shot learning, base-to-novel class generalization,\nand transfer learning.\n","authors":["Nilakshan Kunananthaseelan","Jing Zhang","Mehrtash Harandi"],"pdf_url":"https://arxiv.org/pdf/2312.10945v1.pdf","comment":"The 38th Annual AAAI Conference on Artificial Intelligence"},{"id":"http://arxiv.org/abs/2312.08793v2","updated":"2023-12-18T05:23:30Z","published":"2023-12-14T10:27:15Z","title":"Forbidden Facts: An Investigation of Competing Objectives in Llama-2","summary":"  LLMs often face competing pressures (for example helpfulness vs.\nharmlessness). To understand how models resolve such conflicts, we study\nLlama-2-chat models on the forbidden fact task. Specifically, we instruct\nLlama-2 to truthfully complete a factual recall statement while forbidding it\nfrom saying the correct answer. This often makes the model give incorrect\nanswers. We decompose Llama-2 into 1000+ components, and rank each one with\nrespect to how useful it is for forbidding the correct answer. We find that in\naggregate, around 35 components are enough to reliably implement the full\nsuppression behavior. However, these components are fairly heterogeneous and\nmany operate using faulty heuristics. We discover that one of these heuristics\ncan be exploited via a manually designed adversarial attack which we call The\nCalifornia Attack. Our results highlight some roadblocks standing in the way of\nbeing able to successfully interpret advanced ML systems. Project website\navailable at https://forbiddenfacts.github.io .\n","authors":["Tony T. Wang","Miles Wang","Kaivalya Hariharan","Nir Shavit"],"pdf_url":"https://arxiv.org/pdf/2312.08793v2.pdf","comment":"Accepted to the ATTRIB and SoLaR workshops at NeurIPS 2023; (v2:\n  fixed typos)"},{"id":"http://arxiv.org/abs/2312.08583v2","updated":"2023-12-18T05:08:23Z","published":"2023-12-14T01:06:37Z","title":"ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric\n  Strategy for Diverse Generative Tasks","summary":"  This study examines 4-bit quantization methods like GPTQ in large language\nmodels (LLMs), highlighting GPTQ's overfitting and limited enhancement in\nZero-Shot tasks. While prior works merely focusing on zero-shot measurement, we\nextend task scope to more generative categories such as code generation and\nabstractive summarization, in which we found that INT4 quantization can\nsignificantly underperform. However, simply shifting to higher precision\nformats like FP6 has been particularly challenging, thus overlooked, due to\npoor performance caused by the lack of sophisticated integration and system\nacceleration strategies on current AI hardware. Our results show that FP6, even\nwith a coarse-grain quantization scheme, performs robustly across various\nalgorithms and tasks, demonstrating its superiority in accuracy and\nversatility. Notably, with the FP6 quantization, \\codestar-15B model performs\ncomparably to its FP16 counterpart in code generation, and for smaller models\nlike the 406M it closely matches their baselines in summarization. Neither can\nbe achieved by INT4. To better accommodate various AI hardware and achieve the\nbest system performance, we propose a novel 4+2 design for FP6 to achieve\nsimilar latency to the state-of-the-art INT4 fine-grain quantization. With our\ndesign, FP6 can become a promising solution to the current 4-bit quantization\nmethods used in LLMs.\n","authors":["Xiaoxia Wu","Haojun Xia","Stephen Youn","Zhen Zheng","Shiyang Chen","Arash Bakhtiari","Michael Wyatt","Reza Yazdani Aminabadi","Yuxiong He","Olatunji Ruwase","Leon Song","Zhewei Yao"],"pdf_url":"https://arxiv.org/pdf/2312.08583v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00347v2","updated":"2023-12-18T04:59:01Z","published":"2023-12-01T04:51:01Z","title":"RTQ: Rethinking Video-language Understanding Based on Image-text Model","summary":"  Recent advancements in video-language understanding have been established on\nthe foundation of image-text models, resulting in promising outcomes due to the\nshared knowledge between images and videos. However, video-language\nunderstanding presents unique challenges due to the inclusion of highly complex\nsemantic details, which result in information redundancy, temporal dependency,\nand scene complexity. Current techniques have only partially tackled these\nissues, and our quantitative analysis indicates that some of these methods are\ncomplementary. In light of this, we propose a novel framework called RTQ\n(Refine, Temporal model, and Query), which addresses these challenges\nsimultaneously. The approach involves refining redundant information within\nframes, modeling temporal relations among frames, and querying task-specific\ninformation from the videos. Remarkably, our model demonstrates outstanding\nperformance even in the absence of video-language pre-training, and the results\nare comparable with or superior to those achieved by state-of-the-art\npre-training methods. Code is available at\nhttps://github.com/SCZwangxiao/RTQ-MM2023.\n","authors":["Xiao Wang","Yaoyu Li","Tian Gan","Zheng Zhang","Jingjing Lv","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2312.00347v2.pdf","comment":"Accepted by ACM MM 2023 as Oral representation"},{"id":"http://arxiv.org/abs/2312.01040v2","updated":"2023-12-18T04:57:05Z","published":"2023-12-02T05:54:06Z","title":"From Beginner to Expert: Modeling Medical Knowledge into General LLMs","summary":"  Recently, large language model (LLM) based artificial intelligence (AI)\nsystems have demonstrated remarkable capabilities in natural language\nunderstanding and generation. However, these models face a significant\nchallenge when it comes to sensitive applications, such as reasoning over\nmedical knowledge and answering medical questions in a physician-like manner.\nPrior studies attempted to overcome this challenge by increasing the model size\n(>100B) to learn more general medical knowledge, while there is still room for\nimprovement in LLMs with smaller-scale model sizes (<100B). In this work, we\nstart from a pre-trained general LLM model (AntGLM-10B) and fine-tune it from a\nmedical beginner towards a medical expert (called AntGLM-Med-10B), which\nleverages a 3-stage optimization procedure, i.e., general medical knowledge\ninjection, medical domain instruction tuning, and specific medical task\nadaptation. Our contributions are threefold: (1) We specifically investigate\nhow to adapt a pre-trained general LLM in medical domain, especially for a\nspecific medical task. (2) We collect and construct large-scale medical\ndatasets for each stage of the optimization process. These datasets encompass\nvarious data types and tasks, such as question-answering, medical reasoning,\nmulti-choice questions, and medical conversations. (3) Specifically for\nmulti-choice questions in the medical domain, we propose a novel\nVerification-of-Choice approach for prompting engineering, which significantly\nenhances the reasoning ability of LLMs. Remarkably, by combining the above\napproaches, our AntGLM-Med-10B model can outperform the most of LLMs on\nPubMedQA, including both general and medical LLMs, even when these LLMs have\nlarger model size.\n","authors":["Qiang Li","Xiaoyan Yang","Haowen Wang","Qin Wang","Lei Liu","Junjie Wang","Yang Zhang","Mingyuan Chu","Sen Hu","Yicheng Chen","Yue Shen","Cong Fan","Wangshu Zhang","Teng Xu","Jinjie Gu","Jing Zheng","Guannan Zhang Ant Group"],"pdf_url":"https://arxiv.org/pdf/2312.01040v2.pdf","comment":"Developed by Ant Group for PubMedQA leaderboard"},{"id":"http://arxiv.org/abs/2305.03453v4","updated":"2023-12-18T04:49:01Z","published":"2023-05-05T11:56:30Z","title":"T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Mixed Large\n  Language Model Signals for Science Question Answering","summary":"  Large Language Models (LLMs) have recently demonstrated exceptional\nperformance in various Natural Language Processing (NLP) tasks. They have also\nshown the ability to perform chain-of-thought (CoT) reasoning to solve complex\nproblems. Recent studies have explored CoT reasoning in complex multimodal\nscenarios, such as the science question answering task, by fine-tuning\nmultimodal models with high-quality human-annotated CoT rationales. However,\ncollecting high-quality COT rationales is usually time-consuming and costly.\nBesides, the annotated rationales are hardly accurate due to the external\nessential information missed. To address these issues, we propose a novel\nmethod termed T-SciQ that aims at teaching science question answering with LLM\nsignals. The T-SciQ approach generates high-quality CoT rationales as teaching\nsignals and is advanced to train much smaller models to perform CoT reasoning\nin complex modalities. Additionally, we introduce a novel data mixing strategy\nto produce more effective teaching data samples for simple and complex science\nquestion answer problems. Extensive experimental results show that our T-SciQ\nmethod achieves a new state-of-the-art performance on the ScienceQA benchmark,\nwith an accuracy of 96.18%. Moreover, our approach outperforms the most\npowerful fine-tuned baseline by 4.5%. The code is publicly available at\nhttps://github.com/T-SciQ/T-SciQ.\n","authors":["Lei Wang","Yi Hu","Jiabang He","Xing Xu","Ning Liu","Hui Liu","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2305.03453v4.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2308.09936v3","updated":"2023-12-18T04:33:17Z","published":"2023-08-19T07:53:43Z","title":"BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual\n  Questions","summary":"  Vision Language Models (VLMs), which extend Large Language Models (LLM) by\nincorporating visual understanding capability, have demonstrated significant\nadvancements in addressing open-ended visual question-answering (VQA) tasks.\nHowever, these models cannot accurately interpret images infused with text, a\ncommon occurrence in real-world scenarios. Standard procedures for extracting\ninformation from images often involve learning a fixed set of query embeddings.\nThese embeddings are designed to encapsulate image contexts and are later used\nas soft prompt inputs in LLMs. Yet, this process is limited to the token count,\npotentially curtailing the recognition of scenes with text-rich context. To\nimprove upon them, the present study introduces BLIVA: an augmented version of\nInstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings\nfrom InstructBLIP and also directly projects encoded patch embeddings into the\nLLM, a technique inspired by LLaVA. This approach assists the model to capture\nintricate details potentially missed during the query decoding process.\nEmpirical evidence demonstrates that our model, BLIVA, significantly enhances\nperformance in processing text-rich VQA benchmarks (up to 17.76% in OCR-VQA\nbenchmark) and in undertaking general (not particularly text-rich) VQA\nbenchmarks (up to 7.9% in Visual Spatial Reasoning benchmark), and achieved\n17.72% overall improvement in a comprehensive multimodal LLM benchmark (MME),\ncomparing to our baseline InstructBLIP. BLIVA demonstrates significant\ncapability in decoding real-world images, irrespective of text presence. To\ndemonstrate the broad industry applications enabled by BLIVA, we evaluate the\nmodel using a new dataset comprising YouTube thumbnails paired with\nquestion-answer sets across 11 diverse categories. Our code and models are\nfreely accessible at https://github.com/mlpc-ucsd/BLIVA.\n","authors":["Wenbo Hu","Yifan Xu","Yi Li","Weiyue Li","Zeyuan Chen","Zhuowen Tu"],"pdf_url":"https://arxiv.org/pdf/2308.09936v3.pdf","comment":"Accepted at AAAI Conference on Artificial Intelligence (AAAI-24)"},{"id":"http://arxiv.org/abs/2311.08189v3","updated":"2023-12-18T04:01:35Z","published":"2023-11-14T14:22:47Z","title":"All Data on the Table: Novel Dataset and Benchmark for Cross-Modality\n  Scientific Information Extraction","summary":"  Extracting key information from scientific papers has the potential to help\nresearchers work more efficiently and accelerate the pace of scientific\nprogress. Over the last few years, research on Scientific Information\nExtraction (SciIE) witnessed the release of several new systems and benchmarks.\nHowever, existing paper-focused datasets mostly focus only on specific parts of\na manuscript (e.g., abstracts) and are single-modality (i.e., text- or\ntable-only), due to complex processing and expensive annotations. Moreover,\ncore information can be present in either text or tables or across both. To\nclose this gap in data availability and enable cross-modality IE, while\nalleviating labeling costs, we propose a semi-supervised pipeline for\nannotating entities in text, as well as entities and relations in tables, in an\niterative procedure. Based on this pipeline, we release novel resources for the\nscientific community, including a high-quality benchmark, a large-scale corpus,\nand a semi-supervised annotation pipeline. We further report the performance of\nstate-of-the-art IE models on the proposed benchmark dataset, as a baseline.\nLastly, we explore the potential capability of large language models such as\nChatGPT for the current task. Our new dataset, results, and analysis validate\nthe effectiveness and efficiency of our semi-supervised pipeline, and we\ndiscuss its remaining limitations.\n","authors":["Yuhan Li","Jian Wu","Zhiwei Yu","Börje F. Karlsson","Wei Shen","Manabu Okumura","Chin-Yew Lin"],"pdf_url":"https://arxiv.org/pdf/2311.08189v3.pdf","comment":"Work in progress; 17 pages, 6 figures, 11 tables"},{"id":"http://arxiv.org/abs/2311.16502v2","updated":"2023-12-18T03:47:39Z","published":"2023-11-27T17:33:21Z","title":"MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning\n  Benchmark for Expert AGI","summary":"  We introduce MMMU: a new benchmark designed to evaluate multimodal models on\nmassive multi-discipline tasks demanding college-level subject knowledge and\ndeliberate reasoning. MMMU includes 11.5K meticulously collected multimodal\nquestions from college exams, quizzes, and textbooks, covering six core\ndisciplines: Art & Design, Business, Science, Health & Medicine, Humanities &\nSocial Science, and Tech & Engineering. These questions span 30 subjects and\n183 subfields, comprising 30 highly heterogeneous image types, such as charts,\ndiagrams, maps, tables, music sheets, and chemical structures. Unlike existing\nbenchmarks, MMMU focuses on advanced perception and reasoning with\ndomain-specific knowledge, challenging models to perform tasks akin to those\nfaced by experts. The evaluation of 14 open-source LMMs as well as the\nproprietary GPT-4V(ision) and Gemini highlights the substantial challenges\nposed by MMMU. Even the advanced GPT-4V and Gemini Ultra only achieve\naccuracies of 56% and 59% respectively, indicating significant room for\nimprovement. We believe MMMU will stimulate the community to build\nnext-generation multimodal foundation models towards expert artificial general\nintelligence.\n","authors":["Xiang Yue","Yuansheng Ni","Kai Zhang","Tianyu Zheng","Ruoqi Liu","Ge Zhang","Samuel Stevens","Dongfu Jiang","Weiming Ren","Yuxuan Sun","Cong Wei","Botao Yu","Ruibin Yuan","Renliang Sun","Ming Yin","Boyuan Zheng","Zhenzhu Yang","Yibo Liu","Wenhao Huang","Huan Sun","Yu Su","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2311.16502v2.pdf","comment":"117 pages, 99 figures"},{"id":"http://arxiv.org/abs/2312.10905v1","updated":"2023-12-18T03:21:58Z","published":"2023-12-18T03:21:58Z","title":"Satellite Captioning: Large Language Models to Augment Labeling","summary":"  With the growing capabilities of modern object detection networks and\ndatasets to train them, it has gotten more straightforward and, importantly,\nless laborious to get up and running with a model that is quite adept at\ndetecting any number of various objects. However, while image datasets for\nobject detection have grown and continue to proliferate (the current most\nextensive public set, ImageNet, contains over 14m images with over 14m\ninstances), the same cannot be said for textual caption datasets. While they\nhave certainly been growing in recent years, caption datasets present a much\nmore difficult challenge due to language differences, grammar, and the time it\ntakes for humans to generate them. Current datasets have certainly provided\nmany instances to work with, but it becomes problematic when a captioner may\nhave a more limited vocabulary, one may not be adequately fluent in the\nlanguage, or there are simple grammatical mistakes. These difficulties are\nincreased when the images get more specific, such as remote sensing images.\nThis paper aims to address this issue of potential information and\ncommunication shortcomings in caption datasets. To provide a more precise\nanalysis, we specify our domain of images to be remote sensing images in the\nRSICD dataset and experiment with the captions provided here. Our findings\nindicate that ChatGPT grammar correction is a simple and effective way to\nincrease the performance accuracy of caption models by making data captions\nmore diverse and grammatically correct.\n","authors":["Grant Rosario","David Noever"],"pdf_url":"https://arxiv.org/pdf/2312.10905v1.pdf","comment":"9 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2308.10144v2","updated":"2023-12-18T03:11:52Z","published":"2023-08-20T03:03:34Z","title":"ExpeL: LLM Agents Are Experiential Learners","summary":"  The recent surge in research interest in applying large language models\n(LLMs) to decision-making tasks has flourished by leveraging the extensive\nworld knowledge embedded in LLMs. While there is a growing demand to tailor\nLLMs for custom decision-making tasks, finetuning them for specific tasks is\nresource-intensive and may diminish the model's generalization capabilities.\nMoreover, state-of-the-art language models like GPT-4 and Claude are primarily\naccessible through API calls, with their parametric weights remaining\nproprietary and unavailable to the public. This scenario emphasizes the growing\nneed for new methodologies that allow learning from agent experiences without\nrequiring parametric updates. To address these problems, we introduce the\nExperiential Learning (ExpeL) agent. Our agent autonomously gathers experiences\nand extracts knowledge using natural language from a collection of training\ntasks. At inference, the agent recalls its extracted insights and past\nexperiences to make informed decisions. Our empirical results highlight the\nrobust learning efficacy of the ExpeL agent, indicating a consistent\nenhancement in its performance as it accumulates experiences. We further\nexplore the emerging capabilities and transfer learning potential of the ExpeL\nagent through qualitative observations and additional experiments.\n","authors":["Andrew Zhao","Daniel Huang","Quentin Xu","Matthieu Lin","Yong-Jin Liu","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2308.10144v2.pdf","comment":"Accepted by the 38th Annual AAAI Conference on Artificial\n  Intelligence (AAAI-24)"},{"id":"http://arxiv.org/abs/2312.10897v1","updated":"2023-12-18T02:55:14Z","published":"2023-12-18T02:55:14Z","title":"Generalized Category Discovery with Large Language Models in the Loop","summary":"  Generalized Category Discovery (GCD) is a crucial task that aims to recognize\nboth known and novel categories from a set of unlabeled data by utilizing a few\nlabeled data with only known categories. Due to the lack of supervision and\ncategory information, current methods usually perform poorly on novel\ncategories and struggle to reveal semantic meanings of the discovered clusters,\nwhich limits their applications in the real world. To mitigate above issues, we\npropose Loop, an end-to-end active-learning framework that introduces Large\nLanguage Models (LLMs) into the training loop, which can boost model\nperformance and generate category names without relying on any human efforts.\nSpecifically, we first propose Local Inconsistent Sampling (LIS) to select\nsamples that have a higher probability of falling to wrong clusters, based on\nneighborhood prediction consistency and entropy of cluster assignment\nprobabilities. Then we propose a Scalable Query strategy to allow LLMs to\nchoose true neighbors of the selected samples from multiple candidate samples.\nBased on the feedback from LLMs, we perform Refined Neighborhood Contrastive\nLearning (RNCL) to pull samples and their neighbors closer to learn\nclustering-friendly representations. Finally, we select representative samples\nfrom clusters corresponding to novel categories to allow LLMs to generate\ncategory names for them. Extensive experiments on three benchmark datasets show\nthat Loop outperforms SOTA models by a large margin and generates accurate\ncategory names for the discovered clusters. We will release our code and data\nafter publication.\n","authors":["Wenbin An","Wenkai Shi","Feng Tian","Haonan Lin","QianYing Wang","Yaqiang Wu","Mingxiang Cai","Luyan Wang","Yan Chen","Haiping Zhu","Ping Chen"],"pdf_url":"https://arxiv.org/pdf/2312.10897v1.pdf","comment":"Preprint"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2312.10890v1","updated":"2023-12-18T02:37:30Z","published":"2023-12-18T02:37:30Z","title":"Low-latency Space-time Supersampling for Real-time Rendering","summary":"  With the rise of real-time rendering and the evolution of display devices,\nthere is a growing demand for post-processing methods that offer\nhigh-resolution content in a high frame rate. Existing techniques often suffer\nfrom quality and latency issues due to the disjointed treatment of frame\nsupersampling and extrapolation. In this paper, we recognize the shared context\nand mechanisms between frame supersampling and extrapolation, and present a\nnovel framework, Space-time Supersampling (STSS). By integrating them into a\nunified framework, STSS can improve the overall quality with lower latency. To\nimplement an efficient architecture, we treat the aliasing and warping holes\nunified as reshading regions and put forth two key components to compensate the\nregions, namely Random Reshading Masking (RRM) and Efficient Reshading Module\n(ERM). Extensive experiments demonstrate that our approach achieves superior\nvisual fidelity compared to state-of-the-art (SOTA) methods. Notably, the\nperformance is achieved within only 4ms, saving up to 75\\% of time against the\nconventional two-stage pipeline that necessitates 17ms.\n","authors":["Ruian He","Shili Zhou","Yuqi Sun","Ri Cheng","Weimin Tan","Bo Yan"],"pdf_url":"https://arxiv.org/pdf/2312.10890v1.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2312.09909v2","updated":"2023-12-18T02:36:23Z","published":"2023-12-15T16:17:34Z","title":"TMP: Temporal Motion Propagation for Online Video Super-Resolution","summary":"  Online video super-resolution (online-VSR) highly relies on an effective\nalignment module to aggregate temporal information, while the strict latency\nrequirement makes accurate and efficient alignment very challenging. Though\nmuch progress has been achieved, most of the existing online-VSR methods\nestimate the motion fields of each frame separately to perform alignment, which\nis computationally redundant and ignores the fact that the motion fields of\nadjacent frames are correlated. In this work, we propose an efficient Temporal\nMotion Propagation (TMP) method, which leverages the continuity of motion field\nto achieve fast pixel-level alignment among consecutive frames. Specifically,\nwe first propagate the offsets from previous frames to the current frame, and\nthen refine them in the neighborhood, which significantly reduces the matching\nspace and speeds up the offset estimation process. Furthermore, to enhance the\nrobustness of alignment, we perform spatial-wise weighting on the warped\nfeatures, where the positions with more precise offsets are assigned higher\nimportance. Experiments on benchmark datasets demonstrate that the proposed TMP\nmethod achieves leading online-VSR accuracy as well as inference speed. The\nsource code of TMP can be found at https://github.com/xtudbxk/TMP.\n","authors":["Zhengqiang Zhang","Ruihuang Li","Shi Guo","Yang Cao","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.09909v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08916v2","updated":"2023-12-18T02:06:58Z","published":"2023-12-14T13:21:52Z","title":"Progressive Feature Self-reinforcement for Weakly Supervised Semantic\n  Segmentation","summary":"  Compared to conventional semantic segmentation with pixel-level supervision,\nWeakly Supervised Semantic Segmentation (WSSS) with image-level labels poses\nthe challenge that it always focuses on the most discriminative regions,\nresulting in a disparity between fully supervised conditions. A typical\nmanifestation is the diminished precision on the object boundaries, leading to\na deteriorated accuracy of WSSS. To alleviate this issue, we propose to\nadaptively partition the image content into deterministic regions (e.g.,\nconfident foreground and background) and uncertain regions (e.g., object\nboundaries and misclassified categories) for separate processing. For uncertain\ncues, we employ an activation-based masking strategy and seek to recover the\nlocal information with self-distilled knowledge. We further assume that the\nunmasked confident regions should be robust enough to preserve the global\nsemantics. Building upon this, we introduce a complementary self-enhancement\nmethod that constrains the semantic consistency between these confident regions\nand an augmented image with the same class labels. Extensive experiments\nconducted on PASCAL VOC 2012 and MS COCO 2014 demonstrate that our proposed\nsingle-stage approach for WSSS not only outperforms state-of-the-art benchmarks\nremarkably but also surpasses multi-stage methodologies that trade complexity\nfor accuracy. The code can be found at\n\\url{https://github.com/Jessie459/feature-self-reinforcement}.\n","authors":["Jingxuan He","Lechao Cheng","Chaowei Fang","Zunlei Feng","Tingting Mu","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2312.08916v2.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.10877v1","updated":"2023-12-18T01:49:42Z","published":"2023-12-18T01:49:42Z","title":"Mimic: Speaking Style Disentanglement for Speech-Driven 3D Facial\n  Animation","summary":"  Speech-driven 3D facial animation aims to synthesize vivid facial animations\nthat accurately synchronize with speech and match the unique speaking style.\nHowever, existing works primarily focus on achieving precise lip\nsynchronization while neglecting to model the subject-specific speaking style,\noften resulting in unrealistic facial animations. To the best of our knowledge,\nthis work makes the first attempt to explore the coupled information between\nthe speaking style and the semantic content in facial motions. Specifically, we\nintroduce an innovative speaking style disentanglement method, which enables\narbitrary-subject speaking style encoding and leads to a more realistic\nsynthesis of speech-driven facial animations. Subsequently, we propose a novel\nframework called \\textbf{Mimic} to learn disentangled representations of the\nspeaking style and content from facial motions by building two latent spaces\nfor style and content, respectively. Moreover, to facilitate disentangled\nrepresentation learning, we introduce four well-designed constraints: an\nauxiliary style classifier, an auxiliary inverse classifier, a content\ncontrastive loss, and a pair of latent cycle losses, which can effectively\ncontribute to the construction of the identity-related style space and\nsemantic-related content space. Extensive qualitative and quantitative\nexperiments conducted on three publicly available datasets demonstrate that our\napproach outperforms state-of-the-art methods and is capable of capturing\ndiverse speaking styles for speech-driven 3D facial animation. The source code\nand supplementary video are publicly available at:\nhttps://zeqing-wang.github.io/Mimic/\n","authors":["Hui Fu","Zeqing Wang","Ke Gong","Keze Wang","Tianshui Chen","Haojie Li","Haifeng Zeng","Wenxiong Kang"],"pdf_url":"https://arxiv.org/pdf/2312.10877v1.pdf","comment":"7 pages, 6 figures, accepted by AAAI-24"},{"id":"http://arxiv.org/abs/2312.10872v1","updated":"2023-12-18T01:23:22Z","published":"2023-12-18T01:23:22Z","title":"Country-Scale Cropland Mapping in Data-Scarce Settings Using Deep\n  Learning: A Case Study of Nigeria","summary":"  Cropland maps are a core and critical component of remote-sensing-based\nagricultural monitoring, providing dense and up-to-date information about\nagricultural development. Machine learning is an effective tool for large-scale\nagricultural mapping, but relies on geo-referenced ground-truth data for model\ntraining and testing, which can be scarce or time-consuming to obtain. In this\nstudy, we explore the usefulness of combining a global cropland dataset and a\nhand-labeled dataset to train machine learning models for generating a new\ncropland map for Nigeria in 2020 at 10 m resolution. We provide the models with\npixel-wise time series input data from remote sensing sources such as\nSentinel-1 and 2, ERA5 climate data, and DEM data, in addition to binary labels\nindicating cropland presence. We manually labeled 1827 evenly distributed\npixels across Nigeria, splitting them into 50\\% training, 25\\% validation, and\n25\\% test sets used to fit the models and test our output map. We evaluate and\ncompare the performance of single- and multi-headed Long Short-Term Memory\n(LSTM) neural network classifiers, a Random Forest classifier, and three\nexisting 10 m resolution global land cover maps (Google's Dynamic World, ESRI's\nLand Cover, and ESA's WorldCover) on our proposed test set. Given the regional\nvariations in cropland appearance, we additionally experimented with excluding\nor sub-setting the global crowd-sourced Geowiki cropland dataset, to\nempirically assess the trade-off between data quantity and data quality in\nterms of the similarity to the target data distribution of Nigeria. We find\nthat the existing WorldCover map performs the best with an F1-score of 0.825\nand accuracy of 0.870 on the test set, followed by a single-headed LSTM model\ntrained with our hand-labeled training samples and the Geowiki data points in\nNigeria, with a F1-score of 0.814 and accuracy of 0.842.\n","authors":["Joaquin Gajardo","Michele Volpi","Daniel Onwude","Thijs Defraeye"],"pdf_url":"https://arxiv.org/pdf/2312.10872v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10912v2","updated":"2023-12-18T00:37:52Z","published":"2023-10-17T01:12:08Z","title":"Towards Training-free Open-world Segmentation via Image Prompt\n  Foundation Models","summary":"  The realm of computer vision has witnessed a paradigm shift with the advent\nof foundational models, mirroring the transformative influence of large\nlanguage models in the domain of natural language processing. This paper delves\ninto the exploration of open-world segmentation, presenting a novel approach\ncalled Image Prompt Segmentation (IPSeg) that harnesses the power of vision\nfoundational models. IPSeg lies the principle of a training-free paradigm,\nwhich capitalizes on image prompt techniques. Specifically, IPSeg utilizes a\nsingle image containing a subjective visual concept as a flexible prompt to\nquery vision foundation models like DINOv2 and Stable Diffusion. Our approach\nextracts robust features for the prompt image and input image, then matches the\ninput representations to the prompt representations via a novel feature\ninteraction module to generate point prompts highlighting target objects in the\ninput image. The generated point prompts are further utilized to guide the\nSegment Anything Model to segment the target object in the input image. The\nproposed method stands out by eliminating the need for exhaustive training\nsessions, thereby offering a more efficient and scalable solution. Experiments\non COCO, PASCAL VOC, and other datasets demonstrate IPSeg's efficacy for\nflexible open-world segmentation using intuitive image prompts. This work\npioneers tapping foundation models for open-world understanding through visual\nconcepts conveyed in images.\n","authors":["Lv Tang","Peng-Tao Jiang","Hao-Ke Xiao","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2310.10912v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10854v1","updated":"2023-12-18T00:05:28Z","published":"2023-12-18T00:05:28Z","title":"The Right Losses for the Right Gains: Improving the Semantic Consistency\n  of Deep Text-to-Image Generation with Distribution-Sensitive Losses","summary":"  One of the major challenges in training deep neural networks for\ntext-to-image generation is the significant linguistic discrepancy between\nground-truth captions of each image in most popular datasets. The large\ndifference in the choice of words in such captions results in synthesizing\nimages that are semantically dissimilar to each other and to their ground-truth\ncounterparts. Moreover, existing models either fail to generate the\nfine-grained details of the image or require a huge number of parameters that\nrenders them inefficient for text-to-image synthesis. To fill this gap in the\nliterature, we propose using the contrastive learning approach with a novel\ncombination of two loss functions: fake-to-fake loss to increase the semantic\nconsistency between generated images of the same caption, and fake-to-real loss\nto reduce the gap between the distributions of real images and fake ones. We\ntest this approach on two baseline models: SSAGAN and AttnGAN (with style\nblocks to enhance the fine-grained details of the images.) Results show that\nour approach improves the qualitative results on AttnGAN with style blocks on\nthe CUB dataset. Additionally, on the challenging COCO dataset, our approach\nachieves competitive results against the state-of-the-art Lafite model,\noutperforms the FID score of SSAGAN model by 44.\n","authors":["Mahmoud Ahmed","Omer Moussa","Ismail Shaheen","Mohamed Abdelfattah","Amr Abdalla","Marwan Eid","Hesham Eraqi","Mohamed Moustafa"],"pdf_url":"https://arxiv.org/pdf/2312.10854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11748v1","updated":"2023-12-18T23:21:00Z","published":"2023-12-18T23:21:00Z","title":"Ultrasound Image Enhancement using CycleGAN and Perceptual Loss","summary":"  Purpose: The objective of this work is to introduce an advanced framework\ndesigned to enhance ultrasound images, especially those captured by portable\nhand-held devices, which often produce lower quality images due to hardware\nconstraints. Additionally, this framework is uniquely capable of effectively\nhandling non-registered input ultrasound image pairs, addressing a common\nchallenge in medical imaging. Materials and Methods: In this retrospective\nstudy, we utilized an enhanced generative adversarial network (CycleGAN) model\nfor ultrasound image enhancement across five organ systems. Perceptual loss,\nderived from deep features of pretrained neural networks, is applied to ensure\nthe human-perceptual quality of the enhanced images. These images are compared\nwith paired images acquired from high resolution devices to demonstrate the\nmodel's ability to generate realistic high-quality images across organ systems.\nResults: Preliminary validation of the framework reveals promising performance\nmetrics. The model generates images that result in a Structural Similarity\nIndex (SSI) score of 0.722, Locally Normalized Cross-Correlation (LNCC) score\nof 0.902 and 28.802 for the Peak Signal-to-Noise Ratio (PSNR) metric.\nConclusion: This work presents a significant advancement in medical imaging\nthrough the development of a CycleGAN model enhanced with Perceptual Loss (PL),\neffectively bridging the quality gap between ultrasound images from varied\ndevices. By training on paired images, the model not only improves image\nquality but also ensures the preservation of vital anatomic structural content.\nThis approach may improve equity in access to healthcare by enhancing portable\ndevice capabilities, although further validation and optimizations are\nnecessary for broader clinical application.\n","authors":["Shreeram Athreya","Ashwath Radhachandran","Vedrana Ivezić","Vivek Sant","Corey W. Arnold","William Speier"],"pdf_url":"https://arxiv.org/pdf/2312.11748v1.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2205.03553v3","updated":"2023-12-18T23:04:51Z","published":"2022-05-07T04:55:05Z","title":"From heavy rain removal to detail restoration: A faster and better\n  network","summary":"  The profound accumulation of precipitation during intense rainfall events can\nmarkedly degrade the quality of images, leading to the erosion of textural\ndetails. Despite the improvements observed in existing learning-based methods\nspecialized for heavy rain removal, it is discerned that a significant\nproportion of these methods tend to overlook the precise reconstruction of the\nintricate details. In this work, we introduce a simple dual-stage progressive\nenhancement network, denoted as DPENet, aiming to achieve effective deraining\nwhile preserving the structural accuracy of rain-free images. This approach\ncomprises two key modules, a rain streaks removal network (R$^2$Net) focusing\non accurate rain removal, and a details reconstruction network (DRNet) designed\nto recover the textural details of rain-free images. Firstly, we introduce a\ndilated dense residual block (DDRB) within R$^2$Net, enabling the aggregation\nof high-level and low-level features. Secondly, an enhanced residual pixel-wise\nattention block (ERPAB) is integrated into DRNet to facilitate the\nincorporation of contextual information. To further enhance the fidelity of our\napproach, we employ a comprehensive loss function that accentuates both the\nmarginal and regional accuracy of rain-free images. Extensive experiments\nconducted on publicly available benchmarks demonstrates the noteworthy\nefficiency and effectiveness of our proposed DPENet. The source code and\npre-trained models are currently available at\n\\url{https://github.com/chdwyb/DPENet}.\n","authors":["Yuanbo Wen","Tao Gao","Jing Zhang","Kaihao Zhang","Ting Chen"],"pdf_url":"https://arxiv.org/pdf/2205.03553v3.pdf","comment":"Accepted by Pattern Recognition"},{"id":"http://arxiv.org/abs/2309.10625v2","updated":"2023-12-18T21:50:51Z","published":"2023-09-19T14:04:04Z","title":"NoisyNN: Exploring the Influence of Information Entropy Change in\n  Learning Systems","summary":"  We explore the impact of entropy change in deep learning systems via noise\ninjection at different levels, i.e., the latent space and input image. The\nseries of models that employ our methodology are collectively known as Noisy\nNeural Networks (NoisyNN), with examples such as NoisyViT and NoisyCNN. Noise\nis conventionally viewed as a harmful perturbation in various deep learning\narchitectures, such as convolutional neural networks (CNNs) and vision\ntransformers (ViTs), as well as different learning tasks like image\nclassification and transfer learning. However, this work shows noise can be an\neffective way to change the entropy of the learning system. We demonstrate that\nspecific noise can boost the performance of various deep architectures under\ncertain conditions. We theoretically prove the enhancement gained from positive\nnoise by reducing the task complexity defined by information entropy and\nexperimentally show the significant performance gain in large image datasets,\nsuch as the ImageNet. Herein, we use the information entropy to define the\ncomplexity of the task. We categorize the noise into two types, positive noise\n(PN) and harmful noise (HN), based on whether the noise can help reduce the\ncomplexity of the task. Extensive experiments of CNNs and ViTs have shown\nperformance improvements by proactively injecting positive noise, where we\nachieved an unprecedented top 1 accuracy of over 95$\\%$ on ImageNet. Both\ntheoretical analysis and empirical evidence have confirmed that the presence of\npositive noise, can benefit the learning process, while the traditionally\nperceived harmful noise indeed impairs deep learning models. The different\nroles of noise offer new explanations for deep models on specific tasks and\nprovide a new paradigm for improving model performance. Moreover, it reminds us\nthat we can influence the performance of learning systems via information\nentropy change.\n","authors":["Xiaowei Yu","Yao Xue","Lu Zhang","Li Wang","Tianming Liu","Dajiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2309.10625v2.pdf","comment":"Information Entropy, NoisyNN, ViT, CNN"},{"id":"http://arxiv.org/abs/2312.11716v1","updated":"2023-12-18T21:27:34Z","published":"2023-12-18T21:27:34Z","title":"Squeezed Edge YOLO: Onboard Object Detection on Edge Devices","summary":"  Demand for efficient onboard object detection is increasing due to its key\nrole in autonomous navigation. However, deploying object detection models such\nas YOLO on resource constrained edge devices is challenging due to the high\ncomputational requirements of such models. In this paper, an compressed object\ndetection model named Squeezed Edge YOLO is examined. This model is compressed\nand optimized to kilobytes of parameters in order to fit onboard such edge\ndevices. To evaluate Squeezed Edge YOLO, two use cases - human and shape\ndetection - are used to show the model accuracy and performance. Moreover, the\nmodel is deployed onboard a GAP8 processor with 8 RISC-V cores and an NVIDIA\nJetson Nano with 4GB of memory. Experimental results show Squeezed Edge YOLO\nmodel size is optimized by a factor of 8x which leads to 76% improvements in\nenergy efficiency and 3.3x faster throughout.\n","authors":["Edward Humes","Mozhgan Navardi","Tinoosh Mohsenin"],"pdf_url":"https://arxiv.org/pdf/2312.11716v1.pdf","comment":"ML with New Compute Paradigms (MLNCP) Workshop at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2312.11707v1","updated":"2023-12-18T21:07:03Z","published":"2023-12-18T21:07:03Z","title":"Unified framework for diffusion generative models in SO(3): applications\n  in computer vision and astrophysics","summary":"  Diffusion-based generative models represent the current state-of-the-art for\nimage generation. However, standard diffusion models are based on Euclidean\ngeometry and do not translate directly to manifold-valued data. In this work,\nwe develop extensions of both score-based generative models (SGMs) and\nDenoising Diffusion Probabilistic Models (DDPMs) to the Lie group of 3D\nrotations, SO(3). SO(3) is of particular interest in many disciplines such as\nrobotics, biochemistry and astronomy/cosmology science. Contrary to more\ngeneral Riemannian manifolds, SO(3) admits a tractable solution to heat\ndiffusion, and allows us to implement efficient training of diffusion models.\nWe apply both SO(3) DDPMs and SGMs to synthetic densities on SO(3) and\ndemonstrate state-of-the-art results. Additionally, we demonstrate the\npracticality of our model on pose estimation tasks and in predicting correlated\ngalaxy orientations for astrophysics/cosmology.\n","authors":["Yesukhei Jagvaral","Francois Lanusse","Rachel Mandelbaum"],"pdf_url":"https://arxiv.org/pdf/2312.11707v1.pdf","comment":"Accepted at AAAI-2024 Main Track"},{"id":"http://arxiv.org/abs/2312.07374v3","updated":"2023-12-18T20:17:55Z","published":"2023-12-12T15:43:36Z","title":"Relax Image-Specific Prompt Requirement in SAM: A Single Generic Prompt\n  for Segmenting Camouflaged Objects","summary":"  Camouflaged object detection (COD) approaches heavily rely on pixel-level\nannotated datasets. Weakly-supervised COD (WSCOD) approaches use sparse\nannotations like scribbles or points to reduce annotation effort, but this can\nlead to decreased accuracy. The Segment Anything Model (SAM) shows remarkable\nsegmentation ability with sparse prompts like points. However, manual prompt is\nnot always feasible, as it may not be accessible in real-world application.\nAdditionally, it only provides localization information instead of semantic\none, which can intrinsically cause ambiguity in interpreting the targets. In\nthis work, we aim to eliminate the need for manual prompt. The key idea is to\nemploy Cross-modal Chains of Thought Prompting (CCTP) to reason visual prompts\nusing the semantic information given by a generic text prompt. To that end, we\nintroduce a test-time adaptation per-instance mechanism called Generalizable\nSAM (GenSAM) to automatically enerate and optimize visual prompts the generic\ntask prompt for WSCOD. In particular, CCTP maps a single generic text prompt\nonto image-specific consensus foreground and background heatmaps using\nvision-language models, acquiring reliable visual prompts. Moreover, to\ntest-time adapt the visual prompts, we further propose Progressive Mask\nGeneration (PMG) to iteratively reweight the input image, guiding the model to\nfocus on the targets in a coarse-to-fine manner. Crucially, all network\nparameters are fixed, avoiding the need for additional training. Experiments\ndemonstrate the superiority of GenSAM. Experiments on three benchmarks\ndemonstrate that GenSAM outperforms point supervision approaches and achieves\ncomparable results to scribble supervision ones, solely relying on general task\ndescriptions as prompts. our codes is in: https://lwpyh.github.io/GenSAM/.\n","authors":["Jian Hu","Jiayi Lin","Weitong Cai","Shaogang Gong"],"pdf_url":"https://arxiv.org/pdf/2312.07374v3.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2312.11666v1","updated":"2023-12-18T19:19:32Z","published":"2023-12-18T19:19:32Z","title":"HAAR: Text-Conditioned Generative Model of 3D Strand-based Human\n  Hairstyles","summary":"  We present HAAR, a new strand-based generative model for 3D human hairstyles.\nSpecifically, based on textual inputs, HAAR produces 3D hairstyles that could\nbe used as production-level assets in modern computer graphics engines. Current\nAI-based generative models take advantage of powerful 2D priors to reconstruct\n3D content in the form of point clouds, meshes, or volumetric functions.\nHowever, by using the 2D priors, they are intrinsically limited to only\nrecovering the visual parts. Highly occluded hair structures can not be\nreconstructed with those methods, and they only model the ''outer shell'',\nwhich is not ready to be used in physics-based rendering or simulation\npipelines. In contrast, we propose a first text-guided generative method that\nuses 3D hair strands as an underlying representation. Leveraging 2D visual\nquestion-answering (VQA) systems, we automatically annotate synthetic hair\nmodels that are generated from a small set of artist-created hairstyles. This\nallows us to train a latent diffusion model that operates in a common hairstyle\nUV space. In qualitative and quantitative studies, we demonstrate the\ncapabilities of the proposed model and compare it to existing hairstyle\ngeneration approaches.\n","authors":["Vanessa Sklyarova","Egor Zakharov","Otmar Hilliges","Michael J. Black","Justus Thies"],"pdf_url":"https://arxiv.org/pdf/2312.11666v1.pdf","comment":"For more results please refer to the project page\n  https://haar.is.tue.mpg.de/"},{"id":"http://arxiv.org/abs/2312.11463v1","updated":"2023-12-18T18:59:51Z","published":"2023-12-18T18:59:51Z","title":"Appearance-based Refinement for Object-Centric Motion Segmentation","summary":"  The goal of this paper is to discover, segment, and track independently\nmoving objects in complex visual scenes. Previous approaches have explored the\nuse of optical flow for motion segmentation, leading to imperfect predictions\ndue to partial motion, background distraction, and object articulations and\ninteractions. To address this issue, we introduce an appearance-based\nrefinement method that leverages temporal consistency in video streams to\ncorrect inaccurate flow-based proposals. Our approach involves a simple\nselection mechanism that identifies accurate flow-predicted masks as exemplars,\nand an object-centric architecture that refines problematic masks based on\nexemplar information. The model is pre-trained on synthetic data and then\nadapted to real-world videos in a self-supervised manner, eliminating the need\nfor human annotations. Its performance is evaluated on multiple video\nsegmentation benchmarks, including DAVIS, YouTubeVOS, SegTrackv2, and FBMS-59.\nWe achieve competitive performance on single-object segmentation, while\nsignificantly outperforming existing models on the more challenging problem of\nmulti-object segmentation. Finally, we investigate the benefits of using our\nmodel as a prompt for a per-frame Segment Anything Model.\n","authors":["Junyu Xie","Weidi Xie","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2312.11463v1.pdf","comment":"Total 26 pages, 13 figures (including main text: 9 pages, 5 figures)"},{"id":"http://arxiv.org/abs/2312.11461v1","updated":"2023-12-18T18:59:12Z","published":"2023-12-18T18:59:12Z","title":"GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning","summary":"  Gaussian splatting has emerged as a powerful 3D representation that harnesses\nthe advantages of both explicit (mesh) and implicit (NeRF) 3D representations.\nIn this paper, we seek to leverage Gaussian splatting to generate realistic\nanimatable avatars from textual descriptions, addressing the limitations (e.g.,\nflexibility and efficiency) imposed by mesh or NeRF-based representations.\nHowever, a naive application of Gaussian splatting cannot generate high-quality\nanimatable avatars and suffers from learning instability; it also cannot\ncapture fine avatar geometries and often leads to degenerate body parts. To\ntackle these problems, we first propose a primitive-based 3D Gaussian\nrepresentation where Gaussians are defined inside pose-driven primitives to\nfacilitate animation. Second, to stabilize and amortize the learning of\nmillions of Gaussians, we propose to use neural implicit fields to predict the\nGaussian attributes (e.g., colors). Finally, to capture fine avatar geometries\nand extract detailed meshes, we propose a novel SDF-based implicit mesh\nlearning approach for 3D Gaussians that regularizes the underlying geometries\nand extracts highly detailed textured meshes. Our proposed method, GAvatar,\nenables the large-scale generation of diverse animatable avatars using only\ntext prompts. GAvatar significantly surpasses existing methods in terms of both\nappearance and geometry quality, and achieves extremely fast rendering (100\nfps) at 1K resolution.\n","authors":["Ye Yuan","Xueting Li","Yangyi Huang","Shalini De Mello","Koki Nagano","Jan Kautz","Umar Iqbal"],"pdf_url":"https://arxiv.org/pdf/2312.11461v1.pdf","comment":"Project website: https://nvlabs.github.io/GAvatar"},{"id":"http://arxiv.org/abs/2312.11460v1","updated":"2023-12-18T18:59:06Z","published":"2023-12-18T18:59:06Z","title":"Hybrid Internal Model: A Simple and Efficient Learner for Agile Legged\n  Locomotion","summary":"  Robust locomotion control depends on accurate state estimations. However, the\nsensors of most legged robots can only provide partial and noisy observations,\nmaking the estimation particularly challenging, especially for external states\nlike terrain frictions and elevation maps. Inspired by the classical Internal\nModel Control principle, we consider these external states as disturbances and\nintroduce Hybrid Internal Model (HIM) to estimate them according to the\nresponse of the robot. The response, which we refer to as the hybrid internal\nembedding, contains the robot's explicit velocity and implicit stability\nrepresentation, corresponding to two primary goals for locomotion tasks:\nexplicitly tracking velocity and implicitly maintaining stability. We use\ncontrastive learning to optimize the embedding to be close to the robot's\nsuccessor state, in which the response is naturally embedded. HIM has several\nappealing benefits: It only needs the robot's proprioceptions, i.e., those from\njoint encoders and IMU as observations. It innovatively maintains consistent\nobservations between simulation reference and reality that avoids information\nloss in mimicking learning. It exploits batch-level information that is more\nrobust to noises and keeps better sample efficiency. It only requires 1 hour of\ntraining on an RTX 4090 to enable a quadruped robot to traverse any terrain\nunder any disturbances. A wealth of real-world experiments demonstrates its\nagility, even in high-difficulty tasks and cases never occurred during the\ntraining process, revealing remarkable open-world generalizability.\n","authors":["Junfeng Long","Zirui Wang","Quanyi Li","Jiawei Gao","Liu Cao","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2312.11460v1.pdf","comment":"Use 1 hour to train a quadruped robot capable of traversing any\n  terrain under any disturbances in the open world, Project Page:\n  https://github.com/OpenRobotLab/HIMLoco"},{"id":"http://arxiv.org/abs/2312.11459v1","updated":"2023-12-18T18:59:05Z","published":"2023-12-18T18:59:05Z","title":"VolumeDiffusion: Flexible Text-to-3D Generation with Efficient\n  Volumetric Encoder","summary":"  This paper introduces a pioneering 3D volumetric encoder designed for\ntext-to-3D generation. To scale up the training data for the diffusion model, a\nlightweight network is developed to efficiently acquire feature volumes from\nmulti-view images. The 3D volumes are then trained on a diffusion model for\ntext-to-3D generation using a 3D U-Net. This research further addresses the\nchallenges of inaccurate object captions and high-dimensional feature volumes.\nThe proposed model, trained on the public Objaverse dataset, demonstrates\npromising outcomes in producing diverse and recognizable samples from text\nprompts. Notably, it empowers finer control over object part characteristics\nthrough textual cues, fostering model creativity by seamlessly combining\nmultiple concepts within a single object. This research significantly\ncontributes to the progress of 3D generation by introducing an efficient,\nflexible, and scalable representation methodology. Code is available at\nhttps://github.com/tzco/VolumeDiffusion.\n","authors":["Zhicong Tang","Shuyang Gu","Chunyu Wang","Ting Zhang","Jianmin Bao","Dong Chen","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2312.11459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11458v1","updated":"2023-12-18T18:59:03Z","published":"2023-12-18T18:59:03Z","title":"GauFRe: Gaussian Deformation Fields for Real-time Dynamic Novel View\n  Synthesis","summary":"  We propose a method for dynamic scene reconstruction using deformable 3D\nGaussians that is tailored for monocular video. Building upon the efficiency of\nGaussian splatting, our approach extends the representation to accommodate\ndynamic elements via a deformable set of Gaussians residing in a canonical\nspace, and a time-dependent deformation field defined by a multi-layer\nperceptron (MLP). Moreover, under the assumption that most natural scenes have\nlarge regions that remain static, we allow the MLP to focus its\nrepresentational power by additionally including a static Gaussian point cloud.\nThe concatenated dynamic and static point clouds form the input for the\nGaussian Splatting rasterizer, enabling real-time rendering. The differentiable\npipeline is optimized end-to-end with a self-supervised rendering loss. Our\nmethod achieves results that are comparable to state-of-the-art dynamic neural\nradiance field methods while allowing much faster optimization and rendering.\nProject website: https://lynl7130.github.io/gaufre/index.html\n","authors":["Yiqing Liang","Numair Khan","Zhengqin Li","Thu Nguyen-Phuoc","Douglas Lanman","James Tompkin","Lei Xiao"],"pdf_url":"https://arxiv.org/pdf/2312.11458v1.pdf","comment":"10 pages, 8 figures, 4 tables"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2312.10885v1","updated":"2023-12-18T02:18:33Z","published":"2023-12-18T02:18:33Z","title":"A novel diffusion recommendation algorithm based on multi-scale cnn and\n  residual lstm","summary":"  Sequential recommendation aims to infer user preferences from historical\ninteraction sequences and predict the next item that users may be interested in\nthe future. The current mainstream design approach is to represent items as\nfixed vectors, capturing the underlying relationships between items and user\npreferences based on the order of interactions. However, relying on a single\nfixed-item embedding may weaken the modeling capability of the system, and the\nglobal dynamics and local saliency exhibited by user preferences need to be\ndistinguished. To address these issues, this paper proposes a novel diffusion\nrecommendation algorithm based on multi-scale cnn and residual lstm (AREAL). We\nintroduce diffusion models into the recommend system, representing items as\nprobability distributions instead of fixed vectors. This approach enables\nadaptive reflection of multiple aspects of the items and generates item\ndistributions in a denoising manner. We use multi-scale cnn and residual lstm\nmethods to extract the local and global dependency features of user history\ninteractions, and use attention mechanism to distinguish weights as the guide\nfeatures of reverse diffusion recovery. The effectiveness of the proposed\nmethod is validated through experiments conducted on two real-world datasets.\nSpecifically, AREAL obtains improvements over the best baselines by 2.63% and\n4.25% in terms of HR@20 and 5.05% and 3.94% in terms of NDCG@20 on all\ndatasets.\n","authors":["Yong Niu","Xing Xing","Zhichun Jia","Ruidi Liu","Mindong Xin"],"pdf_url":"https://arxiv.org/pdf/2312.10885v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10864v1","updated":"2023-12-18T00:57:03Z","published":"2023-12-18T00:57:03Z","title":"On-Device Recommender Systems: A Tutorial on The New-Generation\n  Recommendation Paradigm","summary":"  Given the sheer volume of contemporary e-commerce applications, recommender\nsystems (RSs) have gained significant attention in both academia and industry.\nHowever, traditional cloud-based RSs face inevitable challenges, such as\nresource-intensive computation, reliance on network access, and privacy\nbreaches. In response, a new paradigm called on-device recommender systems\n(ODRSs) has emerged recently in various industries like Taobao, Google, and\nKuaishou. ODRSs unleash the computational capacity of user devices with\nlightweight recommendation models tailored for resource-constrained\nenvironments, enabling real-time inference with users' local data. This\ntutorial aims to systematically introduce methodologies of ODRSs, including (1)\nan overview of existing research on ODRSs; (2) a comprehensive taxonomy of\nODRSs, where the core technical content to be covered span across three major\nODRS research directions, including on-device deployment and inference,\non-device training, and privacy/security of ODRSs; (3) limitations and future\ndirections of ODRSs. This tutorial expects to lay the foundation and spark new\ninsights for follow-up research and applications concerning this new\nrecommendation paradigm.\n","authors":["Hongzhi Yin","Tong Chen","Liang Qu","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2312.10864v1.pdf","comment":"Technical tutorial; to appear at The Web Conference 2024"},{"id":"http://arxiv.org/abs/2310.19251v2","updated":"2023-12-18T00:26:07Z","published":"2023-10-30T03:37:32Z","title":"Pre-trained Recommender Systems: A Causal Debiasing Perspective","summary":"  Recent studies on pre-trained vision/language models have demonstrated the\npractical benefit of a new, promising solution-building paradigm in AI where\nmodels can be pre-trained on broad data describing a generic task space and\nthen adapted successfully to solve a wide range of downstream tasks, even when\ntraining data is severely limited (e.g., in zero- or few-shot learning\nscenarios). Inspired by such progress, we investigate in this paper the\npossibilities and challenges of adapting such a paradigm to the context of\nrecommender systems, which is less investigated from the perspective of\npre-trained model. In particular, we propose to develop a generic recommender\nthat captures universal interaction patterns by training on generic user-item\ninteraction data extracted from different domains, which can then be fast\nadapted to improve few-shot learning performance in unseen new domains (with\nlimited data).\n  However, unlike vision/language data which share strong conformity in the\nsemantic space, universal patterns underlying recommendation data collected\nacross different domains (e.g., different countries or different E-commerce\nplatforms) are often occluded by both in-domain and cross-domain biases\nimplicitly imposed by the cultural differences in their user and item bases, as\nwell as their uses of different e-commerce platforms. As shown in our\nexperiments, such heterogeneous biases in the data tend to hinder the\neffectiveness of the pre-trained model. To address this challenge, we further\nintroduce and formalize a causal debiasing perspective, which is substantiated\nvia a hierarchical Bayesian deep learning model, named PreRec. Our empirical\nstudies on real-world data show that the proposed model could significantly\nimprove the recommendation performance in zero- and few-shot learning settings\nunder both cross-market and cross-platform scenarios.\n","authors":["Ziqian Lin","Hao Ding","Nghia Hoang","Branislav Kveton","Anoop Deoras","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2310.19251v2.pdf","comment":"8 pages, WSDM 24"},{"id":"http://arxiv.org/abs/2312.09602v2","updated":"2023-12-18T05:18:58Z","published":"2023-12-15T08:33:06Z","title":"Multi-Modality is All You Need for Transferable Recommender Systems","summary":"  ID-based Recommender Systems (RecSys), where each item is assigned a unique\nidentifier and subsequently converted into an embedding vector, have dominated\nthe designing of RecSys. Though prevalent, such ID-based paradigm is not\nsuitable for developing transferable RecSys and is also susceptible to the\ncold-start issue. In this paper, we unleash the boundaries of the ID-based\nparadigm and propose a Pure Multi-Modality based Recommender system (PMMRec),\nwhich relies solely on the multi-modal contents of the items (e.g., texts and\nimages) and learns transition patterns general enough to transfer across\ndomains and platforms. Specifically, we design a plug-and-play framework\narchitecture consisting of multi-modal item encoders, a fusion module, and a\nuser encoder. To align the cross-modal item representations, we propose a novel\nnext-item enhanced cross-modal contrastive learning objective, which is\nequipped with both inter- and intra-modality negative samples and explicitly\nincorporates the transition patterns of user behaviors into the item encoders.\nTo ensure the robustness of user representations, we propose a novel noised\nitem detection objective and a robustness-aware contrastive learning objective,\nwhich work together to denoise user sequences in a self-supervised manner.\nPMMRec is designed to be loosely coupled, so after being pre-trained on the\nsource data, each component can be transferred alone, or in conjunction with\nother components, allowing PMMRec to achieve versatility under both\nmulti-modality and single-modality transfer learning settings. Extensive\nexperiments on 4 sources and 10 target datasets demonstrate that PMMRec\nsurpasses the state-of-the-art recommenders in both recommendation performance\nand transferability. Our code and dataset is available at:\nhttps://github.com/ICDE24/PMMRec.\n","authors":["Youhua Li","Hanwen Du","Yongxin Ni","Pengpeng Zhao","Qi Guo","Fajie Yuan","Xiaofang Zhou"],"pdf_url":"https://arxiv.org/pdf/2312.09602v2.pdf","comment":"ICDE'24 Accepted"},{"id":"http://arxiv.org/abs/2312.06165v2","updated":"2023-12-18T15:13:23Z","published":"2023-12-11T07:10:50Z","title":"RecJPQ: Training Large-Catalogue Sequential Recommenders","summary":"  Sequential Recommendation is a popular recommendation task that uses the\norder of user-item interaction to model evolving users' interests and\nsequential patterns in their behaviour. Current state-of-the-art\nTransformer-based models for sequential recommendation, such as BERT4Rec and\nSASRec, generate sequence embeddings and compute scores for catalogue items,\nbut the increasing catalogue size makes training these models costly. The Joint\nProduct Quantisation (JPQ) method, originally proposed for passage retrieval,\nmarkedly reduces the size of the retrieval index with minimal effect on model\neffectiveness, by replacing passage embeddings with a limited number of shared\nsub-embeddings. This paper introduces RecJPQ, a novel adaptation of JPQ for\nsequential recommendations, which takes the place of item embeddings tensor and\nreplaces item embeddings with a concatenation of a limited number of shared\nsub-embeddings and, therefore, limits the number of learnable model parameters.\nThe main idea of RecJPQ is to split items into sub-item entities before\ntraining the main recommendation model, which is inspired by splitting words\ninto tokens and training tokenisers in language models. We apply RecJPQ to\nSASRec, BERT4Rec, and GRU4rec models on three large-scale sequential datasets.\nOur results showed that RecJPQ could notably reduce the model size (e.g., 48%\nreduction for the Gowalla dataset with no effectiveness degradation). RecJPQ\ncan also improve model performance through a regularisation effect (e.g. +0.96%\nNDCG@10 improvement on the Booking.com dataset). Overall, RecJPQ allows the\ntraining of state-of-the-art transformer recommenders in industrial\napplications, where datasets with millions of items are common.\n","authors":["Aleksandr V. Petrov","Craig Macdonald"],"pdf_url":"https://arxiv.org/pdf/2312.06165v2.pdf","comment":"Accepted by ACM WSDM 2024"},{"id":"http://arxiv.org/abs/2312.06683v2","updated":"2023-12-18T10:53:15Z","published":"2023-12-09T08:05:20Z","title":"AT4CTR: Auxiliary Match Tasks for Enhancing Click-Through Rate\n  Prediction","summary":"  Click-through rate (CTR) prediction is a vital task in industrial\nrecommendation systems. Most existing methods focus on the network architecture\ndesign of the CTR model for better accuracy and suffer from the data sparsity\nproblem. Especially in industrial recommendation systems, the widely applied\nnegative sample down-sampling technique due to resource limitation worsens the\nproblem, resulting in a decline in performance. In this paper, we propose\n\\textbf{A}uxiliary Match \\textbf{T}asks for enhancing\n\\textbf{C}lick-\\textbf{T}hrough \\textbf{R}ate prediction accuracy (AT4CTR) by\nalleviating the data sparsity problem. Specifically, we design two match tasks\ninspired by collaborative filtering to enhance the relevance modeling between\nuser and item. As the \"click\" action is a strong signal which indicates the\nuser's preference towards the item directly, we make the first match task aim\nat pulling closer the representation between the user and the item regarding\nthe positive samples. Since the user's past click behaviors can also be treated\nas the user him/herself, we apply the next item prediction as the second match\ntask. For both the match tasks, we choose the InfoNCE as their loss function.\nThe two match tasks can provide meaningful training signals to speed up the\nmodel's convergence and alleviate the data sparsity. We conduct extensive\nexperiments on one public dataset and one large-scale industrial recommendation\ndataset. The result demonstrates the effectiveness of the proposed auxiliary\nmatch tasks. AT4CTR has been deployed in the real industrial advertising system\nand has gained remarkable revenue.\n","authors":["Qi Liu","Xuyang Hou","Defu Lian","Zhe Wang","Haoran Jin","Jia Cheng","Jun Lei"],"pdf_url":"https://arxiv.org/pdf/2312.06683v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01916v2","updated":"2023-12-18T02:55:44Z","published":"2023-12-04T14:20:16Z","title":"PEACE: Prototype lEarning Augmented transferable framework for\n  Cross-domain rEcommendation","summary":"  To help merchants/customers to provide/access a variety of services through\nminiapps, online service platforms have occupied a critical position in the\neffective content delivery, in which how to recommend items in the new domain\nlaunched by the service provider for customers has become more urgent. However,\nthe non-negligible gap between the source and diversified target domains poses\na considerable challenge to cross-domain recommendation systems, which often\nleads to performance bottlenecks in industrial settings. While entity graphs\nhave the potential to serve as a bridge between domains, rudimentary\nutilization still fail to distill useful knowledge and even induce the negative\ntransfer issue. To this end, we propose PEACE, a Prototype lEarning Augmented\ntransferable framework for Cross-domain rEcommendation. For domain gap\nbridging, PEACE is built upon a multi-interest and entity-oriented pre-training\narchitecture which could not only benefit the learning of generalized knowledge\nin a multi-granularity manner, but also help leverage more structural\ninformation in the entity graph. Then, we bring the prototype learning into the\npre-training over source domains, so that representations of users and items\nare greatly improved by the contrastive prototype learning module and the\nprototype enhanced attention mechanism for adaptive knowledge utilization. To\nease the pressure of online serving, PEACE is carefully deployed in a\nlightweight manner, and significant performance improvements are observed in\nboth online and offline environments.\n","authors":["Chunjing Gan","Bo Huang","Binbin Hu","Jian Ma","Ziqi Liu","Zhiqiang Zhang","Jun Zhou","Guannan Zhang","Wenliang Zhong"],"pdf_url":"https://arxiv.org/pdf/2312.01916v2.pdf","comment":"Accepted by WSDM 2024"},{"id":"http://arxiv.org/abs/2312.11703v1","updated":"2023-12-18T21:03:46Z","published":"2023-12-18T21:03:46Z","title":"Shaping Political Discourse using multi-source News Summarization","summary":"  Multi-document summarization is the process of automatically generating a\nconcise summary of multiple documents related to the same topic. This summary\ncan help users quickly understand the key information from a large collection\nof documents. Multi-document summarization systems are more complex than\nsingle-document summarization systems due to the need to identify and combine\ninformation from multiple sources. In this paper, we have developed a machine\nlearning model that generates a concise summary of a topic from multiple news\ndocuments. The model is designed to be unbiased by sampling its input equally\nfrom all the different aspects of the topic, even if the majority of the news\nsources lean one way.\n","authors":["Charles Rajan","Nishit Asnani","Shreya Singh"],"pdf_url":"https://arxiv.org/pdf/2312.11703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11361v1","updated":"2023-12-18T17:18:04Z","published":"2023-12-18T17:18:04Z","title":"NoMIRACL: Knowing When You Don't Know for Robust Multilingual\n  Retrieval-Augmented Generation","summary":"  Retrieval-augmented generation (RAG) grounds large language model (LLM)\noutput by leveraging external knowledge sources to reduce factual\nhallucinations. However, prior works lack a comprehensive evaluation of\ndifferent language families, making it challenging to evaluate LLM robustness\nagainst errors in external retrieved knowledge. To overcome this, we establish\nNoMIRACL, a human-annotated dataset for evaluating LLM robustness in RAG across\n18 typologically diverse languages. NoMIRACL includes both a non-relevant and a\nrelevant subset. Queries in the non-relevant subset contain passages manually\njudged as non-relevant or noisy, whereas queries in the relevant subset include\nat least a single judged relevant passage. We measure LLM robustness using two\nmetrics: (i) hallucination rate, measuring model tendency to hallucinate an\nanswer, when the answer is not present in passages in the non-relevant subset,\nand (ii) error rate, measuring model inaccuracy to recognize relevant passages\nin the relevant subset. We build a GPT-4 baseline which achieves a 33.2%\nhallucination rate on the non-relevant and a 14.9% error rate on the relevant\nsubset on average. Our evaluation reveals that GPT-4 hallucinates frequently in\nhigh-resource languages, such as French or English. This work highlights an\nimportant avenue for future research to improve LLM robustness to learn how to\nbetter reject non-relevant information in RAG.\n","authors":["Nandan Thakur","Luiz Bonifacio","Xinyu Zhang","Odunayo Ogundepo","Ehsan Kamalloo","David Alfonso-Hermelo","Xiaoguang Li","Qun Liu","Boxing Chen","Mehdi Rezagholizadeh","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2312.11361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11356v1","updated":"2023-12-18T17:12:35Z","published":"2023-12-18T17:12:35Z","title":"The Problem of Coherence in Natural Language Explanations of\n  Recommendations","summary":"  Providing natural language explanations for recommendations is particularly\nuseful from the perspective of a non-expert user. Although several methods for\nproviding such explanations have recently been proposed, we argue that an\nimportant aspect of explanation quality has been overlooked in their\nexperimental evaluation. Specifically, the coherence between generated text and\npredicted rating, which is a necessary condition for an explanation to be\nuseful, is not properly captured by currently used evaluation measures. In this\npaper, we highlight the issue of explanation and prediction coherence by 1)\npresenting results from a manual verification of explanations generated by one\nof the state-of-the-art approaches 2) proposing a method of automatic coherence\nevaluation 3) introducing a new transformer-based method that aims to produce\nmore coherent explanations than the state-of-the-art approaches 4) performing\nan experimental evaluation which demonstrates that this method significantly\nimproves the explanation coherence without affecting the other aspects of\nrecommendation performance.\n","authors":["Jakub Raczyński","Mateusz Lango","Jerzy Stefanowski"],"pdf_url":"https://arxiv.org/pdf/2312.11356v1.pdf","comment":"ECAI 2023"},{"id":"http://arxiv.org/abs/2312.11336v1","updated":"2023-12-18T16:41:22Z","published":"2023-12-18T16:41:22Z","title":"DRDT: Dynamic Reflection with Divergent Thinking for LLM-based\n  Sequential Recommendation","summary":"  The rise of Large Language Models (LLMs) has sparked interest in their\napplication to sequential recommendation tasks as they can provide supportive\nitem information. However, due to the inherent complexities of sequential\nrecommendation, such as sequential patterns across datasets, noise within\nsequences, and the temporal evolution of user preferences, existing LLM\nreasoning strategies, such as in-context learning and chain-of-thought are not\nfully effective. To address these challenges, we introduce a novel reasoning\nprinciple: Dynamic Reflection with Divergent Thinking within a\nretriever-reranker framework. Our approach starts with a collaborative\nin-context demonstration retriever, which collects sequences exhibiting\ncollaborative behaviors as in-context examples. Following this, we abstract\nhigh-level user preferences across multiple aspects, providing a more nuanced\nunderstanding of user interests and circumventing the noise within the raw\nsequences. The cornerstone of our methodology is dynamic reflection, a process\nthat emulates human learning through probing, critiquing, and reflecting, using\nuser feedback to tailor the analysis more effectively to the target user in a\ntemporal manner. We evaluate our approach on three datasets using six\npre-trained LLMs. The superior performance observed across these models\ndemonstrates the efficacy of our reasoning strategy, notably achieved without\nthe need to fine-tune the LLMs. With our principle, we managed to outperform\nGPT-Turbo-3.5 on three datasets using 7b models e.g., Vicuna-7b and Openchat-7b\non NDCG@10. This research not only highlights the potential of LLMs in\nenhancing sequential recommendation systems but also underscores the importance\nof developing tailored reasoning strategies to fully harness their\ncapabilities.\n","authors":["Yu Wang","Zhiwei Liu","Jianguo Zhang","Weiran Yao","Shelby Heinecke","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2312.11336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.10405v7","updated":"2023-12-18T16:09:49Z","published":"2023-01-25T04:45:06Z","title":"Editing Language Model-based Knowledge Graph Embeddings","summary":"  Recently decades have witnessed the empirical success of framing Knowledge\nGraph (KG) embeddings via language models. However, language model-based KG\nembeddings are usually deployed as static artifacts, making them difficult to\nmodify post-deployment without re-training after deployment. To address this\nissue, we propose a new task of editing language model-based KG embeddings in\nthis paper. This task is designed to facilitate rapid, data-efficient updates\nto KG embeddings without compromising the performance of other aspects. We\nbuild four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and\nevaluate several knowledge editing baselines demonstrating the limited ability\nof previous models to handle the proposed challenging task. We further propose\na simple yet strong baseline dubbed KGEditor, which utilizes additional\nparametric layers of the hypernetwork to edit/add facts. Our comprehensive\nexperimental results reveal that KGEditor excels in updating specific facts\nwithout impacting the overall performance, even when faced with limited\ntraining resources. Code and datasets are available in\nhttps://github.com/zjunlp/PromptKG/tree/main/deltaKG.\n","authors":["Siyuan Cheng","Bozhong Tian","Xi Chen","Ningyu Zhang","Qingbing Liu","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2301.10405v7.pdf","comment":"AAAI 2024. The project website is\n  https://zjunlp.github.io/project/KGE_Editing/"},{"id":"http://arxiv.org/abs/2210.05662v2","updated":"2023-12-18T14:13:03Z","published":"2022-10-11T17:56:55Z","title":"Understanding or Manipulation: Rethinking Online Performance Gains of\n  Modern Recommender Systems","summary":"  Recommender systems are expected to be assistants that help human users find\nrelevant information automatically without explicit queries. As recommender\nsystems evolve, increasingly sophisticated learning techniques are applied and\nhave achieved better performance in terms of user engagement metrics such as\nclicks and browsing time. The increase in the measured performance, however,\ncan have two possible attributions: a better understanding of user preferences,\nand a more proactive ability to utilize human bounded rationality to seduce\nuser over-consumption. A natural following question is whether current\nrecommendation algorithms are manipulating user preferences. If so, can we\nmeasure the manipulation level? In this paper, we present a general framework\nfor benchmarking the degree of manipulations of recommendation algorithms, in\nboth slate recommendation and sequential recommendation scenarios. The\nframework consists of four stages, initial preference calculation, training\ndata collection, algorithm training and interaction, and metrics calculation\nthat involves two proposed metrics. We benchmark some representative\nrecommendation algorithms in both synthetic and real-world datasets under the\nproposed framework. We have observed that a high online click-through rate does\nnot necessarily mean a better understanding of user initial preference, but\nends in prompting users to choose more documents they initially did not favor.\nMoreover, we find that the training data have notable impacts on the\nmanipulation degrees, and algorithms with more powerful modeling abilities are\nmore sensitive to such impacts. The experiments also verified the usefulness of\nthe proposed metrics for measuring the degree of manipulations. We advocate\nthat future recommendation algorithm studies should be treated as an\noptimization problem with constrained user preference manipulations.\n","authors":["Zhengbang Zhu","Rongjun Qin","Junjie Huang","Xinyi Dai","Yang Yu","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2210.05662v2.pdf","comment":"33 pages, 11 figures, 4 tables, ACM Transactions on Information\n  Systems"},{"id":"http://arxiv.org/abs/2306.02841v4","updated":"2023-12-18T12:06:56Z","published":"2023-06-05T12:46:40Z","title":"CTRL: Connect Collaborative and Language Model for CTR Prediction","summary":"  Traditional click-through rate (CTR) prediction models convert the tabular\ndata into one-hot vectors and leverage the collaborative relations among\nfeatures for inferring the user's preference over items. This modeling paradigm\ndiscards essential semantic information. Though some works like P5 and CTR-BERT\nhave explored the potential of using Pre-trained Language Models (PLMs) to\nextract semantic signals for CTR prediction, they are computationally expensive\nand suffer from low efficiency. Besides, the beneficial collaborative relations\nare not considered, hindering the recommendation performance. To solve these\nproblems, in this paper, we propose a novel framework \\textbf{CTRL}, which is\nindustrial-friendly and model-agnostic with superior inference efficiency.\nSpecifically, the original tabular data is first converted into textual data.\nBoth tabular data and converted textual data are regarded as two different\nmodalities and are separately fed into the collaborative CTR model and\npre-trained language model. A cross-modal knowledge alignment procedure is\nperformed to fine-grained align and integrate the collaborative and semantic\nsignals, and the lightweight collaborative model can be deployed online for\nefficient serving after fine-tuned with supervised signals. Experimental\nresults on three public datasets show that CTRL outperforms the\nstate-of-the-art (SOTA) CTR models significantly. Moreover, we further verify\nits effectiveness on a large-scale industrial recommender system.\n","authors":["Xiangyang Li","Bo Chen","Lu Hou","Ruiming Tang"],"pdf_url":"https://arxiv.org/pdf/2306.02841v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11036v1","updated":"2023-12-18T09:13:41Z","published":"2023-12-18T09:13:41Z","title":"UniGen: A Unified Generative Framework for Retrieval and Question\n  Answering with Large Language Models","summary":"  Generative information retrieval, encompassing two major tasks of Generative\nDocument Retrieval (GDR) and Grounded Answer Generation (GAR), has gained\nsignificant attention in the area of information retrieval and natural language\nprocessing. Existing methods for GDR and GAR rely on separate retrieval and\nreader modules, which hinder simultaneous optimization. To overcome this, we\npresent \\textbf{UniGen}, a \\textbf{Uni}fied \\textbf{Gen}erative framework for\nretrieval and question answering that integrates both tasks into a single\ngenerative model leveraging the capabilities of large language models. UniGen\nemploys a shared encoder and two distinct decoders for generative retrieval and\nquestion answering. To facilitate the learning of both tasks, we introduce\nconnectors, generated by large language models, to bridge the gaps between\nquery inputs and generation targets, as well as between document identifiers\nand answers. Furthermore, we propose an iterative enhancement strategy that\nleverages generated answers and retrieved documents to iteratively improve both\ntasks. Through extensive experiments on the MS MARCO and NQ datasets, we\ndemonstrate the effectiveness of UniGen, showcasing its superior performance in\nboth the retrieval and the question answering tasks.\n","authors":["Xiaoxi Li","Yujia Zhou","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2312.11036v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11018v1","updated":"2023-12-18T08:35:10Z","published":"2023-12-18T08:35:10Z","title":"Hypergrah-Enhanced Dual Convolutional Network for Bundle Recommendation","summary":"  Bundle recommendations strive to offer users a set of items as a package\nnamed bundle, enhancing convenience and contributing to the seller's revenue.\nWhile previous approaches have demonstrated notable performance, we argue that\nthey may compromise the ternary relationship among users, items, and bundles.\nThis compromise can result in information loss, ultimately impacting the\noverall model performance. To address this gap, we develop a unified model for\nbundle recommendation, termed hypergraph-enhanced dual convolutional neural\nnetwork (HED). Our approach is characterized by two key aspects. Firstly, we\nconstruct a complete hypergraph to capture interaction dynamics among users,\nitems, and bundles. Secondly, we incorporate U-B interaction information to\nenhance the information representation derived from users and bundle embedding\nvectors. Extensive experimental results on the Youshu and Netease datasets have\ndemonstrated that HED surpasses state-of-the-art baselines, proving its\neffectiveness. In addition, various ablation studies and sensitivity analyses\nrevealed the working mechanism and proved our effectiveness. Codes and datasets\nare available at https://github.com/AAI-Lab/HED\n","authors":["Kangbo Liu","Yang Li","Yaoxin Wu","Zhaoxuan Wang","Xiaoxu Wang"],"pdf_url":"https://arxiv.org/pdf/2312.11018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10968v1","updated":"2023-12-18T06:45:31Z","published":"2023-12-18T06:45:31Z","title":"PARs: Predicate-based Association Rules for Efficient and Accurate\n  Model-Agnostic Anomaly Explanation","summary":"  While new and effective methods for anomaly detection are frequently\nintroduced, many studies prioritize the detection task without considering the\nneed for explainability. Yet, in real-world applications, anomaly explanation,\nwhich aims to provide explanation of why specific data instances are identified\nas anomalies, is an equally important task. In this work, we present a novel\napproach for efficient and accurate model-agnostic anomaly explanation for\ntabular data using Predicate-based Association Rules (PARs). PARs can provide\nintuitive explanations not only about which features of the anomaly instance\nare abnormal, but also the reasons behind their abnormality. Our user study\nindicates that the anomaly explanation form of PARs is better comprehended and\npreferred by regular users of anomaly detection systems as compared to existing\nmodel-agnostic explanation options. Furthermore, we conduct extensive\nexperiments on various benchmark datasets, demonstrating that PARs compare\nfavorably to state-of-the-art model-agnostic methods in terms of computing\nefficiency and explanation accuracy on anomaly explanation tasks. The code for\nPARs tool is available at https://github.com/NSIBF/PARs-EXAD.\n","authors":["Cheng Feng"],"pdf_url":"https://arxiv.org/pdf/2312.10968v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.04923v2","updated":"2023-12-18T06:43:02Z","published":"2023-07-10T22:14:56Z","title":"Ranking with Long-Term Constraints","summary":"  The feedback that users provide through their choices (e.g., clicks,\npurchases) is one of the most common types of data readily available for\ntraining search and recommendation algorithms. However, myopically training\nsystems based on choice data may only improve short-term engagement, but not\nthe long-term sustainability of the platform and the long-term benefits to its\nusers, content providers, and other stakeholders. In this paper, we thus\ndevelop a new framework in which decision makers (e.g., platform operators,\nregulators, users) can express long-term goals for the behavior of the platform\n(e.g., fairness, revenue distribution, legal requirements). These goals take\nthe form of exposure or impact targets that go well beyond individual sessions,\nand we provide new control-based algorithms to achieve these goals. In\nparticular, the controllers are designed to achieve the stated long-term goals\nwith minimum impact on short-term engagement. Beyond the principled theoretical\nderivation of the controllers, we evaluate the algorithms on both synthetic and\nreal-world data. While all controllers perform well, we find that they provide\ninteresting trade-offs in efficiency, robustness, and the ability to plan\nahead.\n","authors":["Kianté Brantley","Zhichong Fang","Sarah Dean","Thorsten Joachims"],"pdf_url":"https://arxiv.org/pdf/2307.04923v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10967v1","updated":"2023-12-18T06:41:23Z","published":"2023-12-18T06:41:23Z","title":"Knowledge Graphs and Pre-trained Language Models enhanced Representation\n  Learning for Conversational Recommender Systems","summary":"  Conversational recommender systems (CRS) utilize natural language\ninteractions and dialogue history to infer user preferences and provide\naccurate recommendations. Due to the limited conversation context and\nbackground knowledge, existing CRSs rely on external sources such as knowledge\ngraphs to enrich the context and model entities based on their inter-relations.\nHowever, these methods ignore the rich intrinsic information within entities.\nTo address this, we introduce the Knowledge-Enhanced Entity Representation\nLearning (KERL) framework, which leverages both the knowledge graph and a\npre-trained language model to improve the semantic understanding of entities\nfor CRS. In our KERL framework, entity textual descriptions are encoded via a\npre-trained language model, while a knowledge graph helps reinforce the\nrepresentation of these entities. We also employ positional encoding to\neffectively capture the temporal information of entities in a conversation. The\nenhanced entity representation is then used to develop a recommender component\nthat fuses both entity and contextual representations for more informed\nrecommendations, as well as a dialogue component that generates informative\nentity-related information in the response text. A high-quality knowledge graph\nwith aligned entity descriptions is constructed to facilitate our study, namely\nthe Wiki Movie Knowledge Graph (WikiMKG). The experimental results show that\nKERL achieves state-of-the-art results in both recommendation and response\ngeneration tasks.\n","authors":["Zhangchi Qiu","Ye Tao","Shirui Pan","Alan Wee-Chung Liew"],"pdf_url":"https://arxiv.org/pdf/2312.10967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.00229v3","updated":"2023-12-18T06:33:57Z","published":"2022-12-01T02:26:52Z","title":"NIR-Prompt: A Multi-task Generalized Neural Information Retrieval\n  Training Framework","summary":"  Information retrieval aims to find information that meets users' needs from\nthe corpus. Different needs correspond to different IR tasks such as document\nretrieval, open-domain question answering, retrieval-based dialogue, etc.,\nwhile they share the same schema to estimate the relationship between texts. It\nindicates that a good IR model can generalize to different tasks and domains.\nHowever, previous studies indicate that state-of-the-art neural information\nretrieval (NIR) models, e.g, pre-trained language models (PLMs) are hard to\ngeneralize. Mainly because the end-to-end fine-tuning paradigm makes the model\noveremphasize task-specific signals and domain biases but loses the ability to\ncapture generalized essential signals. To address this problem, we propose a\nnovel NIR training framework named NIR-Prompt for retrieval and reranking\nstages based on the idea of decoupling signal capturing and combination.\nNIR-Prompt exploits Essential Matching Module (EMM) to capture the essential\nmatching signals and gets the description of tasks by Matching Description\nModule (MDM). The description is used as task-adaptation information to combine\nthe essential matching signals to adapt to different tasks. Experiments under\nin-domain multi-task, out-of-domain multi-task, and new task adaptation\nsettings show that NIR-Prompt can improve the generalization of PLMs in NIR for\nboth retrieval and reranking stages compared with baselines.\n","authors":["Shicheng Xu","Liang Pang","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2212.00229v3.pdf","comment":"This article is the extension of arXiv:2204.02725 and accepted by\n  TOIS"},{"id":"http://arxiv.org/abs/2312.10947v1","updated":"2023-12-18T05:53:44Z","published":"2023-12-18T05:53:44Z","title":"LabelCraft: Empowering Short Video Recommendations with Automated Label\n  Crafting","summary":"  Short video recommendations often face limitations due to the quality of user\nfeedback, which may not accurately depict user interests. To tackle this\nchallenge, a new task has emerged: generating more dependable labels from\noriginal feedback. Existing label generation methods rely on manual rules,\ndemanding substantial human effort and potentially misaligning with the desired\nobjectives of the platform. To transcend these constraints, we introduce\nLabelCraft, a novel automated label generation method explicitly optimizing\npivotal operational metrics for platform success. By formulating label\ngeneration as a higher-level optimization problem above recommender model\noptimization, LabelCraft introduces a trainable labeling model for automatic\nlabel mechanism modeling. Through meta-learning techniques, LabelCraft\neffectively addresses the bi-level optimization hurdle posed by the recommender\nand labeling models, enabling the automatic acquisition of intricate label\ngeneration mechanisms.Extensive experiments on real-world datasets corroborate\nLabelCraft's excellence across varied operational metrics, encompassing usage\ntime, user engagement, and retention. Codes are available at\nhttps://github.com/baiyimeng/LabelCraft.\n","authors":["Yimeng Bai","Yang Zhang","Jing Lu","Jianxin Chang","Xiaoxue Zang","Yanan Niu","Yang Song","Fuli Feng"],"pdf_url":"https://arxiv.org/pdf/2312.10947v1.pdf","comment":"Accepted by WSDM'24"},{"id":"http://arxiv.org/abs/2312.11569v1","updated":"2023-12-18T04:37:45Z","published":"2023-12-18T04:37:45Z","title":"Application of AI in Nutrition","summary":"  In healthcare, artificial intelligence (AI) has been changing the way doctors\nand health experts take care of people. This paper will cover how AI is making\nmajor changes in the health care system, especially with nutrition. Various\nmachine learning and deep learning algorithms have been developed to extract\nvaluable information from healthcare data which help doctors, nutritionists,\nand health experts to make better decisions and make our lifestyle healthy.\nThis paper provides an overview of the current state of AI applications in\nhealthcare with a focus on the utilization of AI-driven recommender systems in\nnutrition. It will discuss the positive outcomes and challenges that arise when\nAI is used in this field. This paper addresses the challenges to develop AI\nrecommender systems in healthcare, providing a well-rounded perspective on the\ncomplexities. Real-world examples and research findings are presented to\nunderscore the tangible and significant impact AI recommender systems have in\nthe field of healthcare, particularly in nutrition. The ongoing efforts of\napplying AI in nutrition lay the groundwork for a future where personalized\nrecommendations play a pivotal role in guiding individuals toward healthier\nlifestyles.\n","authors":["Ritu Ramakrishnan","Tianxiang Xing","Tianfeng Chen","Ming-Hao Lee","Jinzhu Gao"],"pdf_url":"https://arxiv.org/pdf/2312.11569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.00899v2","updated":"2023-12-18T01:10:59Z","published":"2023-06-01T16:56:04Z","title":"Pitfalls in Link Prediction with Graph Neural Networks: Understanding\n  the Impact of Target-link Inclusion & Better Practices","summary":"  While Graph Neural Networks (GNNs) are remarkably successful in a variety of\nhigh-impact applications, we demonstrate that, in link prediction, the common\npractices of including the edges being predicted in the graph at training\nand/or test have outsized impact on the performance of low-degree nodes. We\ntheoretically and empirically investigate how these practices impact node-level\nperformance across different degrees. Specifically, we explore three issues\nthat arise: (I1) overfitting; (I2) distribution shift; and (I3) implicit test\nleakage. The former two issues lead to poor generalizability to the test data,\nwhile the latter leads to overestimation of the model's performance and\ndirectly impacts the deployment of GNNs. To address these issues in a\nsystematic way, we introduce an effective and efficient GNN training framework,\nSpotTarget, which leverages our insight on low-degree nodes: (1) at training\ntime, it excludes a (training) edge to be predicted if it is incident to at\nleast one low-degree node; and (2) at test time, it excludes all test edges to\nbe predicted (thus, mimicking real scenarios of using GNNs, where the test data\nis not included in the graph). SpotTarget helps researchers and practitioners\nadhere to best practices for learning from graph data, which are frequently\noverlooked even by the most widely-used frameworks. Our experiments on various\nreal-world datasets show that SpotTarget makes GNNs up to 15x more accurate in\nsparse graphs, and significantly improves their performance for low-degree\nnodes in dense graphs.\n","authors":["Jing Zhu","Yuhang Zhou","Vassilis N. Ioannidis","Shengyi Qian","Wei Ai","Xiang Song","Danai Koutra"],"pdf_url":"https://arxiv.org/pdf/2306.00899v2.pdf","comment":"Extended Version of our WSDM'24 paper. 8 pages, 2 page appendix"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2312.10884v1","updated":"2023-12-18T02:15:40Z","published":"2023-12-18T02:15:40Z","title":"Contextual Reinforcement Learning for Offshore Wind Farm Bidding","summary":"  We propose a framework for applying reinforcement learning to contextual\ntwo-stage stochastic optimization and apply this framework to the problem of\nenergy market bidding of an off-shore wind farm. Reinforcement learning could\npotentially be used to learn close to optimal solutions for first stage\nvariables of a two-stage stochastic program under different contexts. Under the\nproposed framework, these solutions would be learned without having to solve\nthe full two-stage stochastic program. We present initial results of training\nusing the DDPG algorithm and present intended future steps to improve\nperformance.\n","authors":["David Cole","Himanshu Sharma","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2312.10884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10879v1","updated":"2023-12-18T01:52:59Z","published":"2023-12-18T01:52:59Z","title":"Development and Evaluation of Ensemble Learning-based Environmental\n  Methane Detection and Intensity Prediction Models","summary":"  The environmental impacts of global warming driven by methane (CH4) emissions\nhave catalyzed significant research initiatives in developing novel\ntechnologies that enable proactive and rapid detection of CH4. Several\ndata-driven machine learning (ML) models were tested to determine how well they\nidentified fugitive CH4 and its related intensity in the affected areas.\nVarious meteorological characteristics, including wind speed, temperature,\npressure, relative humidity, water vapor, and heat flux, were included in the\nsimulation. We used the ensemble learning method to determine the\nbest-performing weighted ensemble ML models built upon several weaker\nlower-layer ML models to (i) detect the presence of CH4 as a classification\nproblem and (ii) predict the intensity of CH4 as a regression problem.\n","authors":["Reek Majumder","Jacquan Pollard","M Sabbir Salek","David Werth","Gurcan Comert","Adrian Gale","Sakib Mahmud Khan","Samuel Darko","Mashrur Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2312.10879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09323v2","updated":"2023-12-18T01:49:05Z","published":"2023-12-07T19:58:37Z","title":"Perspectives on the State and Future of Deep Learning -- 2023","summary":"  The goal of this series is to chronicle opinions and issues in the field of\nmachine learning as they stand today and as they change over time. The plan is\nto host this survey periodically until the AI singularity\npaperclip-frenzy-driven doomsday, keeping an updated list of topical questions\nand interviewing new community members for each edition. In this issue, we\nprobed people's opinions on interpretable AI, the value of benchmarking in\nmodern NLP, the state of progress towards understanding deep learning, and the\nfuture of academia.\n","authors":["Micah Goldblum","Anima Anandkumar","Richard Baraniuk","Tom Goldstein","Kyunghyun Cho","Zachary C Lipton","Melanie Mitchell","Preetum Nakkiran","Max Welling","Andrew Gordon Wilson"],"pdf_url":"https://arxiv.org/pdf/2312.09323v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03360v2","updated":"2023-12-18T01:43:56Z","published":"2023-12-06T08:55:55Z","title":"Teaching Specific Scientific Knowledge into Large Language Models\n  through Additional Training","summary":"  Through additional training, we explore embedding specialized scientific\nknowledge into the Llama 2 Large Language Model (LLM). Key findings reveal that\neffective knowledge integration requires reading texts from multiple\nperspectives, especially in instructional formats. We utilize text augmentation\nto tackle the scarcity of specialized texts, including style conversions and\ntranslations. Hyperparameter optimization proves crucial, with different size\nmodels (7b, 13b, and 70b) reasonably undergoing additional training. Validating\nour methods, we construct a dataset of 65,000 scientific papers. Although we\nhave succeeded in partially embedding knowledge, the study highlights the\ncomplexities and limitations of incorporating specialized information into\nLLMs, suggesting areas for further improvement.\n","authors":["Kan Hatakeyama-Sato","Yasuhiko Igarashi","Shun Katakami","Yuta Nabae","Teruaki Hayakawa"],"pdf_url":"https://arxiv.org/pdf/2312.03360v2.pdf","comment":"added token information for some texts, and fixed typo"},{"id":"http://arxiv.org/abs/2310.17658v3","updated":"2023-12-18T01:42:26Z","published":"2023-10-18T15:24:34Z","title":"Is Channel Independent strategy optimal for Time Series Forecasting?","summary":"  There has been an emergence of various models for long-term time series\nforecasting. Recent studies have demonstrated that a single linear layer, using\nChannel Dependent (CD) or Channel Independent (CI) modeling, can even\noutperform a large number of sophisticated models. However, current research\nprimarily considers CD and CI as two complementary yet mutually exclusive\napproaches, unable to harness these two extremes simultaneously. And it is also\na challenging issue that both CD and CI are static strategies that cannot be\ndetermined to be optimal for a specific dataset without extensive experiments.\nIn this paper, we reconsider whether the current CI strategy is the best\nsolution for time series forecasting. First, we propose a simple yet effective\nstrategy called CSC, which stands for $\\mathbf{C}$hannel\n$\\mathbf{S}$elf-$\\mathbf{C}$lustering strategy, for linear models. Our Channel\nSelf-Clustering (CSC) enhances CI strategy's performance improvements while\nreducing parameter size, for exmpale by over 10 times on electricity dataset,\nand significantly cutting training time. Second, we further propose Channel\nRearrangement (CR), a method for deep models inspired by the self-clustering.\nCR attains competitive performance against baselines. Finally, we also discuss\nwhether it is best to forecast the future values using the historical values of\nthe same channel as inputs. We hope our findings and methods could inspire new\nsolutions beyond CD/CI.\n","authors":["Yuan Peiwen","Zhu Changsheng"],"pdf_url":"https://arxiv.org/pdf/2310.17658v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05429v2","updated":"2023-12-18T01:29:42Z","published":"2023-12-09T01:26:22Z","title":"Mitigating Nonlinear Algorithmic Bias in Binary Classification","summary":"  This paper proposes the use of causal modeling to detect and mitigate\nalgorithmic bias that is nonlinear in the protected attribute. We provide a\ngeneral overview of our approach. We use the German Credit data set, which is\navailable for download from the UC Irvine Machine Learning Repository, to\ndevelop (1) a prediction model, which is treated as a black box, and (2) a\ncausal model for bias mitigation. In this paper, we focus on age bias and the\nproblem of binary classification. We show that the probability of getting\ncorrectly classified as \"low risk\" is lowest among young people. The\nprobability increases with age nonlinearly. To incorporate the nonlinearity\ninto the causal model, we introduce a higher order polynomial term. Based on\nthe fitted causal model, the de-biased probability estimates are computed,\nshowing improved fairness with little impact on overall classification\naccuracy. Causal modeling is intuitive and, hence, its use can enhance\nexplicability and promotes trust among different stakeholders of AI.\n","authors":["Wendy Hui","Wai Kwong Lau"],"pdf_url":"https://arxiv.org/pdf/2312.05429v2.pdf","comment":"5 pages, 3 figures, 12 tables. arXiv admin note: text overlap with\n  arXiv:2310.12421"},{"id":"http://arxiv.org/abs/2312.10858v1","updated":"2023-12-18T00:21:47Z","published":"2023-12-18T00:21:47Z","title":"Variable Importance in High-Dimensional Settings Requires Grouping","summary":"  Explaining the decision process of machine learning algorithms is nowadays\ncrucial for both model's performance enhancement and human comprehension. This\ncan be achieved by assessing the variable importance of single variables, even\nfor high-capacity non-linear methods, e.g. Deep Neural Networks (DNNs). While\nonly removal-based approaches, such as Permutation Importance (PI), can bring\nstatistical validity, they return misleading results when variables are\ncorrelated. Conditional Permutation Importance (CPI) bypasses PI's limitations\nin such cases. However, in high-dimensional settings, where high correlations\nbetween the variables cancel their conditional importance, the use of CPI as\nwell as other methods leads to unreliable results, besides prohibitive\ncomputation costs. Grouping variables statistically via clustering or some\nprior knowledge gains some power back and leads to better interpretations. In\nthis work, we introduce BCPI (Block-Based Conditional Permutation Importance),\na new generic framework for variable importance computation with statistical\nguarantees handling both single and group cases. Furthermore, as handling\ngroups with high cardinality (such as a set of observations of a given\nmodality) are both time-consuming and resource-intensive, we also introduce a\nnew stacking approach extending the DNN architecture with sub-linear layers\nadapted to the group structure. We show that the ensuing approach extended with\nstacking controls the type-I error even with highly-correlated groups and shows\ntop accuracy across benchmarks. Furthermore, we perform a real-world data\nanalysis in a large-scale medical dataset where we aim to show the consistency\nbetween our results and the literature for a biomarker prediction.\n","authors":["Ahmad Chamma","Bertrand Thirion","Denis A. Engemann"],"pdf_url":"https://arxiv.org/pdf/2312.10858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10854v1","updated":"2023-12-18T00:05:28Z","published":"2023-12-18T00:05:28Z","title":"The Right Losses for the Right Gains: Improving the Semantic Consistency\n  of Deep Text-to-Image Generation with Distribution-Sensitive Losses","summary":"  One of the major challenges in training deep neural networks for\ntext-to-image generation is the significant linguistic discrepancy between\nground-truth captions of each image in most popular datasets. The large\ndifference in the choice of words in such captions results in synthesizing\nimages that are semantically dissimilar to each other and to their ground-truth\ncounterparts. Moreover, existing models either fail to generate the\nfine-grained details of the image or require a huge number of parameters that\nrenders them inefficient for text-to-image synthesis. To fill this gap in the\nliterature, we propose using the contrastive learning approach with a novel\ncombination of two loss functions: fake-to-fake loss to increase the semantic\nconsistency between generated images of the same caption, and fake-to-real loss\nto reduce the gap between the distributions of real images and fake ones. We\ntest this approach on two baseline models: SSAGAN and AttnGAN (with style\nblocks to enhance the fine-grained details of the images.) Results show that\nour approach improves the qualitative results on AttnGAN with style blocks on\nthe CUB dataset. Additionally, on the challenging COCO dataset, our approach\nachieves competitive results against the state-of-the-art Lafite model,\noutperforms the FID score of SSAGAN model by 44.\n","authors":["Mahmoud Ahmed","Omer Moussa","Ismail Shaheen","Mohamed Abdelfattah","Amr Abdalla","Marwan Eid","Hesham Eraqi","Mohamed Moustafa"],"pdf_url":"https://arxiv.org/pdf/2312.10854v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2312.00347v2","updated":"2023-12-18T04:59:01Z","published":"2023-12-01T04:51:01Z","title":"RTQ: Rethinking Video-language Understanding Based on Image-text Model","summary":"  Recent advancements in video-language understanding have been established on\nthe foundation of image-text models, resulting in promising outcomes due to the\nshared knowledge between images and videos. However, video-language\nunderstanding presents unique challenges due to the inclusion of highly complex\nsemantic details, which result in information redundancy, temporal dependency,\nand scene complexity. Current techniques have only partially tackled these\nissues, and our quantitative analysis indicates that some of these methods are\ncomplementary. In light of this, we propose a novel framework called RTQ\n(Refine, Temporal model, and Query), which addresses these challenges\nsimultaneously. The approach involves refining redundant information within\nframes, modeling temporal relations among frames, and querying task-specific\ninformation from the videos. Remarkably, our model demonstrates outstanding\nperformance even in the absence of video-language pre-training, and the results\nare comparable with or superior to those achieved by state-of-the-art\npre-training methods. Code is available at\nhttps://github.com/SCZwangxiao/RTQ-MM2023.\n","authors":["Xiao Wang","Yaoyu Li","Tian Gan","Zheng Zhang","Jingjing Lv","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2312.00347v2.pdf","comment":"Accepted by ACM MM 2023 as Oral representation"},{"id":"http://arxiv.org/abs/2310.17796v3","updated":"2023-12-18T15:09:20Z","published":"2023-10-26T21:57:21Z","title":"ControlLLM: Augment Language Models with Tools by Searching on Graphs","summary":"  We present ControlLLM, a novel framework that enables large language models\n(LLMs) to utilize multi-modal tools for solving complex real-world tasks.\nDespite the remarkable performance of LLMs, they still struggle with tool\ninvocation due to ambiguous user prompts, inaccurate tool selection and\nparameterization, and inefficient tool scheduling. To overcome these\nchallenges, our framework comprises three key components: (1) a \\textit{task\ndecomposer} that breaks down a complex task into clear subtasks with\nwell-defined inputs and outputs; (2) a \\textit{Thoughts-on-Graph (ToG)\nparadigm} that searches the optimal solution path on a pre-built tool graph,\nwhich specifies the parameter and dependency relations among different tools;\nand (3) an \\textit{execution engine with a rich toolbox} that interprets the\nsolution path and runs the tools efficiently on different computational\ndevices. We evaluate our framework on diverse tasks involving image, audio, and\nvideo processing, demonstrating its superior accuracy, efficiency, and\nversatility compared to existing methods. The code is at\nhttps://github.com/OpenGVLab/ControlLLM.\n","authors":["Zhaoyang Liu","Zeqiang Lai","Zhangwei Gao","Erfei Cui","Ziheng Li","Xizhou Zhu","Lewei Lu","Qifeng Chen","Yu Qiao","Jifeng Dai","Wenhai Wang"],"pdf_url":"https://arxiv.org/pdf/2310.17796v3.pdf","comment":"24 pages, 9 figures, 12 tables"},{"id":"http://arxiv.org/abs/2312.11576v1","updated":"2023-12-18T09:24:35Z","published":"2023-12-18T09:24:35Z","title":"Emotion Based Prediction in the Context of Optimized Trajectory Planning\n  for Immersive Learning","summary":"  In the virtual elements of immersive learning, the use of Google Expedition\nand touch-screen-based emotion are examined. The objective is to investigate\npossible ways to combine these technologies to enhance virtual learning\nenvironments and learners emotional engagement. Pedagogical application,\naffordances, and cognitive load are the corresponding measures that are\ninvolved. Students will gain insight into the reason behind their significantly\nhigher post-assessment Prediction Systems scores compared to preassessment\nscores through this work that leverages technology. This suggests that it is\neffective to include emotional elements in immersive learning scenarios. The\nresults of this study may help develop new strategies by leveraging the\nfeatures of immersive learning technology in educational technologies to\nimprove virtual reality and augmented reality experiences. Furthermore, the\neffectiveness of immersive learning environments can be raised by utilizing\nmagnetic, optical, or hybrid trackers that considerably improve object\ntracking.\n","authors":["Akey Sungheetha","Rajesh Sharma R","Chinnaiyan R"],"pdf_url":"https://arxiv.org/pdf/2312.11576v1.pdf","comment":"5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2312.11023v1","updated":"2023-12-18T08:55:42Z","published":"2023-12-18T08:55:42Z","title":"Frequency Spectrum is More Effective for Multimodal Representation and\n  Fusion: A Multimodal Spectrum Rumor Detector","summary":"  Multimodal content, such as mixing text with images, presents significant\nchallenges to rumor detection in social media. Existing multimodal rumor\ndetection has focused on mixing tokens among spatial and sequential locations\nfor unimodal representation or fusing clues of rumor veracity across\nmodalities. However, they suffer from less discriminative unimodal\nrepresentation and are vulnerable to intricate location dependencies in the\ntime-consuming fusion of spatial and sequential tokens. This work makes the\nfirst attempt at multimodal rumor detection in the frequency domain, which\nefficiently transforms spatial features into the frequency spectrum and obtains\nhighly discriminative spectrum features for multimodal representation and\nfusion. A novel Frequency Spectrum Representation and fUsion network (FSRU)\nwith dual contrastive learning reveals the frequency spectrum is more effective\nfor multimodal representation and fusion, extracting the informative components\nfor rumor detection. FSRU involves three novel mechanisms: utilizing the\nFourier transform to convert features in the spatial domain to the frequency\ndomain, the unimodal spectrum compression, and the cross-modal spectrum\nco-selection module in the frequency domain. Substantial experiments show that\nFSRU achieves satisfactory multimodal rumor detection performance.\n","authors":["An Lao","Qi Zhang","Chongyang Shi","Longbing Cao","Kun Yi","Liang Hu","Duoqian Miao"],"pdf_url":"https://arxiv.org/pdf/2312.11023v1.pdf","comment":"12 pages, AAAI-2024"},{"id":"http://arxiv.org/abs/2312.10980v1","updated":"2023-12-18T07:03:35Z","published":"2023-12-18T07:03:35Z","title":"Liquid Leak Detection Using Thermal Images","summary":"  This paper presents a comprehensive solution to address the critical\nchallenge of liquid leaks in the oil and gas industry, leveraging advanced\ncomputer vision and deep learning methodologies. Employing You Only Look Once\n(YOLO) and Real-Time Detection Transformer (RT DETR) models, our project\nfocuses on enhancing early identification of liquid leaks in key infrastructure\ncomponents such as pipelines, pumps, and tanks. Through the integration of\nsurveillance thermal cameras and sensors, the combined YOLO and RT DETR models\ndemonstrate remarkable efficacy in the continuous monitoring and analysis of\nvisual data within oil and gas facilities. YOLO's real-time object detection\ncapabilities swiftly recognize leaks and their patterns, while RT DETR excels\nin discerning specific leak-related features, particularly in thermal images.\nThis approach significantly improves the accuracy and speed of leak detection,\nultimately mitigating environmental and financial risks associated with liquid\nleaks.\n","authors":["Kalpak Bansod","Yanshan Wan","Yugesh Rai"],"pdf_url":"https://arxiv.org/pdf/2312.10980v1.pdf","comment":"13 pages, 9 figures"},{"id":"http://arxiv.org/abs/2309.11082v2","updated":"2023-12-18T06:47:29Z","published":"2023-09-20T06:08:11Z","title":"Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial\n  Margin Contrastive Learning","summary":"  In recent years, the explosion of web videos makes text-video retrieval\nincreasingly essential and popular for video filtering, recommendation, and\nsearch. Text-video retrieval aims to rank relevant text/video higher than\nirrelevant ones. The core of this task is to precisely measure the cross-modal\nsimilarity between texts and videos. Recently, contrastive learning methods\nhave shown promising results for text-video retrieval, most of which focus on\nthe construction of positive and negative pairs to learn text and video\nrepresentations. Nevertheless, they do not pay enough attention to hard\nnegative pairs and lack the ability to model different levels of semantic\nsimilarity. To address these two issues, this paper improves contrastive\nlearning using two novel techniques. First, to exploit hard examples for robust\ndiscriminative power, we propose a novel Dual-Modal Attention-Enhanced Module\n(DMAE) to mine hard negative pairs from textual and visual clues. By further\nintroducing a Negative-aware InfoNCE (NegNCE) loss, we are able to adaptively\nidentify all these hard negatives and explicitly highlight their impacts in the\ntraining loss. Second, our work argues that triplet samples can better model\nfine-grained semantic similarity compared to pairwise samples. We thereby\npresent a new Triplet Partial Margin Contrastive Learning (TPM-CL) module to\nconstruct partial order triplet samples by automatically generating\nfine-grained hard negatives for matched text-video pairs. The proposed TPM-CL\ndesigns an adaptive token masking strategy with cross-modal interaction to\nmodel subtle semantic differences. Extensive experiments demonstrate that the\nproposed approach outperforms existing methods on four widely-used text-video\nretrieval datasets, including MSR-VTT, MSVD, DiDeMo and ActivityNet.\n","authors":["Chen Jiang","Hong Liu","Xuzheng Yu","Qing Wang","Yuan Cheng","Jia Xu","Zhongyi Liu","Qingpei Guo","Wei Chu","Ming Yang","Yuan Qi"],"pdf_url":"https://arxiv.org/pdf/2309.11082v2.pdf","comment":"Accepted by ACM MM 2023"},{"id":"http://arxiv.org/abs/2312.10949v1","updated":"2023-12-18T05:55:46Z","published":"2023-12-18T05:55:46Z","title":"Leveraged Mel spectrograms using Harmonic and Percussive Components in\n  Speech Emotion Recognition","summary":"  Speech Emotion Recognition (SER) affective technology enables the intelligent\nembedded devices to interact with sensitivity. Similarly, call centre employees\nrecognise customers' emotions from their pitch, energy, and tone of voice so as\nto modify their speech for a high-quality interaction with customers. This work\nexplores, for the first time, the effects of the harmonic and percussive\ncomponents of Mel spectrograms in SER. We attempt to leverage the Mel\nspectrogram by decomposing distinguishable acoustic features for exploitation\nin our proposed architecture, which includes a novel feature map generator\nalgorithm, a CNN-based network feature extractor and a multi-layer perceptron\n(MLP) classifier. This study specifically focuses on effective data\naugmentation techniques for building an enriched hybrid-based feature map. This\nprocess results in a function that outputs a 2D image so that it can be used as\ninput data for a pre-trained CNN-VGG16 feature extractor. Furthermore, we also\ninvestigate other acoustic features such as MFCCs, chromagram, spectral\ncontrast, and the tonnetz to assess our proposed framework. A test accuracy of\n92.79% on the Berlin EMO-DB database is achieved. Our result is higher than\nprevious works using CNN-VGG16.\n","authors":["David Hason Rudd","Huan Huo","Guandong Xu"],"pdf_url":"https://arxiv.org/pdf/2312.10949v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2312.10937v1","updated":"2023-12-18T05:24:03Z","published":"2023-12-18T05:24:03Z","title":"An Extended Variational Mode Decomposition Algorithm Developed Speech\n  Emotion Recognition Performance","summary":"  Emotion recognition (ER) from speech signals is a robust approach since it\ncannot be imitated like facial expression or text based sentiment analysis.\nValuable information underlying the emotions are significant for human-computer\ninteractions enabling intelligent machines to interact with sensitivity in the\nreal world. Previous ER studies through speech signal processing have focused\nexclusively on associations between different signal mode decomposition methods\nand hidden informative features. However, improper decomposition parameter\nselections lead to informative signal component losses due to mode duplicating\nand mixing. In contrast, the current study proposes VGG-optiVMD, an empowered\nvariational mode decomposition algorithm, to distinguish meaningful speech\nfeatures and automatically select the number of decomposed modes and optimum\nbalancing parameter for the data fidelity constraint by assessing their effects\non the VGG16 flattening output layer. Various feature vectors were employed to\ntrain the VGG16 network on different databases and assess VGG-optiVMD\nreproducibility and reliability. One, two, and three-dimensional feature\nvectors were constructed by concatenating Mel-frequency cepstral coefficients,\nChromagram, Mel spectrograms, Tonnetz diagrams, and spectral centroids. Results\nconfirmed a synergistic relationship between the fine-tuning of the signal\nsample rate and decomposition parameters with classification accuracy,\nachieving state-of-the-art 96.09% accuracy in predicting seven emotions on the\nBerlin EMO-DB database.\n","authors":["David Hason Rudd","Huan Huo","Guandong Xu"],"pdf_url":"https://arxiv.org/pdf/2312.10937v1.pdf","comment":"12 pages"}]},"2023-12-17T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2312.08579v2","updated":"2023-12-17T23:20:49Z","published":"2023-12-14T00:50:14Z","title":"Identifying Planetary Names in Astronomy Papers: A Multi-Step Approach","summary":"  The automatic identification of planetary feature names in astronomy\npublications presents numerous challenges. These features include craters,\ndefined as roughly circular depressions resulting from impact or volcanic\nactivity; dorsas, which are elongate raised structures or wrinkle ridges; and\nlacus, small irregular patches of dark, smooth material on the Moon, referred\nto as \"lake\" (Planetary Names Working Group, n.d.). Many feature names overlap\nwith places or people's names that they are named after, for example, Syria,\nTempe, Einstein, and Sagan, to name a few (U.S. Geological Survey, n.d.). Some\nfeature names have been used in many contexts, for instance, Apollo, which can\nrefer to mission, program, sample, astronaut, seismic, seismometers, core, era,\ndata, collection, instrument, and station, in addition to the crater on the\nMoon. Some feature names can appear in the text as adjectives, like the lunar\ncraters Black, Green, and White. Some feature names in other contexts serve as\ndirections, like craters West and South on the Moon. Additionally, some\nfeatures share identical names across different celestial bodies, requiring\ndisambiguation, such as the Adams crater, which exists on both the Moon and\nMars. We present a multi-step pipeline combining rule-based filtering,\nstatistical relevance analysis, part-of-speech (POS) tagging, named entity\nrecognition (NER) model, hybrid keyword harvesting, knowledge graph (KG)\nmatching, and inference with a locally installed large language model (LLM) to\nreliably identify planetary names despite these challenges. When evaluated on a\ndataset of astronomy papers from the Astrophysics Data System (ADS), this\nmethodology achieves an F1-score over 0.97 in disambiguating planetary feature\nnames.\n","authors":["Golnaz Shapurian","Michael J Kurtz","Alberto Accomazzi"],"pdf_url":"https://arxiv.org/pdf/2312.08579v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14865v2","updated":"2023-12-17T23:14:37Z","published":"2023-11-24T23:00:36Z","title":"Improving Cross-Domain Hate Speech Generalizability with Emotion\n  Knowledge","summary":"  Reliable automatic hate speech (HS) detection systems must adapt to the\nin-flow of diverse new data to curtail hate speech. However, hate speech\ndetection systems commonly lack generalizability in identifying hate speech\ndissimilar to data used in training, impeding their robustness in real-world\ndeployments. In this work, we propose a hate speech generalization framework\nthat leverages emotion knowledge in a multitask architecture to improve the\ngeneralizability of hate speech detection in a cross-domain setting. We\ninvestigate emotion corpora with varying emotion categorical scopes to\ndetermine the best corpus scope for supplying emotion knowledge to foster\ngeneralized hate speech detection. We further assess the relationship between\nusing pretrained Transformers models adapted for hate speech and its effect on\nour emotion-enriched hate speech generalization model. We perform extensive\nexperiments on six publicly available datasets sourced from different online\ndomains and show that our emotion-enriched HS detection generalization method\ndemonstrates consistent generalization improvement in cross-domain evaluation,\nincreasing generalization performance up to 18.1% and average cross-domain\nperformance up to 8.5%, according to the F1 measure.\n","authors":["Shi Yin Hong","Susan Gauch"],"pdf_url":"https://arxiv.org/pdf/2311.14865v2.pdf","comment":"Accepted to Pacific Asia Conference on Language, Information and\n  Computation (PACLIC 37)"},{"id":"http://arxiv.org/abs/2311.07723v3","updated":"2023-12-17T21:18:14Z","published":"2023-11-13T20:07:36Z","title":"Generalization Analogies: A Testbed for Generalizing AI Oversight to\n  Hard-To-Measure Domains","summary":"  As AI systems become more intelligent and their behavior becomes more\nchallenging to assess, they may learn to game the flaws of human feedback\ninstead of genuinely striving to follow instructions; however, this risk can be\nmitigated by controlling how LLMs generalize human feedback to situations where\nit is unreliable. To better understand how reward models generalize, we craft\n69 distribution shifts spanning 8 categories. We find that reward models do not\nlearn to evaluate `instruction-following' by default and instead favor personas\nthat resemble internet text. Techniques for interpreting reward models'\ninternal representations achieve better generalization than standard\nfine-tuning, but still frequently fail to distinguish instruction-following\nfrom conflated behaviors. We consolidate the 15 most challenging distribution\nshifts into the GENeralization analogIES (GENIES) benchmark, which we hope will\nenable progress toward controlling reward model generalization.\n","authors":["Joshua Clymer","Garrett Baker","Rohan Subramani","Sam Wang"],"pdf_url":"https://arxiv.org/pdf/2311.07723v3.pdf","comment":"Code: https://github.com/Joshuaclymer/GENIES Website:\n  https://joshuaclymer.github.io/generalization-analogies-website/"},{"id":"http://arxiv.org/abs/2312.10813v1","updated":"2023-12-17T20:42:43Z","published":"2023-12-17T20:42:43Z","title":"Re-parameterized Low-rank Prompt: Generalize a Vision-Language Model\n  within 0.5K Parameters","summary":"  With the development of large pre-trained vision-language models, how to\neffectively transfer the knowledge of such foundational models to downstream\ntasks becomes a hot topic, especially in a data-deficient scenario. Recently,\nprompt tuning has become a popular solution. When adapting the vision-language\nmodels, researchers freeze the parameters in the backbone and only design and\ntune the prompts. On the one hand, the delicate design of prompt tuning\nexhibits strong performance. On the other hand, complicated structures and\nupdate rules largely increase the computation and storage cost. Motivated by\nthe observation that the evolution pattern of the generalization capability in\nvisual-language models aligns harmoniously with the trend of rank variations in\nthe prompt matrix during adaptation, we design a new type of prompt,\nRe-parameterized Low-rank Prompt (RLP), for both efficient and effective\nadaptation. Our method could largely reduce the number of tunable parameters\nand storage space, which is quite beneficial in resource-limited scenarios.\nExtensive experiments further demonstrate the superiority of RLP. In\nparticular, RLP shows comparable or even stronger performance than the latest\nstate-of-the-art methods with an extremely small number of parameters. On a\nseries of tasks over 11 datasets, RLP significantly increases the average\ndownstream accuracy of classic prompt tuning by up to 5.25% using merely 0.5K\nparameters.\n","authors":["Tianxiang Hao","Mengyao Lyu","Hui Chen","Sicheng Zhao","Jungong Han","Guiguang Ding"],"pdf_url":"https://arxiv.org/pdf/2312.10813v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10793v1","updated":"2023-12-17T18:44:26Z","published":"2023-12-17T18:44:26Z","title":"Understanding the Instruction Mixture for Large Language Model","summary":"  While instructions fine-tuning of large language models (LLMs) has been\nproven to enhance performance across various applications, the influence of the\ninstruction dataset mixture on LLMs has not been thoroughly explored. In this\nstudy, we classify instructions into three main types: NLP downstream tasks,\ncoding, and general chatting, and investigate their impact on LLMs. Our\nfindings reveal that specific types of instructions are more beneficial for\nparticular uses, while it may cause harms to other aspects, emphasizing the\nimportance of meticulously designing the instruction mixture to maximize model\nperformance. This study sheds light on the instruction mixture and paves the\nway for future research.\n","authors":["Renxi Wang","Minghao Wu","Yuxia Wang","Xudong Han","Chiyu Zhang","Haonan Li"],"pdf_url":"https://arxiv.org/pdf/2312.10793v1.pdf","comment":"Instruction Tuning, Large Language Model, Alignment"},{"id":"http://arxiv.org/abs/2312.10771v1","updated":"2023-12-17T17:26:50Z","published":"2023-12-17T17:26:50Z","title":"kNN-ICL: Compositional Task-Oriented Parsing Generalization with Nearest\n  Neighbor In-Context Learning","summary":"  Task-Oriented Parsing (TOP) enables conversational assistants to interpret\nuser commands expressed in natural language, transforming them into structured\noutputs that combine elements of both natural language and intent/slot tags.\nRecently, Large Language Models (LLMs) have achieved impressive performance in\nsynthesizing computer programs based on a natural language prompt, mitigating\nthe gap between natural language and structured programs. Our paper focuses on\nharnessing the capabilities of LLMs for semantic parsing tasks, addressing the\nfollowing three key research questions: 1) How can LLMs be effectively utilized\nfor semantic parsing tasks? 2) What defines an effective prompt? and 3) How can\nLLM overcome the length constraint and streamline prompt design by including\nall examples as prompts? We introduce k Nearest Neighbor In-Context\nLearning(kNN-ICL), which simplifies prompt engineering by allowing it to be\nbuilt on top of any design strategy while providing access to all demo\nexamples. Extensive experiments show that: 1)Simple ICL without kNN search can\nachieve a comparable performance with strong supervised models on the TOP\ntasks, and 2) kNN-ICL significantly improves the comprehension of complex\nrequests by seamlessly integrating ICL with a nearest-neighbor approach.\nNotably, this enhancement is achieved without the need for additional data or\nspecialized prompts.\n","authors":["Wenting Zhao","Ye Liu","Yao Wan","Yibo Wang","Qingyang Wu","Zhongfen Deng","Jiangshu Du","Shuaiqi Liu","Yunlong Xu","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2312.10771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10770v1","updated":"2023-12-17T17:23:43Z","published":"2023-12-17T17:23:43Z","title":"Identification of Knowledge Neurons in Protein Language Models","summary":"  Neural language models have become powerful tools for learning complex\nrepresentations of entities in natural language processing tasks. However,\ntheir interpretability remains a significant challenge, particularly in domains\nlike computational biology where trust in model predictions is crucial. In this\nwork, we aim to enhance the interpretability of protein language models,\nspecifically the state-of-the-art ESM model, by identifying and characterizing\nknowledge neurons - components that express understanding of key information.\nAfter fine-tuning the ESM model for the task of enzyme sequence classification,\nwe compare two knowledge neuron selection methods that preserve a subset of\nneurons from the original model. The two methods, activation-based and\nintegrated gradient-based selection, consistently outperform a random baseline.\nIn particular, these methods show that there is a high density of knowledge\nneurons in the key vector prediction networks of self-attention modules. Given\nthat key vectors specialize in understanding different features of input\nsequences, these knowledge neurons could capture knowledge of different enzyme\nsequence motifs. In the future, the types of knowledge captured by each neuron\ncould be characterized.\n","authors":["Divya Nori","Shivali Singireddy","Marina Ten Have"],"pdf_url":"https://arxiv.org/pdf/2312.10770v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14196v3","updated":"2023-12-17T17:05:09Z","published":"2023-05-23T16:15:31Z","title":"ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding","summary":"  We introduce ZeroSCROLLS, a zero-shot benchmark for natural language\nunderstanding over long texts, which contains only test and small validation\nsets, without training data. We adapt six tasks from the SCROLLS benchmark, and\nadd four new datasets, including two novel information fusing tasks, such as\naggregating the percentage of positive reviews. Using ZeroSCROLLS, we conduct a\ncomprehensive evaluation of both open-source and closed large language models,\nfinding that Claude outperforms ChatGPT, and that GPT-4 achieves the highest\naverage score. However, there is still room for improvement on multiple open\nchallenges in ZeroSCROLLS, such as aggregation tasks, where models struggle to\npass the naive baseline. As the state of the art is a moving target, we invite\nresearchers to evaluate their ideas on the live ZeroSCROLLS leaderboard.\n","authors":["Uri Shaham","Maor Ivgi","Avia Efrat","Jonathan Berant","Omer Levy"],"pdf_url":"https://arxiv.org/pdf/2305.14196v3.pdf","comment":"Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2312.10750v1","updated":"2023-12-17T15:56:05Z","published":"2023-12-17T15:56:05Z","title":"Distinguishing Translations by Human, NMT, and ChatGPT: A Linguistic and\n  Statistical Approach","summary":"  The growing popularity of neural machine translation (NMT) and LLMs\nrepresented by ChatGPT underscores the need for a deeper understanding of their\ndistinct characteristics and relationships. Such understanding is crucial for\nlanguage professionals and researchers to make informed decisions and tactful\nuse of these cutting-edge translation technology, but remains underexplored.\nThis study aims to fill this gap by investigating three key questions: (1) the\ndistinguishability of ChatGPT-generated translations from NMT and human\ntranslation (HT), (2) the linguistic characteristics of each translation type,\nand (3) the degree of resemblance between ChatGPT-produced translations and HT\nor NMT. To achieve these objectives, we employ statistical testing, machine\nlearning algorithms, and multidimensional analysis (MDA) to analyze\nSpokesperson's Remarks and their translations. After extracting a wide range of\nlinguistic features, supervised classifiers demonstrate high accuracy in\ndistinguishing the three translation types, whereas unsupervised clustering\ntechniques do not yield satisfactory results. Another major finding is that\nChatGPT-produced translations exhibit greater similarity with NMT than HT in\nmost MDA dimensions, which is further corroborated by distance computing and\nvisualization. These novel insights shed light on the interrelationships among\nthe three translation types and have implications for the future advancements\nof NMT and generative AI.\n","authors":["Zhaokun Jiang","Qianxi Lv","Ziyin Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.10750v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10748v1","updated":"2023-12-17T15:50:05Z","published":"2023-12-17T15:50:05Z","title":"Multi-Label Classification of COVID-Tweets Using Large Language Models","summary":"  Vaccination is important to minimize the risk and spread of various diseases.\nIn recent years, vaccination has been a key step in countering the COVID-19\npandemic. However, many people are skeptical about the use of vaccines for\nvarious reasons, including the politics involved, the potential side effects of\nvaccines, etc. The goal in this task is to build an effective multi-label\nclassifier to label a social media post (particularly, a tweet) according to\nthe specific concern(s) towards vaccines as expressed by the author of the\npost. We tried three different models-(a) Supervised BERT-large-uncased, (b)\nSupervised HateXplain model, and (c) Zero-Shot GPT-3.5 Turbo model. The\nSupervised BERT-large-uncased model performed best in our case. We achieved a\nmacro-F1 score of 0.66, a Jaccard similarity score of 0.66, and received the\nsixth rank among other submissions. Code is available\nat-https://github.com/anonmous1981/AISOME\n","authors":["Aniket Deroy","Subhankar Maity"],"pdf_url":"https://arxiv.org/pdf/2312.10748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10746v1","updated":"2023-12-17T15:37:03Z","published":"2023-12-17T15:37:03Z","title":"Knowledge Trees: Gradient Boosting Decision Trees on Knowledge Neurons\n  as Probing Classifier","summary":"  To understand how well a large language model captures certain semantic or\nsyntactic features, researchers typically apply probing classifiers. However,\nthe accuracy of these classifiers is critical for the correct interpretation of\nthe results. If a probing classifier exhibits low accuracy, this may be due\neither to the fact that the language model does not capture the property under\ninvestigation, or to shortcomings in the classifier itself, which is unable to\nadequately capture the characteristics encoded in the internal representations\nof the model. Consequently, for more effective diagnosis, it is necessary to\nuse the most accurate classifiers possible for a particular type of task.\nLogistic regression on the output representation of the transformer neural\nnetwork layer is most often used to probing the syntactic properties of the\nlanguage model.\n  We show that using gradient boosting decision trees at the Knowledge Neuron\nlayer, i.e., at the hidden layer of the feed-forward network of the transformer\nas a probing classifier for recognizing parts of a sentence is more\nadvantageous than using logistic regression on the output representations of\nthe transformer layer. This approach is also preferable to many other methods.\nThe gain in error rate, depending on the preset, ranges from 9-54%\n","authors":["Sergey A. Saltykov"],"pdf_url":"https://arxiv.org/pdf/2312.10746v1.pdf","comment":"10 pages, 7 figures, 4 tables"},{"id":"http://arxiv.org/abs/2312.10741v1","updated":"2023-12-17T15:26:16Z","published":"2023-12-17T15:26:16Z","title":"StyleSinger: Style Transfer for Out-Of-Domain Singing Voice Synthesis","summary":"  Style transfer for out-of-domain (OOD) singing voice synthesis (SVS) focuses\non generating high-quality singing voices with unseen styles (such as timbre,\nemotion, pronunciation, and articulation skills) derived from reference singing\nvoice samples. However, the endeavor to model the intricate nuances of singing\nvoice styles is an arduous task, as singing voices possess a remarkable degree\nof expressiveness. Moreover, existing SVS methods encounter a decline in the\nquality of synthesized singing voices in OOD scenarios, as they rest upon the\nassumption that the target vocal attributes are discernible during the training\nphase. To overcome these challenges, we propose StyleSinger, the first singing\nvoice synthesis model for zero-shot style transfer of out-of-domain reference\nsinging voice samples. StyleSinger incorporates two critical approaches for\nenhanced effectiveness: 1) the Residual Style Adaptor (RSA) which employs a\nresidual quantization module to capture diverse style characteristics in\nsinging voices, and 2) the Uncertainty Modeling Layer Normalization (UMLN) to\nperturb the style attributes within the content representation during the\ntraining phase and thus improve the model generalization. Our extensive\nevaluations in zero-shot style transfer undeniably establish that StyleSinger\noutperforms baseline models in both audio quality and similarity to the\nreference singing voice samples. Access to singing voice samples can be found\nat https://stylesinger.github.io/.\n","authors":["Yu Zhang","Rongjie Huang","Ruiqi Li","JinZheng He","Yan Xia","Feiyang Chen","Xinyu Duan","Baoxing Huai","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2312.10741v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.10730v1","updated":"2023-12-17T14:28:28Z","published":"2023-12-17T14:28:28Z","title":"Mixed Distillation Helps Smaller Language Model Better Reasoning","summary":"  Despite the remarkable performance of large language models (LLMs) in recent\nNLP tasks, their deployment poses substantial challenges due to high\ncomputational and memory demands. Recent research has concentrated on improving\nopen-source smaller models through knowledge distillation from LLMs to reduce\ncomputational resource costs with promising outcomes. Nevertheless, they\nfrequently fall short of attaining LLM-level performance, particularly in tasks\ndemanding advanced reasoning. In this work, we introduce the \\textbf{Mixed\nDistillation} framework, which capitalizes on the strengths of\nProgram-of-Thought (PoT) and Chain-of-Thought (CoT) capabilities within LLMs\nand distills these capabilities to smaller models. Regarding these two\ncapabilities, the PoT is dedicated to enhancing the performance of reasoning\nresults generated by smaller models, while CoT simultaneously optimizes the\nresults. Our Mixed Distillation framework offers a promising approach to\nenhance the capabilities of smaller models, bridging the gap with LLMs, and\ndemonstrating better performance across various tasks. Specifically, on the\nSVAMP dataset, employing a 7 billion parameter Llama2 and CodeLlama in a mixed\ndistillation framework not only boosts distillation capabilities beyond\nsingle-path distillation methods but also outperforms the LLM (GPT-3.5-turbo)\nin terms of reasoning accuracy. Through sampling in multiple-path reasoning,\nthe models achieve impressive accuracy performances of 85% and 85.5%,\nrespectively, signifying advancements over previous distillation methods.\n","authors":["Li Chenglin","Chen Qianglong","Wang Caiyu","Zhang Yin"],"pdf_url":"https://arxiv.org/pdf/2312.10730v1.pdf","comment":"Working in Progress, 11 pages, 9 figures"},{"id":"http://arxiv.org/abs/2311.08152v2","updated":"2023-12-17T13:02:27Z","published":"2023-11-14T13:27:07Z","title":"Towards Reasoning in Large Language Models via Multi-Agent Peer Review\n  Collaboration","summary":"  Large Language Models (LLMs) have shown remarkable capabilities in general\nnatural language processing tasks but often fall short in complex reasoning\ntasks. Recent studies have explored human-like problem-solving strategies, such\nas self-correct, to push further the boundary of single-model reasoning\nability. In this work, we let a single model \"step outside the box\" by engaging\nmultiple models to correct each other. We introduce a multi-agent collaboration\nstrategy that emulates the academic peer review process. Each agent\nindependently constructs its own solution, provides reviews on the solutions of\nothers, and assigns confidence levels to its reviews. Upon receiving peer\nreviews, agents revise their initial solutions. Extensive experiments on three\ndifferent types of reasoning tasks show that our collaboration approach\ndelivers superior accuracy across all ten datasets compared to existing\nmethods. Further study underscores the effectiveness of integrating confidence\nin reviews, demonstrates the superiority of feedback exchange over mere\nsolution sharing, and highlights the role of capability and diversity in\nfostering successful collaboration.\n","authors":["Zhenran Xu","Senbao Shi","Baotian Hu","Jindi Yu","Dongfang Li","Min Zhang","Yuxiang Wu"],"pdf_url":"https://arxiv.org/pdf/2311.08152v2.pdf","comment":"16 pages, 6 figures, 11 tables. Work in progress"},{"id":"http://arxiv.org/abs/2312.10700v1","updated":"2023-12-17T12:27:15Z","published":"2023-12-17T12:27:15Z","title":"Cross-Domain Robustness of Transformer-based Keyphrase Generation","summary":"  Modern models for text generation show state-of-the-art results in many\nnatural language processing tasks. In this work, we explore the effectiveness\nof abstractive text summarization models for keyphrase selection. A list of\nkeyphrases is an important element of a text in databases and repositories of\nelectronic documents. In our experiments, abstractive text summarization models\nfine-tuned for keyphrase generation show quite high results for a target text\ncorpus. However, in most cases, the zero-shot performance on other corpora and\ndomains is significantly lower. We investigate cross-domain limitations of\nabstractive text summarization models for keyphrase generation. We present an\nevaluation of the fine-tuned BART models for the keyphrase selection task\nacross six benchmark corpora for keyphrase extraction including scientific\ntexts from two domains and news texts. We explore the role of transfer learning\nbetween different domains to improve the BART model performance on small text\ncorpora. Our experiments show that preliminary fine-tuning on out-of-domain\ncorpora can be effective under conditions of a limited number of samples.\n","authors":["Anna Glazkova","Dmitry Morozov"],"pdf_url":"https://arxiv.org/pdf/2312.10700v1.pdf","comment":"Presented at the XXV International Conference \"Data Analytics and\n  Management in Data Intensive Domains\" (DAMDID/RCDL), October 2023"},{"id":"http://arxiv.org/abs/2207.08012v4","updated":"2023-12-17T11:12:37Z","published":"2022-07-16T20:37:46Z","title":"Meta-Referential Games to Learn Compositional Learning Behaviours","summary":"  Human beings use compositionality to generalise from past experiences to\nnovel experiences. We assume a separation of our experiences into fundamental\natomic components that can be recombined in novel ways to support our ability\nto engage with novel experiences. We frame this as the ability to learn to\ngeneralise compositionally, and we will refer to behaviours making use of this\nability as compositional learning behaviours (CLBs). A central problem to\nlearning CLBs is the resolution of a binding problem (BP). While it is another\nfeat of intelligence that human beings perform with ease, it is not the case\nfor state-of-the-art artificial agents. Thus, in order to build artificial\nagents able to collaborate with human beings, we propose to develop a novel\nbenchmark to investigate agents' abilities to exhibit CLBs by solving a\ndomain-agnostic version of the BP. We take inspiration from the language\nemergence and grounding framework of referential games and propose a\nmeta-learning extension of referential games, entitled Meta-Referential Games,\nand use this framework to build our benchmark, the Symbolic Behaviour Benchmark\n(S2B). We provide baseline results and error analysis showing that our\nbenchmark is a compelling challenge that we hope will spur the research\ncommunity towards developing more capable artificial agents.\n","authors":["Kevin Denamganaï","Sondess Missaoui","James Alfred Walker"],"pdf_url":"https://arxiv.org/pdf/2207.08012v4.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2311.11267v2","updated":"2023-12-17T11:06:09Z","published":"2023-11-19T08:40:01Z","title":"Rethinking Large Language Models in Mental Health Applications","summary":"  Large Language Models (LLMs) have become valuable assets in mental health,\nshowing promise in both classification tasks and counseling applications. This\npaper offers a perspective on using LLMs in mental health applications. It\ndiscusses the instability of generative models for prediction and the potential\nfor generating hallucinatory outputs, underscoring the need for ongoing audits\nand evaluations to maintain their reliability and dependability. The paper also\ndistinguishes between the often interchangeable terms ``explainability'' and\n``interpretability'', advocating for developing inherently interpretable\nmethods instead of relying on potentially hallucinated self-explanations\ngenerated by LLMs. Despite the advancements in LLMs, human counselors'\nempathetic understanding, nuanced interpretation, and contextual awareness\nremain irreplaceable in the sensitive and complex realm of mental health\ncounseling. The use of LLMs should be approached with a judicious and\nconsiderate mindset, viewing them as tools that complement human expertise\nrather than seeking to replace it.\n","authors":["Shaoxiong Ji","Tianlin Zhang","Kailai Yang","Sophia Ananiadou","Erik Cambria"],"pdf_url":"https://arxiv.org/pdf/2311.11267v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10679v1","updated":"2023-12-17T10:45:50Z","published":"2023-12-17T10:45:50Z","title":"Bengali Intent Classification with Generative Adversarial BERT","summary":"  Intent classification is a fundamental task in natural language\nunderstanding, aiming to categorize user queries or sentences into predefined\nclasses to understand user intent. The most challenging aspect of this\nparticular task lies in effectively incorporating all possible classes of\nintent into a dataset while ensuring adequate linguistic variation. Plenty of\nresearch has been conducted in the related domains in rich-resource languages\nlike English. In this study, we introduce BNIntent30, a comprehensive Bengali\nintent classification dataset containing 30 intent classes. The dataset is\nexcerpted and translated from the CLINIC150 dataset containing a diverse range\nof user intents categorized over 150 classes. Furthermore, we propose a novel\napproach for Bengali intent classification using Generative Adversarial BERT to\nevaluate the proposed dataset, which we call GAN-BnBERT. Our approach leverages\nthe power of BERT-based contextual embeddings to capture salient linguistic\nfeatures and contextual information from the text data, while the generative\nadversarial network (GAN) component complements the model's ability to learn\ndiverse representations of existing intent classes through generative modeling.\nOur experimental results demonstrate that the GAN-BnBERT model achieves\nsuperior performance on the newly introduced BNIntent30 dataset, surpassing the\nexisting Bi-LSTM and the stand-alone BERT-based classification model.\n","authors":["Mehedi Hasan","Mohammad Jahid Ibna Basher","Md. Tanvir Rouf Shawon"],"pdf_url":"https://arxiv.org/pdf/2312.10679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.15494v2","updated":"2023-12-17T10:30:11Z","published":"2023-07-28T11:42:31Z","title":"ETHER: Aligning Emergent Communication for Hindsight Experience Replay","summary":"  Natural language instruction following is paramount to enable collaboration\nbetween artificial agents and human beings. Natural language-conditioned\nreinforcement learning (RL) agents have shown how natural languages'\nproperties, such as compositionality, can provide a strong inductive bias to\nlearn complex policies. Previous architectures like HIGhER combine the benefit\nof language-conditioning with Hindsight Experience Replay (HER) to deal with\nsparse rewards environments. Yet, like HER, HIGhER relies on an oracle\npredicate function to provide a feedback signal highlighting which linguistic\ndescription is valid for which state. This reliance on an oracle limits its\napplication. Additionally, HIGhER only leverages the linguistic information\ncontained in successful RL trajectories, thus hurting its final performance and\ndata-efficiency. Without early successful trajectories, HIGhER is no better\nthan DQN upon which it is built. In this paper, we propose the Emergent Textual\nHindsight Experience Replay (ETHER) agent, which builds on HIGhER and addresses\nboth of its limitations by means of (i) a discriminative visual referential\ngame, commonly studied in the subfield of Emergent Communication (EC), used\nhere as an unsupervised auxiliary task and (ii) a semantic grounding scheme to\nalign the emergent language with the natural language of the\ninstruction-following benchmark. We show that the referential game's agents\nmake an artificial language emerge that is aligned with the natural-like\nlanguage used to describe goals in the BabyAI benchmark and that it is\nexpressive enough so as to also describe unsuccessful RL trajectories and thus\nprovide feedback to the RL agent to leverage the linguistic, structured\ninformation contained in all trajectories. Our work shows that EC is a viable\nunsupervised auxiliary task for RL and provides missing pieces to make HER more\nwidely applicable.\n","authors":["Kevin Denamganaï","Daniel Hernandez","Ozan Vardal","Sondess Missaoui","James Alfred Walker"],"pdf_url":"https://arxiv.org/pdf/2307.15494v2.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2312.10665v1","updated":"2023-12-17T09:44:27Z","published":"2023-12-17T09:44:27Z","title":"Silkie: Preference Distillation for Large Visual Language Models","summary":"  This paper explores preference distillation for large vision language models\n(LVLMs), improving their ability to generate helpful and faithful responses\nanchoring the visual context. We first build a vision-language feedback\n(VLFeedback) dataset utilizing AI annotation. Specifically, responses are\ngenerated by models sampled from 12 LVLMs, conditioned on multi-modal\ninstructions sourced from various datasets. We adopt GPT-4V to assess the\ngenerated outputs regarding helpfulness, visual faithfulness, and ethical\nconsiderations. Furthermore, the preference supervision is distilled into\nQwen-VL-Chat through the direct preference optimization (DPO) method. The\nresulting model Silkie, achieves 6.9% and 9.5% relative improvement on the MME\nbenchmark regarding the perception and cognition capabilities, respectively.\nSilkie also demonstrates reduced hallucination by setting a new\nstate-of-the-art score of 3.02 on the MMHal-Bench benchmark. Further analysis\nshows that DPO with our VLFeedback dataset mainly boosts the fine-grained\nperception and complex cognition abilities of LVLMs, leading to more\ncomprehensive improvements compared to human-annotated preference datasets.\n","authors":["Lei Li","Zhihui Xie","Mukai Li","Shunian Chen","Peiyi Wang","Liang Chen","Yazheng Yang","Benyou Wang","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2312.10665v1.pdf","comment":"Project page: https://vlf-silkie.github.io"},{"id":"http://arxiv.org/abs/2310.14805v2","updated":"2023-12-17T09:40:52Z","published":"2023-10-23T11:00:19Z","title":"Cross-Modal Conceptualization in Bottleneck Models","summary":"  Concept Bottleneck Models (CBMs) assume that training examples (e.g., x-ray\nimages) are annotated with high-level concepts (e.g., types of abnormalities),\nand perform classification by first predicting the concepts, followed by\npredicting the label relying on these concepts. The main difficulty in using\nCBMs comes from having to choose concepts that are predictive of the label and\nthen having to label training examples with these concepts. In our approach, we\nadopt a more moderate assumption and instead use text descriptions (e.g.,\nradiology reports), accompanying the images in training, to guide the induction\nof concepts. Our cross-modal approach treats concepts as discrete latent\nvariables and promotes concepts that (1) are predictive of the label, and (2)\ncan be predicted reliably from both the image and text. Through experiments\nconducted on datasets ranging from synthetic datasets (e.g., synthetic images\nwith generated descriptions) to realistic medical imaging datasets, we\ndemonstrate that cross-modal learning encourages the induction of interpretable\nconcepts while also facilitating disentanglement. Our results also suggest that\nthis guidance leads to increased robustness by suppressing the reliance on\nshortcut features.\n","authors":["Danis Alukaev","Semen Kiselev","Ilya Pershin","Bulat Ibragimov","Vladimir Ivanov","Alexey Kornaev","Ivan Titov"],"pdf_url":"https://arxiv.org/pdf/2310.14805v2.pdf","comment":"Accepted at EMNLP 2023; camera-ready version"},{"id":"http://arxiv.org/abs/2312.07398v2","updated":"2023-12-17T09:39:05Z","published":"2023-12-12T16:14:43Z","title":"LLMEval: A Preliminary Study on How to Evaluate Large Language Models","summary":"  Recently, the evaluation of Large Language Models has emerged as a popular\narea of research. The three crucial questions for LLM evaluation are ``what,\nwhere, and how to evaluate''. However, the existing research mainly focuses on\nthe first two questions, which are basically what tasks to give the LLM during\ntesting and what kind of knowledge it should deal with. As for the third\nquestion, which is about what standards to use, the types of evaluators, how to\nscore, and how to rank, there hasn't been much discussion. In this paper, we\nanalyze evaluation methods by comparing various criteria with both manual and\nautomatic evaluation, utilizing onsite, crowd-sourcing, public annotators and\nGPT-4, with different scoring methods and ranking systems. We propose a new\ndataset, LLMEval and conduct evaluations on 20 LLMs. A total of 2,186\nindividuals participated, leading to the generation of 243,337 manual\nannotations and 57,511 automatic evaluation results. We perform comparisons and\nanalyses of different settings and conduct 10 conclusions that can provide some\ninsights for evaluating LLM in the future. The dataset and the results are\npublicly available at https://github.com/llmeval .\n","authors":["Yue Zhang","Ming Zhang","Haipeng Yuan","Shichun Liu","Yongyao Shi","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2312.07398v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10652v1","updated":"2023-12-17T08:52:05Z","published":"2023-12-17T08:52:05Z","title":"Explorers at #SMM4H 2023: Enhancing BERT for Health Applications through\n  Knowledge and Model Fusion","summary":"  An increasing number of individuals are willing to post states and opinions\nin social media, which has become a valuable data resource for studying human\nhealth. Furthermore, social media has been a crucial research point for\nhealthcare now. This paper outlines the methods in our participation in the\n#SMM4H 2023 Shared Tasks, including data preprocessing, continual pre-training\nand fine-tuned optimization strategies. Especially for the Named Entity\nRecognition (NER) task, we utilize the model architecture named W2NER that\neffectively enhances the model generalization ability. Our method achieved\nfirst place in the Task 3. This paper has been peer-reviewed and accepted for\npresentation at the #SMM4H 2023 Workshop.\n","authors":["Xutong Yue","Xilai Wang","Yuxin He","Zhenkun Zhou"],"pdf_url":"https://arxiv.org/pdf/2312.10652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10645v1","updated":"2023-12-17T08:09:27Z","published":"2023-12-17T08:09:27Z","title":"FedMKGC: Privacy-Preserving Federated Multilingual Knowledge Graph\n  Completion","summary":"  Knowledge graph completion (KGC) aims to predict missing facts in knowledge\ngraphs (KGs), which is crucial as modern KGs remain largely incomplete. While\ntraining KGC models on multiple aligned KGs can improve performance, previous\nmethods that rely on transferring raw data among KGs raise privacy concerns. To\naddress this challenge, we propose a new federated learning framework that\nimplicitly aggregates knowledge from multiple KGs without demanding raw data\nexchange and entity alignment. We treat each KG as a client that trains a local\nlanguage model through textbased knowledge representation learning. A central\nserver then aggregates the model weights from clients. As natural language\nprovides a universal representation, the same knowledge thus has similar\nsemantic representations across KGs. As such, the aggregated language model can\nleverage complementary knowledge from multilingual KGs without demanding raw\nuser data sharing. Extensive experiments on a benchmark dataset demonstrate\nthat our method substantially improves KGC on multilingual KGs, achieving\ncomparable performance to state-of-the-art alignment-based models without\nrequiring any labeled alignments or raw user data sharing. Our codes will be\npublicly available.\n","authors":["Wei Tang","Zhiqian Wu","Yixin Cao","Yong Liao","Pengyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2312.10645v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10638v1","updated":"2023-12-17T07:39:07Z","published":"2023-12-17T07:39:07Z","title":"HyperPIE: Hyperparameter Information Extraction from Scientific\n  Publications","summary":"  Automatic extraction of information from publications is key to making\nscientific knowledge machine readable at a large scale. The extracted\ninformation can, for example, facilitate academic search, decision making, and\nknowledge graph construction. An important type of information not covered by\nexisting approaches is hyperparameters. In this paper, we formalize and tackle\nhyperparameter information extraction (HyperPIE) as an entity recognition and\nrelation extraction task. We create a labeled data set covering publications\nfrom a variety of computer science disciplines. Using this data set, we train\nand evaluate BERT-based fine-tuned models as well as five large language\nmodels: GPT-3.5, GALACTICA, Falcon, Vicuna, and WizardLM. For fine-tuned\nmodels, we develop a relation extraction approach that achieves an improvement\nof 29% F1 over a state-of-the-art baseline. For large language models, we\ndevelop an approach leveraging YAML output for structured data extraction,\nwhich achieves an average improvement of 5.5% F1 in entity recognition over\nusing JSON. With our best performing model we extract hyperparameter\ninformation from a large number of unannotated papers, and analyze patterns\nacross disciplines. All our data and source code is publicly available at\nhttps://github.com/IllDepence/hyperpie\n","authors":["Tarek Saier","Mayumi Ohta","Takuto Asakura","Michael Färber"],"pdf_url":"https://arxiv.org/pdf/2312.10638v1.pdf","comment":"accepted at ECIR2024"},{"id":"http://arxiv.org/abs/2308.11730v2","updated":"2023-12-17T07:21:57Z","published":"2023-08-22T18:41:31Z","title":"Knowledge Graph Prompting for Multi-Document Question Answering","summary":"  The `pre-train, prompt, predict' paradigm of large language models (LLMs) has\nachieved remarkable success in open-domain question answering (OD-QA). However,\nfew works explore this paradigm in the scenario of multi-document question\nanswering (MD-QA), a task demanding a thorough understanding of the logical\nassociations among the contents and structures of different documents. To fill\nthis crucial gap, we propose a Knowledge Graph Prompting (KGP) method to\nformulate the right context in prompting LLMs for MD-QA, which consists of a\ngraph construction module and a graph traversal module. For graph construction,\nwe create a knowledge graph (KG) over multiple documents with nodes symbolizing\npassages or document structures (e.g., pages/tables), and edges denoting the\nsemantic/lexical similarity between passages or intra-document structural\nrelations. For graph traversal, we design an LLM-based graph traversal agent\nthat navigates across nodes and gathers supporting passages assisting LLMs in\nMD-QA. The constructed graph serves as the global ruler that regulates the\ntransitional space among passages and reduces retrieval latency. Concurrently,\nthe graph traversal agent acts as a local navigator that gathers pertinent\ncontext to progressively approach the question and guarantee retrieval quality.\nExtensive experiments underscore the efficacy of KGP for MD-QA, signifying the\npotential of leveraging graphs in enhancing the prompt design for LLMs. Our\ncode: https://github.com/YuWVandy/KG-LLM-MDQA.\n","authors":["Yu Wang","Nedim Lipka","Ryan A. Rossi","Alexa Siu","Ruiyi Zhang","Tyler Derr"],"pdf_url":"https://arxiv.org/pdf/2308.11730v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10626v1","updated":"2023-12-17T06:55:04Z","published":"2023-12-17T06:55:04Z","title":"Decoding Concerns: Multi-label Classification of Vaccine Sentiments in\n  Social Media","summary":"  In the realm of public health, vaccination stands as the cornerstone for\nmitigating disease risks and controlling their proliferation. The recent\nCOVID-19 pandemic has highlighted how vaccines play a crucial role in keeping\nus safe. However the situation involves a mix of perspectives, with skepticism\ntowards vaccines prevailing for various reasons such as political dynamics,\napprehensions about side effects, and more. The paper addresses the challenge\nof comprehensively understanding and categorizing these diverse concerns\nexpressed in the context of vaccination. Our focus is on developing a robust\nmulti-label classifier capable of assigning specific concern labels to tweets\nbased on the articulated apprehensions towards vaccines. To achieve this, we\ndelve into the application of a diverse set of advanced natural language\nprocessing techniques and machine learning algorithms including transformer\nmodels like BERT, state of the art GPT 3.5, Classifier Chains & traditional\nmethods like SVM, Random Forest, Naive Bayes. We see that the cutting-edge\nlarge language model outperforms all other methods in this context.\n","authors":["Somsubhra De","Shaurya Vats"],"pdf_url":"https://arxiv.org/pdf/2312.10626v1.pdf","comment":"13 pages, Submitted to the AISoMe Track at FIRE 2023"},{"id":"http://arxiv.org/abs/2312.10617v1","updated":"2023-12-17T06:03:33Z","published":"2023-12-17T06:03:33Z","title":"Deep dive into language traits of AI-generated Abstracts","summary":"  Generative language models, such as ChatGPT, have garnered attention for\ntheir ability to generate human-like writing in various fields, including\nacademic research. The rapid proliferation of generated texts has bolstered the\nneed for automatic identification to uphold transparency and trust in the\ninformation. However, these generated texts closely resemble human writing and\noften have subtle differences in the grammatical structure, tones, and\npatterns, which makes systematic scrutinization challenging. In this work, we\nattempt to detect the Abstracts generated by ChatGPT, which are much shorter in\nlength and bounded. We extract the texts semantic and lexical properties and\nobserve that traditional machine learning models can confidently detect these\nAbstracts.\n","authors":["Vikas Kumar","Amisha Bharti","Devanshu Verma","Vasudha Bhatnagar"],"pdf_url":"https://arxiv.org/pdf/2312.10617v1.pdf","comment":"Accepted for Cods-Comad Conference"},{"id":"http://arxiv.org/abs/2308.11940v3","updated":"2023-12-17T06:01:27Z","published":"2023-08-23T06:21:46Z","title":"Audio Generation with Multiple Conditional Diffusion Model","summary":"  Text-based audio generation models have limitations as they cannot encompass\nall the information in audio, leading to restricted controllability when\nrelying solely on text. To address this issue, we propose a novel model that\nenhances the controllability of existing pre-trained text-to-audio models by\nincorporating additional conditions including content (timestamp) and style\n(pitch contour and energy contour) as supplements to the text. This approach\nachieves fine-grained control over the temporal order, pitch, and energy of\ngenerated audio. To preserve the diversity of generation, we employ a trainable\ncontrol condition encoder that is enhanced by a large language model and a\ntrainable Fusion-Net to encode and fuse the additional conditions while keeping\nthe weights of the pre-trained text-to-audio model frozen. Due to the lack of\nsuitable datasets and evaluation metrics, we consolidate existing datasets into\na new dataset comprising the audio and corresponding conditions and use a\nseries of evaluation metrics to evaluate the controllability performance.\nExperimental results demonstrate that our model successfully achieves\nfine-grained control to accomplish controllable audio generation. Audio samples\nand our dataset are publicly available at\nhttps://conditionaudiogen.github.io/conditionaudiogen/\n","authors":["Zhifang Guo","Jianguo Mao","Rui Tao","Long Yan","Kazushige Ouchi","Hong Liu","Xiangdong Wang"],"pdf_url":"https://arxiv.org/pdf/2308.11940v3.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.10610v1","updated":"2023-12-17T05:13:58Z","published":"2023-12-17T05:13:58Z","title":"Do LLMs Work on Charts? Designing Few-Shot Prompts for Chart Question\n  Answering and Summarization","summary":"  A number of tasks have been proposed recently to facilitate easy access to\ncharts such as chart QA and summarization. The dominant paradigm to solve these\ntasks has been to fine-tune a pretrained model on the task data. However, this\napproach is not only expensive but also not generalizable to unseen tasks. On\nthe other hand, large language models (LLMs) have shown impressive\ngeneralization capabilities to unseen tasks with zero- or few-shot prompting.\nHowever, their application to chart-related tasks is not trivial as these tasks\ntypically involve considering not only the underlying data but also the visual\nfeatures in the chart image. We propose PromptChart, a multimodal few-shot\nprompting framework with LLMs for chart-related applications. By analyzing the\ntasks carefully, we have come up with a set of prompting guidelines for each\ntask to elicit the best few-shot performance from LLMs. We further propose a\nstrategy to inject visual information into the prompts. Our experiments on\nthree different chart-related information consumption tasks show that with\nproperly designed prompts LLMs can excel on the benchmarks, achieving\nstate-of-the-art.\n","authors":["Xuan Long Do","Mohammad Hassanpour","Ahmed Masry","Parsa Kavehzadeh","Enamul Hoque","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2312.10610v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2312.08926v2","updated":"2023-12-17T03:34:36Z","published":"2023-12-14T13:33:50Z","title":"Modeling Complex Mathematical Reasoning via Large Language Model based\n  MathAgent","summary":"  Large language models (LLMs) face challenges in solving complex mathematical\nproblems that require comprehensive capacities to parse the statements,\nassociate domain knowledge, perform compound logical reasoning, and integrate\nthe intermediate rationales. Tackling all these problems once could be arduous\nfor LLMs, thus leading to confusion in generation. In this work, we explore the\npotential of enhancing LLMs with agents by meticulous decomposition and\nmodeling of mathematical reasoning process. Specifically, we propose a formal\ndescription of the mathematical solving and extend LLMs with an agent-based\nzero-shot framework named\n$\\bf{P}$lanner-$\\bf{R}$easoner-$\\bf{E}$xecutor-$\\bf{R}$eflector (PRER). We\nfurther provide and implement two MathAgents that define the logical forms and\ninherent relations via a pool of actions in different grains and orientations:\nMathAgent-M adapts its actions to LLMs, while MathAgent-H aligns with\nhumankind. Experiments on miniF2F and MATH have demonstrated the effectiveness\nof PRER and proposed MathAgents, achieving an increase of\n$12.3\\%$($53.9\\%\\xrightarrow{}66.2\\%$) on the MiniF2F, $9.2\\%$\n($49.8\\%\\xrightarrow{}59.0\\%$) on MATH, and\n$13.2\\%$($23.2\\%\\xrightarrow{}35.4\\%$) for level-5 problems of MATH against\nGPT-4. Further analytical results provide more insightful perspectives on\nexploiting the behaviors of LLMs as agents.\n","authors":["Haoran Liao","Qinyi Du","Shaohua Hu","Hao He","Yanyan Xu","Jidong Tian","Yaohui Jin"],"pdf_url":"https://arxiv.org/pdf/2312.08926v2.pdf","comment":"There are unfair comparisons on miniF2F. This will be fixed in the\n  future"},{"id":"http://arxiv.org/abs/2305.13071v2","updated":"2023-12-17T03:20:13Z","published":"2023-05-22T14:41:09Z","title":"Machine-Created Universal Language for Cross-lingual Transfer","summary":"  There are two primary approaches to addressing cross-lingual transfer:\nmultilingual pre-training, which implicitly aligns the hidden representations\nof various languages, and translate-test, which explicitly translates different\nlanguages into an intermediate language, such as English. Translate-test offers\nbetter interpretability compared to multilingual pre-training. However, it has\nlower performance than multilingual pre-training(Conneau and Lample, 2019;\nConneau et al, 2020) and struggles with word-level tasks due to translation\naltering word order. As a result, we propose a new Machine-created Universal\nLanguage (MUL) as an alternative intermediate language. MUL comprises a set of\ndiscrete symbols forming a universal vocabulary and a natural language to MUL\ntranslator for converting multiple natural languages to MUL. MUL unifies shared\nconcepts from various languages into a single universal word, enhancing\ncross-language transfer. Additionally, MUL retains language-specific words and\nword order, allowing the model to be easily applied to word-level tasks. Our\nexperiments demonstrate that translating into MUL yields improved performance\ncompared to multilingual pre-training, and our analysis indicates that MUL\npossesses strong interpretability. The code is at:\nhttps://github.com/microsoft/Unicoder/tree/master/MCUL.\n","authors":["Yaobo Liang","Quanzhi Zhu","Junhe Zhao","Nan Duan"],"pdf_url":"https://arxiv.org/pdf/2305.13071v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.05091v2","updated":"2023-12-17T02:03:29Z","published":"2023-05-08T23:31:39Z","title":"Knowledge-enhanced Agents for Interactive Text Games","summary":"  Communication via natural language is a key aspect of machine intelligence,\nand it requires computational models to learn and reason about world concepts,\nwith varying levels of supervision. Significant progress has been made on\nfully-supervised non-interactive tasks, such as question-answering and\nprocedural text understanding. Yet, various sequential interactive tasks, as in\ntext-based games, have revealed limitations of existing approaches in terms of\ncoherence, contextual awareness, and their ability to learn effectively from\nthe environment. In this paper, we propose a knowledge-injection framework for\nimproved functional grounding of agents in text-based games. Specifically, we\nconsider two forms of domain knowledge that we inject into learning-based\nagents: memory of previous correct actions and affordances of relevant objects\nin the environment. Our framework supports two representative model classes:\nreinforcement learning agents and language model agents. Furthermore, we devise\nmultiple injection strategies for the above domain knowledge types and agent\narchitectures, including injection via knowledge graphs and augmentation of the\nexisting input encoding strategies. We experiment with four models on the 10\ntasks in the ScienceWorld text-based game environment, to illustrate the impact\nof knowledge injection on various model configurations and challenging task\nsettings. Our findings provide crucial insights into the interplay between task\nproperties, model architectures, and domain knowledge for interactive contexts.\n","authors":["Prateek Chhikara","Jiarui Zhang","Filip Ilievski","Jonathan Francis","Kaixin Ma"],"pdf_url":"https://arxiv.org/pdf/2305.05091v2.pdf","comment":"Published at K-CAP '23"},{"id":"http://arxiv.org/abs/2312.10580v1","updated":"2023-12-17T01:50:27Z","published":"2023-12-17T01:50:27Z","title":"Sentiment Analysis and Text Analysis of the Public Discourse on Twitter\n  about COVID-19 and MPox","summary":"  Mining and analysis of the big data of Twitter conversations have been of\nsignificant interest to the scientific community in the fields of healthcare,\nepidemiology, big data, data science, computer science, and their related\nareas, as can be seen from several works in the last few years that focused on\nsentiment analysis and other forms of text analysis of tweets related to Ebola,\nE-Coli, Dengue, Human Papillomavirus, Middle East Respiratory Syndrome,\nMeasles, Zika virus, H1N1, influenza like illness, swine flu, flu, Cholera,\nListeriosis, cancer, Liver Disease, Inflammatory Bowel Disease, kidney disease,\nlupus, Parkinsons, Diphtheria, and West Nile virus. The recent outbreaks of\nCOVID-19 and MPox have served as catalysts for Twitter usage related to seeking\nand sharing information, views, opinions, and sentiments involving both of\nthese viruses. None of the prior works in this field analyzed tweets focusing\non both COVID-19 and MPox simultaneously. To address this research gap, a total\nof 61,862 tweets that focused on MPox and COVID-19 simultaneously, posted\nbetween 7 May 2022 and 3 March 2023, were studied. The findings and\ncontributions of this study are manifold. First, the results of sentiment\nanalysis using the VADER approach show that nearly half the tweets had a\nnegative sentiment. It was followed by tweets that had a positive sentiment and\ntweets that had a neutral sentiment, respectively. Second, this paper presents\nthe top 50 hashtags used in these tweets. Third, it presents the top 100 most\nfrequently used words in these tweets after performing tokenization, removal of\nstopwords, and word frequency analysis. Finally, a comprehensive comparative\nstudy that compares the contributions of this paper with 49 prior works in this\nfield is presented to further uphold the relevance and novelty of this work.\n","authors":["Nirmalya Thakur"],"pdf_url":"https://arxiv.org/pdf/2312.10580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10579v1","updated":"2023-12-17T01:49:40Z","published":"2023-12-17T01:49:40Z","title":"DER-GCN: Dialogue and Event Relation-Aware Graph Convolutional Neural\n  Network for Multimodal Dialogue Emotion Recognition","summary":"  With the continuous development of deep learning (DL), the task of multimodal\ndialogue emotion recognition (MDER) has recently received extensive research\nattention, which is also an essential branch of DL. The MDER aims to identify\nthe emotional information contained in different modalities, e.g., text, video,\nand audio, in different dialogue scenes. However, existing research has focused\non modeling contextual semantic information and dialogue relations between\nspeakers while ignoring the impact of event relations on emotion. To tackle the\nabove issues, we propose a novel Dialogue and Event Relation-Aware Graph\nConvolutional Neural Network for Multimodal Emotion Recognition (DER-GCN)\nmethod. It models dialogue relations between speakers and captures latent event\nrelations information. Specifically, we construct a weighted multi-relationship\ngraph to simultaneously capture the dependencies between speakers and event\nrelations in a dialogue. Moreover, we also introduce a Self-Supervised Masked\nGraph Autoencoder (SMGAE) to improve the fusion representation ability of\nfeatures and structures. Next, we design a new Multiple Information Transformer\n(MIT) to capture the correlation between different relations, which can provide\na better fuse of the multivariate information between relations. Finally, we\npropose a loss optimization strategy based on contrastive learning to enhance\nthe representation learning ability of minority class features. We conduct\nextensive experiments on the IEMOCAP and MELD benchmark datasets, which verify\nthe effectiveness of the DER-GCN model. The results demonstrate that our model\nsignificantly improves both the average accuracy and the f1 value of emotion\nrecognition.\n","authors":["Wei Ai","Yuntao Shou","Tao Meng","Keqin Li"],"pdf_url":"https://arxiv.org/pdf/2312.10579v1.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2310.03018v2","updated":"2023-12-17T01:49:18Z","published":"2023-10-04T17:58:11Z","title":"Zero Resource Code-switched Speech Benchmark Using Speech Utterance\n  Pairs For Multiple Spoken Languages","summary":"  We introduce a new zero resource code-switched speech benchmark designed to\ndirectly assess the code-switching capabilities of self-supervised speech\nencoders. We showcase a baseline system of language modeling on discrete units\nto demonstrate how the code-switching abilities of speech encoders can be\nassessed in a zero-resource manner. Our experiments encompass a variety of\nwell-known speech encoders, including Wav2vec 2.0, HuBERT, XLSR, etc. We\nexamine the impact of pre-training languages and model size on benchmark\nperformance. Notably, though our results demonstrate that speech encoders with\nmultilingual pre-training, exemplified by XLSR, outperform monolingual variants\n(Wav2vec 2.0, HuBERT) in code-switching scenarios, there is still substantial\nroom for improvement in their code-switching linguistic abilities.\n","authors":["Kuan-Po Huang","Chih-Kai Yang","Yu-Kuan Fu","Ewan Dunbar","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2310.03018v2.pdf","comment":"Accepted by ICASSP 2024 (v2)"},{"id":"http://arxiv.org/abs/2303.01261v3","updated":"2023-12-17T00:06:16Z","published":"2023-03-01T17:23:12Z","title":"ParrotTTS: Text-to-Speech synthesis by exploiting self-supervised\n  representations","summary":"  We present ParrotTTS, a modularized text-to-speech synthesis model leveraging\ndisentangled self-supervised speech representations. It can train a\nmulti-speaker variant effectively using transcripts from a single speaker.\nParrotTTS adapts to a new language in low resource setup and generalizes to\nlanguages not seen while training the self-supervised backbone. Moreover,\nwithout training on bilingual or parallel examples, ParrotTTS can transfer\nvoices across languages while preserving the speaker specific characteristics,\ne.g., synthesizing fluent Hindi speech using a French speaker's voice and\naccent. We present extensive results in monolingual and multi-lingual\nscenarios. ParrotTTS outperforms state-of-the-art multi-lingual TTS models\nusing only a fraction of paired data as latter.\n","authors":["Neil Shah","Saiteja Kosgi","Vishal Tambrahalli","Neha Sahipjohn","Niranjan Pedanekar","Vineet Gandhi"],"pdf_url":"https://arxiv.org/pdf/2303.01261v3.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2312.10843v1","updated":"2023-12-17T23:22:37Z","published":"2023-12-17T23:22:37Z","title":"High-Fidelity Face Swapping with Style Blending","summary":"  Face swapping has gained significant traction, driven by the plethora of\nhuman face synthesis facilitated by deep learning methods. However, previous\nface swapping methods that used generative adversarial networks (GANs) as\nbackbones have faced challenges such as inconsistency in blending, distortions,\nartifacts, and issues with training stability. To address these limitations, we\npropose an innovative end-to-end framework for high-fidelity face swapping.\nFirst, we introduce a StyleGAN-based facial attributes encoder that extracts\nessential features from faces and inverts them into a latent style code,\nencapsulating indispensable facial attributes for successful face swapping.\nSecond, we introduce an attention-based style blending module to effectively\ntransfer Face IDs from source to target. To ensure accurate and quality\ntransferring, a series of constraint measures including contrastive face ID\nlearning, facial landmark alignment, and dual swap consistency is implemented.\nFinally, the blended style code is translated back to the image space via the\nstyle decoder, which is of high training stability and generative capability.\nExtensive experiments on the CelebA-HQ dataset highlight the superior visual\nquality of generated images from our face-swapping methodology when compared to\nother state-of-the-art methods, and the effectiveness of each proposed module.\nSource code and weights will be publicly available.\n","authors":["Xinyu Yang","Hongbo Bo"],"pdf_url":"https://arxiv.org/pdf/2312.10843v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2312.10835v1","updated":"2023-12-17T22:40:38Z","published":"2023-12-17T22:40:38Z","title":"Your Student is Better Than Expected: Adaptive Teacher-Student\n  Collaboration for Text-Conditional Diffusion Models","summary":"  Knowledge distillation methods have recently shown to be a promising\ndirection to speedup the synthesis of large-scale diffusion models by requiring\nonly a few inference steps. While several powerful distillation methods were\nrecently proposed, the overall quality of student samples is typically lower\ncompared to the teacher ones, which hinders their practical usage. In this\nwork, we investigate the relative quality of samples produced by the teacher\ntext-to-image diffusion model and its distilled student version. As our main\nempirical finding, we discover that a noticeable portion of student samples\nexhibit superior fidelity compared to the teacher ones, despite the\n``approximate'' nature of the student. Based on this finding, we propose an\nadaptive collaboration between student and teacher diffusion models for\neffective text-to-image synthesis. Specifically, the distilled model produces\nthe initial sample, and then an oracle decides whether it needs further\nimprovements with a slow teacher model. Extensive experiments demonstrate that\nthe designed pipeline surpasses state-of-the-art text-to-image alternatives for\nvarious inference budgets in terms of human preference. Furthermore, the\nproposed approach can be naturally used in popular applications such as\ntext-guided image editing and controllable generation.\n","authors":["Nikita Starodubcev","Artem Fedorov","Artem Babenko","Dmitry Baranchuk"],"pdf_url":"https://arxiv.org/pdf/2312.10835v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12813v2","updated":"2023-12-17T22:12:37Z","published":"2023-09-22T13:02:14Z","title":"Targeted Activation Penalties Help CNNs Ignore Spurious Signals","summary":"  Neural networks (NNs) can learn to rely on spurious signals in the training\ndata, leading to poor generalisation. Recent methods tackle this problem by\ntraining NNs with additional ground-truth annotations of such signals. These\nmethods may, however, let spurious signals re-emerge in deep convolutional NNs\n(CNNs). We propose Targeted Activation Penalty (TAP), a new method tackling the\nsame problem by penalising activations to control the re-emergence of spurious\nsignals in deep CNNs, while also lowering training times and memory usage. In\naddition, ground-truth annotations can be expensive to obtain. We show that TAP\nstill works well with annotations generated by pre-trained models as effective\nsubstitutes of ground-truth annotations. We demonstrate the power of TAP\nagainst two state-of-the-art baselines on the MNIST benchmark and on two\nclinical image datasets, using four different CNN architectures.\n","authors":["Dekai Zhang","Matthew Williams","Francesca Toni"],"pdf_url":"https://arxiv.org/pdf/2311.12813v2.pdf","comment":"24 pages including appendix; extended version of a paper accepted to\n  AAAI-2024 under the same title"},{"id":"http://arxiv.org/abs/2308.02669v2","updated":"2023-12-17T22:04:14Z","published":"2023-08-03T17:04:41Z","title":"ConceptLab: Creative Concept Generation using VLM-Guided Diffusion Prior\n  Constraints","summary":"  Recent text-to-image generative models have enabled us to transform our words\ninto vibrant, captivating imagery. The surge of personalization techniques that\nhas followed has also allowed us to imagine unique concepts in new scenes.\nHowever, an intriguing question remains: How can we generate a new, imaginary\nconcept that has never been seen before? In this paper, we present the task of\ncreative text-to-image generation, where we seek to generate new members of a\nbroad category (e.g., generating a pet that differs from all existing pets). We\nleverage the under-studied Diffusion Prior models and show that the creative\ngeneration problem can be formulated as an optimization process over the output\nspace of the diffusion prior, resulting in a set of \"prior constraints\". To\nkeep our generated concept from converging into existing members, we\nincorporate a question-answering Vision-Language Model (VLM) that adaptively\nadds new constraints to the optimization problem, encouraging the model to\ndiscover increasingly more unique creations. Finally, we show that our prior\nconstraints can also serve as a strong mixing mechanism allowing us to create\nhybrids between generated concepts, introducing even more flexibility into the\ncreative process.\n","authors":["Elad Richardson","Kfir Goldberg","Yuval Alaluf","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2308.02669v2.pdf","comment":"Project page: https://kfirgoldberg.github.io/ConceptLab/"},{"id":"http://arxiv.org/abs/2312.10825v1","updated":"2023-12-17T21:49:59Z","published":"2023-12-17T21:49:59Z","title":"Latent Space Editing in Transformer-Based Flow Matching","summary":"  This paper strives for image editing via generative models. Flow Matching is\nan emerging generative modeling technique that offers the advantage of simple\nand efficient training. Simultaneously, a new transformer-based U-ViT has\nrecently been proposed to replace the commonly used UNet for better scalability\nand performance in generative modeling. Hence, Flow Matching with a transformer\nbackbone offers the potential for scalable and high-quality generative\nmodeling, but their latent structure and editing ability are as of yet unknown.\nHence, we adopt this setting and explore how to edit images through latent\nspace manipulation. We introduce an editing space, which we call $u$-space,\nthat can be manipulated in a controllable, accumulative, and composable manner.\nAdditionally, we propose a tailored sampling solution to enable sampling with\nthe more efficient adaptive step-size ODE solvers. Lastly, we put forth a\nstraightforward yet powerful method for achieving fine-grained and nuanced\nediting using text prompts. Our framework is simple and efficient, all while\nbeing highly effective at editing images while preserving the essence of the\noriginal content. Our code will be publicly available at https://taohu.me/lfm/\n","authors":["Vincent Tao Hu","David W Zhang","Pascal Mettes","Meng Tang","Deli Zhao","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2312.10825v1.pdf","comment":"AAAI 2024 with Appendix"},{"id":"http://arxiv.org/abs/2312.10818v1","updated":"2023-12-17T21:31:35Z","published":"2023-12-17T21:31:35Z","title":"Facial Emotion Recognition using CNN in PyTorch","summary":"  In this project, we have implemented a model to recognize real-time facial\nemotions given the camera images. Current approaches would read all data and\ninput it into their model, which has high space complexity. Our model is based\non the Convolutional Neural Network utilizing the PyTorch library. We believe\nour implementation will significantly improve the space complexity and provide\na useful contribution to facial emotion recognition. Our motivation is to\nunderstanding clearly about deep learning, particularly in CNNs, and analysis\nreal-life scenarios. Therefore, we tunned the hyper parameter of model such as\nlearning rate, batch size, and number of epochs to meet our needs. In addition,\nwe also used techniques to optimize the networks, such as activation function,\ndropout and max pooling. Finally, we analyzed the result from two optimizer to\nobserve the relationship between number of epochs and accuracy.\n","authors":["Deyuan Qu","Sudip Dhakal","Dominic Carrillo"],"pdf_url":"https://arxiv.org/pdf/2312.10818v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10813v1","updated":"2023-12-17T20:42:43Z","published":"2023-12-17T20:42:43Z","title":"Re-parameterized Low-rank Prompt: Generalize a Vision-Language Model\n  within 0.5K Parameters","summary":"  With the development of large pre-trained vision-language models, how to\neffectively transfer the knowledge of such foundational models to downstream\ntasks becomes a hot topic, especially in a data-deficient scenario. Recently,\nprompt tuning has become a popular solution. When adapting the vision-language\nmodels, researchers freeze the parameters in the backbone and only design and\ntune the prompts. On the one hand, the delicate design of prompt tuning\nexhibits strong performance. On the other hand, complicated structures and\nupdate rules largely increase the computation and storage cost. Motivated by\nthe observation that the evolution pattern of the generalization capability in\nvisual-language models aligns harmoniously with the trend of rank variations in\nthe prompt matrix during adaptation, we design a new type of prompt,\nRe-parameterized Low-rank Prompt (RLP), for both efficient and effective\nadaptation. Our method could largely reduce the number of tunable parameters\nand storage space, which is quite beneficial in resource-limited scenarios.\nExtensive experiments further demonstrate the superiority of RLP. In\nparticular, RLP shows comparable or even stronger performance than the latest\nstate-of-the-art methods with an extremely small number of parameters. On a\nseries of tasks over 11 datasets, RLP significantly increases the average\ndownstream accuracy of classic prompt tuning by up to 5.25% using merely 0.5K\nparameters.\n","authors":["Tianxiang Hao","Mengyao Lyu","Hui Chen","Sicheng Zhao","Jungong Han","Guiguang Ding"],"pdf_url":"https://arxiv.org/pdf/2312.10813v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10806v1","updated":"2023-12-17T20:12:42Z","published":"2023-12-17T20:12:42Z","title":"Cross-Lingual Learning in Multilingual Scene Text Recognition","summary":"  In this paper, we investigate cross-lingual learning (CLL) for multilingual\nscene text recognition (STR). CLL transfers knowledge from one language to\nanother. We aim to find the condition that exploits knowledge from\nhigh-resource languages for improving performance in low-resource languages. To\ndo so, we first examine if two general insights about CLL discussed in previous\nworks are applied to multilingual STR: (1) Joint learning with high- and\nlow-resource languages may reduce performance on low-resource languages, and\n(2) CLL works best between typologically similar languages. Through extensive\nexperiments, we show that two general insights may not be applied to\nmultilingual STR. After that, we show that the crucial condition for CLL is the\ndataset size of high-resource languages regardless of the kind of high-resource\nlanguages. Our code, data, and models are available at\nhttps://github.com/ku21fan/CLL-STR.\n","authors":["Jeonghun Baek","Yusuke Matsui","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2312.10806v1.pdf","comment":"Accepted at ICASSP2024, 5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2312.10798v1","updated":"2023-12-17T19:22:39Z","published":"2023-12-17T19:22:39Z","title":"Land use/land cover classification of fused Sentinel-1 and Sentinel-2\n  imageries using ensembles of Random Forests","summary":"  The study explores the synergistic combination of Synthetic Aperture Radar\n(SAR) and Visible-Near Infrared-Short Wave Infrared (VNIR-SWIR) imageries for\nland use/land cover (LULC) classification. Image fusion, employing Bayesian\nfusion, merges SAR texture bands with VNIR-SWIR imageries. The research aims to\ninvestigate the impact of this fusion on LULC classification. Despite the\npopularity of random forests for supervised classification, their limitations,\nsuch as suboptimal performance with fewer features and accuracy stagnation, are\naddressed. To overcome these issues, ensembles of random forests (RFE) are\ncreated, introducing random rotations using the Forest-RC algorithm. Three\nrotation approaches: principal component analysis (PCA), sparse random rotation\n(SRP) matrix, and complete random rotation (CRP) matrix are employed.\nSentinel-1 SAR data and Sentinel-2 VNIR-SWIR data from the IIT-Kanpur region\nconstitute the training datasets, including SAR, SAR with texture, VNIR-SWIR,\nVNIR-SWIR with texture, and fused VNIR-SWIR with texture. The study evaluates\nclassifier efficacy, explores the impact of SAR and VNIR-SWIR fusion on\nclassification, and significantly enhances the execution speed of Bayesian\nfusion code. The SRP-based RFE outperforms other ensembles for the first two\ndatasets, yielding average overall kappa values of 61.80% and 68.18%, while the\nCRP-based RFE excels for the last three datasets with average overall kappa\nvalues of 95.99%, 96.93%, and 96.30%. The fourth dataset achieves the highest\noverall kappa of 96.93%. Furthermore, incorporating texture with SAR bands\nresults in a maximum overall kappa increment of 10.00%, while adding texture to\nVNIR-SWIR bands yields a maximum increment of approximately 3.45%.\n","authors":["Shivam Pande"],"pdf_url":"https://arxiv.org/pdf/2312.10798v1.pdf","comment":"Thesis for Master of Technology. Created: July 2018. Total pages 123"},{"id":"http://arxiv.org/abs/2312.08746v2","updated":"2023-12-17T19:14:33Z","published":"2023-12-14T08:42:26Z","title":"DreamDrone","summary":"  We introduce DreamDrone, an innovative method for generating unbounded\nflythrough scenes from textual prompts. Central to our method is a novel\nfeature-correspondence-guidance diffusion process, which utilizes the strong\ncorrespondence of intermediate features in the diffusion model. Leveraging this\nguidance strategy, we further propose an advanced technique for editing the\nintermediate latent code, enabling the generation of subsequent novel views\nwith geometric consistency. Extensive experiments reveal that DreamDrone\nsignificantly surpasses existing methods, delivering highly authentic scene\ngeneration with exceptional visual quality. This approach marks a significant\nstep in zero-shot perpetual view generation from textual prompts, enabling the\ncreation of diverse scenes, including natural landscapes like oases and caves,\nas well as complex urban settings such as Lego-style street views. Our code is\npublicly available.\n","authors":["Hanyang Kong","Dongze Lian","Michael Bi Mi","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2312.08746v2.pdf","comment":"16 pages, 12 figures, project page:\n  https://hyokong.github.io/dreamdrone-page/"},{"id":"http://arxiv.org/abs/2312.06833v2","updated":"2023-12-17T18:38:15Z","published":"2023-12-11T20:54:59Z","title":"The unreasonable effectiveness of AI CADe polyp detectors to generalize\n  to new countries","summary":"  $\\textbf{Background and aims}$: Artificial Intelligence (AI) Computer-Aided\nDetection (CADe) is commonly used for polyp detection, but data seen in\nclinical settings can differ from model training. Few studies evaluate how well\nCADe detectors perform on colonoscopies from countries not seen during\ntraining, and none are able to evaluate performance without collecting\nexpensive and time-intensive labels.\n  $\\textbf{Methods}$: We trained a CADe polyp detector on Israeli colonoscopy\nvideos (5004 videos, 1106 hours) and evaluated on Japanese videos (354 videos,\n128 hours) by measuring the True Positive Rate (TPR) versus false alarms per\nminute (FAPM). We introduce a colonoscopy dissimilarity measure called \"MAsked\nmediCal Embedding Distance\" (MACE) to quantify differences between\ncolonoscopies, without labels. We evaluated CADe on all Japan videos and on\nthose with the highest MACE.\n  $\\textbf{Results}$: MACE correctly quantifies that narrow-band imaging (NBI)\nand chromoendoscopy (CE) frames are less similar to Israel data than Japan\nwhitelight (bootstrapped z-test, |z| > 690, p < $10^{-8}$ for both). Despite\ndifferences in the data, CADe performance on Japan colonoscopies was\nnon-inferior to Israel ones without additional training (TPR at 0.5 FAPM: 0.957\nand 0.972 for Israel and Japan; TPR at 1.0 FAPM: 0.972 and 0.989 for Israel and\nJapan; superiority test t > 45.2, p < $10^{-8}$). Despite not being trained on\nNBI or CE, TPR on those subsets were non-inferior to Japan overall\n(non-inferiority test t > 47.3, p < $10^{-8}$, $\\delta$ = 1.5% for both).\n  $\\textbf{Conclusion}$: Differences that prevent CADe detectors from\nperforming well in non-medical settings do not degrade the performance of our\nAI CADe polyp detector when applied to data from a new country. MACE can help\nmedical AI models internationalize by identifying the most \"dissimilar\" data on\nwhich to evaluate models.\n","authors":["Joel Shor","Hiro-o Yamano","Daisuke Tsurumaru","Yotami Intrator","Hiroki Kayama","Joe Ledsam","Atsushi Hamabe","Koji Ando","Mitsuhiko Ota","Haruei Ogino","Hiroshi Nakase","Kaho Kobayashi","Eiji Oki","Roman Goldenberg","Ehud Rivlin","Ichiro Takemasa"],"pdf_url":"https://arxiv.org/pdf/2312.06833v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.02270v2","updated":"2023-12-17T17:39:20Z","published":"2023-09-05T14:33:56Z","title":"SAM-Deblur: Let Segment Anything Boost Image Deblurring","summary":"  Image deblurring is a critical task in the field of image restoration, aiming\nto eliminate blurring artifacts. However, the challenge of addressing\nnon-uniform blurring leads to an ill-posed problem, which limits the\ngeneralization performance of existing deblurring models. To solve the problem,\nwe propose a framework SAM-Deblur, integrating prior knowledge from the Segment\nAnything Model (SAM) into the deblurring task for the first time. In\nparticular, SAM-Deblur is divided into three stages. First, we preprocess the\nblurred images, obtain segment masks via SAM, and propose a mask dropout method\nfor training to enhance model robustness. Then, to fully leverage the\nstructural priors generated by SAM, we propose a Mask Average Pooling (MAP)\nunit specifically designed to average SAM-generated segmented areas, serving as\na plug-and-play component which can be seamlessly integrated into existing\ndeblurring networks. Finally, we feed the fused features generated by the MAP\nUnit into the deblurring model to obtain a sharp image. Experimental results on\nthe RealBlurJ, ReloBlur, and REDS datasets reveal that incorporating our\nmethods improves GoPro-trained NAFNet's PSNR by 0.05, 0.96, and 7.03,\nrespectively. Project page is available at GitHub\n\\href{https://hplqaq.github.io/projects/sam-deblur}{HPLQAQ/SAM-Deblur}.\n","authors":["Siwei Li","Mingxuan Liu","Yating Zhang","Shu Chen","Haoxiang Li","Zifei Dou","Hong Chen"],"pdf_url":"https://arxiv.org/pdf/2309.02270v2.pdf","comment":"Accepted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2304.08796v2","updated":"2023-12-17T17:18:33Z","published":"2023-04-18T08:00:54Z","title":"Deep Unrestricted Document Image Rectification","summary":"  In recent years, tremendous efforts have been made on document image\nrectification, but existing advanced algorithms are limited to processing\nrestricted document images, i.e., the input images must incorporate a complete\ndocument. Once the captured image merely involves a local text region, its\nrectification quality is degraded and unsatisfactory. Our previously proposed\nDocTr, a transformer-assisted network for document image rectification, also\nsuffers from this limitation. In this work, we present DocTr++, a novel unified\nframework for document image rectification, without any restrictions on the\ninput distorted images. Our major technical improvements can be concluded in\nthree aspects. Firstly, we upgrade the original architecture by adopting a\nhierarchical encoder-decoder structure for multi-scale representation\nextraction and parsing. Secondly, we reformulate the pixel-wise mapping\nrelationship between the unrestricted distorted document images and the\ndistortion-free counterparts. The obtained data is used to train our DocTr++\nfor unrestricted document image rectification. Thirdly, we contribute a\nreal-world test set and metrics applicable for evaluating the rectification\nquality. To our best knowledge, this is the first learning-based method for the\nrectification of unrestricted document images. Extensive experiments are\nconducted, and the results demonstrate the effectiveness and superiority of our\nmethod. We hope our DocTr++ will serve as a strong baseline for generic\ndocument image rectification, prompting the further advancement and application\nof learning-based algorithms. The source code and the proposed dataset are\npublicly available at https://github.com/fh2019ustc/DocTr-Plus.\n","authors":["Hao Feng","Shaokai Liu","Jiajun Deng","Wengang Zhou","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2304.08796v2.pdf","comment":"Accepted by TMM 2023"},{"id":"http://arxiv.org/abs/2303.03932v2","updated":"2023-12-17T16:53:44Z","published":"2023-03-07T14:38:28Z","title":"FFT-based Dynamic Token Mixer for Vision","summary":"  Multi-head-self-attention (MHSA)-equipped models have achieved notable\nperformance in computer vision. Their computational complexity is proportional\nto quadratic numbers of pixels in input feature maps, resulting in slow\nprocessing, especially when dealing with high-resolution images. New types of\ntoken-mixer are proposed as an alternative to MHSA to circumvent this problem:\nan FFT-based token-mixer involves global operations similar to MHSA but with\nlower computational complexity. However, despite its attractive properties, the\nFFT-based token-mixer has not been carefully examined in terms of its\ncompatibility with the rapidly evolving MetaFormer architecture. Here, we\npropose a novel token-mixer called Dynamic Filter and novel image recognition\nmodels, DFFormer and CDFFormer, to close the gaps above. The results of image\nclassification and downstream tasks, analysis, and visualization show that our\nmodels are helpful. Notably, their throughput and memory efficiency when\ndealing with high-resolution image recognition is remarkable. Our results\nindicate that Dynamic Filter is one of the token-mixer options that should be\nseriously considered. The code is available at\nhttps://github.com/okojoalg/dfformer\n","authors":["Yuki Tatsunami","Masato Taki"],"pdf_url":"https://arxiv.org/pdf/2303.03932v2.pdf","comment":"The 38th Annual AAAI Conference on Artificial Intelligence (AAAI'24)"},{"id":"http://arxiv.org/abs/2312.10763v1","updated":"2023-12-17T16:53:30Z","published":"2023-12-17T16:53:30Z","title":"M3DBench: Let's Instruct Large Models with Multi-modal 3D Prompts","summary":"  Recently, 3D understanding has become popular to facilitate autonomous agents\nto perform further decisionmaking. However, existing 3D datasets and methods\nare often limited to specific tasks. On the other hand, recent progress in\nLarge Language Models (LLMs) and Multimodal Language Models (MLMs) have\ndemonstrated exceptional general language and imagery tasking performance.\nTherefore, it is interesting to unlock MLM's potential to be 3D generalist for\nwider tasks. However, current MLMs' research has been less focused on 3D tasks\ndue to a lack of large-scale 3D instruction-following datasets. In this work,\nwe introduce a comprehensive 3D instructionfollowing dataset called M3DBench,\nwhich possesses the following characteristics: 1) It supports general\nmultimodal instructions interleaved with text, images, 3D objects, and other\nvisual prompts. 2) It unifies diverse 3D tasks at both region and scene levels,\ncovering a variety of fundamental abilities in real-world 3D environments. 3)\nIt is a large-scale 3D instruction-following dataset with over 320k\ninstruction-response pairs. Furthermore, we establish a new benchmark for\nassessing the performance of large models in understanding multi-modal 3D\nprompts. Extensive experiments demonstrate the effectiveness of our dataset and\nbaseline, supporting general 3D-centric tasks, which can inspire future\nresearch.\n","authors":["Mingsheng Li","Xin Chen","Chi Zhang","Sijin Chen","Hongyuan Zhu","Fukun Yin","Gang Yu","Tao Chen"],"pdf_url":"https://arxiv.org/pdf/2312.10763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10758v1","updated":"2023-12-17T16:29:16Z","published":"2023-12-17T16:29:16Z","title":"SHaRPose: Sparse High-Resolution Representation for Human Pose\n  Estimation","summary":"  High-resolution representation is essential for achieving good performance in\nhuman pose estimation models. To obtain such features, existing works utilize\nhigh-resolution input images or fine-grained image tokens. However, this dense\nhigh-resolution representation brings a significant computational burden. In\nthis paper, we address the following question: \"Only sparse human keypoint\nlocations are detected for human pose estimation, is it really necessary to\ndescribe the whole image in a dense, high-resolution manner?\" Based on dynamic\ntransformer models, we propose a framework that only uses Sparse\nHigh-resolution Representations for human Pose estimation (SHaRPose). In\ndetail, SHaRPose consists of two stages. At the coarse stage, the relations\nbetween image regions and keypoints are dynamically mined while a coarse\nestimation is generated. Then, a quality predictor is applied to decide whether\nthe coarse estimation results should be refined. At the fine stage, SHaRPose\nbuilds sparse high-resolution representations only on the regions related to\nthe keypoints and provides refined high-precision human pose estimations.\nExtensive experiments demonstrate the outstanding performance of the proposed\nmethod. Specifically, compared to the state-of-the-art method ViTPose, our\nmodel SHaRPose-Base achieves 77.4 AP (+0.5 AP) on the COCO validation set and\n76.7 AP (+0.5 AP) on the COCO test-dev set, and infers at a speed of\n$1.4\\times$ faster than ViTPose-Base.\n","authors":["Xiaoqi An","Lin Zhao","Chen Gong","Nannan Wang","Di Wang","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2312.10758v1.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2307.00371v5","updated":"2023-12-17T15:50:36Z","published":"2023-07-01T15:48:33Z","title":"Learning Content-enhanced Mask Transformer for Domain Generalized\n  Urban-Scene Segmentation","summary":"  Domain-generalized urban-scene semantic segmentation (USSS) aims to learn\ngeneralized semantic predictions across diverse urban-scene styles. Unlike\ndomain gap challenges, USSS is unique in that the semantic categories are often\nsimilar in different urban scenes, while the styles can vary significantly due\nto changes in urban landscapes, weather conditions, lighting, and other\nfactors. Existing approaches typically rely on convolutional neural networks\n(CNNs) to learn the content of urban scenes.\n  In this paper, we propose a Content-enhanced Mask TransFormer (CMFormer) for\ndomain-generalized USSS. The main idea is to enhance the focus of the\nfundamental component, the mask attention mechanism, in Transformer\nsegmentation models on content information. To achieve this, we introduce a\nnovel content-enhanced mask attention mechanism. It learns mask queries from\nboth the image feature and its down-sampled counterpart, as lower-resolution\nimage features usually contain more robust content information and are less\nsensitive to style variations. These features are fused into a Transformer\ndecoder and integrated into a multi-resolution content-enhanced mask attention\nlearning scheme.\n  Extensive experiments conducted on various domain-generalized urban-scene\nsegmentation datasets demonstrate that the proposed CMFormer significantly\noutperforms existing CNN-based methods for domain-generalized semantic\nsegmentation, achieving improvements of up to 14.00\\% in terms of mIoU (mean\nintersection over union). The source code is publicly available at\n\\url{https://github.com/BiQiWHU/CMFormer}.\n","authors":["Qi Bi","Shaodi You","Theo Gevers"],"pdf_url":"https://arxiv.org/pdf/2307.00371v5.pdf","comment":"Accepted by AAAI 2024. Camera-ready version with available source\n  code"},{"id":"http://arxiv.org/abs/2312.10747v1","updated":"2023-12-17T15:37:41Z","published":"2023-12-17T15:37:41Z","title":"CEIR: Concept-based Explainable Image Representation Learning","summary":"  In modern machine learning, the trend of harnessing self-supervised learning\nto derive high-quality representations without label dependency has garnered\nsignificant attention. However, the absence of label information, coupled with\nthe inherently high-dimensional nature, improves the difficulty for the\ninterpretation of learned representations. Consequently, indirect evaluations\nbecome the popular metric for evaluating the quality of these features, leading\nto a biased validation of the learned representation rationale. To address\nthese challenges, we introduce a novel approach termed Concept-based\nExplainable Image Representation (CEIR). Initially, using the Concept-based\nModel (CBM) incorporated with pretrained CLIP and concepts generated by GPT-4,\nwe project input images into a concept vector space. Subsequently, a\nVariational Autoencoder (VAE) learns the latent representation from these\nprojected concepts, which serves as the final image representation. Due to the\ncapability of the representation to encapsulate high-level, semantically\nrelevant concepts, the model allows for attributions to a human-comprehensible\nconcept space. This not only enhances interpretability but also preserves the\nrobustness essential for downstream tasks. For instance, our method exhibits\nstate-of-the-art unsupervised clustering performance on benchmarks such as\nCIFAR10, CIFAR100, and STL10. Furthermore, capitalizing on the universality of\nhuman conceptual understanding, CEIR can seamlessly extract the related concept\nfrom open-world images without fine-tuning. This offers a fresh approach to\nautomatic label generation and label manipulation.\n","authors":["Yan Cui","Shuhong Liu","Liuzhuozheng Li","Zhiyuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2312.10747v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2312.10740v1","updated":"2023-12-17T14:57:10Z","published":"2023-12-17T14:57:10Z","title":"Unmasking Deepfake Faces from Videos Using An Explainable Cost-Sensitive\n  Deep Learning Approach","summary":"  Deepfake technology is widely used, which has led to serious worries about\nthe authenticity of digital media, making the need for trustworthy deepfake\nface recognition techniques more urgent than ever. This study employs a\nresource-effective and transparent cost-sensitive deep learning method to\neffectively detect deepfake faces in videos. To create a reliable deepfake\ndetection system, four pre-trained Convolutional Neural Network (CNN) models:\nXceptionNet, InceptionResNetV2, EfficientNetV2S, and EfficientNetV2M were used.\nFaceForensics++ and CelebDf-V2 as benchmark datasets were used to assess the\nperformance of our method. To efficiently process video data, key frame\nextraction was used as a feature extraction technique. Our main contribution is\nto show the models adaptability and effectiveness in correctly identifying\ndeepfake faces in videos. Furthermore, a cost-sensitive neural network method\nwas applied to solve the dataset imbalance issue that arises frequently in\ndeepfake detection. The XceptionNet model on the CelebDf-V2 dataset gave the\nproposed methodology a 98% accuracy, which was the highest possible whereas,\nthe InceptionResNetV2 model, achieves an accuracy of 94% on the FaceForensics++\ndataset. Source Code:\nhttps://github.com/Faysal-MD/Unmasking-Deepfake-Faces-from-Videos-An-Explainable-Cost-Sensitive-Deep-Learning-Approach-IEEE2023\n","authors":["Faysal Mahmud","Yusha Abdullah","Minhajul Islam","Tahsin Aziz"],"pdf_url":"https://arxiv.org/pdf/2312.10740v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10737v1","updated":"2023-12-17T14:52:31Z","published":"2023-12-17T14:52:31Z","title":"Traffic Incident Database with Multiple Labels Including Various\n  Perspective Environmental Information","summary":"  Traffic accident recognition is essential in developing automated driving and\nAdvanced Driving Assistant System technologies.A large dataset of annotated\ntraffic accidents is necessary to improve the accuracy of traffic accident\nrecognition using deep learning models.Conventional traffic accident datasets\nprovide annotations on the presence or absence of traffic accidents and other\nteacher labels, improving traffic accident recognition performance. Therefore,\nwe propose V-TIDB, a large-scale traffic accident recognition dataset annotated\nwith various environmental information as multi-labels. Our proposed dataset\naims to improve the performance of traffic accident recognition by annotating\nten types of environmental information in addition to the presence or absence\nof traffic accidents. V-TIDB is constructed by collecting many videos from the\nInternet and annotating them with appropriate environmental information.In our\nexperiments, we compare the performance of traffic accident recognition when\nonly labels related to the presence or absence of traffic accidents are trained\nand when environmental information is added as a multi-label. In the second\nexperiment, we compare the performance of the training with only contact level\nwhich represents the severity of the traffic accident, and the performance with\nenvironmental information added as a multi-label.The results showed that 6 out\nof 10 environmental information labels improved the performance of recognizing\nthe presence or absence of traffic accidents. In the experiment on the degree\nof recognition of traffic accidents, the performance of recognition of car\nwrecks and contacts was improved for all environmental information. These\nexperiments show that V-TIDB can be used to learn traffic accident recognition\nmodels that take environmental information into account in detail and can be\nused for appropriate traffic accident analysis.\n","authors":["Shota Nishiyama","Takuma Saito","Ryo Nakamura","Go Ohtani","Hirokatsu Kataoka","Kensho Hara"],"pdf_url":"https://arxiv.org/pdf/2312.10737v1.pdf","comment":"Conference paper accepted to IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS), 2023"},{"id":"http://arxiv.org/abs/2312.06198v2","updated":"2023-12-17T14:50:10Z","published":"2023-12-11T08:22:24Z","title":"Optimized View and Geometry Distillation from Multi-view Diffuser","summary":"  Generating multi-view images from a single input view using image-conditioned\ndiffusion models is a recent advancement and has shown considerable potential.\nHowever, issues such as the lack of consistency in synthesized views and\nover-smoothing in extracted geometry persist. Previous methods integrate\nmulti-view consistency modules or impose additional supervisory to enhance view\nconsistency while compromising on the flexibility of camera positioning and\nlimiting the versatility of view synthesis. In this study, we consider the\nradiance field optimized during geometry extraction as a more rigid consistency\nprior, compared to volume and ray aggregation used in previous works. We\nfurther identify and rectify a critical bias in the traditional radiance field\noptimization process through score distillation from a multi-view diffuser. We\nintroduce an Unbiased Score Distillation (USD) that utilizes unconditioned\nnoises from a 2D diffusion model, greatly refining the radiance field fidelity.\nwe leverage the rendered views from the optimized radiance field as the basis\nand develop a two-step specialization process of a 2D diffusion model, which is\nadept at conducting object-specific denoising and generating high-quality\nmulti-view images. Finally, we recover faithful geometry and texture directly\nfrom the refined multi-view images. Empirical evaluations demonstrate that our\noptimized geometry and view distillation technique generates comparable results\nto the state-of-the-art models trained on extensive datasets, all while\nmaintaining freedom in camera positioning. Please see our project page at\nhttps://youjiazhang.github.io/USD/.\n","authors":["Youjia Zhang","Junqing Yu","Zikai Song","Wei Yang"],"pdf_url":"https://arxiv.org/pdf/2312.06198v2.pdf","comment":"Project page: https://youjiazhang.github.io/USD/"},{"id":"http://arxiv.org/abs/2301.02761v2","updated":"2023-12-17T14:25:50Z","published":"2023-01-07T01:35:25Z","title":"Active Learning Guided by Efficient Surrogate Learners","summary":"  Re-training a deep learning model each time a single data point receives a\nnew label is impractical due to the inherent complexity of the training\nprocess. Consequently, existing active learning (AL) algorithms tend to adopt a\nbatch-based approach where, during each AL iteration, a set of data points is\ncollectively chosen for annotation. However, this strategy frequently leads to\nredundant sampling, ultimately eroding the efficacy of the labeling procedure.\nIn this paper, we introduce a new AL algorithm that harnesses the power of a\nGaussian process surrogate in conjunction with the neural network principal\nlearner. Our proposed model adeptly updates the surrogate learner for every new\ndata instance, enabling it to emulate and capitalize on the continuous learning\ndynamics of the neural network without necessitating a complete retraining of\nthe principal model for each individual label. Experiments on four benchmark\ndatasets demonstrate that this approach yields significant enhancements, either\nrivaling or aligning with the performance of state-of-the-art techniques.\n","authors":["Yunpyo An","Suyeong Park","Kwang In Kim"],"pdf_url":"https://arxiv.org/pdf/2301.02761v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.09616v2","updated":"2023-12-17T14:17:38Z","published":"2023-08-18T15:19:17Z","title":"Far3D: Expanding the Horizon for Surround-view 3D Object Detection","summary":"  Recently 3D object detection from surround-view images has made notable\nadvancements with its low deployment cost. However, most works have primarily\nfocused on close perception range while leaving long-range detection less\nexplored. Expanding existing methods directly to cover long distances poses\nchallenges such as heavy computation costs and unstable convergence. To address\nthese limitations, this paper proposes a novel sparse query-based framework,\ndubbed Far3D. By utilizing high-quality 2D object priors, we generate 3D\nadaptive queries that complement the 3D global queries. To efficiently capture\ndiscriminative features across different views and scales for long-range\nobjects, we introduce a perspective-aware aggregation module. Additionally, we\npropose a range-modulated 3D denoising approach to address query error\npropagation and mitigate convergence issues in long-range tasks. Significantly,\nFar3D demonstrates SoTA performance on the challenging Argoverse 2 dataset,\ncovering a wide range of 150 meters, surpassing several LiDAR-based approaches.\nMeanwhile, Far3D exhibits superior performance compared to previous methods on\nthe nuScenes dataset. The code is available at\nhttps://github.com/megvii-research/Far3D.\n","authors":["Xiaohui Jiang","Shuailin Li","Yingfei Liu","Shihao Wang","Fan Jia","Tiancai Wang","Lijin Han","Xiangyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2308.09616v2.pdf","comment":"Accepted by AAAI-2024"},{"id":"http://arxiv.org/abs/2312.10726v1","updated":"2023-12-17T14:17:05Z","published":"2023-12-17T14:17:05Z","title":"Towards Compact 3D Representations via Point Feature Enhancement Masked\n  Autoencoders","summary":"  Learning 3D representation plays a critical role in masked autoencoder (MAE)\nbased pre-training methods for point cloud, including single-modal and\ncross-modal based MAE. Specifically, although cross-modal MAE methods learn\nstrong 3D representations via the auxiliary of other modal knowledge, they\noften suffer from heavy computational burdens and heavily rely on massive\ncross-modal data pairs that are often unavailable, which hinders their\napplications in practice. Instead, single-modal methods with solely point\nclouds as input are preferred in real applications due to their simplicity and\nefficiency. However, such methods easily suffer from limited 3D representations\nwith global random mask input. To learn compact 3D representations, we propose\na simple yet effective Point Feature Enhancement Masked Autoencoders\n(Point-FEMAE), which mainly consists of a global branch and a local branch to\ncapture latent semantic features. Specifically, to learn more compact features,\na share-parameter Transformer encoder is introduced to extract point features\nfrom the global and local unmasked patches obtained by global random and local\nblock mask strategies, followed by a specific decoder to reconstruct.\nMeanwhile, to further enhance features in the local branch, we propose a Local\nEnhancement Module with local patch convolution to perceive fine-grained local\ncontext at larger scales. Our method significantly improves the pre-training\nefficiency compared to cross-modal alternatives, and extensive downstream\nexperiments underscore the state-of-the-art effectiveness, particularly\noutperforming our baseline (Point-MAE) by 5.16%, 5.00%, and 5.04% in three\nvariants of ScanObjectNN, respectively. The code is available at\nhttps://github.com/zyh16143998882/AAAI24-PointFEMAE.\n","authors":["Yaohua Zha","Huizhen Ji","Jinmin Li","Rongsheng Li","Tao Dai","Bin Chen","Zhi Wang","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2312.10726v1.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2312.10725v1","updated":"2023-12-17T14:14:31Z","published":"2023-12-17T14:14:31Z","title":"Addressing Sample Inefficiency in Multi-View Representation Learning","summary":"  Non-contrastive self-supervised learning (NC-SSL) methods like BarlowTwins\nand VICReg have shown great promise for label-free representation learning in\ncomputer vision. Despite the apparent simplicity of these techniques,\nresearchers must rely on several empirical heuristics to achieve competitive\nperformance, most notably using high-dimensional projector heads and two\naugmentations of the same image. In this work, we provide theoretical insights\non the implicit bias of the BarlowTwins and VICReg loss that can explain these\nheuristics and guide the development of more principled recommendations. Our\nfirst insight is that the orthogonality of the features is more critical than\nprojector dimensionality for learning good representations. Based on this, we\nempirically demonstrate that low-dimensional projector heads are sufficient\nwith appropriate regularization, contrary to the existing heuristic. Our second\ntheoretical insight suggests that using multiple data augmentations better\nrepresents the desiderata of the SSL objective. Based on this, we demonstrate\nthat leveraging more augmentations per sample improves representation quality\nand trainability. In particular, it improves optimization convergence, leading\nto better features emerging earlier in the training. Remarkably, we demonstrate\nthat we can reduce the pretraining dataset size by up to 4x while maintaining\naccuracy and improving convergence simply by using more data augmentations.\nCombining these insights, we present practical pretraining recommendations that\nimprove wall-clock time by 2x and improve performance on CIFAR-10/STL-10\ndatasets using a ResNet-50 backbone. Thus, this work provides a theoretical\ninsight into NC-SSL and produces practical recommendations for enhancing its\nsample and compute efficiency.\n","authors":["Kumar Krishna Agrawal","Arna Ghosh","Adam Oberman","Blake Richards"],"pdf_url":"https://arxiv.org/pdf/2312.10725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.06252v4","updated":"2023-12-17T13:53:42Z","published":"2023-05-10T15:33:15Z","title":"Embedded Feature Similarity Optimization with Specific Parameter\n  Initialization for 2D/3D Medical Image Registration","summary":"  We present a novel deep learning-based framework: Embedded Feature Similarity\nOptimization with Specific Parameter Initialization (SOPI) for 2D/3D medical\nimage registration which is a most challenging problem due to the difficulty\nsuch as dimensional mismatch, heavy computation load and lack of golden\nevaluation standard. The framework we design includes a parameter specification\nmodule to efficiently choose initialization pose parameter and a\nfine-registration module to align images. The proposed framework takes\nextracting multi-scale features into consideration using a novel composite\nconnection encoder with special training techniques. We compare the method with\nboth learning-based methods and optimization-based methods on a in-house\nCT/X-ray dataset as well as simulated data to further evaluate performance. Our\nexperiments demonstrate that the method in this paper has improved the\nregistration performance, and thereby outperforms the existing methods in terms\nof accuracy and running time. We also show the potential of the proposed method\nas an initial pose estimator. The code is available at\nhttps://github.com/m1nhengChen/SOPI\n","authors":["Minheng Chen","Zhirun Zhang","Shuheng Gu","Youyong Kong"],"pdf_url":"https://arxiv.org/pdf/2305.06252v4.pdf","comment":"14 pages, 5 figures, accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.10718v1","updated":"2023-12-17T13:39:04Z","published":"2023-12-17T13:39:04Z","title":"CogCartoon: Towards Practical Story Visualization","summary":"  The state-of-the-art methods for story visualization demonstrate a\nsignificant demand for training data and storage, as well as limited\nflexibility in story presentation, thereby rendering them impractical for\nreal-world applications. We introduce CogCartoon, a practical story\nvisualization method based on pre-trained diffusion models. To alleviate\ndependence on data and storage, we propose an innovative strategy of\ncharacter-plugin generation that can represent a specific character as a\ncompact 316 KB plugin by using a few training samples. To facilitate enhanced\nflexibility, we employ a strategy of plugin-guided and layout-guided inference,\nenabling users to seamlessly incorporate new characters and custom layouts into\nthe generated image results at their convenience. We have conducted\ncomprehensive qualitative and quantitative studies, providing compelling\nevidence for the superiority of CogCartoon over existing methodologies.\nMoreover, CogCartoon demonstrates its power in tackling challenging tasks,\nincluding long story visualization and realistic style story visualization.\n","authors":["Zhongyang Zhu","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2312.10718v1.pdf","comment":"17 pages, 9 figures"},{"id":"http://arxiv.org/abs/2312.10714v1","updated":"2023-12-17T13:16:49Z","published":"2023-12-17T13:16:49Z","title":"Primitive-based 3D Human-Object Interaction Modelling and Programming","summary":"  Embedding Human and Articulated Object Interaction (HAOI) in 3D is an\nimportant direction for a deeper human activity understanding. Different from\nprevious works that use parametric and CAD models to represent humans and\nobjects, in this work, we propose a novel 3D geometric primitive-based language\nto encode both humans and objects. Given our new paradigm, humans and objects\nare all compositions of primitives instead of heterogeneous entities. Thus,\nmutual information learning may be achieved between the limited 3D data of\nhumans and different object categories. Moreover, considering the simplicity of\nthe expression and the richness of the information it contains, we choose the\nsuperquadric as the primitive representation. To explore an effective embedding\nof HAOI for the machine, we build a new benchmark on 3D HAOI consisting of\nprimitives together with their images and propose a task requiring machines to\nrecover 3D HAOI using primitives from images. Moreover, we propose a baseline\nof single-view 3D reconstruction on HAOI. We believe this primitive-based 3D\nHAOI representation would pave the way for 3D HAOI studies. Our code and data\nare available at https://mvig-rhos.com/p3haoi.\n","authors":["Siqi Liu","Yong-Lu Li","Zhou Fang","Xinpeng Liu","Yang You","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2312.10714v1.pdf","comment":"AAAI2024"},{"id":"http://arxiv.org/abs/2312.10713v1","updated":"2023-12-17T13:12:34Z","published":"2023-12-17T13:12:34Z","title":"Synthesizing Black-box Anti-forensics DeepFakes with High Visual Quality","summary":"  DeepFake, an AI technology for creating facial forgeries, has garnered global\nattention. Amid such circumstances, forensics researchers focus on developing\ndefensive algorithms to counter these threats. In contrast, there are\ntechniques developed for enhancing the aggressiveness of DeepFake, e.g.,\nthrough anti-forensics attacks, to disrupt forensic detectors. However, such\nattacks often sacrifice image visual quality for improved undetectability. To\naddress this issue, we propose a method to generate novel adversarial\nsharpening masks for launching black-box anti-forensics attacks. Unlike many\nexisting arts, with such perturbations injected, DeepFakes could achieve high\nanti-forensics performance while exhibiting pleasant sharpening visual effects.\nAfter experimental evaluations, we prove that the proposed method could\nsuccessfully disrupt the state-of-the-art DeepFake detectors. Besides, compared\nwith the images processed by existing DeepFake anti-forensics methods, the\nvisual qualities of anti-forensics DeepFakes rendered by the proposed method\nare significantly refined.\n","authors":["Bing Fan","Shu Hu","Feng Ding"],"pdf_url":"https://arxiv.org/pdf/2312.10713v1.pdf","comment":"Accepted for publication at ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.10701v1","updated":"2023-12-17T12:28:30Z","published":"2023-12-17T12:28:30Z","title":"Bengali License Plate Recognition: Unveiling Clarity with CNN and\n  GFP-GAN","summary":"  Automated License Plate Recognition(ALPR) is a system that automatically\nreads and extracts data from vehicle license plates using image processing and\ncomputer vision techniques. The Goal of LPR is to identify and read the license\nplate number accurately and quickly, even under challenging, conditions such as\npoor lighting, angled or obscured plates, and different plate fonts and\nlayouts. The proposed method consists of processing the Bengali low-resolution\nblurred license plates and identifying the plate's characters. The processes\ninclude image restoration using GFPGAN, Maximizing contrast, Morphological\nimage processing like dilation, feature extraction and Using Convolutional\nNeural Networks (CNN), character segmentation and recognition are accomplished.\nA dataset of 1292 images of Bengali digits and characters was prepared for this\nproject.\n","authors":["Noushin Afrin","Md Mahamudul Hasan","Mohammed Fazlay Elahi Safin","Khondakar Rifat Amin","Md Zahidul Haque","Farzad Ahmed","Md. Tanvir Rouf Shawon"],"pdf_url":"https://arxiv.org/pdf/2312.10701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10696v1","updated":"2023-12-17T12:11:38Z","published":"2023-12-17T12:11:38Z","title":"An Interpretable Deep Learning Approach for Skin Cancer Categorization","summary":"  Skin cancer is a serious worldwide health issue, precise and early detection\nis essential for better patient outcomes and effective treatment. In this\nresearch, we use modern deep learning methods and explainable artificial\nintelligence (XAI) approaches to address the problem of skin cancer detection.\nTo categorize skin lesions, we employ four cutting-edge pre-trained models:\nXceptionNet, EfficientNetV2S, InceptionResNetV2, and EfficientNetV2M. Image\naugmentation approaches are used to reduce class imbalance and improve the\ngeneralization capabilities of our models. Our models decision-making process\ncan be clarified because of the implementation of explainable artificial\nintelligence (XAI). In the medical field, interpretability is essential to\nestablish credibility and make it easier to implement AI driven diagnostic\ntechnologies into clinical workflows. We determined the XceptionNet\narchitecture to be the best performing model, achieving an accuracy of 88.72%.\nOur study shows how deep learning and explainable artificial intelligence (XAI)\ncan improve skin cancer diagnosis, laying the groundwork for future\ndevelopments in medical image analysis. These technologies ability to allow for\nearly and accurate detection could enhance patient care, lower healthcare\ncosts, and raise the survival rates for those with skin cancer. Source Code:\nhttps://github.com/Faysal-MD/An-Interpretable-Deep-Learning?Approach-for-Skin-Cancer-Categorization-IEEE2023\n","authors":["Faysal Mahmud","Md. Mahin Mahfiz","Md. Zobayer Ibna Kabir","Yusha Abdullah"],"pdf_url":"https://arxiv.org/pdf/2312.10696v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05845v4","updated":"2023-12-17T12:10:16Z","published":"2023-07-11T23:36:49Z","title":"PIGEON: Predicting Image Geolocations","summary":"  Planet-scale image geolocalization remains a challenging problem due to the\ndiversity of images originating from anywhere in the world. Although approaches\nbased on vision transformers have made significant progress in geolocalization\naccuracy, success in prior literature is constrained to narrow distributions of\nimages of landmarks, and performance has not generalized to unseen places. We\npresent a new geolocalization system that combines semantic geocell creation,\nmulti-task contrastive pretraining, and a novel loss function. Additionally,\nour work is the first to perform retrieval over location clusters for guess\nrefinements. We train two models for evaluations on street-level data and\ngeneral-purpose image geolocalization; the first model, PIGEON, is trained on\ndata from the game of Geoguessr and is capable of placing over 40% of its\nguesses within 25 kilometers of the target location globally. We also develop a\nbot and deploy PIGEON in a blind experiment against humans, ranking in the top\n0.01% of players. We further challenge one of the world's foremost professional\nGeoguessr players to a series of six matches with millions of viewers, winning\nall six games. Our second model, PIGEOTTO, differs in that it is trained on a\ndataset of images from Flickr and Wikipedia, achieving state-of-the-art results\non a wide range of image geolocalization benchmarks, outperforming the previous\nSOTA by up to 7.7 percentage points on the city accuracy level and up to 38.8\npercentage points on the country level. Our findings suggest that PIGEOTTO is\nthe first image geolocalization model that effectively generalizes to unseen\nplaces and that our approach can pave the way for highly accurate, planet-scale\nimage geolocalization systems. Our code is available on GitHub.\n","authors":["Lukas Haas","Michal Skreta","Silas Alberti","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2307.05845v4.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2312.10692v1","updated":"2023-12-17T11:59:14Z","published":"2023-12-17T11:59:14Z","title":"Pedestrian Attribute Recognition via CLIP based Prompt Vision-Language\n  Fusion","summary":"  Existing pedestrian attribute recognition (PAR) algorithms adopt pre-trained\nCNN (e.g., ResNet) as their backbone network for visual feature learning, which\nmight obtain sub-optimal results due to the insufficient employment of the\nrelations between pedestrian images and attribute labels. In this paper, we\nformulate PAR as a vision-language fusion problem and fully exploit the\nrelations between pedestrian images and attribute labels. Specifically, the\nattribute phrases are first expanded into sentences, and then the pre-trained\nvision-language model CLIP is adopted as our backbone for feature embedding of\nvisual images and attribute descriptions. The contrastive learning objective\nconnects the vision and language modalities well in the CLIP-based feature\nspace, and the Transformer layers used in CLIP can capture the long-range\nrelations between pixels. Then, a multi-modal Transformer is adopted to fuse\nthe dual features effectively and feed-forward network is used to predict\nattributes. To optimize our network efficiently, we propose the region-aware\nprompt tuning technique to adjust very few parameters (i.e., only the prompt\nvectors and classification heads) and fix both the pre-trained VL model and\nmulti-modal Transformer. Our proposed PAR algorithm only adjusts 0.75%\nlearnable parameters compared with the fine-tuning strategy. It also achieves\nnew state-of-the-art performance on both standard and zero-shot settings for\nPAR, including RAPv1, RAPv2, WIDER, PA100K, and PETA-ZS, RAP-ZS datasets. The\nsource code and pre-trained models will be released on\nhttps://github.com/Event-AHU/OpenPAR.\n","authors":["Xiao Wang","Jiandong Jin","Chenglong Li","Jin Tang","Cheng Zhang","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2312.10692v1.pdf","comment":"In Peer Review"},{"id":"http://arxiv.org/abs/2312.10686v1","updated":"2023-12-17T11:11:02Z","published":"2023-12-17T11:11:02Z","title":"Out-of-Distribution Detection in Long-Tailed Recognition with Calibrated\n  Outlier Class Learning","summary":"  Existing out-of-distribution (OOD) methods have shown great success on\nbalanced datasets but become ineffective in long-tailed recognition (LTR)\nscenarios where 1) OOD samples are often wrongly classified into head classes\nand/or 2) tail-class samples are treated as OOD samples. To address these\nissues, current studies fit a prior distribution of auxiliary/pseudo OOD data\nto the long-tailed in-distribution (ID) data. However, it is difficult to\nobtain such an accurate prior distribution given the unknowingness of real OOD\nsamples and heavy class imbalance in LTR. A straightforward solution to avoid\nthe requirement of this prior is to learn an outlier class to encapsulate the\nOOD samples. The main challenge is then to tackle the aforementioned confusion\nbetween OOD samples and head/tail-class samples when learning the outlier\nclass. To this end, we introduce a novel calibrated outlier class learning\n(COCL) approach, in which 1) a debiased large margin learning method is\nintroduced in the outlier class learning to distinguish OOD samples from both\nhead and tail classes in the representation space and 2) an outlier-class-aware\nlogit calibration method is defined to enhance the long-tailed classification\nconfidence. Extensive empirical results on three popular benchmarks CIFAR10-LT,\nCIFAR100-LT, and ImageNet-LT demonstrate that COCL substantially outperforms\nstate-of-the-art OOD detection methods in LTR while being able to improve the\nclassification accuracy on ID data. Code is available at\nhttps://github.com/mala-lab/COCL.\n","authors":["Wenjun Miao","Guansong Pang","Tianqi Li","Xiao Bai","Jin Zheng"],"pdf_url":"https://arxiv.org/pdf/2312.10686v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08815v2","updated":"2023-12-17T10:52:14Z","published":"2023-03-15T17:59:13Z","title":"Lane Graph as Path: Continuity-preserving Path-wise Modeling for Online\n  Lane Graph Construction","summary":"  Online lane graph construction is a promising but challenging task in\nautonomous driving. Previous methods usually model the lane graph at the pixel\nor piece level, and recover the lane graph by pixel-wise or piece-wise\nconnection, which breaks down the continuity of the lane. Human drivers focus\non and drive along the continuous and complete paths instead of considering\nlane pieces. Autonomous vehicles also require path-specific guidance from lane\ngraph for trajectory planning. We argue that the path, which indicates the\ntraffic flow, is the primitive of the lane graph. Motivated by this, we propose\nto model the lane graph in a novel path-wise manner, which well preserves the\ncontinuity of the lane and encodes traffic information for planning. We present\na path-based online lane graph construction method, termed LaneGAP, which\nend-to-end learns the path and recovers the lane graph via a Path2Graph\nalgorithm. We qualitatively and quantitatively demonstrate the superiority of\nLaneGAP over conventional pixel-based and piece-based methods on challenging\nnuScenes and Argoverse2 datasets. Abundant visualizations show LaneGAP can cope\nwith diverse traffic conditions. Code and models will be released at\n\\url{https://github.com/hustvl/LaneGAP} for facilitating future research.\n","authors":["Bencheng Liao","Shaoyu Chen","Bo Jiang","Tianheng Cheng","Qian Zhang","Wenyu Liu","Chang Huang","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.08815v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10680v1","updated":"2023-12-17T10:46:46Z","published":"2023-12-17T10:46:46Z","title":"DomainForensics: Exposing Face Forgery across Domains via Bi-directional\n  Adaptation","summary":"  Recent DeepFake detection methods have shown excellent performance on public\ndatasets but are significantly degraded on new forgeries. Solving this problem\nis important, as new forgeries emerge daily with the continuously evolving\ngenerative techniques. Many efforts have been made for this issue by seeking\nthe commonly existing traces empirically on data level. In this paper, we\nrethink this problem and propose a new solution from the unsupervised domain\nadaptation perspective. Our solution, called DomainForensics, aims to transfer\nthe forgery knowledge from known forgeries to new forgeries. Unlike recent\nefforts, our solution does not focus on data view but on learning strategies of\nDeepFake detectors to capture the knowledge of new forgeries through the\nalignment of domain discrepancies. In particular, unlike the general domain\nadaptation methods which consider the knowledge transfer in the semantic class\ncategory, thus having limited application, our approach captures the subtle\nforgery traces. We describe a new bi-directional adaptation strategy dedicated\nto capturing the forgery knowledge across domains. Specifically, our strategy\nconsiders both forward and backward adaptation, to transfer the forgery\nknowledge from the source domain to the target domain in forward adaptation and\nthen reverse the adaptation from the target domain to the source domain in\nbackward adaptation. In forward adaptation, we perform supervised training for\nthe DeepFake detector in the source domain and jointly employ adversarial\nfeature adaptation to transfer the ability to detect manipulated faces from\nknown forgeries to new forgeries. In backward adaptation, we further improve\nthe knowledge transfer by coupling adversarial adaptation with\nself-distillation on new forgeries. This enables the detector to expose new\nforgery features from unlabeled data and avoid forgetting the known knowledge\nof known...\n","authors":["Qingxuan Lv","Yuezun Li","Junyu Dong","Sheng Chen","Hui Yu","Huiyu Zhou","Shu Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.10680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10674v1","updated":"2023-12-17T10:16:47Z","published":"2023-12-17T10:16:47Z","title":"A Framework of Full-Process Generation Design for Park Green Spaces\n  Based on Remote Sensing Segmentation-GAN-Diffusion","summary":"  The development of generative design driven by artificial intelligence\nalgorithms is speedy. There are two research gaps in the current research: 1)\nMost studies only focus on the relationship between design elements and pay\nlittle attention to the external information of the site; 2) GAN and other\ntraditional generative algorithms generate results with low resolution and\ninsufficient details. To address these two problems, we integrate GAN, Stable\ndiffusion multimodal large-scale image pre-training model to construct a\nfull-process park generative design method: 1) First, construct a\nhigh-precision remote sensing object extraction system for automated extraction\nof urban environmental information; 2) Secondly, use GAN to construct a park\ndesign generation system based on the external environment, which can quickly\ninfer and generate design schemes from urban environmental information; 3)\nFinally, introduce Stable Diffusion to optimize the design plan, fill in\ndetails, and expand the resolution of the plan by 64 times. This method can\nachieve a fully unmanned design automation workflow. The research results show\nthat: 1) The relationship between the inside and outside of the site will\naffect the algorithm generation results. 2) Compared with traditional GAN\nalgorithms, Stable diffusion significantly improve the information richness of\nthe generated results.\n","authors":["Ran Chen","Xingjian Yi","Jing Zhao","Yueheng He","Bainian Chen","Xueqi Yao","Fangjun Liu","Haoran Li","Zeke Lian"],"pdf_url":"https://arxiv.org/pdf/2312.10674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10671v1","updated":"2023-12-17T10:07:03Z","published":"2023-12-17T10:07:03Z","title":"Open3DIS: Open-vocabulary 3D Instance Segmentation with 2D Mask Guidance","summary":"  We introduce Open3DIS, a novel solution designed to tackle the problem of\nOpen-Vocabulary Instance Segmentation within 3D scenes. Objects within 3D\nenvironments exhibit diverse shapes, scales, and colors, making precise\ninstance-level identification a challenging task. Recent advancements in\nOpen-Vocabulary scene understanding have made significant strides in this area\nby employing class-agnostic 3D instance proposal networks for object\nlocalization and learning queryable features for each 3D mask. While these\nmethods produce high-quality instance proposals, they struggle with identifying\nsmall-scale and geometrically ambiguous objects. The key idea of our method is\na new module that aggregates 2D instance masks across frames and maps them to\ngeometrically coherent point cloud regions as high-quality object proposals\naddressing the above limitations. These are then combined with 3D\nclass-agnostic instance proposals to include a wide range of objects in the\nreal world. To validate our approach, we conducted experiments on three\nprominent datasets, including ScanNet200, S3DIS, and Replica, demonstrating\nsignificant performance gains in segmenting objects with diverse categories\nover the state-of-the-art approaches.\n","authors":["Phuc D. A. Nguyen","Tuan Duc Ngo","Chuang Gan","Evangelos Kalogerakis","Anh Tran","Cuong Pham","Khoi Nguyen"],"pdf_url":"https://arxiv.org/pdf/2312.10671v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2304.01168v5","updated":"2023-12-17T10:00:55Z","published":"2023-04-03T17:37:00Z","title":"DeepAccident: A Motion and Accident Prediction Benchmark for V2X\n  Autonomous Driving","summary":"  Safety is the primary priority of autonomous driving. Nevertheless, no\npublished dataset currently supports the direct and explainable safety\nevaluation for autonomous driving. In this work, we propose DeepAccident, a\nlarge-scale dataset generated via a realistic simulator containing diverse\naccident scenarios that frequently occur in real-world driving. The proposed\nDeepAccident dataset includes 57K annotated frames and 285K annotated samples,\napproximately 7 times more than the large-scale nuScenes dataset with 40k\nannotated samples. In addition, we propose a new task, end-to-end motion and\naccident prediction, which can be used to directly evaluate the accident\nprediction ability for different autonomous driving algorithms. Furthermore,\nfor each scenario, we set four vehicles along with one infrastructure to record\ndata, thus providing diverse viewpoints for accident scenarios and enabling V2X\n(vehicle-to-everything) research on perception and prediction tasks. Finally,\nwe present a baseline V2X model named V2XFormer that demonstrates superior\nperformance for motion and accident prediction and 3D object detection compared\nto the single-vehicle model.\n","authors":["Tianqi Wang","Sukmin Kim","Wenxuan Ji","Enze Xie","Chongjian Ge","Junsong Chen","Zhenguo Li","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2304.01168v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10665v1","updated":"2023-12-17T09:44:27Z","published":"2023-12-17T09:44:27Z","title":"Silkie: Preference Distillation for Large Visual Language Models","summary":"  This paper explores preference distillation for large vision language models\n(LVLMs), improving their ability to generate helpful and faithful responses\nanchoring the visual context. We first build a vision-language feedback\n(VLFeedback) dataset utilizing AI annotation. Specifically, responses are\ngenerated by models sampled from 12 LVLMs, conditioned on multi-modal\ninstructions sourced from various datasets. We adopt GPT-4V to assess the\ngenerated outputs regarding helpfulness, visual faithfulness, and ethical\nconsiderations. Furthermore, the preference supervision is distilled into\nQwen-VL-Chat through the direct preference optimization (DPO) method. The\nresulting model Silkie, achieves 6.9% and 9.5% relative improvement on the MME\nbenchmark regarding the perception and cognition capabilities, respectively.\nSilkie also demonstrates reduced hallucination by setting a new\nstate-of-the-art score of 3.02 on the MMHal-Bench benchmark. Further analysis\nshows that DPO with our VLFeedback dataset mainly boosts the fine-grained\nperception and complex cognition abilities of LVLMs, leading to more\ncomprehensive improvements compared to human-annotated preference datasets.\n","authors":["Lei Li","Zhihui Xie","Mukai Li","Shunian Chen","Peiyi Wang","Liang Chen","Yazheng Yang","Benyou Wang","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2312.10665v1.pdf","comment":"Project page: https://vlf-silkie.github.io"},{"id":"http://arxiv.org/abs/2312.10656v1","updated":"2023-12-17T09:05:56Z","published":"2023-12-17T09:05:56Z","title":"VidToMe: Video Token Merging for Zero-Shot Video Editing","summary":"  Diffusion models have made significant advances in generating high-quality\nimages, but their application to video generation has remained challenging due\nto the complexity of temporal motion. Zero-shot video editing offers a solution\nby utilizing pre-trained image diffusion models to translate source videos into\nnew ones. Nevertheless, existing methods struggle to maintain strict temporal\nconsistency and efficient memory consumption. In this work, we propose a novel\napproach to enhance temporal consistency in generated videos by merging\nself-attention tokens across frames. By aligning and compressing temporally\nredundant tokens across frames, our method improves temporal coherence and\nreduces memory consumption in self-attention computations. The merging strategy\nmatches and aligns tokens according to the temporal correspondence between\nframes, facilitating natural temporal consistency in generated video frames. To\nmanage the complexity of video processing, we divide videos into chunks and\ndevelop intra-chunk local token merging and inter-chunk global token merging,\nensuring both short-term video continuity and long-term content consistency.\nOur video editing approach seamlessly extends the advancements in image editing\nto video editing, rendering favorable results in temporal consistency over\nstate-of-the-art methods.\n","authors":["Xirui Li","Chao Ma","Xiaokang Yang","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2312.10656v1.pdf","comment":"Project page: https://vidtome-diffusion.github.io"},{"id":"http://arxiv.org/abs/2303.12787v3","updated":"2023-12-17T08:30:49Z","published":"2023-03-22T17:57:36Z","title":"EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for\n  Monocular Object Pose Estimation","summary":"  Locating 3D objects from a single RGB image via Perspective-n-Point (PnP) is\na long-standing problem in computer vision. Driven by end-to-end deep learning,\nrecent studies suggest interpreting PnP as a differentiable layer, allowing for\npartial learning of 2D-3D point correspondences by backpropagating the\ngradients of pose loss. Yet, learning the entire correspondences from scratch\nis highly challenging, particularly for ambiguous pose solutions, where the\nglobally optimal pose is theoretically non-differentiable w.r.t. the points. In\nthis paper, we propose the EPro-PnP, a probabilistic PnP layer for general\nend-to-end pose estimation, which outputs a distribution of pose with\ndifferentiable probability density on the SE(3) manifold. The 2D-3D coordinates\nand corresponding weights are treated as intermediate variables learned by\nminimizing the KL divergence between the predicted and target pose\ndistribution. The underlying principle generalizes previous approaches, and\nresembles the attention mechanism. EPro-PnP can enhance existing correspondence\nnetworks, closing the gap between PnP-based method and the task-specific\nleaders on the LineMOD 6DoF pose estimation benchmark. Furthermore, EPro-PnP\nhelps to explore new possibilities of network design, as we demonstrate a novel\ndeformable correspondence network with the state-of-the-art pose accuracy on\nthe nuScenes 3D object detection benchmark. Our code is available at\nhttps://github.com/tjiiv-cprg/EPro-PnP-v2.\n","authors":["Hansheng Chen","Wei Tian","Pichao Wang","Fan Wang","Lu Xiong","Hao Li"],"pdf_url":"https://arxiv.org/pdf/2303.12787v3.pdf","comment":"Code available at https://github.com/tjiiv-cprg/EPro-PnP-v2. Revised\n  and fixed typos. arXiv admin note: substantial text overlap with\n  arXiv:2203.13254"},{"id":"http://arxiv.org/abs/2312.10649v1","updated":"2023-12-17T08:30:00Z","published":"2023-12-17T08:30:00Z","title":"PNeRFLoc: Visual Localization with Point-based Neural Radiance Fields","summary":"  Due to the ability to synthesize high-quality novel views, Neural Radiance\nFields (NeRF) have been recently exploited to improve visual localization in a\nknown environment. However, the existing methods mostly utilize NeRFs for data\naugmentation to improve the regression model training, and the performance on\nnovel viewpoints and appearances is still limited due to the lack of geometric\nconstraints. In this paper, we propose a novel visual localization framework,\n\\ie, PNeRFLoc, based on a unified point-based representation. On the one hand,\nPNeRFLoc supports the initial pose estimation by matching 2D and 3D feature\npoints as traditional structure-based methods; on the other hand, it also\nenables pose refinement with novel view synthesis using rendering-based\noptimization. Specifically, we propose a novel feature adaption module to close\nthe gaps between the features for visual localization and neural rendering. To\nimprove the efficacy and efficiency of neural rendering-based optimization, we\nalso develop an efficient rendering-based framework with a warping loss\nfunction. Furthermore, several robustness techniques are developed to handle\nillumination changes and dynamic objects for outdoor scenarios. Experiments\ndemonstrate that PNeRFLoc performs the best on synthetic data when the NeRF\nmodel can be well learned and performs on par with the SOTA method on the\nvisual localization benchmark datasets.\n","authors":["Boming Zhao","Luwei Yang","Mao Mao","Hujun Bao","Zhaopeng Cui"],"pdf_url":"https://arxiv.org/pdf/2312.10649v1.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2306.00800v3","updated":"2023-12-17T08:24:37Z","published":"2023-06-01T15:28:41Z","title":"FigGen: Text to Scientific Figure Generation","summary":"  The generative modeling landscape has experienced tremendous growth in recent\nyears, particularly in generating natural images and art. Recent techniques\nhave shown impressive potential in creating complex visual compositions while\ndelivering impressive realism and quality. However, state-of-the-art methods\nhave been focusing on the narrow domain of natural images, while other\ndistributions remain unexplored. In this paper, we introduce the problem of\ntext-to-figure generation, that is creating scientific figures of papers from\ntext descriptions. We present FigGen, a diffusion-based approach for\ntext-to-figure as well as the main challenges of the proposed task. Code and\nmodels are available at https://github.com/joanrod/figure-diffusion\n","authors":["Juan A Rodriguez","David Vazquez","Issam Laradji","Marco Pedersoli","Pau Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2306.00800v3.pdf","comment":"Published at ICLR 2023 as a Tiny Paper"},{"id":"http://arxiv.org/abs/2312.00377v3","updated":"2023-12-17T07:59:20Z","published":"2023-12-01T06:48:03Z","title":"SynFundus: A synthetic fundus images dataset with millions of samples\n  and multi-disease annotations","summary":"  In the field of medical imaging, there are seldom large-scale public datasets\nwith high-quality annotations due to data privacy and annotation cost. To\naddress this issue, we release SynFundus-1M, a high-quality synthetic dataset\ncontaining over \\textbf{1 million} fundus images w.r.t. 11 disease types.\nMoreover, we intentionally diversify the readability of the images and\naccordingly provide 4 types of the quality score for each image. To the best of\nour knowledge, SynFundus-1M is currently the largest fundus dataset with the\nmost sophisticated annotations. All the images are generated by a Denoising\nDiffusion Probabilistic Model, named SynFundus-Generator. Trained with over 1.3\nmillion private fundus images, our SynFundus-Generator achieves significant\nsuperior performance in generating fundus images compared to some recent\nrelated works. Furthermore, we blend some synthetic images from SynFundus-1M\nwith real fundus images, and ophthalmologists can hardly distinguish the\nsynthetic images from real ones. Through extensive experiments, we demonstrate\nthat both convolutional neural networs (CNN) and Vision Transformer (ViT) can\nbenefit from SynFundus-1M by pretraining or training directly. Compared to\ndatasets like ImageNet or EyePACS, models trained on SynFundus-1M not only\nachieve better performance but also faster convergence on various downstream\ntasks.\n","authors":["Fangxin Shang","Jie Fu","Yehui Yang","Haifeng Huang","Junwei Liu","Lei Ma"],"pdf_url":"https://arxiv.org/pdf/2312.00377v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10639v1","updated":"2023-12-17T07:51:38Z","published":"2023-12-17T07:51:38Z","title":"Artificial intelligence optical hardware empowers high-resolution\n  hyperspectral video understanding at 1.2 Tb/s","summary":"  Foundation models, exemplified by GPT technology, are discovering new\nhorizons in artificial intelligence by executing tasks beyond their designers'\nexpectations. While the present generation provides fundamental advances in\nunderstanding language and images, the next frontier is video comprehension.\nProgress in this area must overcome the 1 Tb/s data rate demanded to grasp\nreal-time multidimensional video information. This speed limit lies well beyond\nthe capabilities of the existing generation of hardware, imposing a roadblock\nto further advances. This work introduces a hardware-accelerated integrated\noptoelectronic platform for multidimensional video understanding in real-time.\nThe technology platform combines artificial intelligence hardware, processing\ninformation optically, with state-of-the-art machine vision networks, resulting\nin a data processing speed of 1.2 Tb/s with hundreds of frequency bands and\nmegapixel spatial resolution at video rates. Such performance, validated in the\nAI tasks of video semantic segmentation and object understanding in indoor and\naerial applications, surpasses the speed of the closest technologies with\nsimilar spectral resolution by three to four orders of magnitude. This platform\nopens up new avenues for research in real-time AI video understanding of\nmultidimensional visual information, helping the empowerment of future\nhuman-machine interactions and cognitive processing developments.\n","authors":["Maksim Makarenko","Qizhou Wang","Arturo Burguete-Lopez","Silvio Giancola","Bernard Ghanem","Luca Passone","Andrea Fratalocchi"],"pdf_url":"https://arxiv.org/pdf/2312.10639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10637v1","updated":"2023-12-17T07:38:43Z","published":"2023-12-17T07:38:43Z","title":"An Evaluation of GPT-4V and Gemini in Online VQA","summary":"  A comprehensive evaluation is critical to assess the capabilities of large\nmultimodal models (LMM). In this study, we evaluate the state-of-the-art LMMs,\nnamely GPT-4V and Gemini, utilizing the VQAonline dataset. VQAonline is an\nend-to-end authentic VQA dataset sourced from a diverse range of everyday\nusers. Compared previous benchmarks, VQAonline well aligns with real-world\ntasks. It enables us to effectively evaluate the generality of an LMM, and\nfacilitates a direct comparison with human performance. To comprehensively\nevaluate GPT-4V and Gemini, we generate seven types of metadata for around\n2,000 visual questions, such as image type and the required image processing\ncapabilities. Leveraging this array of metadata, we analyze the zero-shot\nperformance of GPT-4V and Gemini, and identify the most challenging questions\nfor both models.\n","authors":["Mengchen Liu","Chongyan Chen"],"pdf_url":"https://arxiv.org/pdf/2312.10637v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2312.10634v1","updated":"2023-12-17T07:33:06Z","published":"2023-12-17T07:33:06Z","title":"Anomaly Score: Evaluating Generative Models and Individual Generated\n  Images based on Complexity and Vulnerability","summary":"  With the advancement of generative models, the assessment of generated images\nbecomes more and more important. Previous methods measure distances between\nfeatures of reference and generated images from trained vision models. In this\npaper, we conduct an extensive investigation into the relationship between the\nrepresentation space and input space around generated images. We first propose\ntwo measures related to the presence of unnatural elements within images:\ncomplexity, which indicates how non-linear the representation space is, and\nvulnerability, which is related to how easily the extracted feature changes by\nadversarial input changes. Based on these, we introduce a new metric to\nevaluating image-generative models called anomaly score (AS). Moreover, we\npropose AS-i (anomaly score for individual images) that can effectively\nevaluate generated images individually. Experimental results demonstrate the\nvalidity of the proposed approach.\n","authors":["Jaehui Hwang","Junghyuk Lee","Jong-Seok Lee"],"pdf_url":"https://arxiv.org/pdf/2312.10634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06988v4","updated":"2023-12-17T07:06:56Z","published":"2023-12-12T05:12:22Z","title":"MWSIS: Multimodal Weakly Supervised Instance Segmentation with 2D Box\n  Annotations for Autonomous Driving","summary":"  Instance segmentation is a fundamental research in computer vision,\nespecially in autonomous driving. However, manual mask annotation for instance\nsegmentation is quite time-consuming and costly. To address this problem, some\nprior works attempt to apply weakly supervised manner by exploring 2D or 3D\nboxes. However, no one has ever successfully segmented 2D and 3D instances\nsimultaneously by only using 2D box annotations, which could further reduce the\nannotation cost by an order of magnitude. Thus, we propose a novel framework\ncalled Multimodal Weakly Supervised Instance Segmentation (MWSIS), which\nincorporates various fine-grained label generation and correction modules for\nboth 2D and 3D modalities to improve the quality of pseudo labels, along with a\nnew multimodal cross-supervision approach, named Consistency Sparse Cross-modal\nSupervision (CSCS), to reduce the inconsistency of multimodal predictions by\nresponse distillation. Particularly, transferring the 3D backbone to downstream\ntasks not only improves the performance of the 3D detectors, but also\noutperforms fully supervised instance segmentation with only 5% fully\nsupervised annotations. On the Waymo dataset, the proposed framework\ndemonstrates significant improvements over the baseline, especially achieving\n2.59% mAP and 12.75% mAP increases for 2D and 3D instance segmentation tasks,\nrespectively. The code is available at\nhttps://github.com/jiangxb98/mwsis-plugin.\n","authors":["Guangfeng Jiang","Jun Liu","Yuzhi Wu","Wenlong Liao","Tao He","Pai Peng"],"pdf_url":"https://arxiv.org/pdf/2312.06988v4.pdf","comment":"AAAI2024"},{"id":"http://arxiv.org/abs/2311.15145v2","updated":"2023-12-17T07:06:31Z","published":"2023-11-26T00:06:12Z","title":"Choosing Wisely and Learning Deeply: Selective Cross-Modality\n  Distillation via CLIP for Domain Generalization","summary":"  Domain Generalization (DG), a crucial research area, seeks to train models\nacross multiple domains and test them on unseen ones. In this paper, we\nintroduce a novel approach, namely, Selective Cross-Modality Distillation for\nDomain Generalization (SCMD). SCMD leverages the capabilities of large\nvision-language models, specifically the CLIP model, to train a more efficient\nmodel, ensuring it acquires robust generalization capabilities across unseen\ndomains. Our primary contribution is a unique selection framework strategically\ndesigned to identify hard-to-learn samples for distillation. In parallel, we\nintroduce a novel cross-modality module. This module seamlessly combines the\nprojected features of the student model with the text embeddings from CLIP,\nensuring the alignment of similarity distributions. We assess SCMD's\nperformance on various benchmarks, where it empowers a ResNet50 to deliver\nstate-of-the-art performance, surpassing existing domain generalization\nmethods. Furthermore, we provide a theoretical analysis of our selection\nstrategy, offering deeper insight into its effectiveness and potential in the\nfield of DG.\n","authors":["Jixuan Leng","Yijiang Li","Haohan Wang"],"pdf_url":"https://arxiv.org/pdf/2311.15145v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10628v1","updated":"2023-12-17T06:58:31Z","published":"2023-12-17T06:58:31Z","title":"T2M-HiFiGPT: Generating High Quality Human Motion from Textual\n  Descriptions with Residual Discrete Representations","summary":"  In this study, we introduce T2M-HiFiGPT, a novel conditional generative\nframework for synthesizing human motion from textual descriptions. This\nframework is underpinned by a Residual Vector Quantized Variational AutoEncoder\n(RVQ-VAE) and a double-tier Generative Pretrained Transformer (GPT)\narchitecture. We demonstrate that our CNN-based RVQ-VAE is capable of producing\nhighly accurate 2D temporal-residual discrete motion representations. Our\nproposed double-tier GPT structure comprises a temporal GPT and a residual GPT.\nThe temporal GPT efficiently condenses information from previous frames and\ntextual descriptions into a 1D context vector. This vector then serves as a\ncontext prompt for the residual GPT, which generates the final residual\ndiscrete indices. These indices are subsequently transformed back into motion\ndata by the RVQ-VAE decoder. To mitigate the exposure bias issue, we employ\nstraightforward code corruption techniques for RVQ and a conditional dropout\nstrategy, resulting in enhanced synthesis performance. Remarkably, T2M-HiFiGPT\nnot only simplifies the generative process but also surpasses existing methods\nin both performance and parameter efficacy, including the latest\ndiffusion-based and GPT-based models. On the HumanML3D and KIT-ML datasets, our\nframework achieves exceptional results across nearly all primary metrics. We\nfurther validate the efficacy of our framework through comprehensive ablation\nstudies on the HumanML3D dataset, examining the contribution of each component.\nOur findings reveal that RVQ-VAE is more adept at capturing precise 3D human\nmotion with comparable computational demand compared to its VQ-VAE\ncounterparts. As a result, T2M-HiFiGPT enables the generation of human motion\nwith significantly increased accuracy, outperforming recent state-of-the-art\napproaches such as T2M-GPT and Att-T2M.\n","authors":["Congyi Wang"],"pdf_url":"https://arxiv.org/pdf/2312.10628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05797v2","updated":"2023-12-17T06:57:36Z","published":"2023-12-10T07:12:15Z","title":"Multimodality in Online Education: A Comparative Study","summary":"  The commencement of the decade brought along with it a grave pandemic and in\nresponse the movement of education forums predominantly into the online world.\nWith a surge in the usage of online video conferencing platforms and tools to\nbetter gauge student understanding, there needs to be a mechanism to assess\nwhether instructors can grasp the extent to which students understand the\nsubject and their response to the educational stimuli. The current systems\nconsider only a single cue with a lack of focus in the educational domain.\nThus, there is a necessity for the measurement of an all-encompassing holistic\noverview of the students' reaction to the subject matter. This paper highlights\nthe need for a multimodal approach to affect recognition and its deployment in\nthe online classroom while considering four cues, posture and gesture, facial,\neye tracking and verbal recognition. It compares the various machine learning\nmodels available for each cue and provides the most suitable approach given the\navailable dataset and parameters of classroom footage. A multimodal approach\nderived from weighted majority voting is proposed by combining the most fitting\nmodels from this analysis of individual cues based on accuracy, ease of\nprocuring data corpus, sensitivity and any major drawbacks.\n","authors":["Praneeta Immadisetty","Pooja Rajesh","Akshita Gupta","Anala M R","Soumya A","K. N. Subramanya"],"pdf_url":"https://arxiv.org/pdf/2312.05797v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10616v1","updated":"2023-12-17T05:59:06Z","published":"2023-12-17T05:59:06Z","title":"DistilVPR: Cross-Modal Knowledge Distillation for Visual Place\n  Recognition","summary":"  The utilization of multi-modal sensor data in visual place recognition (VPR)\nhas demonstrated enhanced performance compared to single-modal counterparts.\nNonetheless, integrating additional sensors comes with elevated costs and may\nnot be feasible for systems that demand lightweight operation, thereby\nimpacting the practical deployment of VPR. To address this issue, we resort to\nknowledge distillation, which empowers single-modal students to learn from\ncross-modal teachers without introducing additional sensors during inference.\nDespite the notable advancements achieved by current distillation approaches,\nthe exploration of feature relationships remains an under-explored area. In\norder to tackle the challenge of cross-modal distillation in VPR, we present\nDistilVPR, a novel distillation pipeline for VPR. We propose leveraging feature\nrelationships from multiple agents, including self-agents and cross-agents for\nteacher and student neural networks. Furthermore, we integrate various\nmanifolds, characterized by different space curvatures for exploring feature\nrelationships. This approach enhances the diversity of feature relationships,\nincluding Euclidean, spherical, and hyperbolic relationship modules, thereby\nenhancing the overall representational capacity. The experiments demonstrate\nthat our proposed pipeline achieves state-of-the-art performance compared to\nother distillation baselines. We also conduct necessary ablation studies to\nshow design effectiveness. The code is released at:\nhttps://github.com/sijieaaa/DistilVPR\n","authors":["Sijie Wang","Rui She","Qiyu Kang","Xingchao Jian","Kai Zhao","Yang Song","Wee Peng Tay"],"pdf_url":"https://arxiv.org/pdf/2312.10616v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.10613v1","updated":"2023-12-17T05:30:35Z","published":"2023-12-17T05:30:35Z","title":"p-Laplacian Adaptation for Generative Pre-trained Vision-Language Models","summary":"  Vision-Language models (VLMs) pre-trained on large corpora have demonstrated\nnotable success across a range of downstream tasks. In light of the rapidly\nincreasing size of pre-trained VLMs, parameter-efficient transfer learning\n(PETL) has garnered attention as a viable alternative to full fine-tuning. One\nsuch approach is the adapter, which introduces a few trainable parameters into\nthe pre-trained models while preserving the original parameters during\nadaptation. In this paper, we present a novel modeling framework that recasts\nadapter tuning after attention as a graph message passing process on attention\ngraphs, where the projected query and value features and attention matrix\nconstitute the node features and the graph adjacency matrix, respectively.\nWithin this framework, tuning adapters in VLMs necessitates handling\nheterophilic graphs, owing to the disparity between the projected query and\nvalue space. To address this challenge, we propose a new adapter architecture,\n$p$-adapter, which employs $p$-Laplacian message passing in Graph Neural\nNetworks (GNNs). Specifically, the attention weights are re-normalized based on\nthe features, and the features are then aggregated using the calibrated\nattention matrix, enabling the dynamic exploitation of information with varying\nfrequencies in the heterophilic attention graphs. We conduct extensive\nexperiments on different pre-trained VLMs and multi-modal tasks, including\nvisual question answering, visual entailment, and image captioning. The\nexperimental results validate our method's significant superiority over other\nPETL methods.\n","authors":["Haoyuan Wu","Xinyun Zhang","Peng Xu","Peiyu Liao","Xufeng Yao","Bei Yu"],"pdf_url":"https://arxiv.org/pdf/2312.10613v1.pdf","comment":"Accepted by AAAI24. The first two authors contributed equally to this\n  paper"},{"id":"http://arxiv.org/abs/2312.10611v1","updated":"2023-12-17T05:27:31Z","published":"2023-12-17T05:27:31Z","title":"Bi-directional Adapter for Multi-modal Tracking","summary":"  Due to the rapid development of computer vision, single-modal (RGB) object\ntracking has made significant progress in recent years. Considering the\nlimitation of single imaging sensor, multi-modal images (RGB, Infrared, etc.)\nare introduced to compensate for this deficiency for all-weather object\ntracking in complex environments. However, as acquiring sufficient multi-modal\ntracking data is hard while the dominant modality changes with the open\nenvironment, most existing techniques fail to extract multi-modal complementary\ninformation dynamically, yielding unsatisfactory tracking performance. To\nhandle this problem, we propose a novel multi-modal visual prompt tracking\nmodel based on a universal bi-directional adapter, cross-prompting multiple\nmodalities mutually. Our model consists of a universal bi-directional adapter\nand multiple modality-specific transformer encoder branches with sharing\nparameters. The encoders extract features of each modality separately by using\na frozen pre-trained foundation model. We develop a simple but effective light\nfeature adapter to transfer modality-specific information from one modality to\nanother, performing visual feature prompt fusion in an adaptive manner. With\nadding fewer (0.32M) trainable parameters, our model achieves superior tracking\nperformance in comparison with both the full fine-tuning methods and the prompt\nlearning-based methods. Our code is available:\nhttps://github.com/SparkTempest/BAT.\n","authors":["Bing Cao","Junliang Guo","Pengfei Zhu","Qinghua Hu"],"pdf_url":"https://arxiv.org/pdf/2312.10611v1.pdf","comment":"Accepted by AAAI 2024. Code is available at\n  https://github.com/SparkTempest/BAT"},{"id":"http://arxiv.org/abs/2303.08977v2","updated":"2023-12-17T05:00:40Z","published":"2023-03-15T22:56:04Z","title":"DeblurSR: Event-Based Motion Deblurring Under the Spiking Representation","summary":"  We present DeblurSR, a novel motion deblurring approach that converts a\nblurry image into a sharp video. DeblurSR utilizes event data to compensate for\nmotion ambiguities and exploits the spiking representation to parameterize the\nsharp output video as a mapping from time to intensity. Our key contribution,\nthe Spiking Representation (SR), is inspired by the neuromorphic principles\ndetermining how biological neurons communicate with each other in living\norganisms. We discuss why the spikes can represent sharp edges and how the\nspiking parameters are interpreted from the neuromorphic perspective. DeblurSR\nhas higher output quality and requires fewer computing resources than\nstate-of-the-art event-based motion deblurring methods. We additionally show\nthat our approach easily extends to video super-resolution when combined with\nrecent advances in implicit neural representation. The implementation and\nanimated visualization of DeblurSR are available at\nhttps://github.com/chensong1995/DeblurSR.\n","authors":["Chen Song","Chandrajit Bajaj","Qixing Huang"],"pdf_url":"https://arxiv.org/pdf/2303.08977v2.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.10608v1","updated":"2023-12-17T04:50:24Z","published":"2023-12-17T04:50:24Z","title":"Robust 3D Tracking with Quality-Aware Shape Completion","summary":"  3D single object tracking remains a challenging problem due to the sparsity\nand incompleteness of the point clouds. Existing algorithms attempt to address\nthe challenges in two strategies. The first strategy is to learn dense\ngeometric features based on the captured sparse point cloud. Nevertheless, it\nis quite a formidable task since the learned dense geometric features are with\nhigh uncertainty for depicting the shape of the target object. The other\nstrategy is to aggregate the sparse geometric features of multiple templates to\nenrich the shape information, which is a routine solution in 2D tracking.\nHowever, aggregating the coarse shape representations can hardly yield a\nprecise shape representation. Different from 2D pixels, 3D points of different\nframes can be directly fused by coordinate transform, i.e., shape completion.\nConsidering that, we propose to construct a synthetic target representation\ncomposed of dense and complete point clouds depicting the target shape\nprecisely by shape completion for robust 3D tracking. Specifically, we design a\nvoxelized 3D tracking framework with shape completion, in which we propose a\nquality-aware shape completion mechanism to alleviate the adverse effect of\nnoisy historical predictions. It enables us to effectively construct and\nleverage the synthetic target representation. Besides, we also develop a\nvoxelized relation modeling module and box refinement module to improve\ntracking performance. Favorable performance against state-of-the-art algorithms\non three benchmarks demonstrates the effectiveness and generalization ability\nof our method.\n","authors":["Jingwen Zhang","Zikun Zhou","Guangming Lu","Jiandong Tian","Wenjie Pei"],"pdf_url":"https://arxiv.org/pdf/2312.10608v1.pdf","comment":"A detailed version of the paper accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2307.03856v2","updated":"2023-12-17T04:49:55Z","published":"2023-07-07T22:30:24Z","title":"Novel Categories Discovery Via Constraints on Empirical Prediction\n  Statistics","summary":"  Novel Categories Discovery (NCD) aims to cluster novel data based on the\nclass semantics of known classes using the open-world partial class space\nannotated dataset. As an alternative to the traditional pseudo-labeling-based\napproaches, we leverage the connection between the data sampling and the\nprovided multinoulli (categorical) distribution of novel classes. We introduce\nconstraints on individual and collective statistics of predicted novel class\nprobabilities to implicitly achieve semantic-based clustering. More\nspecifically, we align the class neuron activation distributions under\nMonte-Carlo sampling of novel classes in large batches by matching their\nempirical first-order (mean) and second-order (covariance) statistics with the\nmultinoulli distribution of the labels while applying instance information\nconstraints and prediction consistency under label-preserving augmentations. We\nthen explore a directional statistics-based probability formation that learns\nthe mixture of Von Mises-Fisher distribution of class labels in a unit\nhypersphere. We demonstrate the discriminative ability of our approach to\nrealize semantic clustering of novel samples in image, video, and time-series\nmodalities. We perform extensive ablation studies regarding data, networks, and\nframework components to provide better insights. Our approach maintains 94%,\n93%, 85%, and 93% (approx.) classification accuracy in labeled data while\nachieving 90%, 84%, 72%, and 75% (approx.) clustering accuracy for novel\ncategories in Cifar10, UCF101, MPSC-ARL, and SHAR datasets that match\nstate-of-the-art approaches without any external clustering.\n","authors":["Zahid Hasan","Abu Zaher Md Faridee","Masud Ahmed","Sanjay Purushotham","Heesung Kwon","Hyungtae Lee","Nirmalya Roy"],"pdf_url":"https://arxiv.org/pdf/2307.03856v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10604v1","updated":"2023-12-17T04:45:15Z","published":"2023-12-17T04:45:15Z","title":"A Dual Domain Multi-exposure Image Fusion Network based on the\n  Spatial-Frequency Integration","summary":"  Multi-exposure image fusion aims to generate a single high-dynamic image by\nintegrating images with different exposures. Existing deep learning-based\nmulti-exposure image fusion methods primarily focus on spatial domain fusion,\nneglecting the global modeling ability of the frequency domain. To effectively\nleverage the global illumination modeling ability of the frequency domain, we\npropose a novelty perspective on multi-exposure image fusion via the\nSpatial-Frequency Integration Framework, named MEF-SFI. Initially, we revisit\nthe properties of the Fourier transform on the 2D image, and verify the\nfeasibility of multi-exposure image fusion on the frequency domain where the\namplitude and phase component is able to guide the integration of the\nillumination information. Subsequently, we present the deep Fourier-based\nmulti-exposure image fusion framework, which consists of a spatial path and\nfrequency path for local and global modeling separately. Specifically, we\nintroduce a Spatial-Frequency Fusion Block to facilitate efficient interaction\nbetween dual domains and capture complementary information from input images\nwith different exposures. Finally, we combine a dual domain loss function to\nensure the retention of complementary information in both the spatial and\nfrequency domains. Extensive experiments on the PQA-MEF dataset demonstrate\nthat our method achieves visual-appealing fusion results against\nstate-of-the-art multi-exposure image fusion approaches. Our code is available\nat https://github.com/SSyangguang/MEF-freq.\n","authors":["Guang Yang","Jie Li","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2312.10604v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10602v1","updated":"2023-12-17T04:41:07Z","published":"2023-12-17T04:41:07Z","title":"A Weighted K-Center Algorithm for Data Subset Selection","summary":"  The success of deep learning hinges on enormous data and large models, which\nrequire labor-intensive annotations and heavy computation costs. Subset\nselection is a fundamental problem that can play a key role in identifying\nsmaller portions of the training data, which can then be used to produce\nsimilar models as the ones trained with full data. Two prior methods are shown\nto achieve impressive results: (1) margin sampling that focuses on selecting\npoints with high uncertainty, and (2) core-sets or clustering methods such as\nk-center for informative and diverse subsets. We are not aware of any work that\ncombines these methods in a principled manner. To this end, we develop a novel\nand efficient factor 3-approximation algorithm to compute subsets based on the\nweighted sum of both k-center and uncertainty sampling objective functions. To\nhandle large datasets, we show a parallel algorithm to run on multiple machines\nwith approximation guarantees. The proposed algorithm achieves similar or\nbetter performance compared to other strong baselines on vision datasets such\nas CIFAR-10, CIFAR-100, and ImageNet.\n","authors":["Srikumar Ramalingam","Pranjal Awasthi","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2312.10602v1.pdf","comment":"data selection, k-center, subset selection,"},{"id":"http://arxiv.org/abs/2312.10600v1","updated":"2023-12-17T04:26:42Z","published":"2023-12-17T04:26:42Z","title":"Cut your annotation cost: An empirical study on the use of weak, noisy,\n  and SAM-generated annotations for segmentation network training","summary":"  Deep neural networks (DNNs) have been deployed for many image segmentation\ntasks and achieved outstanding performance. However, preparing a dataset for\ntraining segmentation DNNs is laborious and costly since typically pixel-level\nannotations are provided for each object of interest. To alleviate this issue,\none can provide only weak labels such as bounding boxes or scribbles, or less\naccurate (noisy) annotations of the objects. These are significantly faster to\ngenerate and thus result in more annotated images given the same time budget.\nHowever, the reduction in quality might negatively affect the segmentation\nperformance of the resulting model. In this study, we perform a thorough\ncost-effectiveness evaluation of several weak and noisy labels. We considered\n11 variants of annotation strategies and 4 datasets. We conclude that the\ncommon practice of accurately outlining the objects of interest is virtually\nnever the optimal approach when the annotation time is limited, even if notable\nannotation time is available (10s of hours). Annotation approaches that stood\nout in such scenarios were (1) polygon-based annotation with few vertices, and\n(2) box annotations combined with the Segment Anything Model (SAM). In\nsituations where unlimited annotation time was available, precise annotations\nstill lead to the highest segmentation model performance.\n","authors":["Yixin Zhang","Shen Zhao","Hanxue Gu","Maciej A. Mazurowski"],"pdf_url":"https://arxiv.org/pdf/2312.10600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01597v2","updated":"2023-12-17T04:10:33Z","published":"2023-12-04T03:18:46Z","title":"SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference","summary":"  Recent advances in contrastive language-image pretraining (CLIP) have\ndemonstrated strong capabilities in zero-shot classification by aligning visual\nrepresentations with target text embeddings in an image level. However, in\ndense prediction tasks, CLIP often struggles to localize visual features within\nan image and fails to give accurate pixel-level predictions, which prevents it\nfrom functioning as a generalized visual foundation model. In this work, we aim\nto enhance CLIP's potential for semantic segmentation with minimal\nmodifications to its pretrained models. By rethinking self-attention, we\nsurprisingly find that CLIP can adapt to dense prediction tasks by simply\nintroducing a novel Correlative Self-Attention (CSA) mechanism. Specifically,\nwe replace the traditional self-attention block of CLIP vision encoder's last\nlayer by our CSA module and reuse its pretrained projection matrices of query,\nkey, and value, leading to a training-free adaptation approach for CLIP's\nzero-shot semantic segmentation. Extensive experiments show the advantage of\nCSA: we obtain a 38.2% average zero-shot mIoU across eight semantic\nsegmentation benchmarks highlighted in this paper, significantly outperforming\nthe existing SoTA's 33.9% and the vanilla CLIP's 14.1%.\n","authors":["Feng Wang","Jieru Mei","Alan Yuille"],"pdf_url":"https://arxiv.org/pdf/2312.01597v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00242v2","updated":"2023-12-17T03:18:58Z","published":"2023-04-01T06:39:58Z","title":"GLT-T++: Global-Local Transformer for 3D Siamese Tracking with Ranking\n  Loss","summary":"  Siamese trackers based on 3D region proposal network (RPN) have shown\nremarkable success with deep Hough voting. However, using a single seed point\nfeature as the cue for voting fails to produce high-quality 3D proposals.\nAdditionally, the equal treatment of seed points in the voting process,\nregardless of their significance, exacerbates this limitation. To address these\nchallenges, we propose a novel transformer-based voting scheme to generate\nbetter proposals. Specifically, a global-local transformer (GLT) module is\ndevised to integrate object- and patch-aware geometric priors into seed point\nfeatures, resulting in robust and accurate cues for offset learning of seed\npoints. To train the GLT module, we introduce an importance prediction branch\nthat learns the potential importance weights of seed points as a training\nconstraint. Incorporating this transformer-based voting scheme into 3D RPN, a\nnovel Siamese method dubbed GLT-T is developed for 3D single object tracking on\npoint clouds. Moreover, we identify that the highest-scored proposal in the\nSiamese paradigm may not be the most accurate proposal, which limits tracking\nperformance. Towards this concern, we approach the binary score prediction task\nas a ranking problem, and design a target-aware ranking loss and a\nlocalization-aware ranking loss to produce accurate ranking of proposals. With\nthe ranking losses, we further present GLT-T++, an enhanced version of GLT-T.\nExtensive experiments on multiple benchmarks demonstrate that our GLT-T and\nGLT-T++ outperform state-of-the-art methods in terms of tracking accuracy while\nmaintaining a real-time inference speed. The source code will be made available\nat https://github.com/haooozi/GLT-T.\n","authors":["Jiahao Nie","Zhiwei He","Yuxiang Yang","Xudong Lv","Mingyu Gao","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.00242v2.pdf","comment":"Need further revision"},{"id":"http://arxiv.org/abs/2312.10588v1","updated":"2023-12-17T02:31:20Z","published":"2023-12-17T02:31:20Z","title":"Post-Training Quantization for Re-parameterization via Coarse & Fine\n  Weight Splitting","summary":"  Although neural networks have made remarkable advancements in various\napplications, they require substantial computational and memory resources.\nNetwork quantization is a powerful technique to compress neural networks,\nallowing for more efficient and scalable AI deployments. Recently,\nRe-parameterization has emerged as a promising technique to enhance model\nperformance while simultaneously alleviating the computational burden in\nvarious computer vision tasks. However, the accuracy drops significantly when\napplying quantization on the re-parameterized networks. We identify that the\nprimary challenge arises from the large variation in weight distribution across\nthe original branches. To address this issue, we propose a coarse & fine weight\nsplitting (CFWS) method to reduce quantization error of weight, and develop an\nimproved KL metric to determine optimal quantization scales for activation. To\nthe best of our knowledge, our approach is the first work that enables\npost-training quantization applicable on re-parameterized networks. For\nexample, the quantized RepVGG-A1 model exhibits a mere 0.3% accuracy loss. The\ncode is in https://github.com/NeonHo/Coarse-Fine-Weight-Split.git\n","authors":["Dawei Yang","Ning He","Xing Hu","Zhihang Yuan","Jiangyong Yu","Chen Xu","Zhe Jiang"],"pdf_url":"https://arxiv.org/pdf/2312.10588v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2309.16899v3","updated":"2023-12-17T02:19:57Z","published":"2023-09-28T23:58:02Z","title":"On the Contractivity of Plug-and-Play Operators","summary":"  In plug-and-play (PnP) regularization, the proximal operator in algorithms\nsuch as ISTA and ADMM is replaced by a powerful denoiser. This formal\nsubstitution works surprisingly well in practice. In fact, PnP has been shown\nto give state-of-the-art results for various imaging applications. The\nempirical success of PnP has motivated researchers to understand its\ntheoretical underpinnings and, in particular, its convergence. It was shown in\nprior work that for kernel denoisers such as the nonlocal means, PnP-ISTA\nprovably converges under some strong assumptions on the forward model. The\npresent work is motivated by the following questions: Can we relax the\nassumptions on the forward model? Can the convergence analysis be extended to\nPnP-ADMM? Can we estimate the convergence rate? In this letter, we resolve\nthese questions using the contraction mapping theorem: (i) for symmetric\ndenoisers, we show that (under mild conditions) PnP-ISTA and PnP-ADMM exhibit\nlinear convergence; and (ii) for kernel denoisers, we show that PnP-ISTA and\nPnP-ADMM converge linearly for image inpainting. We validate our theoretical\nfindings using reconstruction experiments.\n","authors":["Chirayu D. Athalye","Kunal N. Chaudhury","Bhartendu Kumar"],"pdf_url":"https://arxiv.org/pdf/2309.16899v3.pdf","comment":"Errors in the proof of Lemma 1 and the statement of Theorem 2 were\n  identified after the publication; these have been rectified in the revised\n  version (v2)"},{"id":"http://arxiv.org/abs/2312.06372v2","updated":"2023-12-17T02:19:42Z","published":"2023-12-11T13:28:54Z","title":"Ternary Spike: Learning Ternary Spikes for Spiking Neural Networks","summary":"  The Spiking Neural Network (SNN), as one of the biologically inspired neural\nnetwork infrastructures, has drawn increasing attention recently. It adopts\nbinary spike activations to transmit information, thus the multiplications of\nactivations and weights can be substituted by additions, which brings high\nenergy efficiency. However, in the paper, we theoretically and experimentally\nprove that the binary spike activation map cannot carry enough information,\nthus causing information loss and resulting in accuracy decreasing. To handle\nthe problem, we propose a ternary spike neuron to transmit information. The\nternary spike neuron can also enjoy the event-driven and multiplication-free\noperation advantages of the binary spike neuron but will boost the information\ncapacity. Furthermore, we also embed a trainable factor in the ternary spike\nneuron to learn the suitable spike amplitude, thus our SNN will adopt different\nspike amplitudes along layers, which can better suit the phenomenon that the\nmembrane potential distributions are different along layers. To retain the\nefficiency of the vanilla ternary spike, the trainable ternary spike SNN will\nbe converted to a standard one again via a re-parameterization technique in the\ninference. Extensive experiments with several popular network structures over\nstatic and dynamic datasets show that the ternary spike can consistently\noutperform state-of-the-art methods. Our code is open-sourced at\nhttps://github.com/yfguo91/Ternary-Spike.\n","authors":["Yufei Guo","Yuanpei Chen","Xiaode Liu","Weihang Peng","Yuhan Zhang","Xuhui Huang","Zhe Ma"],"pdf_url":"https://arxiv.org/pdf/2312.06372v2.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2312.10586v1","updated":"2023-12-17T02:18:10Z","published":"2023-12-17T02:18:10Z","title":"Few-Shot Learning from Augmented Label-Uncertain Queries in Bongard-HOI","summary":"  Detecting human-object interactions (HOI) in a few-shot setting remains a\nchallenge. Existing meta-learning methods struggle to extract representative\nfeatures for classification due to the limited data, while existing few-shot\nHOI models rely on HOI text labels for classification. Moreover, some query\nimages may display visual similarity to those outside their class, such as\nsimilar backgrounds between different HOI classes. This makes learning more\nchallenging, especially with limited samples. Bongard-HOI (Jiang et al. 2022)\nepitomizes this HOI few-shot problem, making it the benchmark we focus on in\nthis paper. In our proposed method, we introduce novel label-uncertain query\naugmentation techniques to enhance the diversity of the query inputs, aiming to\ndistinguish the positive HOI class from the negative ones. As these augmented\ninputs may or may not have the same class label as the original inputs, their\nclass label is unknown. Those belonging to a different class become hard\nsamples due to their visual similarity to the original ones. Additionally, we\nintroduce a novel pseudo-label generation technique that enables a mean teacher\nmodel to learn from the augmented label-uncertain inputs. We propose to augment\nthe negative support set for the student model to enrich the semantic\ninformation, fostering diversity that challenges and enhances the student's\nlearning. Experimental results demonstrate that our method sets a new\nstate-of-the-art (SOTA) performance by achieving 68.74% accuracy on the\nBongard-HOI benchmark, a significant improvement over the existing SOTA of\n66.59%. In our evaluation on HICO-FS, a more general few-shot recognition\ndataset, our method achieves 73.27% accuracy, outperforming the previous SOTA\nof 71.20% in the 5-way 5-shot task.\n","authors":["Qinqian Lei","Bo Wang","Robby T. Tan"],"pdf_url":"https://arxiv.org/pdf/2312.10586v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2312.10585v1","updated":"2023-12-17T02:15:49Z","published":"2023-12-17T02:15:49Z","title":"ESDMR-Net: A Lightweight Network With Expand-Squeeze and Dual Multiscale\n  Residual Connections for Medical Image Segmentation","summary":"  Segmentation is an important task in a wide range of computer vision\napplications, including medical image analysis. Recent years have seen an\nincrease in the complexity of medical image segmentation approaches based on\nsophisticated convolutional neural network architectures. This progress has led\nto incremental enhancements in performance on widely recognised benchmark\ndatasets. However, most of the existing approaches are computationally\ndemanding, which limits their practical applicability. This paper presents an\nexpand-squeeze dual multiscale residual network (ESDMR-Net), which is a fully\nconvolutional network that is particularly well-suited for resource-constrained\ncomputing hardware such as mobile devices. ESDMR-Net focuses on extracting\nmultiscale features, enabling the learning of contextual dependencies among\nsemantically distinct features. The ESDMR-Net architecture allows dual-stream\ninformation flow within encoder-decoder pairs. The expansion operation\n(depthwise separable convolution) makes all of the rich features with\nmultiscale information available to the squeeze operation (bottleneck layer),\nwhich then extracts the necessary information for the segmentation task. The\nExpand-Squeeze (ES) block helps the network pay more attention to\nunder-represented classes, which contributes to improved segmentation accuracy.\nTo enhance the flow of information across multiple resolutions or scales, we\nintegrated dual multiscale residual (DMR) blocks into the skip connection. This\nintegration enables the decoder to access features from various levels of\nabstraction, ultimately resulting in more comprehensive feature\nrepresentations. We present experiments on seven datasets from five distinct\nexamples of applications. Our model achieved the best results despite having\nsignificantly fewer trainable parameters, with a reduction of two or even three\norders of magnitude.\n","authors":["Tariq M Khan","Syed S. Naqvi","Erik Meijering"],"pdf_url":"https://arxiv.org/pdf/2312.10585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.12153v3","updated":"2023-12-17T02:12:46Z","published":"2023-06-21T10:03:56Z","title":"DIAS: A Dataset and Benchmark for Intracranial Artery Segmentation in\n  DSA sequences","summary":"  Digital subtraction angiography (DSA) is universally acknowledged as the gold\nstandard for examining lesion angioarchitecture, elucidating arterial blood\nsupply dynamics, and guiding endovascular interventions. The automatic\nsegmentation of intracranial arteries (IA) in DSA, which is pivotal for\nquantifying vascular morphology, plays an essential role in computer-assisted\nstroke research and clinical practices. Nevertheless, research in this specific\ndomain remains constrained, primarily owing to the unavailability of publicly\ndatasets for IA segmentation within the research community. Currently, the\npredominant focus of methodologies lies in the segmentation of single-frame DSA\nusing in-house datasets. These methods, limited by the partial inclusion of\ncontrast in single-frame DSA, encounters challenges in rendering a precise\nrepresentation of vascular structures. In this paper, we introduces DIAS, a\ndataset specifically developed for IA segmentation in DSA sequences. A\ncomprehensive benchmark has been established for evaluating DIAS, covering\nfully, weakly, and semi-supervised segmentation methods. Specifically, we\npropose a vessel sequence segmentation network that captures the spatiotemporal\nrepresentation of intravascular contrast for segmenting vessels in DSA\nsequences. For weakly-supervised learning, we propose a novel scribble\nlearning-based image segmentation framework, incorporating both scribble\nsupervision and consistency regularization. Furthermore, we introduce a random\npatch-based self-training framework that harnesses unlabeled DSA sequences to\nimprove segmentation performance. Our extensive experiments on the DIAS dataset\ndemonstrate the effectiveness of these methods as potential baselines for\nfuture research and clinical applications.\n","authors":["Wentao Liu","Tong Tian","Lemeng Wang","Weijin Xu","Lei Li","Haoyuan Li","Wenyi Zhao","Siyu Tian","Xipeng Pan","Huihua Yang","Feng Gao","Yiming Deng","Ruisheng Su"],"pdf_url":"https://arxiv.org/pdf/2306.12153v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10578v1","updated":"2023-12-17T01:44:29Z","published":"2023-12-17T01:44:29Z","title":"SAME: Sample Reconstruction Against Model Extraction Attacks","summary":"  While deep learning models have shown significant performance across various\ndomains, their deployment needs extensive resources and advanced computing\ninfrastructure. As a solution, Machine Learning as a Service (MLaaS) has\nemerged, lowering the barriers for users to release or productize their deep\nlearning models. However, previous studies have highlighted potential privacy\nand security concerns associated with MLaaS, and one primary threat is model\nextraction attacks. To address this, there are many defense solutions but they\nsuffer from unrealistic assumptions and generalization issues, making them less\npractical for reliable protection. Driven by these limitations, we introduce a\nnovel defense mechanism, SAME, based on the concept of sample reconstruction.\nThis strategy imposes minimal prerequisites on the defender's capabilities,\neliminating the need for auxiliary Out-of-Distribution (OOD) datasets, user\nquery history, white-box model access, and additional intervention during model\ntraining. It is compatible with existing active defense methods. Our extensive\nexperiments corroborate the superior efficacy of SAME over state-of-the-art\nsolutions. Our code is available at https://github.com/xythink/SAME.\n","authors":["Yi Xie","Jie Zhang","Shiqian Zhao","Tianwei Zhang","Xiaofeng Chen"],"pdf_url":"https://arxiv.org/pdf/2312.10578v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2209.02178v2","updated":"2023-12-17T01:31:57Z","published":"2022-09-06T02:11:08Z","title":"Transformer-CNN Cohort: Semi-supervised Semantic Segmentation by the\n  Best of Both Students","summary":"  The popular methods for semi-supervised semantic segmentation mostly adopt a\nunitary network model using convolutional neural networks (CNNs) and enforce\nconsistency of the model's predictions over perturbations applied to the inputs\nor model. However, such a learning paradigm suffers from two critical\nlimitations: a) learning the discriminative features for the unlabeled data; b)\nlearning both global and local information from the whole image. In this paper,\nwe propose a novel Semi-supervised Learning (SSL) approach, called\nTransformer-CNN Cohort (TCC), that consists of two students with one based on\nthe vision transformer (ViT) and the other based on the CNN. Our method subtly\nincorporates the multi-level consistency regularization on the predictions and\nthe heterogeneous feature spaces via pseudo-labeling for the unlabeled data.\nFirst, as the inputs of the ViT student are image patches, the feature maps\nextracted encode crucial class-wise statistics. To this end, we propose\nclass-aware feature consistency distillation (CFCD) that first leverages the\noutputs of each student as the pseudo labels and generates class-aware feature\n(CF) maps for knowledge transfer between the two students. Second, as the ViT\nstudent has more uniform representations for all layers, we propose\nconsistency-aware cross distillation (CCD) to transfer knowledge between the\npixel-wise predictions from the cohort. We validate the TCC framework on\nCityscapes and Pascal VOC 2012 datasets, which outperforms existing SSL methods\nby a large margin.\n","authors":["Xu Zheng","Yunhao Luo","Chong Fu","Kangcheng Liu","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2209.02178v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.19629v2","updated":"2023-12-17T01:19:13Z","published":"2023-10-30T15:22:50Z","title":"RayDF: Neural Ray-surface Distance Fields with Multi-view Consistency","summary":"  In this paper, we study the problem of continuous 3D shape representations.\nThe majority of existing successful methods are coordinate-based implicit\nneural representations. However, they are inefficient to render novel views or\nrecover explicit surface points. A few works start to formulate 3D shapes as\nray-based neural functions, but the learned structures are inferior due to the\nlack of multi-view geometry consistency. To tackle these challenges, we propose\na new framework called RayDF. It consists of three major components: 1) the\nsimple ray-surface distance field, 2) the novel dual-ray visibility classifier,\nand 3) a multi-view consistency optimization module to drive the learned\nray-surface distances to be multi-view geometry consistent. We extensively\nevaluate our method on three public datasets, demonstrating remarkable\nperformance in 3D surface point reconstruction on both synthetic and\nchallenging real-world 3D scenes, clearly surpassing existing coordinate-based\nand ray-based baselines. Most notably, our method achieves a 1000x faster speed\nthan coordinate-based methods to render an 800x800 depth image, showing the\nsuperiority of our method for 3D shape representation. Our code and data are\navailable at https://github.com/vLAR-group/RayDF\n","authors":["Zhuoman Liu","Bo Yang","Yan Luximon","Ajay Kumar","Jinxi Li"],"pdf_url":"https://arxiv.org/pdf/2310.19629v2.pdf","comment":"Added the last 3 authors in the camera-ready version. NeurIPS 2023.\n  Code and data are available at: https://github.com/vLAR-group/RayDF"},{"id":"http://arxiv.org/abs/2312.10571v1","updated":"2023-12-17T00:47:13Z","published":"2023-12-17T00:47:13Z","title":"Multi-level Reasoning for Robotic Assembly: From Sequence Inference to\n  Contact Selection","summary":"  Automating the assembly of objects from their parts is a complex problem with\ninnumerable applications in manufacturing, maintenance, and recycling. Unlike\nexisting research, which is limited to target segmentation, pose regression, or\nusing fixed target blueprints, our work presents a holistic multi-level\nframework for part assembly planning consisting of part assembly sequence\ninference, part motion planning, and robot contact optimization. We present the\nPart Assembly Sequence Transformer (PAST) -- a sequence-to-sequence neural\nnetwork -- to infer assembly sequences recursively from a target blueprint. We\nthen use a motion planner and optimization to generate part movements and\ncontacts. To train PAST, we introduce D4PAS: a large-scale Dataset for Part\nAssembly Sequences (D4PAS) consisting of physically valid sequences for\nindustrial objects. Experimental results show that our approach generalizes\nbetter than prior methods while needing significantly less computational time\nfor inference.\n","authors":["Xinghao Zhu","Devesh K. Jha","Diego Romeres","Lingfeng Sun","Masayoshi Tomizuka","Anoop Cherian"],"pdf_url":"https://arxiv.org/pdf/2312.10571v1.pdf","comment":"Supplementary video is available at\n  https://www.youtube.com/watch?v=XNYkWSHkAaU&ab_channel=MitsubishiElectricResearchLabs%28MERL%29"},{"id":"http://arxiv.org/abs/2312.10568v1","updated":"2023-12-17T00:29:25Z","published":"2023-12-17T00:29:25Z","title":"IntraSeismic: a coordinate-based learning approach to seismic inversion","summary":"  Seismic imaging is the numerical process of creating a volumetric\nrepresentation of the subsurface geological structures from elastic waves\nrecorded at the surface of the Earth. As such, it is widely utilized in the\nenergy and construction sectors for applications ranging from oil and gas\nprospection, to geothermal production and carbon capture and storage\nmonitoring, to geotechnical assessment of infrastructures. Extracting\nquantitative information from seismic recordings, such as an acoustic impedance\nmodel, is however a highly ill-posed inverse problem, due to the band-limited\nand noisy nature of the data. This paper introduces IntraSeismic, a novel\nhybrid seismic inversion method that seamlessly combines coordinate-based\nlearning with the physics of the post-stack modeling operator. Key features of\nIntraSeismic are i) unparalleled performance in 2D and 3D post-stack seismic\ninversion, ii) rapid convergence rates, iii) ability to seamlessly include hard\nconstraints (i.e., well data) and perform uncertainty quantification, and iv)\npotential data compression and fast randomized access to portions of the\ninverted model. Synthetic and field data applications of IntraSeismic are\npresented to validate the effectiveness of the proposed method.\n","authors":["Juan Romero","Wolfgang Heidrich","Nick Luiken","Matteo Ravasi"],"pdf_url":"https://arxiv.org/pdf/2312.10568v1.pdf","comment":"-"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2311.18550v2","updated":"2023-12-17T18:57:21Z","published":"2023-11-30T13:36:21Z","title":"Search Still Matters: Information Retrieval in the Era of Generative AI","summary":"  Objective: Information retrieval (IR, also known as search) systems are\nubiquitous in modern times. How does the emergence of generative artificial\nintelligence (AI), based on large language models (LLMs), fit into the IR\nprocess? Process: This perspective explores the use of generative AI in the\ncontext of the motivations, considerations, and outcomes of the IR process with\na focus on the academic use of such systems. Conclusions: There are many\ninformation needs, from simple to complex, that motivate use of IR. Users of\nsuch systems, particularly academics, have concerns for authoritativeness,\ntimeliness, and contextualization of search. While LLMs may provide\nfunctionality that aids the IR process, the continued need for search systems,\nand research into their improvement, remains essential.\n","authors":["William R. Hersh"],"pdf_url":"https://arxiv.org/pdf/2311.18550v2.pdf","comment":"7 pages, no figures"},{"id":"http://arxiv.org/abs/2312.10743v1","updated":"2023-12-17T15:28:06Z","published":"2023-12-17T15:28:06Z","title":"A Unified Framework for Multi-Domain CTR Prediction via Large Language\n  Models","summary":"  Click-Through Rate (CTR) prediction is a crucial task in online\nrecommendation platforms as it involves estimating the probability of user\nengagement with advertisements or items by clicking on them. Given the\navailability of various services like online shopping, ride-sharing, food\ndelivery, and professional services on commercial platforms, recommendation\nsystems in these platforms are required to make CTR predictions across multiple\ndomains rather than just a single domain. However, multi-domain click-through\nrate (MDCTR) prediction remains a challenging task in online recommendation due\nto the complex mutual influence between domains. Traditional MDCTR models\ntypically encode domains as discrete identifiers, ignoring rich semantic\ninformation underlying. Consequently, they can hardly generalize to new\ndomains. Besides, existing models can be easily dominated by some specific\ndomains, which results in significant performance drops in the other domains\n(\\ie the ``seesaw phenomenon``). In this paper, we propose a novel solution\nUni-CTR to address the above challenges. Uni-CTR leverages a backbone Large\nLanguage Model (LLM) to learn layer-wise semantic representations that capture\ncommonalities between domains. Uni-CTR also uses several domain-specific\nnetworks to capture the characteristics of each domain. Note that we design a\nmasked loss strategy so that these domain-specific networks are decoupled from\nbackbone LLM. This allows domain-specific networks to remain unchanged when\nincorporating new or removing domains, thereby enhancing the flexibility and\nscalability of the system significantly. Experimental results on three public\ndatasets show that Uni-CTR outperforms the state-of-the-art (SOTA) MDCTR models\nsignificantly. Furthermore, Uni-CTR demonstrates remarkable effectiveness in\nzero-shot prediction. We have applied Uni-CTR in industrial scenarios,\nconfirming its efficiency.\n","authors":["Zichuan Fu","Xiangyang Li","Chuhan Wu","Yichao Wang","Kuicai Dong","Xiangyu Zhao","Mengchen Zhao","Huifeng Guo","Ruiming Tang"],"pdf_url":"https://arxiv.org/pdf/2312.10743v1.pdf","comment":"Still being revised"},{"id":"http://arxiv.org/abs/2208.12063v2","updated":"2023-12-17T12:56:31Z","published":"2022-08-25T12:47:20Z","title":"Partial Matrix Completion","summary":"  The matrix completion problem aims to reconstruct a low-rank matrix based on\na revealed set of possibly noisy entries. Prior works consider completing the\nentire matrix with generalization error guarantees. However, the completion\naccuracy can be drastically different over different entries. This work\nestablishes a new framework of partial matrix completion, where the goal is to\nidentify a large subset of the entries that can be completed with high\nconfidence. We propose an efficient algorithm with the following provable\nguarantees. Given access to samples from an unknown and arbitrary distribution,\nit guarantees: (a) high accuracy over completed entries, and (b) high coverage\nof the underlying distribution. We also consider an online learning variant of\nthis problem, where we propose a low-regret algorithm based on iterative\ngradient updates. Preliminary empirical evaluations are included.\n","authors":["Elad Hazan","Adam Tauman Kalai","Varun Kanade","Clara Mohri","Y. Jennifer Sun"],"pdf_url":"https://arxiv.org/pdf/2208.12063v2.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2312.10661v1","updated":"2023-12-17T09:31:47Z","published":"2023-12-17T09:31:47Z","title":"Wikiformer: Pre-training with Structured Information of Wikipedia for\n  Ad-hoc Retrieval","summary":"  With the development of deep learning and natural language processing\ntechniques, pre-trained language models have been widely used to solve\ninformation retrieval (IR) problems. Benefiting from the pre-training and\nfine-tuning paradigm, these models achieve state-of-the-art performance. In\nprevious works, plain texts in Wikipedia have been widely used in the\npre-training stage. However, the rich structured information in Wikipedia, such\nas the titles, abstracts, hierarchical heading (multi-level title) structure,\nrelationship between articles, references, hyperlink structures, and the\nwriting organizations, has not been fully explored. In this paper, we devise\nfour pre-training objectives tailored for IR tasks based on the structured\nknowledge of Wikipedia. Compared to existing pre-training methods, our approach\ncan better capture the semantic knowledge in the training corpus by leveraging\nthe human-edited structured data from Wikipedia. Experimental results on\nmultiple IR benchmark datasets show the superior performance of our model in\nboth zero-shot and fine-tuning settings compared to existing strong retrieval\nbaselines. Besides, experimental results in biomedical and legal domains\ndemonstrate that our approach achieves better performance in vertical domains\ncompared to previous models, especially in scenarios where long text similarity\nmatching is needed.\n","authors":["Weihang Su","Qingyao Ai","Xiangsheng Li","Jia Chen","Yiqun Liu","Xiaolong Wu","Shengluan Hou"],"pdf_url":"https://arxiv.org/pdf/2312.10661v1.pdf","comment":"Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)"},{"id":"http://arxiv.org/abs/2312.10638v1","updated":"2023-12-17T07:39:07Z","published":"2023-12-17T07:39:07Z","title":"HyperPIE: Hyperparameter Information Extraction from Scientific\n  Publications","summary":"  Automatic extraction of information from publications is key to making\nscientific knowledge machine readable at a large scale. The extracted\ninformation can, for example, facilitate academic search, decision making, and\nknowledge graph construction. An important type of information not covered by\nexisting approaches is hyperparameters. In this paper, we formalize and tackle\nhyperparameter information extraction (HyperPIE) as an entity recognition and\nrelation extraction task. We create a labeled data set covering publications\nfrom a variety of computer science disciplines. Using this data set, we train\nand evaluate BERT-based fine-tuned models as well as five large language\nmodels: GPT-3.5, GALACTICA, Falcon, Vicuna, and WizardLM. For fine-tuned\nmodels, we develop a relation extraction approach that achieves an improvement\nof 29% F1 over a state-of-the-art baseline. For large language models, we\ndevelop an approach leveraging YAML output for structured data extraction,\nwhich achieves an average improvement of 5.5% F1 in entity recognition over\nusing JSON. With our best performing model we extract hyperparameter\ninformation from a large number of unannotated papers, and analyze patterns\nacross disciplines. All our data and source code is publicly available at\nhttps://github.com/IllDepence/hyperpie\n","authors":["Tarek Saier","Mayumi Ohta","Takuto Asakura","Michael Färber"],"pdf_url":"https://arxiv.org/pdf/2312.10638v1.pdf","comment":"accepted at ECIR2024"},{"id":"http://arxiv.org/abs/2308.11730v2","updated":"2023-12-17T07:21:57Z","published":"2023-08-22T18:41:31Z","title":"Knowledge Graph Prompting for Multi-Document Question Answering","summary":"  The `pre-train, prompt, predict' paradigm of large language models (LLMs) has\nachieved remarkable success in open-domain question answering (OD-QA). However,\nfew works explore this paradigm in the scenario of multi-document question\nanswering (MD-QA), a task demanding a thorough understanding of the logical\nassociations among the contents and structures of different documents. To fill\nthis crucial gap, we propose a Knowledge Graph Prompting (KGP) method to\nformulate the right context in prompting LLMs for MD-QA, which consists of a\ngraph construction module and a graph traversal module. For graph construction,\nwe create a knowledge graph (KG) over multiple documents with nodes symbolizing\npassages or document structures (e.g., pages/tables), and edges denoting the\nsemantic/lexical similarity between passages or intra-document structural\nrelations. For graph traversal, we design an LLM-based graph traversal agent\nthat navigates across nodes and gathers supporting passages assisting LLMs in\nMD-QA. The constructed graph serves as the global ruler that regulates the\ntransitional space among passages and reduces retrieval latency. Concurrently,\nthe graph traversal agent acts as a local navigator that gathers pertinent\ncontext to progressively approach the question and guarantee retrieval quality.\nExtensive experiments underscore the efficacy of KGP for MD-QA, signifying the\npotential of leveraging graphs in enhancing the prompt design for LLMs. Our\ncode: https://github.com/YuWVandy/KG-LLM-MDQA.\n","authors":["Yu Wang","Nedim Lipka","Ryan A. Rossi","Alexa Siu","Ruiyi Zhang","Tyler Derr"],"pdf_url":"https://arxiv.org/pdf/2308.11730v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10623v1","updated":"2023-12-17T06:39:10Z","published":"2023-12-17T06:39:10Z","title":"A Survey on Query-based API Recommendation","summary":"  Application Programming Interfaces (APIs) are designed to help developers\nbuild software more effectively. Recommending the right APIs for specific tasks\nhas gained increasing attention among researchers and developers in recent\nyears. To comprehensively understand this research domain, we have surveyed to\nanalyze API recommendation studies published in the last 10 years. Our study\nbegins with an overview of the structure of API recommendation tools.\nSubsequently, we systematically analyze prior research and pose four key\nresearch questions. For RQ1, we examine the volume of published papers and the\nvenues in which these papers appear within the API recommendation field. In\nRQ2, we categorize and summarize the prevalent data sources and collection\nmethods employed in API recommendation research. In RQ3, we explore the types\nof data and common data representations utilized by API recommendation\napproaches. We also investigate the typical data extraction procedures and\ncollection approaches employed by the existing approaches. RQ4 delves into the\nmodeling techniques employed by API recommendation approaches, encompassing\nboth statistical and deep learning models. Additionally, we compile an overview\nof the prevalent ranking strategies and evaluation metrics used for assessing\nAPI recommendation tools. Drawing from our survey findings, we identify current\nchallenges in API recommendation research that warrant further exploration,\nalong with potential avenues for future research.\n","authors":["Moshi Wei","Nima Shiri Harzevili","Alvine Boaye Belle","Junjie Wang","Lin Shi","Song Wang","Zhen Ming Jiang"],"pdf_url":"https://arxiv.org/pdf/2312.10623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08262v2","updated":"2023-12-17T06:05:22Z","published":"2022-12-16T03:13:43Z","title":"Uniform Sequence Better: Time Interval Aware Data Augmentation for\n  Sequential Recommendation","summary":"  Sequential recommendation is an important task to predict the next-item to\naccess based on a sequence of interacted items. Most existing works learn user\npreference as the transition pattern from the previous item to the next one,\nignoring the time interval between these two items. However, we observe that\nthe time interval in a sequence may vary significantly different, and thus\nresult in the ineffectiveness of user modeling due to the issue of\n\\emph{preference drift}. In fact, we conducted an empirical study to validate\nthis observation, and found that a sequence with uniformly distributed time\ninterval (denoted as uniform sequence) is more beneficial for performance\nimprovement than that with greatly varying time interval. Therefore, we propose\nto augment sequence data from the perspective of time interval, which is not\nstudied in the literature. Specifically, we design five operators (Ti-Crop,\nTi-Reorder, Ti-Mask, Ti-Substitute, Ti-Insert) to transform the original\nnon-uniform sequence to uniform sequence with the consideration of variance of\ntime intervals. Then, we devise a control strategy to execute data augmentation\non item sequences in different lengths. Finally, we implement these\nimprovements on a state-of-the-art model CoSeRec and validate our approach on\nfour real datasets. The experimental results show that our approach reaches\nsignificantly better performance than the other 11 competing methods. Our\nimplementation is available: https://github.com/KingGugu/TiCoSeRec.\n","authors":["Yizhou Dang","Enneng Yang","Guibing Guo","Linying Jiang","Xingwei Wang","Xiaoxiao Xu","Qinghui Sun","Hong Liu"],"pdf_url":"https://arxiv.org/pdf/2212.08262v2.pdf","comment":"9 pages, 4 figures, AAAI-2023"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2312.10842v1","updated":"2023-12-17T23:20:51Z","published":"2023-12-17T23:20:51Z","title":"Compositional Inductive Invariant Based Verification of Neural Network\n  Controlled Systems","summary":"  The integration of neural networks into safety-critical systems has shown\ngreat potential in recent years. However, the challenge of effectively\nverifying the safety of Neural Network Controlled Systems (NNCS) persists. This\npaper introduces a novel approach to NNCS safety verification, leveraging the\ninductive invariant method. Verifying the inductiveness of a candidate\ninductive invariant in the context of NNCS is hard because of the scale and\nnonlinearity of neural networks. Our compositional method makes this\nverification process manageable by decomposing the inductiveness proof\nobligation into smaller, more tractable subproblems. Alongside the high-level\nmethod, we present an algorithm capable of automatically verifying the\ninductiveness of given candidates by automatically inferring the necessary\ndecomposition predicates. The algorithm significantly outperforms the baseline\nmethod and shows remarkable reductions in execution time in our case studies,\nshortening the verification time from hours (or timeout) to seconds.\n","authors":["Yuhao Zhou","Stavros Tripakis"],"pdf_url":"https://arxiv.org/pdf/2312.10842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08579v2","updated":"2023-12-17T23:20:49Z","published":"2023-12-14T00:50:14Z","title":"Identifying Planetary Names in Astronomy Papers: A Multi-Step Approach","summary":"  The automatic identification of planetary feature names in astronomy\npublications presents numerous challenges. These features include craters,\ndefined as roughly circular depressions resulting from impact or volcanic\nactivity; dorsas, which are elongate raised structures or wrinkle ridges; and\nlacus, small irregular patches of dark, smooth material on the Moon, referred\nto as \"lake\" (Planetary Names Working Group, n.d.). Many feature names overlap\nwith places or people's names that they are named after, for example, Syria,\nTempe, Einstein, and Sagan, to name a few (U.S. Geological Survey, n.d.). Some\nfeature names have been used in many contexts, for instance, Apollo, which can\nrefer to mission, program, sample, astronaut, seismic, seismometers, core, era,\ndata, collection, instrument, and station, in addition to the crater on the\nMoon. Some feature names can appear in the text as adjectives, like the lunar\ncraters Black, Green, and White. Some feature names in other contexts serve as\ndirections, like craters West and South on the Moon. Additionally, some\nfeatures share identical names across different celestial bodies, requiring\ndisambiguation, such as the Adams crater, which exists on both the Moon and\nMars. We present a multi-step pipeline combining rule-based filtering,\nstatistical relevance analysis, part-of-speech (POS) tagging, named entity\nrecognition (NER) model, hybrid keyword harvesting, knowledge graph (KG)\nmatching, and inference with a locally installed large language model (LLM) to\nreliably identify planetary names despite these challenges. When evaluated on a\ndataset of astronomy papers from the Astrophysics Data System (ADS), this\nmethodology achieves an F1-score over 0.97 in disambiguating planetary feature\nnames.\n","authors":["Golnaz Shapurian","Michael J Kurtz","Alberto Accomazzi"],"pdf_url":"https://arxiv.org/pdf/2312.08579v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10841v1","updated":"2023-12-17T23:10:39Z","published":"2023-12-17T23:10:39Z","title":"Online Boosting Adaptive Learning under Concept Drift for Multistream\n  Classification","summary":"  Multistream classification poses significant challenges due to the necessity\nfor rapid adaptation in dynamic streaming processes with concept drift. Despite\nthe growing research outcomes in this area, there has been a notable oversight\nregarding the temporal dynamic relationships between these streams, leading to\nthe issue of negative transfer arising from irrelevant data. In this paper, we\npropose a novel Online Boosting Adaptive Learning (OBAL) method that\neffectively addresses this limitation by adaptively learning the dynamic\ncorrelation among different streams. Specifically, OBAL operates in a\ndual-phase mechanism, in the first of which we design an Adaptive COvariate\nShift Adaptation (AdaCOSA) algorithm to construct an initialized ensemble model\nusing archived data from various source streams, thus mitigating the covariate\nshift while learning the dynamic correlations via an adaptive re-weighting\nstrategy. During the online process, we employ a Gaussian Mixture Model-based\nweighting mechanism, which is seamlessly integrated with the acquired\ncorrelations via AdaCOSA to effectively handle asynchronous drift. This\napproach significantly improves the predictive performance and stability of the\ntarget stream. We conduct comprehensive experiments on several synthetic and\nreal-world data streams, encompassing various drifting scenarios and types. The\nresults clearly demonstrate that OBAL achieves remarkable advancements in\naddressing multistream classification problems by effectively leveraging\npositive knowledge derived from multiple sources.\n","authors":["En Yu","Jie Lu","Bin Zhang","Guangquan Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.10841v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2311.12813v2","updated":"2023-12-17T22:12:37Z","published":"2023-09-22T13:02:14Z","title":"Targeted Activation Penalties Help CNNs Ignore Spurious Signals","summary":"  Neural networks (NNs) can learn to rely on spurious signals in the training\ndata, leading to poor generalisation. Recent methods tackle this problem by\ntraining NNs with additional ground-truth annotations of such signals. These\nmethods may, however, let spurious signals re-emerge in deep convolutional NNs\n(CNNs). We propose Targeted Activation Penalty (TAP), a new method tackling the\nsame problem by penalising activations to control the re-emergence of spurious\nsignals in deep CNNs, while also lowering training times and memory usage. In\naddition, ground-truth annotations can be expensive to obtain. We show that TAP\nstill works well with annotations generated by pre-trained models as effective\nsubstitutes of ground-truth annotations. We demonstrate the power of TAP\nagainst two state-of-the-art baselines on the MNIST benchmark and on two\nclinical image datasets, using four different CNN architectures.\n","authors":["Dekai Zhang","Matthew Williams","Francesca Toni"],"pdf_url":"https://arxiv.org/pdf/2311.12813v2.pdf","comment":"24 pages including appendix; extended version of a paper accepted to\n  AAAI-2024 under the same title"},{"id":"http://arxiv.org/abs/2312.10825v1","updated":"2023-12-17T21:49:59Z","published":"2023-12-17T21:49:59Z","title":"Latent Space Editing in Transformer-Based Flow Matching","summary":"  This paper strives for image editing via generative models. Flow Matching is\nan emerging generative modeling technique that offers the advantage of simple\nand efficient training. Simultaneously, a new transformer-based U-ViT has\nrecently been proposed to replace the commonly used UNet for better scalability\nand performance in generative modeling. Hence, Flow Matching with a transformer\nbackbone offers the potential for scalable and high-quality generative\nmodeling, but their latent structure and editing ability are as of yet unknown.\nHence, we adopt this setting and explore how to edit images through latent\nspace manipulation. We introduce an editing space, which we call $u$-space,\nthat can be manipulated in a controllable, accumulative, and composable manner.\nAdditionally, we propose a tailored sampling solution to enable sampling with\nthe more efficient adaptive step-size ODE solvers. Lastly, we put forth a\nstraightforward yet powerful method for achieving fine-grained and nuanced\nediting using text prompts. Our framework is simple and efficient, all while\nbeing highly effective at editing images while preserving the essence of the\noriginal content. Our code will be publicly available at https://taohu.me/lfm/\n","authors":["Vincent Tao Hu","David W Zhang","Pascal Mettes","Meng Tang","Deli Zhao","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2312.10825v1.pdf","comment":"AAAI 2024 with Appendix"},{"id":"http://arxiv.org/abs/2311.07723v3","updated":"2023-12-17T21:18:14Z","published":"2023-11-13T20:07:36Z","title":"Generalization Analogies: A Testbed for Generalizing AI Oversight to\n  Hard-To-Measure Domains","summary":"  As AI systems become more intelligent and their behavior becomes more\nchallenging to assess, they may learn to game the flaws of human feedback\ninstead of genuinely striving to follow instructions; however, this risk can be\nmitigated by controlling how LLMs generalize human feedback to situations where\nit is unreliable. To better understand how reward models generalize, we craft\n69 distribution shifts spanning 8 categories. We find that reward models do not\nlearn to evaluate `instruction-following' by default and instead favor personas\nthat resemble internet text. Techniques for interpreting reward models'\ninternal representations achieve better generalization than standard\nfine-tuning, but still frequently fail to distinguish instruction-following\nfrom conflated behaviors. We consolidate the 15 most challenging distribution\nshifts into the GENeralization analogIES (GENIES) benchmark, which we hope will\nenable progress toward controlling reward model generalization.\n","authors":["Joshua Clymer","Garrett Baker","Rohan Subramani","Sam Wang"],"pdf_url":"https://arxiv.org/pdf/2311.07723v3.pdf","comment":"Code: https://github.com/Joshuaclymer/GENIES Website:\n  https://joshuaclymer.github.io/generalization-analogies-website/"},{"id":"http://arxiv.org/abs/2312.10817v1","updated":"2023-12-17T20:57:22Z","published":"2023-12-17T20:57:22Z","title":"Ocean Data Quality Assessment through Outlier Detection-enhanced Active\n  Learning","summary":"  Ocean and climate research benefits from global ocean observation initiatives\nsuch as Argo, GLOSS, and EMSO. The Argo network, dedicated to ocean profiling,\ngenerates a vast volume of observatory data. However, data quality issues from\nsensor malfunctions and transmission errors necessitate stringent quality\nassessment. Existing methods, including machine learning, fall short due to\nlimited labeled data and imbalanced datasets. To address these challenges, we\npropose an ODEAL framework for ocean data quality assessment, employing AL to\nreduce human experts' workload in the quality assessment workflow and\nleveraging outlier detection algorithms for effective model initialization. We\nalso conduct extensive experiments on five large-scale realistic Argo datasets\nto gain insights into our proposed method, including the effectiveness of AL\nquery strategies and the initial set construction approach. The results suggest\nthat our framework enhances quality assessment efficiency by up to 465.5% with\nthe uncertainty-based query strategy compared to random sampling and minimizes\noverall annotation costs by up to 76.9% using the initial set built with\noutlier detectors.\n","authors":["Na Li","Yiyang Qi","Ruyue Xin","Zhiming Zhao"],"pdf_url":"https://arxiv.org/pdf/2312.10817v1.pdf","comment":"2023 IEEE International Conference on Big Data (IEEE BigData 2023)"},{"id":"http://arxiv.org/abs/2312.10815v1","updated":"2023-12-17T20:53:37Z","published":"2023-12-17T20:53:37Z","title":"DePRL: Achieving Linear Convergence Speedup in Personalized\n  Decentralized Learning with Shared Representations","summary":"  Decentralized learning has emerged as an alternative method to the popular\nparameter-server framework which suffers from high communication burden,\nsingle-point failure and scalability issues due to the need of a central\nserver. However, most existing works focus on a single shared model for all\nworkers regardless of the data heterogeneity problem, rendering the resulting\nmodel performing poorly on individual workers. In this work, we propose a novel\npersonalized decentralized learning algorithm named DePRL via shared\nrepresentations. Our algorithm relies on ideas from representation learning\ntheory to learn a low-dimensional global representation collaboratively among\nall workers in a fully decentralized manner, and a user-specific\nlow-dimensional local head leading to a personalized solution for each worker.\nWe show that DePRL achieves, for the first time, a provable linear speedup for\nconvergence with general non-linear representations (i.e., the convergence rate\nis improved linearly with respect to the number of workers). Experimental\nresults support our theoretical findings showing the superiority of our method\nin data heterogeneous environments.\n","authors":["Guojun Xiong","Gang Yan","Shiqiang Wang","Jian Li"],"pdf_url":"https://arxiv.org/pdf/2312.10815v1.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2312.10813v1","updated":"2023-12-17T20:42:43Z","published":"2023-12-17T20:42:43Z","title":"Re-parameterized Low-rank Prompt: Generalize a Vision-Language Model\n  within 0.5K Parameters","summary":"  With the development of large pre-trained vision-language models, how to\neffectively transfer the knowledge of such foundational models to downstream\ntasks becomes a hot topic, especially in a data-deficient scenario. Recently,\nprompt tuning has become a popular solution. When adapting the vision-language\nmodels, researchers freeze the parameters in the backbone and only design and\ntune the prompts. On the one hand, the delicate design of prompt tuning\nexhibits strong performance. On the other hand, complicated structures and\nupdate rules largely increase the computation and storage cost. Motivated by\nthe observation that the evolution pattern of the generalization capability in\nvisual-language models aligns harmoniously with the trend of rank variations in\nthe prompt matrix during adaptation, we design a new type of prompt,\nRe-parameterized Low-rank Prompt (RLP), for both efficient and effective\nadaptation. Our method could largely reduce the number of tunable parameters\nand storage space, which is quite beneficial in resource-limited scenarios.\nExtensive experiments further demonstrate the superiority of RLP. In\nparticular, RLP shows comparable or even stronger performance than the latest\nstate-of-the-art methods with an extremely small number of parameters. On a\nseries of tasks over 11 datasets, RLP significantly increases the average\ndownstream accuracy of classic prompt tuning by up to 5.25% using merely 0.5K\nparameters.\n","authors":["Tianxiang Hao","Mengyao Lyu","Hui Chen","Sicheng Zhao","Jungong Han","Guiguang Ding"],"pdf_url":"https://arxiv.org/pdf/2312.10813v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10812v1","updated":"2023-12-17T20:39:54Z","published":"2023-12-17T20:39:54Z","title":"Learning to Act without Actions","summary":"  Pre-training large models on vast amounts of web data has proven to be an\neffective approach for obtaining powerful, general models in several domains,\nincluding language and vision. However, this paradigm has not yet taken hold in\ndeep reinforcement learning (RL). This gap is due to the fact that the most\nabundant form of embodied behavioral data on the web consists of videos, which\ndo not include the action labels required by existing methods for training\npolicies from offline data. We introduce Latent Action Policies from\nObservation (LAPO), a method to infer latent actions and, consequently,\nlatent-action policies purely from action-free demonstrations. Our experiments\non challenging procedurally-generated environments show that LAPO can act as an\neffective pre-training method to obtain RL policies that can then be rapidly\nfine-tuned to expert-level performance. Our approach serves as a key stepping\nstone to enabling the pre-training of powerful, generalist RL models on the\nvast amounts of action-free demonstrations readily available on the web.\n","authors":["Dominik Schmidt","Minqi Jiang"],"pdf_url":"https://arxiv.org/pdf/2312.10812v1.pdf","comment":"Under review at ICLR 2024"},{"id":"http://arxiv.org/abs/2305.17021v2","updated":"2023-12-17T20:26:53Z","published":"2023-05-26T15:26:59Z","title":"GLOBE-CE: A Translation-Based Approach for Global Counterfactual\n  Explanations","summary":"  Counterfactual explanations have been widely studied in explainability, with\na range of application dependent methods prominent in fairness, recourse and\nmodel understanding. The major shortcoming associated with these methods,\nhowever, is their inability to provide explanations beyond the local or\ninstance-level. While many works touch upon the notion of a global explanation,\ntypically suggesting to aggregate masses of local explanations in the hope of\nascertaining global properties, few provide frameworks that are both reliable\nand computationally tractable. Meanwhile, practitioners are requesting more\nefficient and interactive explainability tools. We take this opportunity to\npropose Global & Efficient Counterfactual Explanations (GLOBE-CE), a flexible\nframework that tackles the reliability and scalability issues associated with\ncurrent state-of-the-art, particularly on higher dimensional datasets and in\nthe presence of continuous features. Furthermore, we provide a unique\nmathematical analysis of categorical feature translations, utilising it in our\nmethod. Experimental evaluation with publicly available datasets and user\nstudies demonstrate that GLOBE-CE performs significantly better than the\ncurrent state-of-the-art across multiple metrics (e.g., speed, reliability).\n","authors":["Dan Ley","Saumitra Mishra","Daniele Magazzeni"],"pdf_url":"https://arxiv.org/pdf/2305.17021v2.pdf","comment":"Published as a conference paper at ICML 2023 (9 page main text, 3\n  page references, 16 page appendix)"},{"id":"http://arxiv.org/abs/2312.10809v1","updated":"2023-12-17T20:21:49Z","published":"2023-12-17T20:21:49Z","title":"Deep-Dispatch: A Deep Reinforcement Learning-Based Vehicle Dispatch\n  Algorithm for Advanced Air Mobility","summary":"  Near future air taxi operations with electric vertical take-off and landing\n(eVTOL) aircraft will be constrained by the need for frequent recharging of\neVTOLs, limited takeoff and landing pads in vertiports, and subject to\ntime-varying demand and electricity prices, making the eVTOL dispatch problem\nunique and particularly challenging to solve. Previously, we have developed\noptimization models to address this problem. Such optimization models however\nsuffer from prohibitively high computational run times when the scale of the\nproblem increases, making them less practical for real world implementation. To\novercome this issue, we have developed two deep reinforcement learning-based\neVTOL dispatch algorithms, namely single-agent and multi-agent deep Q-learning\neVTOL dispatch algorithms, where the objective is to maximize operating profit.\nAn eVTOL-based passenger transportation simulation environment was built to\nassess the performance of our algorithms across $36$ numerical cases with\nvarying number of eVTOLs, vertiports, and demand. The results indicate that the\nmulti-agent eVTOL dispatch algorithm can closely approximate the optimal\ndispatch policy with significantly less computational expenses compared to the\nbenchmark optimization model. The multi-agent algorithm was found to outperform\nthe single-agent counterpart with respect to both profits generated and\ntraining time.\n","authors":["Elaheh Sabziyan Varnousfaderani","Syed A. M. Shihab","Esrat F. Dulia"],"pdf_url":"https://arxiv.org/pdf/2312.10809v1.pdf","comment":"14 figures"},{"id":"http://arxiv.org/abs/2312.10808v1","updated":"2023-12-17T20:21:33Z","published":"2023-12-17T20:21:33Z","title":"Non-Euclidean Spatial Graph Neural Network","summary":"  Spatial networks are networks whose graph topology is constrained by their\nembedded spatial space. Understanding the coupled spatial-graph properties is\ncrucial for extracting powerful representations from spatial networks.\nTherefore, merely combining individual spatial and network representations\ncannot reveal the underlying interaction mechanism of spatial networks.\nBesides, existing spatial network representation learning methods can only\nconsider networks embedded in Euclidean space, and can not well exploit the\nrich geometric information carried by irregular and non-uniform non-Euclidean\nspace. In order to address this issue, in this paper we propose a novel generic\nframework to learn the representation of spatial networks that are embedded in\nnon-Euclidean manifold space. Specifically, a novel message-passing-based\nneural network is proposed to combine graph topology and spatial geometry,\nwhere spatial geometry is extracted as messages on the edges. We theoretically\nguarantee that the learned representations are provably invariant to important\nsymmetries such as rotation or translation, and simultaneously maintain\nsufficient ability in distinguishing different geometric structures. The\nstrength of our proposed method is demonstrated through extensive experiments\non both synthetic and real-world datasets.\n","authors":["Zheng Zhang","Sirui Li","Jingcheng Zhou","Junxiang Wang","Abhinav Angirekula","Allen Zhang","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2312.10808v1.pdf","comment":"Accepted by SDM 2024"},{"id":"http://arxiv.org/abs/2311.13648v2","updated":"2023-12-17T20:19:50Z","published":"2023-11-22T19:04:05Z","title":"Evaluating Pretrained models for Deployable Lifelong Learning","summary":"  We create a novel benchmark for evaluating a Deployable Lifelong Learning\nsystem for Visual Reinforcement Learning (RL) that is pretrained on a curated\ndataset, and propose a novel Scalable Lifelong Learning system capable of\nretaining knowledge from the previously learnt RL tasks. Our benchmark measures\nthe efficacy of a deployable Lifelong Learning system that is evaluated on\nscalability, performance and resource utilization. Our proposed system, once\npretrained on the dataset, can be deployed to perform continual learning on\nunseen tasks. Our proposed method consists of a Few Shot Class Incremental\nLearning (FSCIL) based task-mapper and an encoder/backbone trained entirely\nusing the pretrain dataset. The policy parameters corresponding to the\nrecognized task are then loaded to perform the task. We show that this system\ncan be scaled to incorporate a large number of tasks due to the small memory\nfootprint and fewer computational resources. We perform experiments on our DeLL\n(Deployment for Lifelong Learning) benchmark on the Atari games to determine\nthe efficacy of the system.\n","authors":["Kiran Lekkala","Eshan Bhargava","Yunhao Ge","Laurent Itti"],"pdf_url":"https://arxiv.org/pdf/2311.13648v2.pdf","comment":"In submission to CoLLA 2024. Also published in the Proceedings of\n  WACV 2024 Workshop on Pretraining"},{"id":"http://arxiv.org/abs/2312.10802v1","updated":"2023-12-17T19:47:49Z","published":"2023-12-17T19:47:49Z","title":"GO-DICE: Goal-Conditioned Option-Aware Offline Imitation Learning via\n  Stationary Distribution Correction Estimation","summary":"  Offline imitation learning (IL) refers to learning expert behavior solely\nfrom demonstrations, without any additional interaction with the environment.\nDespite significant advances in offline IL, existing techniques find it\nchallenging to learn policies for long-horizon tasks and require significant\nre-training when task specifications change. Towards addressing these\nlimitations, we present GO-DICE an offline IL technique for goal-conditioned\nlong-horizon sequential tasks. GO-DICE discerns a hierarchy of sub-tasks from\ndemonstrations and uses these to learn separate policies for sub-task\ntransitions and action execution, respectively; this hierarchical policy\nlearning facilitates long-horizon reasoning. Inspired by the expansive\nDICE-family of techniques, policy learning at both the levels transpires within\nthe space of stationary distributions. Further, both policies are learnt with\ngoal conditioning to minimize need for retraining when task goals change.\nExperimental results substantiate that GO-DICE outperforms recent baselines, as\nevidenced by a marked improvement in the completion rate of increasingly\nchallenging pick-and-place Mujoco robotic tasks. GO-DICE is also capable of\nleveraging imperfect demonstration and partial task segmentation when\navailable, both of which boost task performance relative to learning from\nexpert demonstrations alone.\n","authors":["Abhinav Jain","Vaibhav Unhelkar"],"pdf_url":"https://arxiv.org/pdf/2312.10802v1.pdf","comment":"Extended version of an identically-titled paper accepted at AAAI 2024"},{"id":"http://arxiv.org/abs/2312.10801v1","updated":"2023-12-17T19:44:20Z","published":"2023-12-17T19:44:20Z","title":"Scope Compliance Uncertainty Estimate","summary":"  The zeitgeist of the digital era has been dominated by an expanding\nintegration of Artificial Intelligence~(AI) in a plethora of applications\nacross various domains. With this expansion, however, questions of the safety\nand reliability of these methods come have become more relevant than ever.\nConsequently, a run-time ML model safety system has been developed to ensure\nthe model's operation within the intended context, especially in applications\nwhose environments are greatly variable such as Autonomous Vehicles~(AVs).\nSafeML is a model-agnostic approach for performing such monitoring, using\ndistance measures based on statistical testing of the training and operational\ndatasets; comparing them to a predetermined threshold, returning a binary value\nwhether the model should be trusted in the context of the observed data or be\ndeemed unreliable. Although a systematic framework exists for this approach,\nits performance is hindered by: (1) a dependency on a number of design\nparameters that directly affect the selection of a safety threshold and\ntherefore likely affect its robustness, (2) an inherent assumption of certain\ndistributions for the training and operational sets, as well as (3) a high\ncomputational complexity for relatively large sets. This work addresses these\nlimitations by changing the binary decision to a continuous metric.\nFurthermore, all data distribution assumptions are made obsolete by\nimplementing non-parametric approaches, and the computational speed increased\nby introducing a new distance measure based on the Empirical Characteristics\nFunctions~(ECF).\n","authors":["Al-Harith Farhad","Ioannis Sorokos","Mohammed Naveed Akram","Koorosh Aslansefat","Daniel Schneider"],"pdf_url":"https://arxiv.org/pdf/2312.10801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10795v1","updated":"2023-12-17T19:12:33Z","published":"2023-12-17T19:12:33Z","title":"Learning to Learn in Interactive Constraint Acquisition","summary":"  Constraint Programming (CP) has been successfully used to model and solve\ncomplex combinatorial problems. However, modeling is often not trivial and\nrequires expertise, which is a bottleneck to wider adoption. In Constraint\nAcquisition (CA), the goal is to assist the user by automatically learning the\nmodel. In (inter)active CA, this is done by interactively posting queries to\nthe user, e.g., asking whether a partial solution satisfies their (unspecified)\nconstraints or not. While interac tive CA methods learn the constraints, the\nlearning is related to symbolic concept learning, as the goal is to learn an\nexact representation. However, a large number of queries is still required to\nlearn the model, which is a major limitation. In this paper, we aim to\nalleviate this limitation by tightening the connection of CA and Machine\nLearning (ML), by, for the first time in interactive CA, exploiting statistical\nML methods. We propose to use probabilistic classification models to guide\ninteractive CA to generate more promising queries. We discuss how to train\nclassifiers to predict whether a candidate expression from the bias is a\nconstraint of the problem or not, using both relation-based and scope-based\nfeatures. We then show how the predictions can be used in all layers of\ninteractive CA: the query generation, the scope finding, and the lowest-level\nconstraint finding. We experimentally evaluate our proposed methods using\ndifferent classifiers and show that our methods greatly outperform the state of\nthe art, decreasing the number of queries needed to converge by up to 72%.\n","authors":["Dimos Tsouros","Senne Berden","Tias Guns"],"pdf_url":"https://arxiv.org/pdf/2312.10795v1.pdf","comment":"Accepted in AAAI"},{"id":"http://arxiv.org/abs/2312.10794v1","updated":"2023-12-17T19:06:29Z","published":"2023-12-17T19:06:29Z","title":"A mathematical perspective on Transformers","summary":"  Transformers play a central role in the inner workings of large language\nmodels. We develop a mathematical framework for analyzing Transformers based on\ntheir interpretation as interacting particle systems, which reveals that\nclusters emerge in long time. Our study explores the underlying theory and\noffers new perspectives for mathematicians as well as computer scientists.\n","authors":["Borjan Geshkovski","Cyril Letrouit","Yury Polyanskiy","Philippe Rigollet"],"pdf_url":"https://arxiv.org/pdf/2312.10794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06833v2","updated":"2023-12-17T18:38:15Z","published":"2023-12-11T20:54:59Z","title":"The unreasonable effectiveness of AI CADe polyp detectors to generalize\n  to new countries","summary":"  $\\textbf{Background and aims}$: Artificial Intelligence (AI) Computer-Aided\nDetection (CADe) is commonly used for polyp detection, but data seen in\nclinical settings can differ from model training. Few studies evaluate how well\nCADe detectors perform on colonoscopies from countries not seen during\ntraining, and none are able to evaluate performance without collecting\nexpensive and time-intensive labels.\n  $\\textbf{Methods}$: We trained a CADe polyp detector on Israeli colonoscopy\nvideos (5004 videos, 1106 hours) and evaluated on Japanese videos (354 videos,\n128 hours) by measuring the True Positive Rate (TPR) versus false alarms per\nminute (FAPM). We introduce a colonoscopy dissimilarity measure called \"MAsked\nmediCal Embedding Distance\" (MACE) to quantify differences between\ncolonoscopies, without labels. We evaluated CADe on all Japan videos and on\nthose with the highest MACE.\n  $\\textbf{Results}$: MACE correctly quantifies that narrow-band imaging (NBI)\nand chromoendoscopy (CE) frames are less similar to Israel data than Japan\nwhitelight (bootstrapped z-test, |z| > 690, p < $10^{-8}$ for both). Despite\ndifferences in the data, CADe performance on Japan colonoscopies was\nnon-inferior to Israel ones without additional training (TPR at 0.5 FAPM: 0.957\nand 0.972 for Israel and Japan; TPR at 1.0 FAPM: 0.972 and 0.989 for Israel and\nJapan; superiority test t > 45.2, p < $10^{-8}$). Despite not being trained on\nNBI or CE, TPR on those subsets were non-inferior to Japan overall\n(non-inferiority test t > 47.3, p < $10^{-8}$, $\\delta$ = 1.5% for both).\n  $\\textbf{Conclusion}$: Differences that prevent CADe detectors from\nperforming well in non-medical settings do not degrade the performance of our\nAI CADe polyp detector when applied to data from a new country. MACE can help\nmedical AI models internationalize by identifying the most \"dissimilar\" data on\nwhich to evaluate models.\n","authors":["Joel Shor","Hiro-o Yamano","Daisuke Tsurumaru","Yotami Intrator","Hiroki Kayama","Joe Ledsam","Atsushi Hamabe","Koji Ando","Mitsuhiko Ota","Haruei Ogino","Hiroshi Nakase","Kaho Kobayashi","Eiji Oki","Roman Goldenberg","Ehud Rivlin","Ichiro Takemasa"],"pdf_url":"https://arxiv.org/pdf/2312.06833v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10787v1","updated":"2023-12-17T18:22:08Z","published":"2023-12-17T18:22:08Z","title":"Learning Discrete-Time Major-Minor Mean Field Games","summary":"  Recent techniques based on Mean Field Games (MFGs) allow the scalable\nanalysis of multi-player games with many similar, rational agents. However,\nstandard MFGs remain limited to homogeneous players that weakly influence each\nother, and cannot model major players that strongly influence other players,\nseverely limiting the class of problems that can be handled. We propose a novel\ndiscrete time version of major-minor MFGs (M3FGs), along with a learning\nalgorithm based on fictitious play and partitioning the probability simplex.\nImportantly, M3FGs generalize MFGs with common noise and can handle not only\nrandom exogeneous environment states but also major players. A key challenge is\nthat the mean field is stochastic and not deterministic as in standard MFGs.\nOur theoretical investigation verifies both the M3FG model and its algorithmic\nsolution, showing firstly the well-posedness of the M3FG model starting from a\nfinite game of interest, and secondly convergence and approximation guarantees\nof the fictitious play algorithm. Then, we empirically verify the obtained\ntheoretical results, ablating some of the theoretical assumptions made, and\nshow successful equilibrium learning in three example problems. Overall, we\nestablish a learning framework for a novel and broad class of tractable games.\n","authors":["Kai Cui","Gökçe Dayanıklı","Mathieu Laurière","Matthieu Geist","Olivier Pietquin","Heinz Koeppl"],"pdf_url":"https://arxiv.org/pdf/2312.10787v1.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2308.07170v2","updated":"2023-12-17T17:46:27Z","published":"2023-08-14T14:26:52Z","title":"Human Voice Pitch Estimation: A Convolutional Network with Auto-Labeled\n  and Synthetic Data","summary":"  In the domain of music and sound processing, pitch extraction plays a pivotal\nrole. Our research presents a specialized convolutional neural network designed\nfor pitch extraction, particularly from the human singing voice in acapella\nperformances. Notably, our approach combines synthetic data with auto-labeled\nacapella sung audio, creating a robust training environment. Evaluation across\ndatasets comprising synthetic sounds, opera recordings, and time-stretched\nvowels demonstrates its efficacy. This work paves the way for enhanced pitch\nextraction in both music and voice settings.\n","authors":["Jeremy Cochoy"],"pdf_url":"https://arxiv.org/pdf/2308.07170v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10770v1","updated":"2023-12-17T17:23:43Z","published":"2023-12-17T17:23:43Z","title":"Identification of Knowledge Neurons in Protein Language Models","summary":"  Neural language models have become powerful tools for learning complex\nrepresentations of entities in natural language processing tasks. However,\ntheir interpretability remains a significant challenge, particularly in domains\nlike computational biology where trust in model predictions is crucial. In this\nwork, we aim to enhance the interpretability of protein language models,\nspecifically the state-of-the-art ESM model, by identifying and characterizing\nknowledge neurons - components that express understanding of key information.\nAfter fine-tuning the ESM model for the task of enzyme sequence classification,\nwe compare two knowledge neuron selection methods that preserve a subset of\nneurons from the original model. The two methods, activation-based and\nintegrated gradient-based selection, consistently outperform a random baseline.\nIn particular, these methods show that there is a high density of knowledge\nneurons in the key vector prediction networks of self-attention modules. Given\nthat key vectors specialize in understanding different features of input\nsequences, these knowledge neurons could capture knowledge of different enzyme\nsequence motifs. In the future, the types of knowledge captured by each neuron\ncould be characterized.\n","authors":["Divya Nori","Shivali Singireddy","Marina Ten Have"],"pdf_url":"https://arxiv.org/pdf/2312.10770v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14196v3","updated":"2023-12-17T17:05:09Z","published":"2023-05-23T16:15:31Z","title":"ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding","summary":"  We introduce ZeroSCROLLS, a zero-shot benchmark for natural language\nunderstanding over long texts, which contains only test and small validation\nsets, without training data. We adapt six tasks from the SCROLLS benchmark, and\nadd four new datasets, including two novel information fusing tasks, such as\naggregating the percentage of positive reviews. Using ZeroSCROLLS, we conduct a\ncomprehensive evaluation of both open-source and closed large language models,\nfinding that Claude outperforms ChatGPT, and that GPT-4 achieves the highest\naverage score. However, there is still room for improvement on multiple open\nchallenges in ZeroSCROLLS, such as aggregation tasks, where models struggle to\npass the naive baseline. As the state of the art is a moving target, we invite\nresearchers to evaluate their ideas on the live ZeroSCROLLS leaderboard.\n","authors":["Uri Shaham","Maor Ivgi","Avia Efrat","Jonathan Berant","Omer Levy"],"pdf_url":"https://arxiv.org/pdf/2305.14196v3.pdf","comment":"Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2303.03932v2","updated":"2023-12-17T16:53:44Z","published":"2023-03-07T14:38:28Z","title":"FFT-based Dynamic Token Mixer for Vision","summary":"  Multi-head-self-attention (MHSA)-equipped models have achieved notable\nperformance in computer vision. Their computational complexity is proportional\nto quadratic numbers of pixels in input feature maps, resulting in slow\nprocessing, especially when dealing with high-resolution images. New types of\ntoken-mixer are proposed as an alternative to MHSA to circumvent this problem:\nan FFT-based token-mixer involves global operations similar to MHSA but with\nlower computational complexity. However, despite its attractive properties, the\nFFT-based token-mixer has not been carefully examined in terms of its\ncompatibility with the rapidly evolving MetaFormer architecture. Here, we\npropose a novel token-mixer called Dynamic Filter and novel image recognition\nmodels, DFFormer and CDFFormer, to close the gaps above. The results of image\nclassification and downstream tasks, analysis, and visualization show that our\nmodels are helpful. Notably, their throughput and memory efficiency when\ndealing with high-resolution image recognition is remarkable. Our results\nindicate that Dynamic Filter is one of the token-mixer options that should be\nseriously considered. The code is available at\nhttps://github.com/okojoalg/dfformer\n","authors":["Yuki Tatsunami","Masato Taki"],"pdf_url":"https://arxiv.org/pdf/2303.03932v2.pdf","comment":"The 38th Annual AAAI Conference on Artificial Intelligence (AAAI'24)"},{"id":"http://arxiv.org/abs/2312.08533v2","updated":"2023-12-17T16:47:29Z","published":"2023-12-13T21:46:09Z","title":"World Models via Policy-Guided Trajectory Diffusion","summary":"  World models are a powerful tool for developing intelligent agents. By\npredicting the outcome of a sequence of actions, world models enable policies\nto be optimised via on-policy reinforcement learning (RL) using synthetic data,\ni.e. in \"in imagination\". Existing world models are autoregressive in that they\ninterleave predicting the next state with sampling the next action from the\npolicy. Prediction error inevitably compounds as the trajectory length grows.\nIn this work, we propose a novel world modelling approach that is not\nautoregressive and generates entire on-policy trajectories in a single pass\nthrough a diffusion model. Our approach, Policy-Guided Trajectory Diffusion\n(PolyGRAD), leverages a denoising model in addition to the gradient of the\naction distribution of the policy to diffuse a trajectory of initially random\nstates and actions into an on-policy synthetic trajectory. We analyse the\nconnections between PolyGRAD, score-based generative models, and\nclassifier-guided diffusion models. Our results demonstrate that PolyGRAD\noutperforms state-of-the-art baselines in terms of trajectory prediction error\nfor moderate-length trajectories, with the exception of autoregressive\ndiffusion. At short horizons, PolyGRAD obtains comparable errors to\nautoregressive diffusion, but with significantly lower computational\nrequirements. Our experiments also demonstrate that PolyGRAD enables performant\npolicies to be trained via on-policy RL in imagination for MuJoCo continuous\ncontrol domains. Thus, PolyGRAD introduces a new paradigm for scalable and\nnon-autoregressive on-policy world modelling.\n","authors":["Marc Rigter","Jun Yamada","Ingmar Posner"],"pdf_url":"https://arxiv.org/pdf/2312.08533v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.08536v2","updated":"2023-12-17T16:18:56Z","published":"2023-08-16T17:52:11Z","title":"Can Transformers Learn Optimal Filtering for Unknown Systems?","summary":"  Transformer models have shown great success in natural language processing;\nhowever, their potential remains mostly unexplored for dynamical systems. In\nthis work, we investigate the optimal output estimation problem using\ntransformers, which generate output predictions using all the past ones.\nParticularly, we train the transformer using various distinct systems and then\nevaluate the performance on unseen systems with unknown dynamics. Empirically,\nthe trained transformer adapts exceedingly well to different unseen systems and\neven matches the optimal performance given by the Kalman filter for linear\nsystems. In more complex settings with non-i.i.d. noise, time-varying dynamics,\nand nonlinear dynamics like a quadrotor system with unknown parameters,\ntransformers also demonstrate promising results. To support our experimental\nfindings, we provide statistical guarantees that quantify the amount of\ntraining data required for the transformer to achieve a desired excess risk.\nFinally, we point out some limitations by identifying two classes of problems\nthat lead to degraded performance, highlighting the need for caution when using\ntransformers for control and estimation.\n","authors":["Haldun Balim","Zhe Du","Samet Oymak","Necmiye Ozay"],"pdf_url":"https://arxiv.org/pdf/2308.08536v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10756v1","updated":"2023-12-17T16:12:35Z","published":"2023-12-17T16:12:35Z","title":"Attention-Driven Multichannel Speech Enhancement in Moving Sound Source\n  Scenarios","summary":"  Current multichannel speech enhancement algorithms typically assume a\nstationary sound source, a common mismatch with reality that limits their\nperformance in real-world scenarios. This paper focuses on attention-driven\nspatial filtering techniques designed for dynamic settings. Specifically, we\nstudy the application of linear and nonlinear attention-based methods for\nestimating time-varying spatial covariance matrices used to design the filters.\nWe also investigate the direct estimation of spatial filters by attention-based\nmethods without explicitly estimating spatial statistics. The clean speech\nclips from WSJ0 are employed for simulating speech signals of moving speakers\nin a reverberant environment. The experimental dataset is built by mixing the\nsimulated speech signals with multichannel real noise from CHiME-3. Evaluation\nresults show that the attention-driven approaches are robust and consistently\noutperform conventional spatial filtering approaches in both static and dynamic\nsound environments.\n","authors":["Yuzhu Wang","Archontis Politis","Tuomas Virtanen"],"pdf_url":"https://arxiv.org/pdf/2312.10756v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06872v2","updated":"2023-12-17T15:38:51Z","published":"2023-12-11T22:44:05Z","title":"ELSA: Partial Weight Freezing for Overhead-Free Sparse Network\n  Deployment","summary":"  We present ELSA, a practical solution for creating deep networks that can\neasily be deployed at different levels of sparsity. The core idea is to embed\none or more sparse networks within a single dense network as a proper subset of\nthe weights. At prediction time, any sparse model can be extracted effortlessly\nsimply be zeroing out weights according to a predefined mask. ELSA is simple,\npowerful and highly flexible. It can use essentially any existing technique for\nnetwork sparsification and network training. In particular, it does not\nrestrict the loss function, architecture or the optimization technique. Our\nexperiments show that ELSA's advantages of flexible deployment comes with no or\njust a negligible reduction in prediction quality compared to the standard way\nof using multiple sparse networks that are trained and stored independently.\n","authors":["Paniz Halvachi","Alexandra Peste","Dan Alistarh","Christoph H. Lampert"],"pdf_url":"https://arxiv.org/pdf/2312.06872v2.pdf","comment":"updated to reflect PackNet prior work"},{"id":"http://arxiv.org/abs/2312.10747v1","updated":"2023-12-17T15:37:41Z","published":"2023-12-17T15:37:41Z","title":"CEIR: Concept-based Explainable Image Representation Learning","summary":"  In modern machine learning, the trend of harnessing self-supervised learning\nto derive high-quality representations without label dependency has garnered\nsignificant attention. However, the absence of label information, coupled with\nthe inherently high-dimensional nature, improves the difficulty for the\ninterpretation of learned representations. Consequently, indirect evaluations\nbecome the popular metric for evaluating the quality of these features, leading\nto a biased validation of the learned representation rationale. To address\nthese challenges, we introduce a novel approach termed Concept-based\nExplainable Image Representation (CEIR). Initially, using the Concept-based\nModel (CBM) incorporated with pretrained CLIP and concepts generated by GPT-4,\nwe project input images into a concept vector space. Subsequently, a\nVariational Autoencoder (VAE) learns the latent representation from these\nprojected concepts, which serves as the final image representation. Due to the\ncapability of the representation to encapsulate high-level, semantically\nrelevant concepts, the model allows for attributions to a human-comprehensible\nconcept space. This not only enhances interpretability but also preserves the\nrobustness essential for downstream tasks. For instance, our method exhibits\nstate-of-the-art unsupervised clustering performance on benchmarks such as\nCIFAR10, CIFAR100, and STL10. Furthermore, capitalizing on the universality of\nhuman conceptual understanding, CEIR can seamlessly extract the related concept\nfrom open-world images without fine-tuning. This offers a fresh approach to\nautomatic label generation and label manipulation.\n","authors":["Yan Cui","Shuhong Liu","Liuzhuozheng Li","Zhiyuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2312.10747v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2312.10746v1","updated":"2023-12-17T15:37:03Z","published":"2023-12-17T15:37:03Z","title":"Knowledge Trees: Gradient Boosting Decision Trees on Knowledge Neurons\n  as Probing Classifier","summary":"  To understand how well a large language model captures certain semantic or\nsyntactic features, researchers typically apply probing classifiers. However,\nthe accuracy of these classifiers is critical for the correct interpretation of\nthe results. If a probing classifier exhibits low accuracy, this may be due\neither to the fact that the language model does not capture the property under\ninvestigation, or to shortcomings in the classifier itself, which is unable to\nadequately capture the characteristics encoded in the internal representations\nof the model. Consequently, for more effective diagnosis, it is necessary to\nuse the most accurate classifiers possible for a particular type of task.\nLogistic regression on the output representation of the transformer neural\nnetwork layer is most often used to probing the syntactic properties of the\nlanguage model.\n  We show that using gradient boosting decision trees at the Knowledge Neuron\nlayer, i.e., at the hidden layer of the feed-forward network of the transformer\nas a probing classifier for recognizing parts of a sentence is more\nadvantageous than using logistic regression on the output representations of\nthe transformer layer. This approach is also preferable to many other methods.\nThe gain in error rate, depending on the preset, ranges from 9-54%\n","authors":["Sergey A. Saltykov"],"pdf_url":"https://arxiv.org/pdf/2312.10746v1.pdf","comment":"10 pages, 7 figures, 4 tables"},{"id":"http://arxiv.org/abs/2312.07392v2","updated":"2023-12-17T14:29:10Z","published":"2023-12-12T16:05:55Z","title":"ReRoGCRL: Representation-based Robustness in Goal-Conditioned\n  Reinforcement Learning","summary":"  While Goal-Conditioned Reinforcement Learning (GCRL) has gained attention,\nits algorithmic robustness against adversarial perturbations remains\nunexplored. The attacks and robust representation training methods that are\ndesigned for traditional RL become less effective when applied to GCRL. To\naddress this challenge, we first propose the Semi-Contrastive Representation\nattack, a novel approach inspired by the adversarial contrastive attack. Unlike\nexisting attacks in RL, it only necessitates information from the policy\nfunction and can be seamlessly implemented during deployment. Then, to mitigate\nthe vulnerability of existing GCRL algorithms, we introduce Adversarial\nRepresentation Tactics, which combines Semi-Contrastive Adversarial\nAugmentation with Sensitivity-Aware Regularizer to improve the adversarial\nrobustness of the underlying RL agent against various types of perturbations.\nExtensive experiments validate the superior performance of our attack and\ndefence methods across multiple state-of-the-art GCRL algorithms. Our tool\nReRoGCRL is available at https://github.com/TrustAI/ReRoGCRL.\n","authors":["Xiangyu Yin","Sihao Wu","Jiaxu Liu","Meng Fang","Xingyu Zhao","Xiaowei Huang","Wenjie Ruan"],"pdf_url":"https://arxiv.org/pdf/2312.07392v2.pdf","comment":"This paper has been accepted in AAAI24\n  (https://aaai.org/aaai-conference/)"},{"id":"http://arxiv.org/abs/2301.02761v2","updated":"2023-12-17T14:25:50Z","published":"2023-01-07T01:35:25Z","title":"Active Learning Guided by Efficient Surrogate Learners","summary":"  Re-training a deep learning model each time a single data point receives a\nnew label is impractical due to the inherent complexity of the training\nprocess. Consequently, existing active learning (AL) algorithms tend to adopt a\nbatch-based approach where, during each AL iteration, a set of data points is\ncollectively chosen for annotation. However, this strategy frequently leads to\nredundant sampling, ultimately eroding the efficacy of the labeling procedure.\nIn this paper, we introduce a new AL algorithm that harnesses the power of a\nGaussian process surrogate in conjunction with the neural network principal\nlearner. Our proposed model adeptly updates the surrogate learner for every new\ndata instance, enabling it to emulate and capitalize on the continuous learning\ndynamics of the neural network without necessitating a complete retraining of\nthe principal model for each individual label. Experiments on four benchmark\ndatasets demonstrate that this approach yields significant enhancements, either\nrivaling or aligning with the performance of state-of-the-art techniques.\n","authors":["Yunpyo An","Suyeong Park","Kwang In Kim"],"pdf_url":"https://arxiv.org/pdf/2301.02761v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.13838v2","updated":"2023-12-17T14:14:50Z","published":"2023-08-26T10:09:46Z","title":"Price-Discrimination Game for Distributed Resource Management in\n  Federated Learning","summary":"  In vanilla federated learning (FL) such as FedAvg, the parameter server (PS)\nand multiple distributed clients can form a typical buyer's market, where the\nnumber of PS/buyers of FL services is far less than the number of\nclients/sellers. In order to improve the performance of FL and reduce the cost\nof motivating clients to participate in FL, this paper proposes to\ndifferentiate the pricing for services provided by different clients rather\nthan simply providing the same service pricing for different clients. The price\nis differentiated based on the performance improvements brought to FL and their\nheterogeneity in computing and communication capabilities. To this end, a\nprice-discrimination game (PDG) is formulated to comprehensively address the\ndistributed resource management problems in FL, including multi-objective\ntrade-off, client selection, and incentive mechanism. As the PDG is a\nmixed-integer nonlinear programming (MINLP) problem, a distributed\nsemi-heuristic algorithm with low computational complexity and low\ncommunication overhead is designed to solve it. The simulation result verifies\nthe effectiveness of the proposed approach.\n","authors":["Han Zhang","Halvin Yang","Guopeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2308.13838v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10725v1","updated":"2023-12-17T14:14:31Z","published":"2023-12-17T14:14:31Z","title":"Addressing Sample Inefficiency in Multi-View Representation Learning","summary":"  Non-contrastive self-supervised learning (NC-SSL) methods like BarlowTwins\nand VICReg have shown great promise for label-free representation learning in\ncomputer vision. Despite the apparent simplicity of these techniques,\nresearchers must rely on several empirical heuristics to achieve competitive\nperformance, most notably using high-dimensional projector heads and two\naugmentations of the same image. In this work, we provide theoretical insights\non the implicit bias of the BarlowTwins and VICReg loss that can explain these\nheuristics and guide the development of more principled recommendations. Our\nfirst insight is that the orthogonality of the features is more critical than\nprojector dimensionality for learning good representations. Based on this, we\nempirically demonstrate that low-dimensional projector heads are sufficient\nwith appropriate regularization, contrary to the existing heuristic. Our second\ntheoretical insight suggests that using multiple data augmentations better\nrepresents the desiderata of the SSL objective. Based on this, we demonstrate\nthat leveraging more augmentations per sample improves representation quality\nand trainability. In particular, it improves optimization convergence, leading\nto better features emerging earlier in the training. Remarkably, we demonstrate\nthat we can reduce the pretraining dataset size by up to 4x while maintaining\naccuracy and improving convergence simply by using more data augmentations.\nCombining these insights, we present practical pretraining recommendations that\nimprove wall-clock time by 2x and improve performance on CIFAR-10/STL-10\ndatasets using a ResNet-50 backbone. Thus, this work provides a theoretical\ninsight into NC-SSL and produces practical recommendations for enhancing its\nsample and compute efficiency.\n","authors":["Kumar Krishna Agrawal","Arna Ghosh","Adam Oberman","Blake Richards"],"pdf_url":"https://arxiv.org/pdf/2312.10725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.03832v2","updated":"2023-12-17T13:34:46Z","published":"2023-06-06T16:20:44Z","title":"Sequential Principal-Agent Problems with Communication: Efficient\n  Computation and Learning","summary":"  We study a sequential decision making problem between a principal and an\nagent with incomplete information on both sides. In this model, the principal\nand the agent interact in a stochastic environment, and each is privy to\nobservations about the state not available to the other. The principal has the\npower of commitment, both to elicit information from the agent and to provide\nsignals about her own information. The principal and the agent communicate\ntheir signals to each other, and select their actions independently based on\nthis communication. Each player receives a payoff based on the state and their\njoint actions, and the environment moves to a new state. The interaction\ncontinues over a finite time horizon, and both players act to optimize their\nown total payoffs over the horizon. Our model encompasses as special cases\nstochastic games of incomplete information and POMDPs, as well as sequential\nBayesian persuasion and mechanism design problems. We study both computation of\noptimal policies and learning in our setting. While the general problems are\ncomputationally intractable, we study algorithmic solutions under a conditional\nindependence assumption on the underlying state-observation distributions. We\npresent a polynomial-time algorithm to compute the principal's optimal policy\nup to an additive approximation. Additionally, we show an efficient learning\nalgorithm in the case where the transition probabilities are not known\nbeforehand. The algorithm guarantees sublinear regret for both players.\n","authors":["Jiarui Gan","Rupak Majumdar","Debmalya Mandal","Goran Radanovic"],"pdf_url":"https://arxiv.org/pdf/2306.03832v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10708v1","updated":"2023-12-17T12:56:39Z","published":"2023-12-17T12:56:39Z","title":"The Conditioning Bias in Binary Decision Trees and Random Forests and\n  Its Elimination","summary":"  Decision tree and random forest classification and regression are some of the\nmost widely used in machine learning approaches. Binary decision tree\nimplementations commonly use conditioning in the form 'feature $\\leq$ (or $<$)\nthreshold', with the threshold being the midpoint between two observed feature\nvalues. In this paper, we investigate the bias introduced by the choice of\nconditioning operator (an intrinsic property of implementations) in the\npresence of features with lattice characteristics. We propose techniques to\neliminate this bias, requiring an additional prediction with decision trees and\nincurring no cost for random forests. Using 20 classification and 20 regression\ndatasets, we demonstrate that the bias can lead to statistically significant\ndifferences in terms of AUC and $r^2$ scores. The proposed techniques\nsuccessfully mitigate the bias, compared to the worst-case scenario,\nstatistically significant improvements of up to 0.1-0.2 percentage points of\nAUC and $r^2$ scores were achieved and the improvement of 1.5 percentage points\nof $r^2$ score was measured in the most sensitive case of random forest\nregression. The implementation of the study is available on GitHub at the\nfollowing repository: \\url{https://github.com/gykovacs/conditioning_bias}.\n","authors":["Gábor Timár","György Kovács"],"pdf_url":"https://arxiv.org/pdf/2312.10708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.12063v2","updated":"2023-12-17T12:56:31Z","published":"2022-08-25T12:47:20Z","title":"Partial Matrix Completion","summary":"  The matrix completion problem aims to reconstruct a low-rank matrix based on\na revealed set of possibly noisy entries. Prior works consider completing the\nentire matrix with generalization error guarantees. However, the completion\naccuracy can be drastically different over different entries. This work\nestablishes a new framework of partial matrix completion, where the goal is to\nidentify a large subset of the entries that can be completed with high\nconfidence. We propose an efficient algorithm with the following provable\nguarantees. Given access to samples from an unknown and arbitrary distribution,\nit guarantees: (a) high accuracy over completed entries, and (b) high coverage\nof the underlying distribution. We also consider an online learning variant of\nthis problem, where we propose a low-regret algorithm based on iterative\ngradient updates. Preliminary empirical evaluations are included.\n","authors":["Elad Hazan","Adam Tauman Kalai","Varun Kanade","Clara Mohri","Y. Jennifer Sun"],"pdf_url":"https://arxiv.org/pdf/2208.12063v2.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2312.10707v1","updated":"2023-12-17T12:51:49Z","published":"2023-12-17T12:51:49Z","title":"CLDR: Contrastive Learning Drug Response Models from Natural Language\n  Supervision","summary":"  Deep learning-based drug response prediction (DRP) methods can accelerate the\ndrug discovery process and reduce R\\&D costs. Although the mainstream methods\nachieve high accuracy in predicting response regression values, the\nregression-aware representations of these methods are fragmented and fail to\ncapture the continuity of the sample order. This phenomenon leads to models\noptimized to sub-optimal solution spaces, reducing generalization ability and\nmay result in significant wasted costs in the drug discovery phase. In this\npaper, we propose \\MN, a contrastive learning framework with natural language\nsupervision for the DRP. The \\MN~converts regression labels into text, which is\nmerged with the captions text of the drug response as a second modality of the\nsamples compared to the traditional modalities (graph, sequence). In each\nbatch, two modalities of one sample are considered positive pairs and the other\npairs are considered negative pairs. At the same time, in order to enhance the\ncontinuous representation capability of the numerical text, a common-sense\nnumerical knowledge graph is introduced. We validated several hundred thousand\nsamples from the Genomics of Drug Sensitivity in Cancer dataset, observing the\naverage improvement of the DRP method ranges from 7.8\\% to 31.4\\% with the\napplication of our framework. The experiments prove that the \\MN~effectively\nconstrains the samples to a continuous distribution in the representation\nspace, and achieves impressive prediction performance with only a few epochs of\nfine-tuning after pre-training. The code is available at:\n\\url{https://gitee.com/xiaoyibang/clipdrug.git}.\n","authors":["Kun Li","Wenbin Hu"],"pdf_url":"https://arxiv.org/pdf/2312.10707v1.pdf","comment":"9 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2312.10705v1","updated":"2023-12-17T12:50:10Z","published":"2023-12-17T12:50:10Z","title":"Enhancing Numeric-SAM for Learning with Few Observations","summary":"  A significant challenge in applying planning technology to real-world\nproblems lies in obtaining a planning model that accurately represents the\nproblem's dynamics. Numeric Safe Action Models Learning (N-SAM) is a recently\nproposed algorithm that addresses this challenge. It is an algorithm designed\nto learn the preconditions and effects of actions from observations in domains\nthat may involve both discrete and continuous state variables. N-SAM has\nseveral attractive properties. It runs in polynomial time and is guaranteed to\noutput an action model that is safe, in the sense that plans generated by it\nare applicable and will achieve their intended goals. To preserve this safety\nguarantee, N-SAM must observe a substantial number of examples for each action\nbefore it is included in the learned action model. We address this limitation\nof N-SAM and propose N-SAM*, an enhanced version of N-SAM that always returns\nan action model where every observed action is applicable at least in some\nstate, even if it was only observed once. N-SAM* does so without compromising\nthe safety of the returned action model. We prove that N-SAM* is optimal in\nterms of sample complexity compared to any other algorithm that guarantees\nsafety. An empirical study on a set of benchmark domains shows that the action\nmodels returned by N-SAM* enable solving significantly more problems compared\nto the action models returned by N-SAM.\n","authors":["Argaman Mordoch","Shahaf S. Shperberg","Roni Stern","Berndan Juba"],"pdf_url":"https://arxiv.org/pdf/2312.10705v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10702v1","updated":"2023-12-17T12:33:50Z","published":"2023-12-17T12:33:50Z","title":"Can persistent homology whiten Transformer-based black-box models? A\n  case study on BERT compression","summary":"  Large Language Models (LLMs) like BERT have gained significant prominence due\nto their remarkable performance in various natural language processing tasks.\nHowever, they come with substantial computational and memory costs.\nAdditionally, they are essentially black-box models, challenging to explain and\ninterpret. In this article, we propose Optimus BERT Compression and\nExplainability (OBCE), a methodology to bring explainability to BERT models\nusing persistent homology, aiming to measure the importance of each neuron by\nstudying the topological characteristics of their outputs. As a result, we can\ncompress BERT significantly by reducing the number of parameters (58.47% of the\noriginal parameters for BERT Base, 52.3% for BERT Large). We evaluated our\nmethodology on the standard GLUE Benchmark, comparing the results with\nstate-of-the-art techniques and achieving outstanding results. Consequently,\nour methodology can \"whiten\" BERT models by providing explainability to its\nneurons and reducing the model's size, making it more suitable for deployment\non resource-constrained devices.\n","authors":["Luis Balderas","Miguel Lastra","José M. Benítez"],"pdf_url":"https://arxiv.org/pdf/2312.10702v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10700v1","updated":"2023-12-17T12:27:15Z","published":"2023-12-17T12:27:15Z","title":"Cross-Domain Robustness of Transformer-based Keyphrase Generation","summary":"  Modern models for text generation show state-of-the-art results in many\nnatural language processing tasks. In this work, we explore the effectiveness\nof abstractive text summarization models for keyphrase selection. A list of\nkeyphrases is an important element of a text in databases and repositories of\nelectronic documents. In our experiments, abstractive text summarization models\nfine-tuned for keyphrase generation show quite high results for a target text\ncorpus. However, in most cases, the zero-shot performance on other corpora and\ndomains is significantly lower. We investigate cross-domain limitations of\nabstractive text summarization models for keyphrase generation. We present an\nevaluation of the fine-tuned BART models for the keyphrase selection task\nacross six benchmark corpora for keyphrase extraction including scientific\ntexts from two domains and news texts. We explore the role of transfer learning\nbetween different domains to improve the BART model performance on small text\ncorpora. Our experiments show that preliminary fine-tuning on out-of-domain\ncorpora can be effective under conditions of a limited number of samples.\n","authors":["Anna Glazkova","Dmitry Morozov"],"pdf_url":"https://arxiv.org/pdf/2312.10700v1.pdf","comment":"Presented at the XXV International Conference \"Data Analytics and\n  Management in Data Intensive Domains\" (DAMDID/RCDL), October 2023"},{"id":"http://arxiv.org/abs/2307.05845v4","updated":"2023-12-17T12:10:16Z","published":"2023-07-11T23:36:49Z","title":"PIGEON: Predicting Image Geolocations","summary":"  Planet-scale image geolocalization remains a challenging problem due to the\ndiversity of images originating from anywhere in the world. Although approaches\nbased on vision transformers have made significant progress in geolocalization\naccuracy, success in prior literature is constrained to narrow distributions of\nimages of landmarks, and performance has not generalized to unseen places. We\npresent a new geolocalization system that combines semantic geocell creation,\nmulti-task contrastive pretraining, and a novel loss function. Additionally,\nour work is the first to perform retrieval over location clusters for guess\nrefinements. We train two models for evaluations on street-level data and\ngeneral-purpose image geolocalization; the first model, PIGEON, is trained on\ndata from the game of Geoguessr and is capable of placing over 40% of its\nguesses within 25 kilometers of the target location globally. We also develop a\nbot and deploy PIGEON in a blind experiment against humans, ranking in the top\n0.01% of players. We further challenge one of the world's foremost professional\nGeoguessr players to a series of six matches with millions of viewers, winning\nall six games. Our second model, PIGEOTTO, differs in that it is trained on a\ndataset of images from Flickr and Wikipedia, achieving state-of-the-art results\non a wide range of image geolocalization benchmarks, outperforming the previous\nSOTA by up to 7.7 percentage points on the city accuracy level and up to 38.8\npercentage points on the country level. Our findings suggest that PIGEOTTO is\nthe first image geolocalization model that effectively generalizes to unseen\nplaces and that our approach can pave the way for highly accurate, planet-scale\nimage geolocalization systems. Our code is available on GitHub.\n","authors":["Lukas Haas","Michal Skreta","Silas Alberti","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2307.05845v4.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2312.10694v1","updated":"2023-12-17T12:08:09Z","published":"2023-12-17T12:08:09Z","title":"Discretionary Trees: Understanding Street-Level Bureaucracy via Machine\n  Learning","summary":"  Street-level bureaucrats interact directly with people on behalf of\ngovernment agencies to perform a wide range of functions, including, for\nexample, administering social services and policing. A key feature of\nstreet-level bureaucracy is that the civil servants, while tasked with\nimplementing agency policy, are also granted significant discretion in how they\nchoose to apply that policy in individual cases. Using that discretion could be\nbeneficial, as it allows for exceptions to policies based on human interactions\nand evaluations, but it could also allow biases and inequities to seep into\nimportant domains of societal resource allocation. In this paper, we use\nmachine learning techniques to understand street-level bureaucrats' behavior.\nWe leverage a rich dataset that combines demographic and other information on\nhouseholds with information on which homelessness interventions they were\nassigned during a period when assignments were not formulaic. We find that\ncaseworker decisions in this time are highly predictable overall, and some, but\nnot all of this predictivity can be captured by simple decision rules. We\ntheorize that the decisions not captured by the simple decision rules can be\nconsidered applications of caseworker discretion. These discretionary decisions\nare far from random in both the characteristics of such households and in terms\nof the outcomes of the decisions. Caseworkers typically only apply discretion\nto households that would be considered less vulnerable. When they do apply\ndiscretion to assign households to more intensive interventions, the marginal\nbenefits to those households are significantly higher than would be expected if\nthe households were chosen at random; there is no similar reduction in marginal\nbenefit to households that are discretionarily allocated less intensive\ninterventions, suggesting that caseworkers are improving outcomes using their\nknowledge.\n","authors":["Gaurab Pokharel","Sanmay Das","Patrick J. Fowler"],"pdf_url":"https://arxiv.org/pdf/2312.10694v1.pdf","comment":"Accepted to AAAI2024 AISI track"},{"id":"http://arxiv.org/abs/2312.10693v1","updated":"2023-12-17T12:02:10Z","published":"2023-12-17T12:02:10Z","title":"An appointment with Reproducing Kernel Hilbert Space generated by\n  Generalized Gaussian RBF as $L^2-$measure","summary":"  Gaussian Radial Basis Function (RBF) Kernels are the most-often-employed\nkernels in artificial intelligence and machine learning routines for providing\noptimally-best results in contrast to their respective counter-parts. However,\na little is known about the application of the Generalized Gaussian Radial\nBasis Function on various machine learning algorithms namely, kernel\nregression, support vector machine (SVM) and pattern-recognition via neural\nnetworks. The results that are yielded by Generalized Gaussian RBF in the\nkernel sense outperforms in stark contrast to Gaussian RBF Kernel, Sigmoid\nFunction and ReLU Function. This manuscript demonstrates the application of the\nGeneralized Gaussian RBF in the kernel sense on the aforementioned machine\nlearning routines along with the comparisons against the aforementioned\nfunctions as well.\n","authors":["Himanshu Singh"],"pdf_url":"https://arxiv.org/pdf/2312.10693v1.pdf","comment":"20 pages, MATLAB CODE, 11 figures, Results presented in AMS Spring\n  Eastern Sectional Meeting on April 2023"},{"id":"http://arxiv.org/abs/2207.08012v4","updated":"2023-12-17T11:12:37Z","published":"2022-07-16T20:37:46Z","title":"Meta-Referential Games to Learn Compositional Learning Behaviours","summary":"  Human beings use compositionality to generalise from past experiences to\nnovel experiences. We assume a separation of our experiences into fundamental\natomic components that can be recombined in novel ways to support our ability\nto engage with novel experiences. We frame this as the ability to learn to\ngeneralise compositionally, and we will refer to behaviours making use of this\nability as compositional learning behaviours (CLBs). A central problem to\nlearning CLBs is the resolution of a binding problem (BP). While it is another\nfeat of intelligence that human beings perform with ease, it is not the case\nfor state-of-the-art artificial agents. Thus, in order to build artificial\nagents able to collaborate with human beings, we propose to develop a novel\nbenchmark to investigate agents' abilities to exhibit CLBs by solving a\ndomain-agnostic version of the BP. We take inspiration from the language\nemergence and grounding framework of referential games and propose a\nmeta-learning extension of referential games, entitled Meta-Referential Games,\nand use this framework to build our benchmark, the Symbolic Behaviour Benchmark\n(S2B). We provide baseline results and error analysis showing that our\nbenchmark is a compelling challenge that we hope will spur the research\ncommunity towards developing more capable artificial agents.\n","authors":["Kevin Denamganaï","Sondess Missaoui","James Alfred Walker"],"pdf_url":"https://arxiv.org/pdf/2207.08012v4.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2201.12975v3","updated":"2023-12-17T10:16:48Z","published":"2022-01-31T03:07:17Z","title":"Rotting Infinitely Many-armed Bandits","summary":"  We consider the infinitely many-armed bandit problem with rotting rewards,\nwhere the mean reward of an arm decreases at each pull of the arm according to\nan arbitrary trend with maximum rotting rate $\\varrho=o(1)$. We show that this\nlearning problem has an $\\Omega(\\max\\{\\varrho^{1/3}T,\\sqrt{T}\\})$ worst-case\nregret lower bound where $T$ is the horizon time. We show that a matching upper\nbound $\\tilde{O}(\\max\\{\\varrho^{1/3}T,\\sqrt{T}\\})$, up to a poly-logarithmic\nfactor, can be achieved by an algorithm that uses a UCB index for each arm and\na threshold value to decide whether to continue pulling an arm or remove the\narm from further consideration, when the algorithm knows the value of the\nmaximum rotting rate $\\varrho$. We also show that an\n$\\tilde{O}(\\max\\{\\varrho^{1/3}T,T^{3/4}\\})$ regret upper bound can be achieved\nby an algorithm that does not know the value of $\\varrho$, by using an adaptive\nUCB index along with an adaptive threshold value.\n","authors":["Jung-hun Kim","Milan Vojnovic","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2201.12975v3.pdf","comment":"ICML2022"},{"id":"http://arxiv.org/abs/2312.10672v1","updated":"2023-12-17T10:13:42Z","published":"2023-12-17T10:13:42Z","title":"Automatic Optimisation of Normalised Neural Networks","summary":"  We propose automatic optimisation methods considering the geometry of matrix\nmanifold for the normalised parameters of neural networks. Layerwise weight\nnormalisation with respect to Frobenius norm is utilised to bound the Lipschitz\nconstant and to enhance gradient reliability so that the trained networks are\nsuitable for control applications. Our approach first initialises the network\nand normalises the data with respect to the $\\ell^{2}$-$\\ell^{2}$ gain of the\ninitialised network. Then, the proposed algorithms take the update structure\nbased on the exponential map on high-dimensional spheres. Given an update\ndirection such as that of the negative Riemannian gradient, we propose two\ndifferent ways to determine the stepsize for descent. The first algorithm\nutilises automatic differentiation of the objective function along the update\ncurve defined on the combined manifold of spheres. The directional second-order\nderivative information can be utilised without requiring explicit construction\nof the Hessian. The second algorithm utilises the majorisation-minimisation\nframework via architecture-aware majorisation for neural networks. With these\nnew developments, the proposed methods avoid manual tuning and scheduling of\nthe learning rate, thus providing an automated pipeline for optimizing\nnormalised neural networks.\n","authors":["Namhoon Cho","Hyo-Sang Shin"],"pdf_url":"https://arxiv.org/pdf/2312.10672v1.pdf","comment":"13 pages, 2 figures, submitted to 2024 L4DC"},{"id":"http://arxiv.org/abs/2303.01213v2","updated":"2023-12-17T10:04:11Z","published":"2023-03-02T12:54:12Z","title":"DSD$^2$: Can We Dodge Sparse Double Descent and Compress the Neural\n  Network Worry-Free?","summary":"  Neoteric works have shown that modern deep learning models can exhibit a\nsparse double descent phenomenon. Indeed, as the sparsity of the model\nincreases, the test performance first worsens since the model is overfitting\nthe training data; then, the overfitting reduces, leading to an improvement in\nperformance, and finally, the model begins to forget critical information,\nresulting in underfitting. Such a behavior prevents using traditional early\nstop criteria. In this work, we have three key contributions. First, we propose\na learning framework that avoids such a phenomenon and improves generalization.\nSecond, we introduce an entropy measure providing more insights into the\ninsurgence of this phenomenon and enabling the use of traditional stop\ncriteria. Third, we provide a comprehensive quantitative analysis of contingent\nfactors such as re-initialization methods, model width and depth, and dataset\nnoise. The contributions are supported by empirical evidence in typical setups.\nOur code is available at https://github.com/VGCQ/DSD2.\n","authors":["Victor Quétu","Enzo Tartaglione"],"pdf_url":"https://arxiv.org/pdf/2303.01213v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01168v5","updated":"2023-12-17T10:00:55Z","published":"2023-04-03T17:37:00Z","title":"DeepAccident: A Motion and Accident Prediction Benchmark for V2X\n  Autonomous Driving","summary":"  Safety is the primary priority of autonomous driving. Nevertheless, no\npublished dataset currently supports the direct and explainable safety\nevaluation for autonomous driving. In this work, we propose DeepAccident, a\nlarge-scale dataset generated via a realistic simulator containing diverse\naccident scenarios that frequently occur in real-world driving. The proposed\nDeepAccident dataset includes 57K annotated frames and 285K annotated samples,\napproximately 7 times more than the large-scale nuScenes dataset with 40k\nannotated samples. In addition, we propose a new task, end-to-end motion and\naccident prediction, which can be used to directly evaluate the accident\nprediction ability for different autonomous driving algorithms. Furthermore,\nfor each scenario, we set four vehicles along with one infrastructure to record\ndata, thus providing diverse viewpoints for accident scenarios and enabling V2X\n(vehicle-to-everything) research on perception and prediction tasks. Finally,\nwe present a baseline V2X model named V2XFormer that demonstrates superior\nperformance for motion and accident prediction and 3D object detection compared\nto the single-vehicle model.\n","authors":["Tianqi Wang","Sukmin Kim","Wenxuan Ji","Enze Xie","Chongjian Ge","Junsong Chen","Zhenguo Li","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2304.01168v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10666v1","updated":"2023-12-17T09:44:41Z","published":"2023-12-17T09:44:41Z","title":"CACTO-SL: Using Sobolev Learning to improve Continuous Actor-Critic with\n  Trajectory Optimization","summary":"  Trajectory Optimization (TO) and Reinforcement Learning (RL) are powerful and\ncomplementary tools to solve optimal control problems. On the one hand, TO can\nefficiently compute locally-optimal solutions, but it tends to get stuck in\nlocal minima if the problem is not convex. On the other hand, RL is typically\nless sensitive to non-convexity, but it requires a much higher computational\neffort. Recently, we have proposed CACTO (Continuous Actor-Critic with\nTrajectory Optimization), an algorithm that uses TO to guide the exploration of\nan actor-critic RL algorithm. In turns, the policy encoded by the actor is used\nto warm-start TO, closing the loop between TO and RL. In this work, we present\nan extension of CACTO exploiting the idea of Sobolev learning. To make the\ntraining of the critic network faster and more data efficient, we enrich it\nwith the gradient of the Value function, computed via a backward pass of the\ndifferential dynamic programming algorithm. Our results show that the new\nalgorithm is more efficient than the original CACTO, reducing the number of TO\nepisodes by a factor ranging from 3 to 10, and consequently the computation\ntime. Moreover, we show that CACTO-SL helps TO to find better minima and to\nproduce more consistent results.\n","authors":["Elisa Alboni","Gianluigi Grandesso","Gastone Pietro Rosati Papini","Justin Carpentier","Andrea Del Prete"],"pdf_url":"https://arxiv.org/pdf/2312.10666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17326v2","updated":"2023-12-17T09:04:47Z","published":"2023-11-29T02:53:32Z","title":"Mostly Beneficial Clustering: Aggregating Data for Operational Decision\n  Making","summary":"  With increasingly volatile market conditions and rapid product innovations,\noperational decision-making for large-scale systems entails solving thousands\nof problems with limited data. Data aggregation is proposed to combine the data\nacross problems to improve the decisions obtained by solving those problems\nindividually. We propose a novel cluster-based Shrunken-SAA approach that can\nexploit the cluster structure among problems when implementing the data\naggregation approaches. We prove that, as the number of problems grows,\nleveraging the given cluster structure among problems yields additional\nbenefits over the data aggregation approaches that neglect such structure. When\nthe cluster structure is unknown, we show that unveiling the cluster structure,\neven at the cost of a few data points, can be beneficial, especially when the\ndistance between clusters of problems is substantial. Our proposed approach can\nbe extended to general cost functions under mild conditions. When the number of\nproblems gets large, the optimality gap of our proposed approach decreases\nexponentially in the distance between the clusters. We explore the performance\nof the proposed approach through the application of managing newsvendor systems\nvia numerical experiments. We investigate the impacts of distance metrics\nbetween problem instances on the performance of the cluster-based Shrunken-SAA\napproach with synthetic data. We further validate our proposed approach with\nreal data and highlight the advantages of cluster-based data aggregation,\nespecially in the small-data large-scale regime, compared to the existing\napproaches.\n","authors":["Chengzhang Li","Zhenkang Peng","Ying Rong"],"pdf_url":"https://arxiv.org/pdf/2311.17326v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12435v2","updated":"2023-12-17T08:45:26Z","published":"2023-11-21T08:44:38Z","title":"Fair Enough? A map of the current limitations of the requirements to\n  have \"fair\" algorithms","summary":"  In the recent years, the raise in the usage and efficiency of Artificial\nIntelligence and, more in general, of Automated Decision-Making systems has\nbrought with it an increasing and welcome awareness of the risks associated\nwith such systems. One of such risks is that of perpetuating or even amplifying\nbias and unjust disparities present in the data from which many of these\nsystems learn to adjust and optimise their decisions. This awareness has on one\nside encouraged several scientific communities to come up with more and more\nappropriate ways and methods to assess, quantify, and possibly mitigate such\nbiases and disparities. On the other hand, it has prompted more and more layers\nof society, including policy makers, to call for \"fair\" algorithms. We believe\nthat while a lot of excellent and multidisciplinary research is currently being\nconducted, what is still fundamentally missing is the awareness that having\n\"fair\" algorithms is per se a nearly meaningless requirement, that needs to be\ncomplemented with a lot of additional societal choices to become actionable.\nNamely, there is a hiatus between what the society is demanding from Automated\nDecision-Making systems, and what this demand actually means in real-world\nscenarios. In this work, we outline the key features of such a hiatus, and\npinpoint a list of fundamental ambiguities and attention points that we as a\nsociety must address in order to give a concrete meaning to the increasing\ndemand of fairness in Automated Decision-Making systems.\n","authors":["Alessandro Castelnovo","Nicole Inverardi","Gabriele Nanino","Ilaria Giuseppina Penco","Daniele Regoli"],"pdf_url":"https://arxiv.org/pdf/2311.12435v2.pdf","comment":"20 pages, 2 figures, 2 tables. V2: added reference, update info on AI\n  Act"},{"id":"http://arxiv.org/abs/2307.01504v2","updated":"2023-12-17T08:36:44Z","published":"2023-07-04T06:27:31Z","title":"All in One: Multi-task Prompting for Graph Neural Networks","summary":"  Recently, ''pre-training and fine-tuning'' has been adopted as a standard\nworkflow for many graph tasks since it can take general graph knowledge to\nrelieve the lack of graph annotations from each application. However, graph\ntasks with node level, edge level, and graph level are far diversified, making\nthe pre-training pretext often incompatible with these multiple tasks. This gap\nmay even cause a ''negative transfer'' to the specific application, leading to\npoor results. Inspired by the prompt learning in natural language processing\n(NLP), which has presented significant effectiveness in leveraging prior\nknowledge for various NLP tasks, we study the prompting topic for graphs with\nthe motivation of filling the gap between pre-trained models and various graph\ntasks. In this paper, we propose a novel multi-task prompting method for graph\nmodels. Specifically, we first unify the format of graph prompts and language\nprompts with the prompt token, token structure, and inserting pattern. In this\nway, the prompting idea from NLP can be seamlessly introduced to the graph\narea. Then, to further narrow the gap between various graph tasks and\nstate-of-the-art pre-training strategies, we further study the task space of\nvarious graph applications and reformulate downstream problems to the\ngraph-level task. Afterward, we introduce meta-learning to efficiently learn a\nbetter initialization for the multi-task prompt of graphs so that our prompting\nframework can be more reliable and general for different tasks. We conduct\nextensive experiments, results from which demonstrate the superiority of our\nmethod.\n","authors":["Xiangguo Sun","Hong Cheng","Jia Li","Bo Liu","Jihong Guan"],"pdf_url":"https://arxiv.org/pdf/2307.01504v2.pdf","comment":"KDD 23 Best Research Paper Award, which is the first for Hong Kong\n  and Mainland China. A Python Library is released as ProG:\n  https://github.com/sheldonresearch/ProG Submitted to SIGKDD'23 in 03 Feb\n  2023; Receive Acceptance in 17 May 2023 (Rating 3 4 4 4); Submit to arXiv 1st\n  time in 4 Jul 2023"},{"id":"http://arxiv.org/abs/2310.13916v2","updated":"2023-12-17T08:25:42Z","published":"2023-10-21T06:13:19Z","title":"Southern Ocean Dynamics Under Climate Change: New Knowledge Through\n  Physics-Guided Machine Learning","summary":"  Complex ocean systems such as the Antarctic Circumpolar Current play key\nroles in the climate, and current models predict shifts in their strength and\narea under climate change. However, the physical processes underlying these\nchanges are not well understood, in part due to the difficulty of\ncharacterizing and tracking changes in ocean physics in complex models. Using\nthe Antarctic Circumpolar Current as a case study, we extend the method\nTracking global Heating with Ocean Regimes (THOR) to a mesoscale eddy\npermitting climate model and identify regions of the ocean characterized by\nsimilar physics, called dynamical regimes, using readily accessible fields from\nclimate models. To this end, we cluster grid cells into dynamical regimes and\ntrain an ensemble of neural networks, allowing uncertainty quantification, to\npredict these regimes and track them under climate change. Finally, we leverage\nthis new knowledge to elucidate the dynamical drivers of the identified regime\nshifts as noted by the neural network using the 'explainability' methods SHAP\nand Layer-wise Relevance Propagation. A region undergoing a profound shift is\nwhere the Antarctic Circumpolar Current intersects the Pacific-Antarctic Ridge,\nan area important for carbon draw-down and fisheries. In this region, THOR\nspecifically reveals a shift in dynamical regime under climate change driven by\nchanges in wind stress and interactions with bathymetry. Using this knowledge\nto guide further exploration, we find that as the Antarctic Circumpolar Current\nshifts north under intensifying wind stress, the dominant dynamical role of\nbathymetry weakens and the flow intensifies.\n","authors":["William Yik","Maike Sonnewald","Mariana C. A. Clare","Redouane Lguensat"],"pdf_url":"https://arxiv.org/pdf/2310.13916v2.pdf","comment":"14 pages, 11 figures, NeurIPS 2023 Workshop: Tackling Climate Change\n  with Machine Learning"},{"id":"http://arxiv.org/abs/2312.10648v1","updated":"2023-12-17T08:24:44Z","published":"2023-12-17T08:24:44Z","title":"Faithful Model Explanations through Energy-Constrained Conformal\n  Counterfactuals","summary":"  Counterfactual explanations offer an intuitive and straightforward way to\nexplain black-box models and offer algorithmic recourse to individuals. To\naddress the need for plausible explanations, existing work has primarily relied\non surrogate models to learn how the input data is distributed. This\neffectively reallocates the task of learning realistic explanations for the\ndata from the model itself to the surrogate. Consequently, the generated\nexplanations may seem plausible to humans but need not necessarily describe the\nbehaviour of the black-box model faithfully. We formalise this notion of\nfaithfulness through the introduction of a tailored evaluation metric and\npropose a novel algorithmic framework for generating Energy-Constrained\nConformal Counterfactuals that are only as plausible as the model permits.\nThrough extensive empirical studies, we demonstrate that ECCCo reconciles the\nneed for faithfulness and plausibility. In particular, we show that for models\nwith gradient access, it is possible to achieve state-of-the-art performance\nwithout the need for surrogate models. To do so, our framework relies solely on\nproperties defining the black-box model itself by leveraging recent advances in\nenergy-based modelling and conformal prediction. To our knowledge, this is the\nfirst venture in this direction for generating faithful counterfactual\nexplanations. Thus, we anticipate that ECCCo can serve as a baseline for future\nresearch. We believe that our work opens avenues for researchers and\npractitioners seeking tools to better distinguish trustworthy from unreliable\nmodels.\n","authors":["Patrick Altmeyer","Mojtaba Farmanbar","Arie van Deursen","Cynthia C. S. Liem"],"pdf_url":"https://arxiv.org/pdf/2312.10648v1.pdf","comment":"7 pages main paper, 34 pages appendix. Pre-print of upcoming\n  proceedings paper (Association for the Advancement of Artificial Intelligence\n  (www.aaai.org))"},{"id":"http://arxiv.org/abs/2310.14845v2","updated":"2023-12-17T08:16:44Z","published":"2023-10-23T12:11:13Z","title":"ULTRA-DP: Unifying Graph Pre-training with Multi-task Graph Dual Prompt","summary":"  Recent research has demonstrated the efficacy of pre-training graph neural\nnetworks (GNNs) to capture the transferable graph semantics and enhance the\nperformance of various downstream tasks. However, the semantic knowledge\nlearned from pretext tasks might be unrelated to the downstream task, leading\nto a semantic gap that limits the application of graph pre-training. To reduce\nthis gap, traditional approaches propose hybrid pre-training to combine various\npretext tasks together in a multi-task learning fashion and learn multi-grained\nknowledge, which, however, cannot distinguish tasks and results in some\ntransferable task-specific knowledge distortion by each other. Moreover, most\nGNNs cannot distinguish nodes located in different parts of the graph, making\nthem fail to learn position-specific knowledge and lead to suboptimal\nperformance. In this work, inspired by the prompt-based tuning in natural\nlanguage processing, we propose a unified framework for graph hybrid\npre-training which injects the task identification and position identification\ninto GNNs through a prompt mechanism, namely multi-task graph dual prompt\n(ULTRA-DP). Based on this framework, we propose a prompt-based transferability\ntest to find the most relevant pretext task in order to reduce the semantic\ngap. To implement the hybrid pre-training tasks, beyond the classical edge\nprediction task (node-node level), we further propose a novel pre-training\nparadigm based on a group of $k$-nearest neighbors (node-group level). The\ncombination of them across different scales is able to comprehensively express\nmore structural semantics and derive richer multi-grained knowledge. Extensive\nexperiments show that our proposed ULTRA-DP can significantly enhance the\nperformance of hybrid pre-training methods and show the generalizability to\nother pre-training tasks and backbone architectures.\n","authors":["Mouxiang Chen","Zemin Liu","Chenghao Liu","Jundong Li","Qiheng Mao","Jianling Sun"],"pdf_url":"https://arxiv.org/pdf/2310.14845v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10642v1","updated":"2023-12-17T07:58:19Z","published":"2023-12-17T07:58:19Z","title":"Episodic Return Decomposition by Difference of Implicitly Assigned\n  Sub-Trajectory Reward","summary":"  Real-world decision-making problems are usually accompanied by delayed\nrewards, which affects the sample efficiency of Reinforcement Learning,\nespecially in the extremely delayed case where the only feedback is the\nepisodic reward obtained at the end of an episode. Episodic return\ndecomposition is a promising way to deal with the episodic-reward setting.\nSeveral corresponding algorithms have shown remarkable effectiveness of the\nlearned step-wise proxy rewards from return decomposition. However, these\nexisting methods lack either attribution or representation capacity, leading to\ninefficient decomposition in the case of long-term episodes. In this paper, we\npropose a novel episodic return decomposition method called Diaster (Difference\nof implicitly assigned sub-trajectory reward). Diaster decomposes any episodic\nreward into credits of two divided sub-trajectories at any cut point, and the\nstep-wise proxy rewards come from differences in expectation. We theoretically\nand empirically verify that the decomposed proxy reward function can guide the\npolicy to be nearly optimal. Experimental results show that our method\noutperforms previous state-of-the-art methods in terms of both sample\nefficiency and performance.\n","authors":["Haoxin Lin","Hongqiu Wu","Jiaji Zhang","Yihao Sun","Junyin Ye","Yang Yu"],"pdf_url":"https://arxiv.org/pdf/2312.10642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10639v3","updated":"2023-12-17T07:34:17Z","published":"2023-09-19T14:20:55Z","title":"Geometric structure of Deep Learning networks and construction of global\n  ${\\mathcal L}^2$ minimizers","summary":"  In this paper, we provide a geometric interpretation of the structure of Deep\nLearning (DL) networks, characterized by $L$ hidden layers, a ReLU ramp\nactivation function, an $\\mathcal{L}^2$ Schatten class (or Hilbert-Schmidt)\ncost function, and input and output spaces $\\mathbb{R}^Q$ with equal dimension\n$Q\\geq1$. The hidden layers are also defined on $\\mathbb{R}^{Q}$; the training\ninput size $N$ can be arbitrarily large - thus, we are considering the\nunderparametrized regime. We apply our recent results on shallow neural\nnetworks to construct an explicit family of minimizers for the global minimum\nof the cost function in the case $L\\geq Q$, which we show to be degenerate. In\nthe context presented here, the hidden layers of the DL network \"curate\" the\ntraining inputs by recursive application of a truncation map that minimizes the\nnoise to signal ratio of the training inputs. Moreover, we determine a set of\n$2^Q-1$ distinct degenerate local minima of the cost function. Our\nconstructions make no use of gradient descent algorithms at all.\n","authors":["Thomas Chen","Patricia Muñoz Ewald"],"pdf_url":"https://arxiv.org/pdf/2309.10639v3.pdf","comment":"AMS Latex, 21 pages. Typos corrected, slightly extended"},{"id":"http://arxiv.org/abs/2312.10634v1","updated":"2023-12-17T07:33:06Z","published":"2023-12-17T07:33:06Z","title":"Anomaly Score: Evaluating Generative Models and Individual Generated\n  Images based on Complexity and Vulnerability","summary":"  With the advancement of generative models, the assessment of generated images\nbecomes more and more important. Previous methods measure distances between\nfeatures of reference and generated images from trained vision models. In this\npaper, we conduct an extensive investigation into the relationship between the\nrepresentation space and input space around generated images. We first propose\ntwo measures related to the presence of unnatural elements within images:\ncomplexity, which indicates how non-linear the representation space is, and\nvulnerability, which is related to how easily the extracted feature changes by\nadversarial input changes. Based on these, we introduce a new metric to\nevaluating image-generative models called anomaly score (AS). Moreover, we\npropose AS-i (anomaly score for individual images) that can effectively\nevaluate generated images individually. Experimental results demonstrate the\nvalidity of the proposed approach.\n","authors":["Jaehui Hwang","Junghyuk Lee","Jong-Seok Lee"],"pdf_url":"https://arxiv.org/pdf/2312.10634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.11730v2","updated":"2023-12-17T07:21:57Z","published":"2023-08-22T18:41:31Z","title":"Knowledge Graph Prompting for Multi-Document Question Answering","summary":"  The `pre-train, prompt, predict' paradigm of large language models (LLMs) has\nachieved remarkable success in open-domain question answering (OD-QA). However,\nfew works explore this paradigm in the scenario of multi-document question\nanswering (MD-QA), a task demanding a thorough understanding of the logical\nassociations among the contents and structures of different documents. To fill\nthis crucial gap, we propose a Knowledge Graph Prompting (KGP) method to\nformulate the right context in prompting LLMs for MD-QA, which consists of a\ngraph construction module and a graph traversal module. For graph construction,\nwe create a knowledge graph (KG) over multiple documents with nodes symbolizing\npassages or document structures (e.g., pages/tables), and edges denoting the\nsemantic/lexical similarity between passages or intra-document structural\nrelations. For graph traversal, we design an LLM-based graph traversal agent\nthat navigates across nodes and gathers supporting passages assisting LLMs in\nMD-QA. The constructed graph serves as the global ruler that regulates the\ntransitional space among passages and reduces retrieval latency. Concurrently,\nthe graph traversal agent acts as a local navigator that gathers pertinent\ncontext to progressively approach the question and guarantee retrieval quality.\nExtensive experiments underscore the efficacy of KGP for MD-QA, signifying the\npotential of leveraging graphs in enhancing the prompt design for LLMs. Our\ncode: https://github.com/YuWVandy/KG-LLM-MDQA.\n","authors":["Yu Wang","Nedim Lipka","Ryan A. Rossi","Alexa Siu","Ruiyi Zhang","Tyler Derr"],"pdf_url":"https://arxiv.org/pdf/2308.11730v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10626v1","updated":"2023-12-17T06:55:04Z","published":"2023-12-17T06:55:04Z","title":"Decoding Concerns: Multi-label Classification of Vaccine Sentiments in\n  Social Media","summary":"  In the realm of public health, vaccination stands as the cornerstone for\nmitigating disease risks and controlling their proliferation. The recent\nCOVID-19 pandemic has highlighted how vaccines play a crucial role in keeping\nus safe. However the situation involves a mix of perspectives, with skepticism\ntowards vaccines prevailing for various reasons such as political dynamics,\napprehensions about side effects, and more. The paper addresses the challenge\nof comprehensively understanding and categorizing these diverse concerns\nexpressed in the context of vaccination. Our focus is on developing a robust\nmulti-label classifier capable of assigning specific concern labels to tweets\nbased on the articulated apprehensions towards vaccines. To achieve this, we\ndelve into the application of a diverse set of advanced natural language\nprocessing techniques and machine learning algorithms including transformer\nmodels like BERT, state of the art GPT 3.5, Classifier Chains & traditional\nmethods like SVM, Random Forest, Naive Bayes. We see that the cutting-edge\nlarge language model outperforms all other methods in this context.\n","authors":["Somsubhra De","Shaurya Vats"],"pdf_url":"https://arxiv.org/pdf/2312.10626v1.pdf","comment":"13 pages, Submitted to the AISoMe Track at FIRE 2023"},{"id":"http://arxiv.org/abs/2312.10618v1","updated":"2023-12-17T06:12:33Z","published":"2023-12-17T06:12:33Z","title":"Sparse Learning and Class Probability Estimation with Weighted Support\n  Vector Machines","summary":"  Classification and probability estimation have broad applications in modern\nmachine learning and data science applications, including biology, medicine,\nengineering, and computer science. The recent development of a class of\nweighted Support Vector Machines (wSVMs) has shown great values in robustly\npredicting the class probability and classification for various problems with\nhigh accuracy. The current framework is based on the $\\ell^2$-norm regularized\nbinary wSVMs optimization problem, which only works with dense features and has\npoor performance at sparse features with redundant noise in most real\napplications. The sparse learning process requires a prescreen of the important\nvariables for each binary wSVMs for accurately estimating pairwise conditional\nprobability. In this paper, we proposed novel wSVMs frameworks that incorporate\nautomatic variable selection with accurate probability estimation for sparse\nlearning problems. We developed efficient algorithms for effective variable\nselection for solving either the $\\ell^1$-norm or elastic net regularized\nbinary wSVMs optimization problems. The binary class probability is then\nestimated either by the $\\ell^2$-norm regularized wSVMs framework with selected\nvariables or by elastic net regularized wSVMs directly. The two-step approach\nof $\\ell^1$-norm followed by $\\ell^2$-norm wSVMs show a great advantage in both\nautomatic variable selection and reliable probability estimators with the most\nefficient time. The elastic net regularized wSVMs offer the best performance in\nterms of variable selection and probability estimation with the additional\nadvantage of variable grouping in the compensation of more computation time for\nhigh dimensional problems. The proposed wSVMs-based sparse learning methods\nhave wide applications and can be further extended to $K$-class problems\nthrough ensemble learning.\n","authors":["Liyun Zeng","Hao Helen Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.10618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08262v2","updated":"2023-12-17T06:05:22Z","published":"2022-12-16T03:13:43Z","title":"Uniform Sequence Better: Time Interval Aware Data Augmentation for\n  Sequential Recommendation","summary":"  Sequential recommendation is an important task to predict the next-item to\naccess based on a sequence of interacted items. Most existing works learn user\npreference as the transition pattern from the previous item to the next one,\nignoring the time interval between these two items. However, we observe that\nthe time interval in a sequence may vary significantly different, and thus\nresult in the ineffectiveness of user modeling due to the issue of\n\\emph{preference drift}. In fact, we conducted an empirical study to validate\nthis observation, and found that a sequence with uniformly distributed time\ninterval (denoted as uniform sequence) is more beneficial for performance\nimprovement than that with greatly varying time interval. Therefore, we propose\nto augment sequence data from the perspective of time interval, which is not\nstudied in the literature. Specifically, we design five operators (Ti-Crop,\nTi-Reorder, Ti-Mask, Ti-Substitute, Ti-Insert) to transform the original\nnon-uniform sequence to uniform sequence with the consideration of variance of\ntime intervals. Then, we devise a control strategy to execute data augmentation\non item sequences in different lengths. Finally, we implement these\nimprovements on a state-of-the-art model CoSeRec and validate our approach on\nfour real datasets. The experimental results show that our approach reaches\nsignificantly better performance than the other 11 competing methods. Our\nimplementation is available: https://github.com/KingGugu/TiCoSeRec.\n","authors":["Yizhou Dang","Enneng Yang","Guibing Guo","Linying Jiang","Xingwei Wang","Xiaoxiao Xu","Qinghui Sun","Hong Liu"],"pdf_url":"https://arxiv.org/pdf/2212.08262v2.pdf","comment":"9 pages, 4 figures, AAAI-2023"},{"id":"http://arxiv.org/abs/2312.10617v1","updated":"2023-12-17T06:03:33Z","published":"2023-12-17T06:03:33Z","title":"Deep dive into language traits of AI-generated Abstracts","summary":"  Generative language models, such as ChatGPT, have garnered attention for\ntheir ability to generate human-like writing in various fields, including\nacademic research. The rapid proliferation of generated texts has bolstered the\nneed for automatic identification to uphold transparency and trust in the\ninformation. However, these generated texts closely resemble human writing and\noften have subtle differences in the grammatical structure, tones, and\npatterns, which makes systematic scrutinization challenging. In this work, we\nattempt to detect the Abstracts generated by ChatGPT, which are much shorter in\nlength and bounded. We extract the texts semantic and lexical properties and\nobserve that traditional machine learning models can confidently detect these\nAbstracts.\n","authors":["Vikas Kumar","Amisha Bharti","Devanshu Verma","Vasudha Bhatnagar"],"pdf_url":"https://arxiv.org/pdf/2312.10617v1.pdf","comment":"Accepted for Cods-Comad Conference"},{"id":"http://arxiv.org/abs/2308.11940v3","updated":"2023-12-17T06:01:27Z","published":"2023-08-23T06:21:46Z","title":"Audio Generation with Multiple Conditional Diffusion Model","summary":"  Text-based audio generation models have limitations as they cannot encompass\nall the information in audio, leading to restricted controllability when\nrelying solely on text. To address this issue, we propose a novel model that\nenhances the controllability of existing pre-trained text-to-audio models by\nincorporating additional conditions including content (timestamp) and style\n(pitch contour and energy contour) as supplements to the text. This approach\nachieves fine-grained control over the temporal order, pitch, and energy of\ngenerated audio. To preserve the diversity of generation, we employ a trainable\ncontrol condition encoder that is enhanced by a large language model and a\ntrainable Fusion-Net to encode and fuse the additional conditions while keeping\nthe weights of the pre-trained text-to-audio model frozen. Due to the lack of\nsuitable datasets and evaluation metrics, we consolidate existing datasets into\na new dataset comprising the audio and corresponding conditions and use a\nseries of evaluation metrics to evaluate the controllability performance.\nExperimental results demonstrate that our model successfully achieves\nfine-grained control to accomplish controllable audio generation. Audio samples\nand our dataset are publicly available at\nhttps://conditionaudiogen.github.io/conditionaudiogen/\n","authors":["Zhifang Guo","Jianguo Mao","Rui Tao","Long Yan","Kazushige Ouchi","Hong Liu","Xiangdong Wang"],"pdf_url":"https://arxiv.org/pdf/2308.11940v3.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.10603v1","updated":"2023-12-17T04:41:59Z","published":"2023-12-17T04:41:59Z","title":"Evaluating AI Vocational Skills Through Professional Testing","summary":"  Using a novel professional certification survey, the study focuses on\nassessing the vocational skills of two highly cited AI models, GPT-3 and\nTurbo-GPT3.5. The approach emphasizes the importance of practical readiness\nover academic performance by examining the models' performances on a benchmark\ndataset consisting of 1149 professional certifications. This study also\nincludes a comparison with human test scores, providing perspective on the\npotential of AI models to match or even surpass human performance in\nprofessional certifications. GPT-3, even without any fine-tuning or exam\npreparation, managed to achieve a passing score (over 70% correct) on 39% of\nthe professional certifications. It showcased proficiency in computer-related\nfields, including cloud and virtualization, business analytics, cybersecurity,\nnetwork setup and repair, and data analytics. Turbo-GPT3.5, on the other hand,\nscored a perfect 100% on the highly regarded Offensive Security Certified\nProfessional (OSCP) exam. This model also demonstrated competency in diverse\nprofessional fields, such as nursing, licensed counseling, pharmacy, and\naviation. Turbo-GPT3.5 exhibited strong performance on customer service tasks,\nindicating potential use cases in enhancing chatbots for call centers and\nroutine advice services. Both models also scored well on sensory and\nexperience-based tests outside a machine's traditional roles, including wine\nsommelier, beer tasting, emotional quotient, and body language reading. The\nstudy found that OpenAI's model improvement from Babbage to Turbo led to a 60%\nbetter performance on the grading scale within a few years. This progress\nindicates that addressing the current model's limitations could yield an AI\ncapable of passing even the most rigorous professional certifications.\n","authors":["David Noever","Matt Ciolino"],"pdf_url":"https://arxiv.org/pdf/2312.10603v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2305.05377"},{"id":"http://arxiv.org/abs/2312.10602v1","updated":"2023-12-17T04:41:07Z","published":"2023-12-17T04:41:07Z","title":"A Weighted K-Center Algorithm for Data Subset Selection","summary":"  The success of deep learning hinges on enormous data and large models, which\nrequire labor-intensive annotations and heavy computation costs. Subset\nselection is a fundamental problem that can play a key role in identifying\nsmaller portions of the training data, which can then be used to produce\nsimilar models as the ones trained with full data. Two prior methods are shown\nto achieve impressive results: (1) margin sampling that focuses on selecting\npoints with high uncertainty, and (2) core-sets or clustering methods such as\nk-center for informative and diverse subsets. We are not aware of any work that\ncombines these methods in a principled manner. To this end, we develop a novel\nand efficient factor 3-approximation algorithm to compute subsets based on the\nweighted sum of both k-center and uncertainty sampling objective functions. To\nhandle large datasets, we show a parallel algorithm to run on multiple machines\nwith approximation guarantees. The proposed algorithm achieves similar or\nbetter performance compared to other strong baselines on vision datasets such\nas CIFAR-10, CIFAR-100, and ImageNet.\n","authors":["Srikumar Ramalingam","Pranjal Awasthi","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2312.10602v1.pdf","comment":"data selection, k-center, subset selection,"},{"id":"http://arxiv.org/abs/2210.07857v4","updated":"2023-12-17T03:33:23Z","published":"2022-10-14T14:29:32Z","title":"Commutativity and Disentanglement from the Manifold Perspective","summary":"  In this paper, we interpret disentanglement as the discovery of local charts\nof the data manifold and trace how this definition naturally leads to an\nequivalent condition for disentanglement: commutativity between factors of\nvariation. We study the impact of this manifold framework to two classes of\nproblems: learning matrix exponential operators and compressing data-generating\nmodels. In each problem, the manifold perspective yields interesting results\nabout the feasibility and fruitful approaches their solutions. We also link our\nmanifold framework to two other common disentanglement paradigms: group\ntheoretic and probabilistic approaches to disentanglement. In each case, we\nshow how these frameworks can be merged with our manifold perspective.\nImportantly, we recover commutativity as a central property in both alternative\nframeworks, further highlighting its importance in disentanglement.\n","authors":["Frank Qiu"],"pdf_url":"https://arxiv.org/pdf/2210.07857v4.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2312.10589v1","updated":"2023-12-17T02:42:11Z","published":"2023-12-17T02:42:11Z","title":"NN-Steiner: A Mixed Neural-algorithmic Approach for the Rectilinear\n  Steiner Minimum Tree Problem","summary":"  Recent years have witnessed rapid advances in the use of neural networks to\nsolve combinatorial optimization problems. Nevertheless, designing the \"right\"\nneural model that can effectively handle a given optimization problem can be\nchallenging, and often there is no theoretical understanding or justification\nof the resulting neural model. In this paper, we focus on the rectilinear\nSteiner minimum tree (RSMT) problem, which is of critical importance in IC\nlayout design and as a result has attracted numerous heuristic approaches in\nthe VLSI literature. Our contributions are two-fold. On the methodology front,\nwe propose NN-Steiner, which is a novel mixed neural-algorithmic framework for\ncomputing RSMTs that leverages the celebrated PTAS algorithmic framework of\nArora to solve this problem (and other geometric optimization problems). Our\nNN-Steiner replaces key algorithmic components within Arora's PTAS by suitable\nneural components. In particular, NN-Steiner only needs four neural network\n(NN) components that are called repeatedly within an algorithmic framework.\nCrucially, each of the four NN components is only of bounded size independent\nof input size, and thus easy to train. Furthermore, as the NN component is\nlearning a generic algorithmic step, once learned, the resulting mixed\nneural-algorithmic framework generalizes to much larger instances not seen in\ntraining. Our NN-Steiner, to our best knowledge, is the first neural\narchitecture of bounded size that has capacity to approximately solve RSMT (and\nvariants). On the empirical front, we show how NN-Steiner can be implemented\nand demonstrate the effectiveness of our resulting approach, especially in\nterms of generalization, by comparing with state-of-the-art methods (both\nneural or non-neural based).\n","authors":["Andrew B. Kahng","Robert R. Nerem","Yusu Wang","Chien-Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2312.10589v1.pdf","comment":"This paper is the full version with appendix of an accepted paper in\n  AAAI'24 with the same paper title"},{"id":"http://arxiv.org/abs/2312.10587v1","updated":"2023-12-17T02:23:25Z","published":"2023-12-17T02:23:25Z","title":"E2E-AT: A Unified Framework for Tackling Uncertainty in Task-aware\n  End-to-end Learning","summary":"  Successful machine learning involves a complete pipeline of data, model, and\ndownstream applications. Instead of treating them separately, there has been a\nprominent increase of attention within the constrained optimization (CO) and\nmachine learning (ML) communities towards combining prediction and optimization\nmodels. The so-called end-to-end (E2E) learning captures the task-based\nobjective for which they will be used for decision making. Although a large\nvariety of E2E algorithms have been presented, it has not been fully\ninvestigated how to systematically address uncertainties involved in such\nmodels. Most of the existing work considers the uncertainties of ML in the\ninput space and improves robustness through adversarial training. We apply the\nsame idea to E2E learning and prove that there is a robustness certification\nprocedure by solving augmented integer programming. Furthermore, we show that\nneglecting the uncertainty of COs during training causes a new trigger for\ngeneralization errors. To include all these components, we propose a unified\nframework that covers the uncertainties emerging in both the input feature\nspace of the ML models and the COs. The framework is described as a robust\noptimization problem and is practically solved via end-to-end adversarial\ntraining (E2E-AT). Finally, the performance of E2E-AT is evaluated by a\nreal-world end-to-end power system operation problem, including load\nforecasting and sequential scheduling tasks.\n","authors":["Wangkun Xu","Jianhong Wang","Fei Teng"],"pdf_url":"https://arxiv.org/pdf/2312.10587v1.pdf","comment":"Accepted by AAAI-24"},{"id":"http://arxiv.org/abs/2308.11978v3","updated":"2023-12-17T02:20:14Z","published":"2023-08-23T07:57:45Z","title":"Will More Expressive Graph Neural Networks do Better on Generative\n  Tasks?","summary":"  Graph generation poses a significant challenge as it involves predicting a\ncomplete graph with multiple nodes and edges based on simply a given label.\nThis task also carries fundamental importance to numerous real-world\napplications, including de-novo drug and molecular design. In recent years,\nseveral successful methods have emerged in the field of graph generation.\nHowever, these approaches suffer from two significant shortcomings: (1) the\nunderlying Graph Neural Network (GNN) architectures used in these methods are\noften underexplored; and (2) these methods are often evaluated on only a\nlimited number of metrics. To fill this gap, we investigate the expressiveness\nof GNNs under the context of the molecular graph generation task, by replacing\nthe underlying GNNs of graph generative models with more expressive GNNs.\nSpecifically, we analyse the performance of six GNNs on six different molecular\ngenerative objectives on the ZINC-250k dataset in two different generative\nframeworks: autoregressive generation models, such as GCPN and GraphAF, and\none-shot generation models, such as GraphEBM. Through our extensive\nexperiments, we demonstrate that advanced GNNs can indeed improve the\nperformance of GCPN, GraphAF, and GraphEBM on molecular generation tasks, but\nGNN expressiveness is not a necessary condition for a good GNN-based generative\nmodel. Moreover, we show that GCPN and GraphAF with advanced GNNs can achieve\nstate-of-the-art results across 17 other non-GNN-based graph generative\napproaches, such as variational autoencoders and Bayesian optimisation models,\non the proposed molecular generative objectives (DRD2, Median1, Median2), which\nare important metrics for de-novo molecular design.\n","authors":["Xiandong Zou","Xiangyu Zhao","Pietro Liò","Yiren Zhao"],"pdf_url":"https://arxiv.org/pdf/2308.11978v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10505v3","updated":"2023-12-17T02:18:45Z","published":"2023-10-16T15:25:14Z","title":"ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method\n  for Aligning Large Language Models","summary":"  Alignment is crucial for training large language models. The predominant\nstrategy is Reinforcement Learning from Human Feedback (RLHF), with Proximal\nPolicy Optimization (PPO) as the de-facto algorithm. Yet, PPO is known to\nstruggle with computational inefficiency, a challenge that this paper aims to\naddress. We identify three important properties of RLHF tasks: fast simulation,\ndeterministic transitions, and trajectory-level rewards, which are not\nleveraged in PPO. Based on these properties, we develop ReMax, a new algorithm\ntailored for RLHF. The design of ReMax builds on the celebrated algorithm\nREINFORCE but is enhanced with a new variance-reduction technique. ReMax offers\nthreefold advantages over PPO: first, it is simple to implement with just 6\nlines of code. It further eliminates more than 4 hyper-parameters in PPO, which\nare laborious to tune. Second, ReMax reduces memory usage by about 50%. To\nillustrate, PPO runs out of memory when fine-tuning a Llama2-7B model on\nA100-80GB GPUs, whereas ReMax can support the training. Even though\nmemory-efficient techniques (e.g., ZeRO and offload) are employed for PPO to\nafford training, ReMax can utilize a larger batch size to increase throughput.\nThird, in terms of wall-clock time, PPO is about twice as slow as ReMax per\niteration. Importantly, these improvements do not sacrifice task performance.\nWe hypothesize that these advantages can be maintained in larger-scale models.\n","authors":["Ziniu Li","Tian Xu","Yushun Zhang","Zhihang Lin","Yang Yu","Ruoyu Sun","Zhi-Quan Luo"],"pdf_url":"https://arxiv.org/pdf/2310.10505v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10585v1","updated":"2023-12-17T02:15:49Z","published":"2023-12-17T02:15:49Z","title":"ESDMR-Net: A Lightweight Network With Expand-Squeeze and Dual Multiscale\n  Residual Connections for Medical Image Segmentation","summary":"  Segmentation is an important task in a wide range of computer vision\napplications, including medical image analysis. Recent years have seen an\nincrease in the complexity of medical image segmentation approaches based on\nsophisticated convolutional neural network architectures. This progress has led\nto incremental enhancements in performance on widely recognised benchmark\ndatasets. However, most of the existing approaches are computationally\ndemanding, which limits their practical applicability. This paper presents an\nexpand-squeeze dual multiscale residual network (ESDMR-Net), which is a fully\nconvolutional network that is particularly well-suited for resource-constrained\ncomputing hardware such as mobile devices. ESDMR-Net focuses on extracting\nmultiscale features, enabling the learning of contextual dependencies among\nsemantically distinct features. The ESDMR-Net architecture allows dual-stream\ninformation flow within encoder-decoder pairs. The expansion operation\n(depthwise separable convolution) makes all of the rich features with\nmultiscale information available to the squeeze operation (bottleneck layer),\nwhich then extracts the necessary information for the segmentation task. The\nExpand-Squeeze (ES) block helps the network pay more attention to\nunder-represented classes, which contributes to improved segmentation accuracy.\nTo enhance the flow of information across multiple resolutions or scales, we\nintegrated dual multiscale residual (DMR) blocks into the skip connection. This\nintegration enables the decoder to access features from various levels of\nabstraction, ultimately resulting in more comprehensive feature\nrepresentations. We present experiments on seven datasets from five distinct\nexamples of applications. Our model achieved the best results despite having\nsignificantly fewer trainable parameters, with a reduction of two or even three\norders of magnitude.\n","authors":["Tariq M Khan","Syed S. Naqvi","Erik Meijering"],"pdf_url":"https://arxiv.org/pdf/2312.10585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10584v1","updated":"2023-12-17T02:14:15Z","published":"2023-12-17T02:14:15Z","title":"Policy Optimization in RLHF: The Impact of Out-of-preference Data","summary":"  Aligning intelligent agents with human preferences and values is important.\nThis paper examines two popular alignment methods: Direct Preference\nOptimization (DPO) and Reward-Model-Based Policy Optimization (RMB-PO). A\nvariant of RMB-PO, referred to as RMB-PO+ is also considered. These methods,\neither explicitly or implicitly, learn a reward model from preference data and\ndiffer in the data used for policy optimization to unlock the generalization\nability of the reward model. In particular, compared with DPO, RMB-PO\nadditionally uses policy-generated data, and RMB-PO+ further leverages new,\npreference-free data. We examine the impact of such out-of-preference data. Our\nstudy, conducted through controlled and synthetic experiments, demonstrates\nthat DPO performs poorly, whereas RMB-PO+ performs the best. In particular,\neven when providing the policy model with a good feature representation, we\nfind that policy optimization with adequate out-of-preference data\nsignificantly improves performance by harnessing the reward model's\ngeneralization capabilities.\n","authors":["Ziniu Li","Tian Xu","Yang Yu"],"pdf_url":"https://arxiv.org/pdf/2312.10584v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.02507v2","updated":"2023-12-17T01:56:44Z","published":"2023-07-05T03:47:28Z","title":"STS-CCL: Spatial-Temporal Synchronous Contextual Contrastive Learning\n  for Urban Traffic Forecasting","summary":"  Efficiently capturing the complex spatiotemporal representations from\nlarge-scale unlabeled traffic data remains to be a challenging task. In\nconsidering of the dilemma, this work employs the advanced contrastive learning\nand proposes a novel Spatial-Temporal Synchronous Contextual Contrastive\nLearning (STS-CCL) model. First, we elaborate the basic and strong augmentation\nmethods for spatiotemporal graph data, which not only perturb the data in terms\nof graph structure and temporal characteristics, but also employ a\nlearning-based dynamic graph view generator for adaptive augmentation. Second,\nwe introduce a Spatial-Temporal Synchronous Contrastive Module (STS-CM) to\nsimultaneously capture the decent spatial-temporal dependencies and realize\ngraph-level contrasting. To further discriminate node individuals in negative\nfiltering, a Semantic Contextual Contrastive method is designed based on\nsemantic features and spatial heterogeneity, achieving node-level contrastive\nlearning along with negative filtering. Finally, we present a hard mutual-view\ncontrastive training scheme and extend the classic contrastive loss to an\nintegrated objective function, yielding better performance. Extensive\nexperiments and evaluations demonstrate that building a predictor upon STS-CCL\ncontrastive learning model gains superior performance than existing traffic\nforecasting benchmarks. The proposed STS-CCL is highly suitable for large\ndatasets with only a few labeled data and other spatiotemporal tasks with data\nscarcity issue.\n","authors":["Lincan Li","Kaixiang Yang","Fengji Luo","Jichao Bi"],"pdf_url":"https://arxiv.org/pdf/2307.02507v2.pdf","comment":"This work was accepted by the 49th IEEE International Conference on\n  Acoustics, Speech, & Signal Processing (ICASSP 2024). We will present our\n  work in Seoul, Korea"},{"id":"http://arxiv.org/abs/2312.10580v1","updated":"2023-12-17T01:50:27Z","published":"2023-12-17T01:50:27Z","title":"Sentiment Analysis and Text Analysis of the Public Discourse on Twitter\n  about COVID-19 and MPox","summary":"  Mining and analysis of the big data of Twitter conversations have been of\nsignificant interest to the scientific community in the fields of healthcare,\nepidemiology, big data, data science, computer science, and their related\nareas, as can be seen from several works in the last few years that focused on\nsentiment analysis and other forms of text analysis of tweets related to Ebola,\nE-Coli, Dengue, Human Papillomavirus, Middle East Respiratory Syndrome,\nMeasles, Zika virus, H1N1, influenza like illness, swine flu, flu, Cholera,\nListeriosis, cancer, Liver Disease, Inflammatory Bowel Disease, kidney disease,\nlupus, Parkinsons, Diphtheria, and West Nile virus. The recent outbreaks of\nCOVID-19 and MPox have served as catalysts for Twitter usage related to seeking\nand sharing information, views, opinions, and sentiments involving both of\nthese viruses. None of the prior works in this field analyzed tweets focusing\non both COVID-19 and MPox simultaneously. To address this research gap, a total\nof 61,862 tweets that focused on MPox and COVID-19 simultaneously, posted\nbetween 7 May 2022 and 3 March 2023, were studied. The findings and\ncontributions of this study are manifold. First, the results of sentiment\nanalysis using the VADER approach show that nearly half the tweets had a\nnegative sentiment. It was followed by tweets that had a positive sentiment and\ntweets that had a neutral sentiment, respectively. Second, this paper presents\nthe top 50 hashtags used in these tweets. Third, it presents the top 100 most\nfrequently used words in these tweets after performing tokenization, removal of\nstopwords, and word frequency analysis. Finally, a comprehensive comparative\nstudy that compares the contributions of this paper with 49 prior works in this\nfield is presented to further uphold the relevance and novelty of this work.\n","authors":["Nirmalya Thakur"],"pdf_url":"https://arxiv.org/pdf/2312.10580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10578v1","updated":"2023-12-17T01:44:29Z","published":"2023-12-17T01:44:29Z","title":"SAME: Sample Reconstruction Against Model Extraction Attacks","summary":"  While deep learning models have shown significant performance across various\ndomains, their deployment needs extensive resources and advanced computing\ninfrastructure. As a solution, Machine Learning as a Service (MLaaS) has\nemerged, lowering the barriers for users to release or productize their deep\nlearning models. However, previous studies have highlighted potential privacy\nand security concerns associated with MLaaS, and one primary threat is model\nextraction attacks. To address this, there are many defense solutions but they\nsuffer from unrealistic assumptions and generalization issues, making them less\npractical for reliable protection. Driven by these limitations, we introduce a\nnovel defense mechanism, SAME, based on the concept of sample reconstruction.\nThis strategy imposes minimal prerequisites on the defender's capabilities,\neliminating the need for auxiliary Out-of-Distribution (OOD) datasets, user\nquery history, white-box model access, and additional intervention during model\ntraining. It is compatible with existing active defense methods. Our extensive\nexperiments corroborate the superior efficacy of SAME over state-of-the-art\nsolutions. Our code is available at https://github.com/xythink/SAME.\n","authors":["Yi Xie","Jie Zhang","Shiqian Zhao","Tianwei Zhang","Xiaofeng Chen"],"pdf_url":"https://arxiv.org/pdf/2312.10578v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2310.19629v2","updated":"2023-12-17T01:19:13Z","published":"2023-10-30T15:22:50Z","title":"RayDF: Neural Ray-surface Distance Fields with Multi-view Consistency","summary":"  In this paper, we study the problem of continuous 3D shape representations.\nThe majority of existing successful methods are coordinate-based implicit\nneural representations. However, they are inefficient to render novel views or\nrecover explicit surface points. A few works start to formulate 3D shapes as\nray-based neural functions, but the learned structures are inferior due to the\nlack of multi-view geometry consistency. To tackle these challenges, we propose\na new framework called RayDF. It consists of three major components: 1) the\nsimple ray-surface distance field, 2) the novel dual-ray visibility classifier,\nand 3) a multi-view consistency optimization module to drive the learned\nray-surface distances to be multi-view geometry consistent. We extensively\nevaluate our method on three public datasets, demonstrating remarkable\nperformance in 3D surface point reconstruction on both synthetic and\nchallenging real-world 3D scenes, clearly surpassing existing coordinate-based\nand ray-based baselines. Most notably, our method achieves a 1000x faster speed\nthan coordinate-based methods to render an 800x800 depth image, showing the\nsuperiority of our method for 3D shape representation. Our code and data are\navailable at https://github.com/vLAR-group/RayDF\n","authors":["Zhuoman Liu","Bo Yang","Yan Luximon","Ajay Kumar","Jinxi Li"],"pdf_url":"https://arxiv.org/pdf/2310.19629v2.pdf","comment":"Added the last 3 authors in the camera-ready version. NeurIPS 2023.\n  Code and data are available at: https://github.com/vLAR-group/RayDF"},{"id":"http://arxiv.org/abs/2312.10573v1","updated":"2023-12-17T01:11:03Z","published":"2023-12-17T01:11:03Z","title":"Random Forest Variable Importance-based Selection Algorithm in Class\n  Imbalance Problem","summary":"  Random Forest is a machine learning method that offers many advantages,\nincluding the ability to easily measure variable importance. Class balancing\ntechnique is a well-known solution to deal with class imbalance problem.\nHowever, it has not been actively studied on RF variable importance. In this\npaper, we study the effect of class balancing on RF variable importance. Our\nsimulation results show that over-sampling is effective in correctly measuring\nvariable importance in class imbalanced situations with small sample size,\nwhile under-sampling fails to differentiate important and non-informative\nvariables. We then propose a variable selection algorithm that utilizes RF\nvariable importance and its confidence interval. Through an experimental study\nusing many real and artificial datasets, we demonstrate that our proposed\nalgorithm efficiently selects an optimal feature set, leading to improved\nprediction performance in class imbalance problem.\n","authors":["Yunbi Nam","Sunwoo Han"],"pdf_url":"https://arxiv.org/pdf/2312.10573v1.pdf","comment":"20 pages, 3 figures"},{"id":"http://arxiv.org/abs/2312.10571v1","updated":"2023-12-17T00:47:13Z","published":"2023-12-17T00:47:13Z","title":"Multi-level Reasoning for Robotic Assembly: From Sequence Inference to\n  Contact Selection","summary":"  Automating the assembly of objects from their parts is a complex problem with\ninnumerable applications in manufacturing, maintenance, and recycling. Unlike\nexisting research, which is limited to target segmentation, pose regression, or\nusing fixed target blueprints, our work presents a holistic multi-level\nframework for part assembly planning consisting of part assembly sequence\ninference, part motion planning, and robot contact optimization. We present the\nPart Assembly Sequence Transformer (PAST) -- a sequence-to-sequence neural\nnetwork -- to infer assembly sequences recursively from a target blueprint. We\nthen use a motion planner and optimization to generate part movements and\ncontacts. To train PAST, we introduce D4PAS: a large-scale Dataset for Part\nAssembly Sequences (D4PAS) consisting of physically valid sequences for\nindustrial objects. Experimental results show that our approach generalizes\nbetter than prior methods while needing significantly less computational time\nfor inference.\n","authors":["Xinghao Zhu","Devesh K. Jha","Diego Romeres","Lingfeng Sun","Masayoshi Tomizuka","Anoop Cherian"],"pdf_url":"https://arxiv.org/pdf/2312.10571v1.pdf","comment":"Supplementary video is available at\n  https://www.youtube.com/watch?v=XNYkWSHkAaU&ab_channel=MitsubishiElectricResearchLabs%28MERL%29"},{"id":"http://arxiv.org/abs/2312.10570v1","updated":"2023-12-17T00:46:16Z","published":"2023-12-17T00:46:16Z","title":"Adversarially Balanced Representation for Continuous Treatment Effect\n  Estimation","summary":"  Individual treatment effect (ITE) estimation requires adjusting for the\ncovariate shift between populations with different treatments, and deep\nrepresentation learning has shown great promise in learning a balanced\nrepresentation of covariates. However the existing methods mostly consider the\nscenario of binary treatments. In this paper, we consider the more practical\nand challenging scenario in which the treatment is a continuous variable (e.g.\ndosage of a medication), and we address the two main challenges of this setup.\nWe propose the adversarial counterfactual regression network (ACFR) that\nadversarially minimizes the representation imbalance in terms of KL divergence,\nand also maintains the impact of the treatment value on the outcome prediction\nby leveraging an attention mechanism. Theoretically we demonstrate that ACFR\nobjective function is grounded in an upper bound on counterfactual outcome\nprediction error. Our experimental evaluation on semi-synthetic datasets\ndemonstrates the empirical superiority of ACFR over a range of state-of-the-art\nmethods.\n","authors":["Amirreza Kazemi","Martin Ester"],"pdf_url":"https://arxiv.org/pdf/2312.10570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10569v1","updated":"2023-12-17T00:42:42Z","published":"2023-12-17T00:42:42Z","title":"Interpretable Causal Inference for Analyzing Wearable, Sensor, and\n  Distributional Data","summary":"  Many modern causal questions ask how treatments affect complex outcomes that\nare measured using wearable devices and sensors. Current analysis approaches\nrequire summarizing these data into scalar statistics (e.g., the mean), but\nthese summaries can be misleading. For example, disparate distributions can\nhave the same means, variances, and other statistics. Researchers can overcome\nthe loss of information by instead representing the data as distributions. We\ndevelop an interpretable method for distributional data analysis that ensures\ntrustworthy and robust decision-making: Analyzing Distributional Data via\nMatching After Learning to Stretch (ADD MALTS). We (i) provide analytical\nguarantees of the correctness of our estimation strategy, (ii) demonstrate via\nsimulation that ADD MALTS outperforms other distributional data analysis\nmethods at estimating treatment effects, and (iii) illustrate ADD MALTS'\nability to verify whether there is enough cohesion between treatment and\ncontrol units within subpopulations to trustworthily estimate treatment\neffects. We demonstrate ADD MALTS' utility by studying the effectiveness of\ncontinuous glucose monitors in mitigating diabetes risks.\n","authors":["Srikar Katta","Harsh Parikh","Cynthia Rudin","Alexander Volfovsky"],"pdf_url":"https://arxiv.org/pdf/2312.10569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10568v1","updated":"2023-12-17T00:29:25Z","published":"2023-12-17T00:29:25Z","title":"IntraSeismic: a coordinate-based learning approach to seismic inversion","summary":"  Seismic imaging is the numerical process of creating a volumetric\nrepresentation of the subsurface geological structures from elastic waves\nrecorded at the surface of the Earth. As such, it is widely utilized in the\nenergy and construction sectors for applications ranging from oil and gas\nprospection, to geothermal production and carbon capture and storage\nmonitoring, to geotechnical assessment of infrastructures. Extracting\nquantitative information from seismic recordings, such as an acoustic impedance\nmodel, is however a highly ill-posed inverse problem, due to the band-limited\nand noisy nature of the data. This paper introduces IntraSeismic, a novel\nhybrid seismic inversion method that seamlessly combines coordinate-based\nlearning with the physics of the post-stack modeling operator. Key features of\nIntraSeismic are i) unparalleled performance in 2D and 3D post-stack seismic\ninversion, ii) rapid convergence rates, iii) ability to seamlessly include hard\nconstraints (i.e., well data) and perform uncertainty quantification, and iv)\npotential data compression and fast randomized access to portions of the\ninverted model. Synthetic and field data applications of IntraSeismic are\npresented to validate the effectiveness of the proposed method.\n","authors":["Juan Romero","Wolfgang Heidrich","Nick Luiken","Matteo Ravasi"],"pdf_url":"https://arxiv.org/pdf/2312.10568v1.pdf","comment":"-"},{"id":"http://arxiv.org/abs/2312.10567v1","updated":"2023-12-17T00:20:02Z","published":"2023-12-17T00:20:02Z","title":"Light-weight CNN-based VVC Inter Partitioning Acceleration","summary":"  The Versatile Video Coding (VVC) standard has been finalized by Joint Video\nExploration Team (JVET) in 2020. Compared to the High Efficiency Video Coding\n(HEVC) standard, VVC offers about 50% compression efficiency gain, in terms of\nBjontegaard Delta-Rate (BD-rate), at the cost of about 10x more encoder\ncomplexity. In this paper, we propose a Convolutional Neural Network\n(CNN)-based method to speed up inter partitioning in VVC. Our method operates\nat the Coding Tree Unit (CTU) level, by splitting each CTU into a fixed grid of\n8x8 blocks. Then each cell in this grid is associated with information about\nthe partitioning depth within that area. A lightweight network for predicting\nthis grid is employed during the rate-distortion optimization to limit the\nQuaternary Tree (QT)-split search and avoid partitions that are unlikely to be\nselected. Experiments show that the proposed method can achieve acceleration\nranging from 17% to 30% in the RandomAccess Group Of Picture 32 (RAGOP32) mode\nof VVC Test Model (VTM)10 with a reasonable efficiency drop ranging from 0.37%\nto 1.18% in terms of BD-rate increase.\n","authors":["Yiqun Liu","Mohsen Abdoli","Thomas Guionnet","Christine Guillemot","Aline Roumy"],"pdf_url":"https://arxiv.org/pdf/2312.10567v1.pdf","comment":"Accepted by IVMSP"}],"Multimedia":[{"id":"http://arxiv.org/abs/2310.14778v2","updated":"2023-12-17T08:35:04Z","published":"2023-10-23T10:29:33Z","title":"Audio-Visual Speaker Tracking: Progress, Challenges, and Future\n  Directions","summary":"  Audio-visual speaker tracking has drawn increasing attention over the past\nfew years due to its academic values and wide application. Audio and visual\nmodalities can provide complementary information for localization and tracking.\nWith audio and visual information, the Bayesian-based filter can solve the\nproblem of data association, audio-visual fusion and track management. In this\npaper, we conduct a comprehensive overview of audio-visual speaker tracking. To\nour knowledge, this is the first extensive survey over the past five years. We\nintroduce the family of Bayesian filters and summarize the methods for\nobtaining audio-visual measurements. In addition, the existing trackers and\ntheir performance on AV16.3 dataset are summarized. In the past few years, deep\nlearning techniques have thrived, which also boosts the development of audio\nvisual speaker tracking. The influence of deep learning techniques in terms of\nmeasurement extraction and state estimation is also discussed. At last, we\ndiscuss the connections between audio-visual speaker tracking and other areas\nsuch as speech separation and distributed speaker tracking.\n","authors":["Jinzheng Zhao","Yong Xu","Xinyuan Qian","Davide Berghi","Peipei Wu","Meng Cui","Jianyuan Sun","Philip J. B. Jackson","Wenwu Wang"],"pdf_url":"https://arxiv.org/pdf/2310.14778v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10567v1","updated":"2023-12-17T00:20:02Z","published":"2023-12-17T00:20:02Z","title":"Light-weight CNN-based VVC Inter Partitioning Acceleration","summary":"  The Versatile Video Coding (VVC) standard has been finalized by Joint Video\nExploration Team (JVET) in 2020. Compared to the High Efficiency Video Coding\n(HEVC) standard, VVC offers about 50% compression efficiency gain, in terms of\nBjontegaard Delta-Rate (BD-rate), at the cost of about 10x more encoder\ncomplexity. In this paper, we propose a Convolutional Neural Network\n(CNN)-based method to speed up inter partitioning in VVC. Our method operates\nat the Coding Tree Unit (CTU) level, by splitting each CTU into a fixed grid of\n8x8 blocks. Then each cell in this grid is associated with information about\nthe partitioning depth within that area. A lightweight network for predicting\nthis grid is employed during the rate-distortion optimization to limit the\nQuaternary Tree (QT)-split search and avoid partitions that are unlikely to be\nselected. Experiments show that the proposed method can achieve acceleration\nranging from 17% to 30% in the RandomAccess Group Of Picture 32 (RAGOP32) mode\nof VVC Test Model (VTM)10 with a reasonable efficiency drop ranging from 0.37%\nto 1.18% in terms of BD-rate increase.\n","authors":["Yiqun Liu","Mohsen Abdoli","Thomas Guionnet","Christine Guillemot","Aline Roumy"],"pdf_url":"https://arxiv.org/pdf/2312.10567v1.pdf","comment":"Accepted by IVMSP"}]},"2023-12-16T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2312.10536v1","updated":"2023-12-16T20:23:53Z","published":"2023-12-16T20:23:53Z","title":"USTHB at NADI 2023 shared task: Exploring Preprocessing and Feature\n  Engineering Strategies for Arabic Dialect Identification","summary":"  In this paper, we conduct an in-depth analysis of several key factors\ninfluencing the performance of Arabic Dialect Identification NADI'2023, with a\nspecific focus on the first subtask involving country-level dialect\nidentification. Our investigation encompasses the effects of surface\npreprocessing, morphological preprocessing, FastText vector model, and the\nweighted concatenation of TF-IDF features. For classification purposes, we\nemploy the Linear Support Vector Classification (LSVC) model. During the\nevaluation phase, our system demonstrates noteworthy results, achieving an F1\nscore of 62.51%. This achievement closely aligns with the average F1 scores\nattained by other systems submitted for the first subtask, which stands at\n72.91%.\n","authors":["Mohamed Lichouri","Khaled Lounnas","Aicha Zitouni","Houda Latrache","Rachida Djeradi"],"pdf_url":"https://arxiv.org/pdf/2312.10536v1.pdf","comment":"Accepted for publication in the conference proceedings of ArabicNLP\n  2023"},{"id":"http://arxiv.org/abs/2312.10528v1","updated":"2023-12-16T19:59:07Z","published":"2023-12-16T19:59:07Z","title":"Cross-Linguistic Offensive Language Detection: BERT-Based Analysis of\n  Bengali, Assamese, & Bodo Conversational Hateful Content from Social Media","summary":"  In today's age, social media reigns as the paramount communication platform,\nproviding individuals with the avenue to express their conjectures,\nintellectual propositions, and reflections. Unfortunately, this freedom often\ncomes with a downside as it facilitates the widespread proliferation of hate\nspeech and offensive content, leaving a deleterious impact on our world. Thus,\nit becomes essential to discern and eradicate such offensive material from the\nrealm of social media. This article delves into the comprehensive results and\nkey revelations from the HASOC-2023 offensive language identification result.\nThe primary emphasis is placed on the meticulous detection of hate speech\nwithin the linguistic domains of Bengali, Assamese, and Bodo, forming the\nframework for Task 4: Annihilate Hates. In this work, we used BERT models,\nincluding XML-Roberta, L3-cube, IndicBERT, BenglaBERT, and BanglaHateBERT. The\nresearch outcomes were promising and showed that XML-Roberta-lagre performed\nbetter than monolingual models in most cases. Our team 'TeamBD' achieved rank\n3rd for Task 4 - Assamese, & 5th for Bengali.\n","authors":["Jhuma Kabir Mim","Mourad Oussalah","Akash Singhal"],"pdf_url":"https://arxiv.org/pdf/2312.10528v1.pdf","comment":"9 pages, 1 figure, 5 tables"},{"id":"http://arxiv.org/abs/2312.10523v1","updated":"2023-12-16T19:12:45Z","published":"2023-12-16T19:12:45Z","title":"Paloma: A Benchmark for Evaluating Language Model Fit","summary":"  Language models (LMs) commonly report perplexity on monolithic data held out\nfrom training. Implicitly or explicitly, this data is composed of\ndomains$\\unicode{x2013}$varying distributions of language. Rather than assuming\nperplexity on one distribution extrapolates to others, Perplexity Analysis for\nLanguage Model Assessment (Paloma), measures LM fit to 585 text domains,\nranging from nytimes.com to r/depression on Reddit. We invite submissions to\nour benchmark and organize results by comparability based on compliance with\nguidelines such as removal of benchmark contamination from pretraining.\nSubmissions can also record parameter and training token count to make\ncomparisons of Pareto efficiency for performance as a function of these\nmeasures of cost. We populate our benchmark with results from 6 baselines\npretrained on popular corpora. In case studies, we demonstrate analyses that\nare possible with Paloma, such as finding that pretraining without data beyond\nCommon Crawl leads to inconsistent fit to many domains.\n","authors":["Ian Magnusson","Akshita Bhagia","Valentin Hofmann","Luca Soldaini","Ananya Harsh Jha","Oyvind Tafjord","Dustin Schwenk","Evan Pete Walsh","Yanai Elazar","Kyle Lo","Dirk Groeneveld","Iz Beltagy","Hannaneh Hajishirzi","Noah A. Smith","Kyle Richardson","Jesse Dodge"],"pdf_url":"https://arxiv.org/pdf/2312.10523v1.pdf","comment":"Project Page: https://paloma.allen.ai/"},{"id":"http://arxiv.org/abs/2312.10493v1","updated":"2023-12-16T16:14:50Z","published":"2023-12-16T16:14:50Z","title":"Debiasing Multimodal Sarcasm Detection with Contrastive Learning","summary":"  Despite commendable achievements made by existing work, prevailing multimodal\nsarcasm detection studies rely more on textual content over visual information.\nIt unavoidably induces spurious correlations between textual words and labels,\nthereby significantly hindering the models' generalization capability. To\naddress this problem, we define the task of out-of-distribution (OOD)\nmultimodal sarcasm detection, which aims to evaluate models' generalizability\nwhen the word distribution is different in training and testing settings.\nMoreover, we propose a novel debiasing multimodal sarcasm detection framework\nwith contrastive learning, which aims to mitigate the harmful effect of biased\ntextual factors for robust OOD generalization. In particular, we first design\ncounterfactual data augmentation to construct the positive samples with\ndissimilar word biases and negative samples with similar word biases.\nSubsequently, we devise an adapted debiasing contrastive learning mechanism to\nempower the model to learn robust task-relevant features and alleviate the\nadverse effect of biased words. Extensive experiments show the superiority of\nthe proposed framework.\n","authors":["Mengzhao Jia","Can Xie","Liqiang Jing"],"pdf_url":"https://arxiv.org/pdf/2312.10493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10479v1","updated":"2023-12-16T15:17:28Z","published":"2023-12-16T15:17:28Z","title":"A Soft Contrastive Learning-based Prompt Model for Few-shot Sentiment\n  Analysis","summary":"  Few-shot text classification has attracted great interest in both academia\nand industry due to the lack of labeled data in many fields. Different from\ngeneral text classification (e.g., topic classification), few-shot sentiment\nclassification is more challenging because the semantic distances among the\nclasses are more subtle. For instance, the semantic distances between the\nsentiment labels in a positive or negative polarity (e.g., ``love\" and ``joy\",\n``remorse\" and ``sadness\") are close, while the distances are large for the\nsentiment labels in two opposite polarities (e.g., ``love\" and ``sadness\"). To\naddress this problem, we propose a Soft Contrastive learning-based Prompt\n(\\texttt{SCP}) model for few-shot sentiment analysis. First, we design a\nsentiment-aware chain of thought prompt module to guide the model to predict\nthe sentiment from coarse grain to fine grain via a series of intermediate\nreasoning steps. Then, we propose a soft contrastive learning algorithm to take\nthe correlation of the labels into account. A series of experiments on several\nsentiment analysis datasets show the great advantages of \\texttt{SCP} by\ncomparing it with SOTA baselines (e.g., ChatGPT).\n","authors":["Jingyi Zhou","Jie Zhou","Jiabao Zhao","Siyin Wang","Haijun Shan","Gui Tao","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2312.10479v1.pdf","comment":"Accepted by ICASSP"},{"id":"http://arxiv.org/abs/2312.08737v2","updated":"2023-12-16T14:50:53Z","published":"2023-12-14T08:30:38Z","title":"JPIS: A Joint Model for Profile-based Intent Detection and Slot Filling\n  with Slot-to-Intent Attention","summary":"  Profile-based intent detection and slot filling are important tasks aimed at\nreducing the ambiguity in user utterances by leveraging user-specific\nsupporting profile information. However, research in these two tasks has not\nbeen extensively explored. To fill this gap, we propose a joint model, namely\nJPIS, designed to enhance profile-based intent detection and slot filling. JPIS\nincorporates the supporting profile information into its encoder and introduces\na slot-to-intent attention mechanism to transfer slot information\nrepresentations to intent detection. Experimental results show that our JPIS\nsubstantially outperforms previous profile-based models, establishing a new\nstate-of-the-art performance in overall accuracy on the Chinese benchmark\ndataset ProSLU.\n","authors":["Thinh Pham","Dat Quoc Nguyen"],"pdf_url":"https://arxiv.org/pdf/2312.08737v2.pdf","comment":"To appear in Proceedings of ICASSP 2024 (Camera-ready version)"},{"id":"http://arxiv.org/abs/2312.10466v1","updated":"2023-12-16T14:47:03Z","published":"2023-12-16T14:47:03Z","title":"RIGHT: Retrieval-augmented Generation for Mainstream Hashtag\n  Recommendation","summary":"  Automatic mainstream hashtag recommendation aims to accurately provide users\nwith concise and popular topical hashtags before publication. Generally,\nmainstream hashtag recommendation faces challenges in the comprehensive\ndifficulty of newly posted tweets in response to new topics, and the accurate\nidentification of mainstream hashtags beyond semantic correctness. However,\nprevious retrieval-based methods based on a fixed predefined mainstream hashtag\nlist excel in producing mainstream hashtags, but fail to understand the\nconstant flow of up-to-date information. Conversely, generation-based methods\ndemonstrate a superior ability to comprehend newly posted tweets, but their\ncapacity is constrained to identifying mainstream hashtags without additional\nfeatures. Inspired by the recent success of the retrieval-augmented technique,\nin this work, we attempt to adopt this framework to combine the advantages of\nboth approaches. Meantime, with the help of the generator component, we could\nrethink how to further improve the quality of the retriever component at a low\ncost. Therefore, we propose RetrIeval-augmented Generative Mainstream HashTag\nRecommender (RIGHT), which consists of three components: 1) a retriever seeks\nrelevant hashtags from the entire tweet-hashtags set; 2) a selector enhances\nmainstream identification by introducing global signals; and 3) a generator\nincorporates input tweets and selected hashtags to directly generate the\ndesired hashtags. The experimental results show that our method achieves\nsignificant improvements over state-of-the-art baselines. Moreover, RIGHT can\nbe easily integrated into large language models, improving the performance of\nChatGPT by more than 10%.\n","authors":["Run-Ze Fan","Yixing Fan","Jiangui Chen","Jiafeng Guo","Ruqing Zhang","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2312.10466v1.pdf","comment":"Accepted by ECIR2024 full paper"},{"id":"http://arxiv.org/abs/2312.10448v1","updated":"2023-12-16T13:41:04Z","published":"2023-12-16T13:41:04Z","title":"Resolving Crash Bugs via Large Language Models: An Empirical Study","summary":"  Crash bugs cause unexpected program behaviors or even termination, requiring\nhigh-priority resolution. However, manually resolving crash bugs is challenging\nand labor-intensive, and researchers have proposed various techniques for their\nautomated localization and repair. ChatGPT, a recent large language model\n(LLM), has garnered significant attention due to its exceptional performance\nacross various domains. This work performs the first investigation into\nChatGPT's capability in resolve real-world crash bugs, focusing on its\neffectiveness in both localizing and repairing code-related and\nenvironment-related crash bugs. Specifically, we initially assess ChatGPT's\nfundamental ability to resolve crash bugs with basic prompts in a single\niteration. We observe that ChatGPT performs better at resolving code-related\ncrash bugs compared to environment-related ones, and its primary challenge in\nresolution lies in inaccurate localization. Additionally, we explore ChatGPT's\npotential with various advanced prompts. Furthermore, by stimulating ChatGPT's\nself-planning, it methodically investigates each potential crash-causing\nenvironmental factor through proactive inquiry, ultimately identifying the root\ncause of the crash. Based on our findings, we propose IntDiagSolver, an\ninteraction methodology designed to facilitate precise crash bug resolution\nthrough continuous interaction with LLMs. Evaluating IntDiagSolver on multiple\nLLMs reveals consistent enhancement in the accuracy of crash bug resolution,\nincluding ChatGPT, Claude, and CodeLlama.\n","authors":["Xueying Du","Mingwei Liu","Juntao Li","Hanlin Wang","Xin Peng","Yiling Lou"],"pdf_url":"https://arxiv.org/pdf/2312.10448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.15222v2","updated":"2023-12-16T13:26:02Z","published":"2023-06-27T05:48:14Z","title":"Learning to Rank in Generative Retrieval","summary":"  Generative retrieval stands out as a promising new paradigm in text retrieval\nthat aims to generate identifier strings of relevant passages as the retrieval\ntarget. This generative paradigm taps into powerful generative language models,\ndistinct from traditional sparse or dense retrieval methods. However, only\nlearning to generate is insufficient for generative retrieval. Generative\nretrieval learns to generate identifiers of relevant passages as an\nintermediate goal and then converts predicted identifiers into the final\npassage rank list. The disconnect between the learning objective of\nautoregressive models and the desired passage ranking target leads to a\nlearning gap. To bridge this gap, we propose a learning-to-rank framework for\ngenerative retrieval, dubbed LTRGR. LTRGR enables generative retrieval to learn\nto rank passages directly, optimizing the autoregressive model toward the final\npassage ranking target via a rank loss. This framework only requires an\nadditional learning-to-rank training phase to enhance current generative\nretrieval systems and does not add any burden to the inference stage. We\nconducted experiments on three public benchmarks, and the results demonstrate\nthat LTRGR achieves state-of-the-art performance among generative retrieval\nmethods. The code and checkpoints are released at\nhttps://github.com/liyongqi67/LTRGR.\n","authors":["Yongqi Li","Nan Yang","Liang Wang","Furu Wei","Wenjie Li"],"pdf_url":"https://arxiv.org/pdf/2306.15222v2.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2310.07521v3","updated":"2023-12-16T12:47:19Z","published":"2023-10-11T14:18:03Z","title":"Survey on Factuality in Large Language Models: Knowledge, Retrieval and\n  Domain-Specificity","summary":"  This survey addresses the crucial issue of factuality in Large Language\nModels (LLMs). As LLMs find applications across diverse domains, the\nreliability and accuracy of their outputs become vital. We define the\nFactuality Issue as the probability of LLMs to produce content inconsistent\nwith established facts. We first delve into the implications of these\ninaccuracies, highlighting the potential consequences and challenges posed by\nfactual errors in LLM outputs. Subsequently, we analyze the mechanisms through\nwhich LLMs store and process facts, seeking the primary causes of factual\nerrors. Our discussion then transitions to methodologies for evaluating LLM\nfactuality, emphasizing key metrics, benchmarks, and studies. We further\nexplore strategies for enhancing LLM factuality, including approaches tailored\nfor specific domains. We focus two primary LLM configurations standalone LLMs\nand Retrieval-Augmented LLMs that utilizes external data, we detail their\nunique challenges and potential enhancements. Our survey offers a structured\nguide for researchers aiming to fortify the factual reliability of LLMs.\n","authors":["Cunxiang Wang","Xiaoze Liu","Yuanhao Yue","Xiangru Tang","Tianhang Zhang","Cheng Jiayang","Yunzhi Yao","Wenyang Gao","Xuming Hu","Zehan Qi","Yidong Wang","Linyi Yang","Jindong Wang","Xing Xie","Zheng Zhang","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.07521v3.pdf","comment":"62 pages; 300+ references"},{"id":"http://arxiv.org/abs/2312.10432v1","updated":"2023-12-16T12:35:28Z","published":"2023-12-16T12:35:28Z","title":"From Dialogue to Diagram: Task and Relationship Extraction from Natural\n  Language for Accelerated Business Process Prototyping","summary":"  The automatic transformation of verbose, natural language descriptions into\nstructured process models remains a challenge of significant complexity - This\npaper introduces a contemporary solution, where central to our approach, is the\nuse of dependency parsing and Named Entity Recognition (NER) for extracting key\nelements from textual descriptions. Additionally, we utilize\nSubject-Verb-Object (SVO) constructs for identifying action relationships and\nintegrate semantic analysis tools, including WordNet, for enriched contextual\nunderstanding. A novel aspect of our system is the application of neural\ncoreference resolution, integrated with the SpaCy framework, enhancing the\nprecision of entity linkage and anaphoric references. Furthermore, the system\nadeptly handles data transformation and visualization, converting extracted\ninformation into BPMN (Business Process Model and Notation) diagrams. This\nmethodology not only streamlines the process of capturing and representing\nbusiness workflows but also significantly reduces the manual effort and\npotential for error inherent in traditional modeling approaches.\n","authors":["Sara Qayyum","Muhammad Moiz Asghar","Muhammad Fouzan Yaseen"],"pdf_url":"https://arxiv.org/pdf/2312.10432v1.pdf","comment":"9 pages, 2 figures"},{"id":"http://arxiv.org/abs/2311.04666v3","updated":"2023-12-16T12:00:34Z","published":"2023-11-08T13:13:23Z","title":"Pre-training LLMs using human-like development data corpus","summary":"  Pre-trained Large Language Models (LLMs) have shown success in a diverse set\nof language inference and understanding tasks. The pre-training stage of LLMs\nlooks at a large corpus of raw textual data. The BabyLM shared task compares\nLLM pre-training to human language acquisition, where the number of tokens seen\nby 13-year-old kids is magnitudes smaller than the number of tokens seen by\nLLMs. In this work, we pre-train and evaluate LLMs on their ability to learn\ncontextual word representations using roughly the same number of tokens as seen\nby children. We provide a strong set of baselines; with different\narchitectures, evaluation of changes in performance across epochs, and reported\npre-training metrics for the strict small and strict tracks of the task. We\nalso try to loosely replicate the RoBERTa baseline given by the task organizers\nto observe the training robustness to hyperparameter selection and\nreplicability. We provide the submission details to the strict and strict-small\ntracks in this report.\n","authors":["Khushi Bhardwaj","Raj Sanjay Shah","Sashank Varma"],"pdf_url":"https://arxiv.org/pdf/2311.04666v3.pdf","comment":"Proceedings of the BabyLM Challenge at the 27th Conference on\n  Computational Natural Language Learning"},{"id":"http://arxiv.org/abs/2305.07490v3","updated":"2023-12-16T10:59:20Z","published":"2023-05-12T14:04:30Z","title":"ArtGPT-4: Towards Artistic-understanding Large Vision-Language Models\n  with Enhanced Adapter","summary":"  In recent years, advancements in large language models have been remarkable,\nwith models such as ChatGPT demonstrating exceptional proficiency in diverse\nlinguistic tasks. The pre-training of large models with billions of parameters,\nposes a formidable challenge, primarily due to the scarcity of datasets of a\ncommensurate scale for effective training. Nevertheless, innovative strategies\nhave emerged, including methods to fine-tune these pre-trained models using\nfewer parameters set, as evidenced by models like MiniGPT-4 and LLaVA. Despite\ntheir potential in various domains, these models remain limited in their\nunderstanding of artistic imagery. They have yet to fully grasp the intricate\nnuances of art images or to provide an objective articulation of the emotions\nthey evoke, in a manner akin to human perception. This work introduces\nArtGPT-4, a pioneering large vision-language model tailored to address the\ndeficiencies of contemporary models in artistic comprehension. ArtGPT-4\nunderwent training on image-text pairs utilizing a Tesla A100 device in a mere\n2 hours, with a dataset comprising approximately 0.52M entries. Impressively,\nthe model can render images with an artistic-understanding and convey the\nemotions they inspire, mirroring human interpretation. Additionally, this work\npresents a unique dataset designed to evaluate the efficacy of vision-language\nmodels. In subsequent evaluations, ArtGPT-4 not only achieved state-of-the-art\nperformance on the ArtEmis and ArtEmis-v2.0 datasets but also exceeded the\nestablished benchmarks introduced in This study, lagging behind professional\nartists' descriptions by a negligible 0.15 points on a 6-point scale. The code\nand the pre-trained model are accessible in\nhttps://huggingface.co/Tyrannosaurus/ArtGPT-4.\n","authors":["Zhengqing Yuan","Xinyi Wang","Kun Wang","Lichao Sun","Yanyang Ye"],"pdf_url":"https://arxiv.org/pdf/2305.07490v3.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2305.10512v2","updated":"2023-12-16T10:18:21Z","published":"2023-05-17T18:38:10Z","title":"IMAD: IMage-Augmented multi-modal Dialogue","summary":"  Currently, dialogue systems have achieved high performance in processing\ntext-based communication. However, they have not yet effectively incorporated\nvisual information, which poses a significant challenge. Furthermore, existing\nmodels that incorporate images in dialogue generation focus on discussing the\nimage itself. Our proposed approach presents a novel perspective on multi-modal\ndialogue systems, which interprets the image in the context of the dialogue. By\ndoing so, we aim to expand the capabilities of current dialogue systems and\ntransition them from single modality (text) to multi-modality. However, there\nis a lack of validated English datasets that contain both images and dialogue\ncontexts for this task. Thus, we propose a two-stage approach to automatically\nconstruct a multi-modal dialogue dataset. In the first stage, we utilize\ntext-to-image similarity and sentence similarity to identify which utterances\ncould be replaced with an image. In the second stage, we replace those\nutterances by selecting a subset of relevant images and filtering them with a\nvisual question answering model. We used this approach, along with additional\nlabeling, to create the IMage Augmented multi-modal Dialogue dataset (IMAD),\nwhich can serve as a validated dataset for this task. Furthermore, we propose a\nbaseline model trained on this dataset, which outperforms model trained on the\nsame data without images and BlenderBot.\n","authors":["Viktor Moskvoretskii","Anton Frolov","Denis Kuznetsov"],"pdf_url":"https://arxiv.org/pdf/2305.10512v2.pdf","comment":"Main part contains 6 pages, 4 figures. It was accepted on AINL. We\n  wait the publication and DOI"},{"id":"http://arxiv.org/abs/2211.00369v2","updated":"2023-12-16T09:38:26Z","published":"2022-11-01T10:34:27Z","title":"A General Search-based Framework for Generating Textual Counterfactual\n  Explanations","summary":"  One of the prominent methods for explaining the decision of a\nmachine-learning classifier is by a counterfactual example. Most current\nalgorithms for generating such examples in the textual domain are based on\ngenerative language models. Generative models, however, are trained to minimize\na specific loss function in order to fulfill certain requirements for the\ngenerated texts. Any change in the requirements may necessitate costly\nretraining, thus potentially limiting their applicability. In this paper, we\npresent a general search-based framework for generating counterfactual\nexplanations in the textual domain. Our framework is model-agnostic,\ndomain-agnostic, anytime, and does not require retraining in order to adapt to\nchanges in the user requirements. We model the task as a search problem in a\nspace where the initial state is the classified text, and the goal state is a\ntext in a given target class. Our framework includes domain-independent\nmodification operators, but can also exploit domain-specific knowledge through\nspecialized operators. The search algorithm attempts to find a text from the\ntarget class with minimal user-specified distance from the original classified\nobject.\n","authors":["Daniel Gilo","Shaul Markovitch"],"pdf_url":"https://arxiv.org/pdf/2211.00369v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10375v1","updated":"2023-12-16T08:23:12Z","published":"2023-12-16T08:23:12Z","title":"Collect and Connect Data Leaves to Feature Concepts: Interactive Graph\n  Generation Toward Well-being","summary":"  Feature concepts and data leaves have been invented using datasets to foster\ncreative thoughts for creating well-being in daily life. The idea, simply put,\nis to attach selected and collected data leaves that are summaries of event\nflows to be discovered from corresponding datasets, on the target feature\nconcept representing the well-being aimed. A graph of existing or expected\ndatasets to be attached to a feature concept is generated semi-automatically.\nRather than sheer automated generative AI, our work addresses the process of\ngenerative artificial and natural intelligence to create the basis for data use\nand reuse.\n","authors":["Yukio Ohsawa","Tomohide Maekawa","Hiroki Yamaguchi","Hiro Yoshida","Kaira Sekiguchi"],"pdf_url":"https://arxiv.org/pdf/2312.10375v1.pdf","comment":"2 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2312.10371v1","updated":"2023-12-16T08:10:10Z","published":"2023-12-16T08:10:10Z","title":"K-ESConv: Knowledge Injection for Emotional Support Dialogue Systems via\n  Prompt Learning","summary":"  Automatic psychological counseling requires mass of professional knowledge\nthat can be found in online counseling forums. Motivated by this, we propose\nK-ESConv, a novel prompt learning based knowledge injection method for\nemotional support dialogue system, transferring forum knowledge to response\ngeneration. We evaluate our model on an emotional support dataset ESConv, where\nthe model retrieves and incorporates knowledge from external professional\nemotional Q\\&A forum. Experiment results show that the proposed method\noutperforms existing baselines on both automatic evaluation and human\nevaluation, which shows that our approach significantly improves the\ncorrelation and diversity of responses and provides more comfort and better\nsuggestion for the seeker.\n","authors":["Wei Chen","Gang Zhao","Xiaojin Zhang","Xiang Bai","Xuanjing Huang","Zhongyu Wei"],"pdf_url":"https://arxiv.org/pdf/2312.10371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10358v1","updated":"2023-12-16T07:05:16Z","published":"2023-12-16T07:05:16Z","title":"CONCSS: Contrastive-based Context Comprehension for Dialogue-appropriate\n  Prosody in Conversational Speech Synthesis","summary":"  Conversational speech synthesis (CSS) incorporates historical dialogue as\nsupplementary information with the aim of generating speech that has\ndialogue-appropriate prosody. While previous methods have already delved into\nenhancing context comprehension, context representation still lacks effective\nrepresentation capabilities and context-sensitive discriminability. In this\npaper, we introduce a contrastive learning-based CSS framework, CONCSS. Within\nthis framework, we define an innovative pretext task specific to CSS that\nenables the model to perform self-supervised learning on unlabeled\nconversational datasets to boost the model's context understanding.\nAdditionally, we introduce a sampling strategy for negative sample augmentation\nto enhance context vectors' discriminability. This is the first attempt to\nintegrate contrastive learning into CSS. We conduct ablation studies on\ndifferent contrastive learning strategies and comprehensive experiments in\ncomparison with prior CSS systems. Results demonstrate that the synthesized\nspeech from our proposed method exhibits more contextually appropriate and\nsensitive prosody.\n","authors":["Yayue Deng","Jinlong Xue","Yukang Jia","Qifei Li","Yichen Han","Fengping Wang","Yingming Gao","Dengfeng Ke","Ya Li"],"pdf_url":"https://arxiv.org/pdf/2312.10358v1.pdf","comment":"5 pages, 2 figures, 3 tables, Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.10355v1","updated":"2023-12-16T06:57:20Z","published":"2023-12-16T06:57:20Z","title":"CoAScore: Chain-of-Aspects Prompting for NLG Evaluation","summary":"  Recently, natural language generation (NLG) evaluation has shifted from a\nsingle-aspect to a multi-aspect paradigm, allowing for a more accurate\nassessment. Large language models (LLMs) achieve superior performance on\nvarious NLG evaluation tasks. However, current work often employs the LLM to\nindependently evaluate different aspects, which largely ignores the rich\ncorrelation between various aspects. To fill this research gap, in this work,\nwe propose an NLG evaluation metric called CoAScore. Powered by LLMs, the\nCoAScore utilizes multi-aspect knowledge through a CoA\n(\\textbf{C}hain-\\textbf{o}f-\\textbf{A}spects) prompting framework when\nassessing the quality of a certain aspect. Specifically, for a given aspect to\nevaluate, we first prompt the LLM to generate a chain of aspects that are\nrelevant to the target aspect and could be useful for the evaluation. We then\ncollect evaluation scores for each generated aspect, and finally, leverage the\nknowledge of these aspects to improve the evaluation of the target aspect. We\nevaluate CoAScore across five NLG evaluation tasks (e.g., summarization, dialog\nresponse generation, etc) and nine aspects (e.g., overall quality, relevance,\ncoherence, etc). Our experimental findings highlight that, in comparison to\nindividual aspect evaluation, CoAScore exhibits a higher correlation with human\njudgments. This improvement significantly outperforms existing unsupervised\nevaluation metrics, whether for assessing overall quality or other aspects. We\nalso conducted extensive ablation studies to validate the effectiveness of the\nthree stages within the CoAScore framework and conducted case studies to show\nhow the LLM performs in these stages. Our code and scripts are available.\n","authors":["Peiyuan Gong","Jiaxin Mao"],"pdf_url":"https://arxiv.org/pdf/2312.10355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08518v4","updated":"2023-12-16T06:50:09Z","published":"2023-03-15T10:53:49Z","title":"UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation","summary":"  Large Language Models (LLMs) are popular for their impressive abilities, but\nthe need for model-specific fine-tuning or task-specific prompt engineering can\nhinder their generalization. We propose UPRISE (Universal Prompt Retrieval for\nImproving zero-Shot Evaluation), which tunes a lightweight and versatile\nretriever that automatically retrieves prompts for a given zero-shot task\ninput. Specifically, we demonstrate universality in a cross-task and\ncross-model scenario: the retriever is tuned on a diverse set of tasks, but\ntested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for\ntuning the retriever, but test the retriever on different LLMs of much larger\nscales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that\nUPRISE mitigates the hallucination problem in our experiments with ChatGPT,\nsuggesting its potential to improve even the strongest LLMs. Our model and code\nare available at https://github.com/microsoft/LMOps.\n","authors":["Daixuan Cheng","Shaohan Huang","Junyu Bi","Yuefeng Zhan","Jianfeng Liu","Yujing Wang","Hao Sun","Furu Wei","Denvy Deng","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.08518v4.pdf","comment":"EMNLP 2023 Main Conference"},{"id":"http://arxiv.org/abs/2312.07913v2","updated":"2023-12-16T06:30:43Z","published":"2023-12-13T06:11:42Z","title":"A Survey of Text Watermarking in the Era of Large Language Models","summary":"  In recent years, significant advancements have been made in the text\ngeneration capabilities of Large Language Models (LLMs), demonstrating\nexceptional performance in downstream tasks such as abstract summarization,\ndialogue generation, and data-to-text conversion. However, their generative\nabilities also pose risks such as the rapid spread of fake news, infringement\nof datasets/LLM copyrights, and challenges to academic integrity. Text\nwatermarking technology emerges as a potential solution. By embedding invisible\nyet detectable patterns in generated texts, it helps in tracking and verifying\ntext origins, thus preventing misuse and piracy.\n  This survey aims to comprehensively summarize current text watermarking\ntechnologies, covering three main aspects: (1) an overview and comparison of\ndifferent text watermarking techniques; (2) evaluation methods for text\nwatermarking algorithms, including their success rate, impact on text quality,\nrobustness, and unforgeability; (3) potential applications of text watermarking\ntechnologies. This survey aims to help researchers thoroughly understanding the\ntext watermarking technologies, thereby fostering further development.\n","authors":["Aiwei Liu","Leyi Pan","Yijian Lu","Jingjing Li","Xuming Hu","Lijie Wen","Irwin King","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2312.07913v2.pdf","comment":"34 pages, 5 figures"},{"id":"http://arxiv.org/abs/2302.01859v2","updated":"2023-12-16T06:06:03Z","published":"2023-02-03T17:05:59Z","title":"Generalizing to Unseen Elements: A Survey on Knowledge Extrapolation for\n  Knowledge Graphs","summary":"  Knowledge graphs (KGs) have become valuable knowledge resources in various\napplications, and knowledge graph embedding (KGE) methods have garnered\nincreasing attention in recent years. However, conventional KGE methods still\nface challenges when it comes to handling unseen entities or relations during\nmodel testing. To address this issue, much effort has been devoted to various\nfields of KGs. In this paper, we use a set of general terminologies to unify\nthese methods and refer to them collectively as Knowledge Extrapolation. We\ncomprehensively summarize these methods, classified by our proposed taxonomy,\nand describe their interrelationships. Additionally, we introduce benchmarks\nand provide comparisons of these methods based on aspects that are not captured\nby the taxonomy. Finally, we suggest potential directions for future research.\n","authors":["Mingyang Chen","Wen Zhang","Yuxia Geng","Zezhong Xu","Jeff Z. Pan","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2302.01859v2.pdf","comment":"Accepted to IJCAI 2023 Survey Track"},{"id":"http://arxiv.org/abs/2312.10329v1","updated":"2023-12-16T05:38:39Z","published":"2023-12-16T05:38:39Z","title":"Perturbation-Invariant Adversarial Training for Neural Ranking Models:\n  Improving the Effectiveness-Robustness Trade-Off","summary":"  Neural ranking models (NRMs) have shown great success in information\nretrieval (IR). But their predictions can easily be manipulated using\nadversarial examples, which are crafted by adding imperceptible perturbations\nto legitimate documents. This vulnerability raises significant concerns about\ntheir reliability and hinders the widespread deployment of NRMs. By\nincorporating adversarial examples into training data, adversarial training has\nbecome the de facto defense approach to adversarial attacks against NRMs.\nHowever, this defense mechanism is subject to a trade-off between effectiveness\nand adversarial robustness. In this study, we establish theoretical guarantees\nregarding the effectiveness-robustness trade-off in NRMs. We decompose the\nrobust ranking error into two components, i.e., a natural ranking error for\neffectiveness evaluation and a boundary ranking error for assessing adversarial\nrobustness. Then, we define the perturbation invariance of a ranking model and\nprove it to be a differentiable upper bound on the boundary ranking error for\nattainable computation. Informed by our theoretical analysis, we design a novel\n\\emph{perturbation-invariant adversarial training} (PIAT) method for ranking\nmodels to achieve a better effectiveness-robustness trade-off. We design a\nregularized surrogate loss, in which one term encourages the effectiveness to\nbe maximized while the regularization term encourages the output to be smooth,\nso as to improve adversarial robustness. Experimental results on several\nranking models demonstrate the superiority of PITA compared to existing\nadversarial defenses.\n","authors":["Yu-An Liu","Ruqing Zhang","Mingkun Zhang","Wei Chen","Maarten de Rijke","Jiafeng Guo","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2312.10329v1.pdf","comment":"Accepted by AAAI 24"},{"id":"http://arxiv.org/abs/2312.10323v1","updated":"2023-12-16T05:02:06Z","published":"2023-12-16T05:02:06Z","title":"Continuous Prompt Generation from Linear Combination of Discrete Prompt\n  Embeddings","summary":"  The wayward quality of continuous prompts stresses the importance of their\ninterpretability as unexpected and unpredictable behaviors appear following\ntraining, especially in the context of large language models automating\npeople-sensitive tasks such as resume screening. In this paper we present a\nnovel method of constructing continuous prompts via discrete prompt embeddings\nand evaluate improvements to continuous prompt interpretability and inference\naccuracy. For a set of manually designed discrete prompts $\\mathcal{D}$, which\nwe tokenize each into tensor form, we train a model to predict the weights such\nthat the linear combinations of those prompts correspond to higher performance\non natural language understanding tasks.\n","authors":["Pascal Passigan","Kidus Yohannes","Joshua Pereira"],"pdf_url":"https://arxiv.org/pdf/2312.10323v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10321v1","updated":"2023-12-16T05:01:23Z","published":"2023-12-16T05:01:23Z","title":"LLM-SQL-Solver: Can LLMs Determine SQL Equivalence?","summary":"  Judging the equivalence between two SQL queries is a fundamental problem with\nmany practical applications in data management and SQL generation (i.e.,\nevaluating the quality of generated SQL queries in text-to-SQL task). While the\nresearch community has reasoned about SQL equivalence for decades, it poses\nconsiderable difficulties and no complete solutions exist. Recently, Large\nLanguage Models (LLMs) have shown strong reasoning capability in conversation,\nquestion answering and solving mathematics challenges. In this paper, we study\nif LLMs can be used to determine the equivalence between SQL queries under two\nnotions of SQL equivalence (semantic equivalence and relaxed equivalence). To\nassist LLMs in generating high quality responses, we present two prompting\ntechniques: Miniature & Mull and Explain & Compare. The former technique is\nused to evaluate the semantic equivalence in which it asks LLMs to execute a\nquery on a simple database instance and then explore if a counterexample exists\nby modifying the database. The latter technique is used to evaluate the relaxed\nequivalence in which it asks LLMs to explain the queries and then compare if\nthey contain significant logical differences. Our experiments demonstrate using\nour techniques, LLMs is a promising tool to help data engineers in writing\nsemantically equivalent SQL queries, however challenges still persist, and is a\nbetter metric for evaluating SQL generation than the popular execution\naccuracy.\n","authors":["Fuheng Zhao","Lawrence Lim","Ishtiyaque Ahmad","Divyakant Agrawal","Amr El Abbadi"],"pdf_url":"https://arxiv.org/pdf/2312.10321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08726v2","updated":"2023-12-16T04:35:53Z","published":"2023-12-14T08:14:13Z","title":"Labels Need Prompts Too: Mask Matching for Natural Language\n  Understanding Tasks","summary":"  Textual label names (descriptions) are typically semantically rich in many\nnatural language understanding (NLU) tasks. In this paper, we incorporate the\nprompting methodology, which is widely used to enrich model input, into the\nlabel side for the first time. Specifically, we propose a Mask Matching method,\nwhich equips an input with a prompt and its label with another, and then makes\npredictions by matching their mask representations. We evaluate our method\nextensively on 8 NLU tasks with 14 datasets. The experimental results show that\nMask Matching significantly outperforms its counterparts of fine-tuning and\nconventional prompt-tuning, setting up state-of-the-art performances in several\ndatasets. Mask Matching is particularly good at handling NLU tasks with large\nlabel counts and informative label names. As pioneering efforts that\ninvestigate the label-side prompt, we also discuss open issues for future\nstudy.\n","authors":["Bo Li","Wei Ye","Quansen Wang","Wen Zhao","Shikun Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.08726v2.pdf","comment":"AAAI2024, Regular Paper"},{"id":"http://arxiv.org/abs/2312.08906v2","updated":"2023-12-16T03:47:54Z","published":"2023-12-14T13:11:35Z","title":"Using eye tracking to investigate what native Chinese speakers notice\n  about linguistic landscape images","summary":"  Linguistic landscape is an important field in sociolinguistic research. Eye\ntracking technology is a common technology in psychological research. There are\nfew cases of using eye movement to study linguistic landscape. This paper uses\neye tracking technology to study the actual fixation of the linguistic\nlandscape and finds that in the two dimensions of fixation time and fixation\ntimes, the fixation of native Chinese speakers to the linguistic landscape is\nhigher than that of the general landscape. This paper argues that this\nphenomenon is due to the higher information density of linguistic landscapes.\nAt the same time, the article also discusses other possible reasons for this\nphenomenon.\n","authors":["Zichao Wei","Yewei Qin"],"pdf_url":"https://arxiv.org/pdf/2312.08906v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.17176v2","updated":"2023-12-16T03:45:19Z","published":"2023-09-29T12:16:19Z","title":"AdaRefiner: Refining Decisions of Language Models with Adaptive Feedback","summary":"  Large Language Models (LLMs) have demonstrated significant success across\nvarious domains. However, their application in complex decision-making tasks\nfrequently necessitates intricate prompt engineering or fine-tuning, leading to\nchallenges in unseen downstream tasks and heavy demands on computational\nresources. Meanwhile, Reinforcement Learning (RL) has been recognized as\neffective in decision-making problems but struggles in environments with sparse\nrewards, such as open-world games. To overcome these challenges, we introduce\nAdaRefiner, a novel framework designed to enhance the synergy between LLMs and\nRL feedback. The key component of AdaRefiner is a lightweight Adapter Language\nModel (LM), which automatically refines task comprehension based on feedback\nfrom RL agents. This method mitigates the need for intricate prompt engineering\nand intensive LLM fine-tuning while maintaining the LLMs' generalization\nabilities and enhancing their decision-making capabilities in downstream tasks.\nEmpirical evaluations of AdaRefiner on 22 diverse tasks within the open-world\ngame Crafter have demonstrated its superior effectiveness, especially in\nguiding agents towards higher-level and common-sense skills. Our work makes\ncontributions to the automatic self-refinement of LLMs with RL feedback,\noffering a more adaptable and efficient solution for complex decision-making\nproblems.\n","authors":["Wanpeng Zhang","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2309.17176v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10302v1","updated":"2023-12-16T03:33:12Z","published":"2023-12-16T03:33:12Z","title":"One Shot Learning as Instruction Data Prospector for Large Language\n  Models","summary":"  Aligning large language models(LLMs) with human is a critical step in\neffectively utilizing their pre-trained capabilities across a wide array of\nlanguage tasks. Current instruction tuning practices often rely on expanding\ndataset size without a clear strategy for ensuring data quality, which can\ninadvertently introduce noise and degrade model performance. To address this\nchallenge, we introduce Nuggets, a novel and efficient methodology that employs\none shot learning to select high-quality instruction data from expansive\ndatasets. Nuggets assesses the potential of individual instruction examples to\nact as effective one shot examples, thereby identifying those that can\nsignificantly enhance diverse task performance. Nuggets utilizes a scoring\nsystem based on the impact of candidate examples on the perplexity of a diverse\nanchor set, facilitating the selection of the most beneficial data for\ninstruction tuning. Through rigorous testing on two benchmarks, including\nMT-Bench and Alpaca-Eval, we demonstrate that instruction tuning with the top\n1% of Nuggets-curated examples substantially outperforms conventional methods\nthat use the full dataset. These findings advocate for a data selection\nparadigm that prioritizes quality, offering a more efficient pathway to align\nLLMs with humans.\n","authors":["Yunshui Li","Binyuan Hui","Xiaobo Xia","Jiaxi Yang","Min Yang","Lei Zhang","Shuzheng Si","Junhao Liu","Tongliang Liu","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2312.10302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.03617v2","updated":"2023-12-16T02:17:19Z","published":"2021-04-08T08:57:13Z","title":"Half-Truth: A Partially Fake Audio Detection Dataset","summary":"  Diverse promising datasets have been designed to hold back the development of\nfake audio detection, such as ASVspoof databases. However, previous datasets\nignore an attacking situation, in which the hacker hides some small fake clips\nin real speech audio. This poses a serious threat since that it is difficult to\ndistinguish the small fake clip from the whole speech utterance. Therefore,\nthis paper develops such a dataset for half-truth audio detection (HAD).\nPartially fake audio in the HAD dataset involves only changing a few words in\nan utterance.The audio of the words is generated with the very latest\nstate-of-the-art speech synthesis technology. We can not only detect fake\nuttrances but also localize manipulated regions in a speech using this dataset.\nSome benchmark results are presented on this dataset. The results show that\npartially fake audio presents much more challenging than fully fake audio for\nfake audio detection. The HAD dataset is publicly available:\nhttps://zenodo.org/records/10377492.\n","authors":["Jiangyan Yi","Ye Bai","Jianhua Tao","Haoxin Ma","Zhengkun Tian","Chenglong Wang","Tao Wang","Ruibo Fu"],"pdf_url":"https://arxiv.org/pdf/2104.03617v2.pdf","comment":"accepted by Interspeech 2021"},{"id":"http://arxiv.org/abs/2310.10605v3","updated":"2023-12-16T01:39:22Z","published":"2023-10-16T17:31:34Z","title":"ForceGen: End-to-end de novo protein generation based on nonlinear\n  mechanical unfolding responses using a protein language diffusion model","summary":"  Through evolution, nature has presented a set of remarkable protein\nmaterials, including elastins, silks, keratins and collagens with superior\nmechanical performances that play crucial roles in mechanobiology. However,\ngoing beyond natural designs to discover proteins that meet specified\nmechanical properties remains challenging. Here we report a generative model\nthat predicts protein designs to meet complex nonlinear mechanical\nproperty-design objectives. Our model leverages deep knowledge on protein\nsequences from a pre-trained protein language model and maps mechanical\nunfolding responses to create novel proteins. Via full-atom molecular\nsimulations for direct validation, we demonstrate that the designed proteins\nare novel, and fulfill the targeted mechanical properties, including unfolding\nenergy and mechanical strength, as well as the detailed unfolding\nforce-separation curves. Our model offers rapid pathways to explore the\nenormous mechanobiological protein sequence space unconstrained by biological\nsynthesis, using mechanical features as target to enable the discovery of\nprotein materials with superior mechanical properties.\n","authors":["Bo Ni","David L. Kaplan","Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2310.10605v3.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2303.17743v3","updated":"2023-12-16T22:42:42Z","published":"2023-03-30T23:30:42Z","title":"FairGen: Towards Fair Graph Generation","summary":"  There have been tremendous efforts over the past decades dedicated to the\ngeneration of realistic graphs in a variety of domains, ranging from social\nnetworks to computer networks, from gene regulatory networks to online\ntransaction networks. Despite the remarkable success, the vast majority of\nthese works are unsupervised in nature and are typically trained to minimize\nthe expected graph reconstruction loss, which would result in the\nrepresentation disparity issue in the generated graphs, i.e., the protected\ngroups (often minorities) contribute less to the objective and thus suffer from\nsystematically higher errors. In this paper, we aim to tailor graph generation\nto downstream mining tasks by leveraging label information and user-preferred\nparity constraints. In particular, we start from the investigation of\nrepresentation disparity in the context of graph generative models. To mitigate\nthe disparity, we propose a fairness-aware graph generative model named\nFairGen. Our model jointly trains a label-informed graph generation module and\na fair representation learning module by progressively learning the behaviors\nof the protected and unprotected groups, from the `easy' concepts to the `hard'\nones. In addition, we propose a generic context sampling strategy for graph\ngenerative models, which is proven to be capable of fairly capturing the\ncontextual information of each group with a high probability. Experimental\nresults on seven real-world data sets, including web-based graphs, demonstrate\nthat FairGen (1) obtains performance on par with state-of-the-art graph\ngenerative models across nine network properties, (2) mitigates the\nrepresentation disparity issues in the generated graphs, and (3) substantially\nboosts the model performance by up to 17% in downstream tasks via data\naugmentation.\n","authors":["Lecheng Zheng","Dawei Zhou","Hanghang Tong","Jiejun Xu","Yada Zhu","Jingrui He"],"pdf_url":"https://arxiv.org/pdf/2303.17743v3.pdf","comment":"Accepted by ICDE 2024"},{"id":"http://arxiv.org/abs/2306.15521v3","updated":"2023-12-16T21:11:07Z","published":"2023-06-27T14:47:43Z","title":"What a MESS: Multi-Domain Evaluation of Zero-Shot Semantic Segmentation","summary":"  While semantic segmentation has seen tremendous improvements in the past,\nthere are still significant labeling efforts necessary and the problem of\nlimited generalization to classes that have not been present during training.\nTo address this problem, zero-shot semantic segmentation makes use of large\nself-supervised vision-language models, allowing zero-shot transfer to unseen\nclasses. In this work, we build a benchmark for Multi-domain Evaluation of\nSemantic Segmentation (MESS), which allows a holistic analysis of performance\nacross a wide range of domain-specific datasets such as medicine, engineering,\nearth monitoring, biology, and agriculture. To do this, we reviewed 120\ndatasets, developed a taxonomy, and classified the datasets according to the\ndeveloped taxonomy. We select a representative subset consisting of 22 datasets\nand propose it as the MESS benchmark. We evaluate eight recently published\nmodels on the proposed MESS benchmark and analyze characteristics for the\nperformance of zero-shot transfer models. The toolkit is available at\nhttps://github.com/blumenstiel/MESS.\n","authors":["Benedikt Blumenstiel","Johannes Jakubik","Hilde Kühne","Michael Vössing"],"pdf_url":"https://arxiv.org/pdf/2306.15521v3.pdf","comment":"37th Conference on Neural Information Processing Systems (NeurIPS\n  2023) Track on Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2312.10540v1","updated":"2023-12-16T20:49:00Z","published":"2023-12-16T20:49:00Z","title":"VecFusion: Vector Font Generation with Diffusion","summary":"  We present VecFusion, a new neural architecture that can generate vector\nfonts with varying topological structures and precise control point positions.\nOur approach is a cascaded diffusion model which consists of a raster diffusion\nmodel followed by a vector diffusion model. The raster model generates\nlow-resolution, rasterized fonts with auxiliary control point information,\ncapturing the global style and shape of the font, while the vector model\nsynthesizes vector fonts conditioned on the low-resolution raster fonts from\nthe first stage. To synthesize long and complex curves, our vector diffusion\nmodel uses a transformer architecture and a novel vector representation that\nenables the modeling of diverse vector geometry and the precise prediction of\ncontrol points. Our experiments show that, in contrast to previous generative\nmodels for vector graphics, our new cascaded vector diffusion model generates\nhigher quality vector fonts, with complex structures and diverse styles.\n","authors":["Vikas Thamizharasan","Difan Liu","Shantanu Agarwal","Matthew Fisher","Michael Gharbi","Oliver Wang","Alec Jacobson","Evangelos Kalogerakis"],"pdf_url":"https://arxiv.org/pdf/2312.10540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10539v1","updated":"2023-12-16T20:38:02Z","published":"2023-12-16T20:38:02Z","title":"DETER: Detecting Edited Regions for Deterring Generative Manipulations","summary":"  Generative AI capabilities have grown substantially in recent years, raising\nrenewed concerns about potential malicious use of generated data, or \"deep\nfakes\". However, deep fake datasets have not kept up with generative AI\nadvancements sufficiently to enable the development of deep fake detection\ntechnology which can meaningfully alert human users in real-world settings.\nExisting datasets typically use GAN-based models and introduce spurious\ncorrelations by always editing similar face regions. To counteract the\nshortcomings, we introduce DETER, a large-scale dataset for DETEcting edited\nimage Regions and deterring modern advanced generative manipulations. DETER\nincludes 300,000 images manipulated by four state-of-the-art generators with\nthree editing operations: face swapping (a standard coarse image manipulation),\ninpainting (a novel manipulation for deep fake datasets), and attribute editing\n(a subtle fine-grained manipulation). While face swapping and attribute editing\nare performed on similar face regions such as eyes and nose, the inpainting\noperation can be performed on random image regions, removing the spurious\ncorrelations of previous datasets. Careful image post-processing is performed\nto ensure deep fakes in DETER look realistic, and human studies confirm that\nhuman deep fake detection rate on DETER is 20.4% lower than on other fake\ndatasets. Equipped with the dataset, we conduct extensive experiments and\nbreak-down analysis using our rich annotations and improved benchmark\nprotocols, revealing future directions and the next set of challenges in\ndeveloping reliable regional fake detection models.\n","authors":["Sai Wang","Ye Zhu","Ruoyu Wang","Amaya Dharmasiri","Olga Russakovsky","Yu Wu"],"pdf_url":"https://arxiv.org/pdf/2312.10539v1.pdf","comment":"First two authors contribute equally to this work. Project page at\n  https://deter2024.github.io/deter/"},{"id":"http://arxiv.org/abs/2106.05152v7","updated":"2023-12-16T20:35:12Z","published":"2021-06-09T15:51:03Z","title":"Rethinking Transfer Learning for Medical Image Classification","summary":"  Transfer learning (TL) from pretrained deep models is a standard practice in\nmodern medical image classification (MIC). However, what levels of features to\nbe reused are problem-dependent, and uniformly finetuning all layers of\npretrained models may be suboptimal. This insight has partly motivated the\nrecent differential TL strategies, such as TransFusion (TF) and layer-wise\nfinetuning (LWFT), which treat the layers in the pretrained models\ndifferentially. In this paper, we add one more strategy into this family,\ncalled TruncatedTL, which reuses and finetunes appropriate bottom layers and\ndirectly discards the remaining layers. This yields not only superior MIC\nperformance but also compact models for efficient inference, compared to other\ndifferential TL methods. Our code is available at:\nhttps://github.com/sun-umn/TTL\n","authors":["Le Peng","Hengyue Liang","Gaoxiang Luo","Taihui Li","Ju Sun"],"pdf_url":"https://arxiv.org/pdf/2106.05152v7.pdf","comment":"Accepted to BMVC2023 (oral)"},{"id":"http://arxiv.org/abs/2312.10534v1","updated":"2023-12-16T20:20:38Z","published":"2023-12-16T20:20:38Z","title":"Rethinking Robustness of Model Attributions","summary":"  For machine learning models to be reliable and trustworthy, their decisions\nmust be interpretable. As these models find increasing use in safety-critical\napplications, it is important that not just the model predictions but also\ntheir explanations (as feature attributions) be robust to small\nhuman-imperceptible input perturbations. Recent works have shown that many\nattribution methods are fragile and have proposed improvements in either these\nmethods or the model training. We observe two main causes for fragile\nattributions: first, the existing metrics of robustness (e.g., top-k\nintersection) over-penalize even reasonable local shifts in attribution,\nthereby making random perturbations to appear as a strong attack, and second,\nthe attribution can be concentrated in a small region even when there are\nmultiple important parts in an image. To rectify this, we propose simple ways\nto strengthen existing metrics and attribution methods that incorporate\nlocality of pixels in robustness metrics and diversity of pixel locations in\nattributions. Towards the role of model training in attributional robustness,\nwe empirically observe that adversarially trained models have more robust\nattributions on smaller datasets, however, this advantage disappears in larger\ndatasets. Code is available at https://github.com/ksandeshk/LENS.\n","authors":["Sandesh Kamath","Sankalp Mittal","Amit Deshpande","Vineeth N Balasubramanian"],"pdf_url":"https://arxiv.org/pdf/2312.10534v1.pdf","comment":"Accepted AAAI 2024"},{"id":"http://arxiv.org/abs/2312.10531v1","updated":"2023-12-16T20:10:23Z","published":"2023-12-16T20:10:23Z","title":"How to Train Neural Field Representations: A Comprehensive Study and\n  Benchmark","summary":"  Neural fields (NeFs) have recently emerged as a versatile method for modeling\nsignals of various modalities, including images, shapes, and scenes.\nSubsequently, a number of works have explored the use of NeFs as\nrepresentations for downstream tasks, e.g. classifying an image based on the\nparameters of a NeF that has been fit to it. However, the impact of the NeF\nhyperparameters on their quality as downstream representation is scarcely\nunderstood and remains largely unexplored. This is in part caused by the large\namount of time required to fit datasets of neural fields.\n  In this work, we propose $\\verb|fit-a-nef|$, a JAX-based library that\nleverages parallelization to enable fast optimization of large-scale NeF\ndatasets, resulting in a significant speed-up. With this library, we perform a\ncomprehensive study that investigates the effects of different hyperparameters\n-- including initialization, network architecture, and optimization strategies\n-- on fitting NeFs for downstream tasks. Our study provides valuable insights\non how to train NeFs and offers guidance for optimizing their effectiveness in\ndownstream applications. Finally, based on the proposed library and our\nanalysis, we propose Neural Field Arena, a benchmark consisting of neural field\nvariants of popular vision datasets, including MNIST, CIFAR, variants of\nImageNet, and ShapeNetv2. Our library and the Neural Field Arena will be\nopen-sourced to introduce standardized benchmarking and promote further\nresearch on neural fields.\n","authors":["Samuele Papa","Riccardo Valperga","David Knigge","Miltiadis Kofinas","Phillip Lippe","Jan-Jakob Sonke","Efstratios Gavves"],"pdf_url":"https://arxiv.org/pdf/2312.10531v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10529v1","updated":"2023-12-16T20:00:34Z","published":"2023-12-16T20:00:34Z","title":"Transformers in Unsupervised Structure-from-Motion","summary":"  Transformers have revolutionized deep learning based computer vision with\nimproved performance as well as robustness to natural corruptions and\nadversarial attacks. Transformers are used predominantly for 2D vision tasks,\nincluding image classification, semantic segmentation, and object detection.\nHowever, robots and advanced driver assistance systems also require 3D scene\nunderstanding for decision making by extracting structure-from-motion (SfM). We\npropose a robust transformer-based monocular SfM method that learns to predict\nmonocular pixel-wise depth, ego vehicle's translation and rotation, as well as\ncamera's focal length and principal point, simultaneously. With experiments on\nKITTI and DDAD datasets, we demonstrate how to adapt different vision\ntransformers and compare them against contemporary CNN-based methods. Our study\nshows that transformer-based architecture, though lower in run-time efficiency,\nachieves comparable performance while being more robust against natural\ncorruptions, as well as untargeted and targeted attacks.\n","authors":["Hemang Chawla","Arnav Varma","Elahe Arani","Bahram Zonooz"],"pdf_url":"https://arxiv.org/pdf/2312.10529v1.pdf","comment":"International Joint Conference on Computer Vision, Imaging and\n  Computer Graphics. Cham: Springer Nature Switzerland, 2022. Published at\n  \"Communications in Computer and Information Science, vol 1815. Springer\n  Nature\". arXiv admin note: text overlap with arXiv:2202.03131"},{"id":"http://arxiv.org/abs/2311.11125v2","updated":"2023-12-16T18:29:01Z","published":"2023-11-18T17:14:07Z","title":"SecondPose: SE(3)-Consistent Dual-Stream Feature Fusion for\n  Category-Level Pose Estimation","summary":"  Category-level object pose estimation, aiming to predict the 6D pose and 3D\nsize of objects from known categories, typically struggles with large\nintra-class shape variation. Existing works utilizing mean shapes often fall\nshort of capturing this variation. To address this issue, we present\nSecondPose, a novel approach integrating object-specific geometric features\nwith semantic category priors from DINOv2. Leveraging the advantage of DINOv2\nin providing SE(3)-consistent semantic features, we hierarchically extract two\ntypes of SE(3)-invariant geometric features to further encapsulate\nlocal-to-global object-specific information. These geometric features are then\npoint-aligned with DINOv2 features to establish a consistent object\nrepresentation under SE(3) transformations, facilitating the mapping from\ncamera space to the pre-defined canonical space, thus further enhancing pose\nestimation. Extensive experiments on NOCS-REAL275 demonstrate that SecondPose\nachieves a 12.4% leap forward over the state-of-the-art. Moreover, on a more\ncomplex dataset HouseCat6D which provides photometrically challenging objects,\nSecondPose still surpasses other competitors by a large margin. The code will\nbe released soon.\n","authors":["Yamei Chen","Yan Di","Guangyao Zhai","Fabian Manhardt","Chenyangguang Zhang","Ruida Zhang","Federico Tombari","Nassir Navab","Benjamin Busam"],"pdf_url":"https://arxiv.org/pdf/2311.11125v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10515v1","updated":"2023-12-16T18:04:56Z","published":"2023-12-16T18:04:56Z","title":"PETDet: Proposal Enhancement for Two-Stage Fine-Grained Object Detection","summary":"  Fine-grained object detection (FGOD) extends object detection with the\ncapability of fine-grained recognition. In recent two-stage FGOD methods, the\nregion proposal serves as a crucial link between detection and fine-grained\nrecognition. However, current methods overlook that some proposal-related\nprocedures inherited from general detection are not equally suitable for FGOD,\nlimiting the multi-task learning from generation, representation, to\nutilization. In this paper, we present PETDet (Proposal Enhancement for\nTwo-stage fine-grained object detection) to better handle the sub-tasks in\ntwo-stage FGOD methods. Firstly, an anchor-free Quality Oriented Proposal\nNetwork (QOPN) is proposed with dynamic label assignment and attention-based\ndecomposition to generate high-quality oriented proposals. Additionally, we\npresent a Bilinear Channel Fusion Network (BCFN) to extract independent and\ndiscriminative features of the proposals. Furthermore, we design a novel\nAdaptive Recognition Loss (ARL) which offers guidance for the R-CNN head to\nfocus on high-quality proposals. Extensive experiments validate the\neffectiveness of PETDet. Quantitative analysis reveals that PETDet with\nResNet50 reaches state-of-the-art performance on various FGOD datasets,\nincluding FAIR1M-v1.0 (42.96 AP), FAIR1M-v2.0 (48.81 AP), MAR20 (85.91 AP) and\nShipRSImageNet (74.90 AP). The proposed method also achieves superior\ncompatibility between accuracy and inference speed. Our code and models will be\nreleased at https://github.com/canoe-Z/PETDet.\n","authors":["Wentao Li","Danpei Zhao","Bo Yuan","Yue Gao","Zhenwei Shi"],"pdf_url":"https://arxiv.org/pdf/2312.10515v1.pdf","comment":"IEEE TGRS 2023"},{"id":"http://arxiv.org/abs/2303.17561v2","updated":"2023-12-16T16:27:57Z","published":"2023-03-30T17:27:22Z","title":"SoftCLIP: Softer Cross-modal Alignment Makes CLIP Stronger","summary":"  During the preceding biennium, vision-language pre-training has achieved\nnoteworthy success on several downstream tasks. Nevertheless, acquiring\nhigh-quality image-text pairs, where the pairs are entirely exclusive of each\nother, remains a challenging task, and noise exists in the commonly used\ndatasets. To address this issue, we propose SoftCLIP, a novel approach that\nrelaxes the strict one-to-one constraint and achieves a soft cross-modal\nalignment by introducing a softened target, which is generated from the\nfine-grained intra-modal self-similarity. The intra-modal guidance is\nindicative to enable two pairs have some local similarities and model\nmany-to-many relationships between the two modalities. Besides, since the\npositive still dominates in the softened target distribution, we disentangle\nthe negatives in the distribution to further boost the relation alignment with\nthe negatives in the cross-modal learning. Extensive experiments demonstrate\nthe effectiveness of SoftCLIP. In particular, on ImageNet zero-shot\nclassification task, using CC3M/CC12M as pre-training dataset, SoftCLIP brings\na top-1 accuracy improvement of 6.8%/7.2% over the CLIP baseline.\n","authors":["Yuting Gao","Jinfeng Liu","Zihan Xu","Tong Wu Enwei Zhang","Wei Liu","Jie Yang","Ke Li","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2303.17561v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18890v2","updated":"2023-12-16T15:24:18Z","published":"2023-10-29T03:35:34Z","title":"Towards Generalized Multi-stage Clustering: Multi-view Self-distillation","summary":"  Existing multi-stage clustering methods independently learn the salient\nfeatures from multiple views and then perform the clustering task.\nParticularly, multi-view clustering (MVC) has attracted a lot of attention in\nmulti-view or multi-modal scenarios. MVC aims at exploring common semantics and\npseudo-labels from multiple views and clustering in a self-supervised manner.\nHowever, limited by noisy data and inadequate feature learning, such a\nclustering paradigm generates overconfident pseudo-labels that mis-guide the\nmodel to produce inaccurate predictions. Therefore, it is desirable to have a\nmethod that can correct this pseudo-label mistraction in multi-stage clustering\nto avoid the bias accumulation. To alleviate the effect of overconfident\npseudo-labels and improve the generalization ability of the model, this paper\nproposes a novel multi-stage deep MVC framework where multi-view\nself-distillation (DistilMVC) is introduced to distill dark knowledge of label\ndistribution. Specifically, in the feature subspace at different hierarchies,\nwe explore the common semantics of multiple views through contrastive learning\nand obtain pseudo-labels by maximizing the mutual information between views.\nAdditionally, a teacher network is responsible for distilling pseudo-labels\ninto dark knowledge, supervising the student network and improving its\npredictive capabilities to enhance the robustness. Extensive experiments on\nreal-world multi-view datasets show that our method has better clustering\nperformance than state-of-the-art methods.\n","authors":["Jiatai Wang","Zhiwei Xu","Xin Wang","Tao Li"],"pdf_url":"https://arxiv.org/pdf/2310.18890v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10483v1","updated":"2023-12-16T15:23:22Z","published":"2023-12-16T15:23:22Z","title":"All Attention U-NET for Semantic Segmentation of Intracranial\n  Hemorrhages In Head CT Images","summary":"  Intracranial hemorrhages in head CT scans serve as a first line tool to help\nspecialists diagnose different types. However, their types have diverse shapes\nin the same type but similar confusing shape, size and location between types.\nTo solve this problem, this paper proposes an all attention U-Net. It uses\nchannel attentions in the U-Net encoder side to enhance class specific feature\nextraction, and space and channel attentions in the U-Net decoder side for more\naccurate shape extraction and type classification. The simulation results show\nup to a 31.8\\% improvement compared to baseline, ResNet50 + U-Net, and better\nperformance than in cases with limited attention.\n","authors":["Chia Shuo Chang","Tian Sheuan Chang","Jiun Lin Yan","Li Ko"],"pdf_url":"https://arxiv.org/pdf/2312.10483v1.pdf","comment":"2022 IEEE Biomedical Circuits and Systems Conference (BioCAS)"},{"id":"http://arxiv.org/abs/2312.10482v1","updated":"2023-12-16T15:21:51Z","published":"2023-12-16T15:21:51Z","title":"A new method color MS-BSIF Features learning for the robust kinship\n  verification","summary":"  the paper presents a new method color MS-BSIF learning and MS-LBP for the\nkinship verification is the machine's ability to identify the genetic and blood\nthe relationship and its degree between the facial images of humans. Facial\nverification of kinship refers to the task of training a machine to recognize\nthe blood relationship between a pair of faces parent and non-parent\n(verification) based on features extracted from facial images, and determining\nthe exact type or degree of this genetic relationship. We use the LBP and color\nBSIF learning features for the comparison and the TXQDA method for\ndimensionality reduction and data classification. We let's test the kinship\nfacial verification application is namely the kinface Cornell database. This\nsystem improves the robustness of learning while controlling efficiency. The\nexperimental results obtained and compared to other methods have proven the\nreliability of our framework and surpass the performance of other\nstate-of-the-art techniques.\n","authors":["Rachid Aliradi","Abdealmalik Ouamane","Abdeslam Amrane"],"pdf_url":"https://arxiv.org/pdf/2312.10482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10470v1","updated":"2023-12-16T15:04:07Z","published":"2023-12-16T15:04:07Z","title":"Enhancing Person Re-Identification through Tensor Feature Fusion","summary":"  In this paper, we present a novel person reidentification (PRe-ID) system\nthat based on tensor feature representation and multilinear subspace learning.\nOur approach utilizes pretrained CNNs for high-level feature extraction, along\nwith Local Maximal Occurrence (LOMO) and Gaussian Of Gaussian (GOG )\ndescriptors. Additionally, Cross-View Quadratic Discriminant Analysis (TXQDA)\nalgorithm is used for multilinear subspace learning, which models the data in a\ntensor framework to enhance discriminative capabilities. Similarity measure\nbased on Mahalanobis distance is used for matching between training and test\npedestrian images. Experimental evaluations on VIPeR and PRID450s datasets\ndemonstrate the effectiveness of our method.\n","authors":["Akram Abderraouf Gharbi","Ammar Chouchane","Mohcene Bessaoudi","Abdelmalik Ouamane","El ouanas Belabbaci"],"pdf_url":"https://arxiv.org/pdf/2312.10470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10464v1","updated":"2023-12-16T14:46:24Z","published":"2023-12-16T14:46:24Z","title":"Unveiling Empirical Pathologies of Laplace Approximation for Uncertainty\n  Estimation","summary":"  In this paper, we critically evaluate Bayesian methods for uncertainty\nestimation in deep learning, focusing on the widely applied Laplace\napproximation and its variants. Our findings reveal that the conventional\nmethod of fitting the Hessian matrix negatively impacts out-of-distribution\n(OOD) detection efficiency. We propose a different point of view, asserting\nthat focusing solely on optimizing prior precision can yield more accurate\nuncertainty estimates in OOD detection while preserving adequate calibration\nmetrics. Moreover, we demonstrate that this property is not connected to the\ntraining stage of a model but rather to its intrinsic properties. Through\nextensive experimental evaluation, we establish the superiority of our\nsimplified approach over traditional methods in the out-of-distribution domain.\n","authors":["Maksim Zhdanov","Stanislav Dereka","Sergey Kolesnikov"],"pdf_url":"https://arxiv.org/pdf/2312.10464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10462v1","updated":"2023-12-16T14:36:43Z","published":"2023-12-16T14:36:43Z","title":"Fusion of Deep and Shallow Features for Face Kinship Verification","summary":"  Kinship verification from face images is a novel and formidable challenge in\nthe realms of pattern recognition and computer vision. This work makes notable\ncontributions by incorporating a preprocessing technique known as Multiscale\nRetinex (MSR), which enhances image quality. Our approach harnesses the\nstrength of complementary deep (VGG16) and shallow texture descriptors (BSIF)\nby combining them at the score level using Logistic Regression (LR) technique.\nWe assess the effectiveness of our approach by conducting comprehensive\nexperiments on three challenging kinship datasets: Cornell Kin Face, UB Kin\nFace and TS Kin Face\n","authors":["Belabbaci El Ouanas","Khammari Mohammed","Chouchane Ammar","Mohcene Bessaoudi","Abdelmalik Ouamane","Akram Abderraouf Gharbi"],"pdf_url":"https://arxiv.org/pdf/2312.10462v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2312.03562"},{"id":"http://arxiv.org/abs/2312.10461v1","updated":"2023-12-16T14:27:06Z","published":"2023-12-16T14:27:06Z","title":"Rethinking the Up-Sampling Operations in CNN-based Generative Network\n  for Generalizable Deepfake Detection","summary":"  Recently, the proliferation of highly realistic synthetic images, facilitated\nthrough a variety of GANs and Diffusions, has significantly heightened the\nsusceptibility to misuse. While the primary focus of deepfake detection has\ntraditionally centered on the design of detection algorithms, an investigative\ninquiry into the generator architectures has remained conspicuously absent in\nrecent years. This paper contributes to this lacuna by rethinking the\narchitectures of CNN-based generators, thereby establishing a generalized\nrepresentation of synthetic artifacts. Our findings illuminate that the\nup-sampling operator can, beyond frequency-based artifacts, produce generalized\nforgery artifacts. In particular, the local interdependence among image pixels\ncaused by upsampling operators is significantly demonstrated in synthetic\nimages generated by GAN or diffusion. Building upon this observation, we\nintroduce the concept of Neighboring Pixel Relationships(NPR) as a means to\ncapture and characterize the generalized structural artifacts stemming from\nup-sampling operations. A comprehensive analysis is conducted on an open-world\ndataset, comprising samples generated by \\tft{28 distinct generative models}.\nThis analysis culminates in the establishment of a novel state-of-the-art\nperformance, showcasing a remarkable \\tft{12.8\\%} improvement over existing\nmethods. The code is available at\nhttps://github.com/chuangchuangtan/NPR-DeepfakeDetection.\n","authors":["Chuangchuang Tan","Yao Zhao","Shikui Wei","Guanghua Gu","Ping Liu","Yunchao Wei"],"pdf_url":"https://arxiv.org/pdf/2312.10461v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2312.10457v1","updated":"2023-12-16T14:03:10Z","published":"2023-12-16T14:03:10Z","title":"Semantic-Aware Autoregressive Image Modeling for Visual Representation\n  Learning","summary":"  The development of autoregressive modeling (AM) in computer vision lags\nbehind natural language processing (NLP) in self-supervised pre-training. This\nis mainly caused by the challenge that images are not sequential signals and\nlack a natural order when applying autoregressive modeling. In this study,\ninspired by human beings' way of grasping an image, i.e., focusing on the main\nobject first, we present a semantic-aware autoregressive image modeling\n(SemAIM) method to tackle this challenge. The key insight of SemAIM is to\nautoregressive model images from the semantic patches to the less semantic\npatches. To this end, we first calculate a semantic-aware permutation of\npatches according to their feature similarities and then perform the\nautoregression procedure based on the permutation. In addition, considering\nthat the raw pixels of patches are low-level signals and are not ideal\nprediction targets for learning high-level semantic representation, we also\nexplore utilizing the patch features as the prediction targets. Extensive\nexperiments are conducted on a broad range of downstream tasks, including image\nclassification, object detection, and instance/semantic segmentation, to\nevaluate the performance of SemAIM. The results demonstrate SemAIM achieves\nstate-of-the-art performance compared with other self-supervised methods.\nSpecifically, with ViT-B, SemAIM achieves 84.1% top-1 accuracy for fine-tuning\non ImageNet, 51.3% AP and 45.4% AP for object detection and instance\nsegmentation on COCO, which outperforms the vanilla MAE by 0.5%, 1.0%, and\n0.5%, respectively.\n","authors":["Kaiyou Song","Shan Zhang","Tong Wang"],"pdf_url":"https://arxiv.org/pdf/2312.10457v1.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2312.10447v1","updated":"2023-12-16T13:39:52Z","published":"2023-12-16T13:39:52Z","title":"Finger biometric recognition with feature selection","summary":"  Biometrics is indispensable in this modern digital era for secure automated\nhuman authentication in various fields of machine learning and pattern\nrecognition. Hand geometry is a promising physiological biometric trait with\nample deployed application areas for identity verification. Due to the\nintricate anatomic foundation of the thumb and substantial inter-finger posture\nvariation, satisfactory performances cannot be achieved while the thumb is\nincluded in the contact-free environment. To overcome the hindrances associated\nwith the thumb, four finger-based (excluding the thumb) biometric approaches\nhave been devised. In this chapter, a four-finger based biometric method has\nbeen presented. Again, selection of salient features is essential to reduce the\nfeature dimensionality by eliminating the insignificant features. Weights are\nassigned according to the discriminative efficiency of the features to\nemphasize on the essential features. Two different strategies namely, the\nglobal and local feature selection methods are adopted based on the adaptive\nforward-selection and backward-elimination (FoBa) algorithm. The identification\nperformances are evaluated using the weighted k-nearest neighbor (wk-NN) and\nrandom forest (RF) classifiers. The experiments are conducted using the\nselected feature subsets over the 300 subjects of the Bosphorus hand database.\nThe best identification accuracy of 98.67%, and equal error rate (EER) of 4.6%\nhave been achieved using the subset of 25 features which are selected by the\nrank-based local FoBa algorithm.\n","authors":["Asish Bera","Debotosh Bhattacharjee","Mita Nasipuri"],"pdf_url":"https://arxiv.org/pdf/2312.10447v1.pdf","comment":"34 pages. The Biometric Computing: Recognition and Registration, 2019"},{"id":"http://arxiv.org/abs/2308.12558v2","updated":"2023-12-16T13:13:21Z","published":"2023-08-24T04:52:32Z","title":"Hyperbolic Audio-visual Zero-shot Learning","summary":"  Audio-visual zero-shot learning aims to classify samples consisting of a pair\nof corresponding audio and video sequences from classes that are not present\nduring training. An analysis of the audio-visual data reveals a large degree of\nhyperbolicity, indicating the potential benefit of using a hyperbolic\ntransformation to achieve curvature-aware geometric learning, with the aim of\nexploring more complex hierarchical data structures for this task. The proposed\napproach employs a novel loss function that incorporates cross-modality\nalignment between video and audio features in the hyperbolic space.\nAdditionally, we explore the use of multiple adaptive curvatures for hyperbolic\nprojections. The experimental results on this very challenging task demonstrate\nthat our proposed hyperbolic approach for zero-shot learning outperforms the\nSOTA method on three datasets: VGGSound-GZSL, UCF-GZSL, and ActivityNet-GZSL\nachieving a harmonic mean (HM) improvement of around 3.0%, 7.0%, and 5.3%,\nrespectively.\n","authors":["Jie Hong","Zeeshan Hayder","Junlin Han","Pengfei Fang","Mehrtash Harandi","Lars Petersson"],"pdf_url":"https://arxiv.org/pdf/2308.12558v2.pdf","comment":"ICCV 2023"},{"id":"http://arxiv.org/abs/2312.10439v1","updated":"2023-12-16T13:06:15Z","published":"2023-12-16T13:06:15Z","title":"Simple Image-level Classification Improves Open-vocabulary Object\n  Detection","summary":"  Open-Vocabulary Object Detection (OVOD) aims to detect novel objects beyond a\ngiven set of base categories on which the detection model is trained. Recent\nOVOD methods focus on adapting the image-level pre-trained vision-language\nmodels (VLMs), such as CLIP, to a region-level object detection task via, eg.,\nregion-level knowledge distillation, regional prompt learning, or region-text\npre-training, to expand the detection vocabulary. These methods have\ndemonstrated remarkable performance in recognizing regional visual concepts,\nbut they are weak in exploiting the VLMs' powerful global scene understanding\nability learned from the billion-scale image-level text descriptions. This\nlimits their capability in detecting hard objects of small, blurred, or\noccluded appearance from novel/base categories, whose detection heavily relies\non contextual information. To address this, we propose a novel approach, namely\nSimple Image-level Classification for Context-Aware Detection Scoring\n(SIC-CADS), to leverage the superior global knowledge yielded from CLIP for\ncomplementing the current OVOD models from a global perspective. The core of\nSIC-CADS is a multi-modal multi-label recognition (MLR) module that learns the\nobject co-occurrence-based contextual information from CLIP to recognize all\npossible object categories in the scene. These image-level MLR scores can then\nbe utilized to refine the instance-level detection scores of the current OVOD\nmodels in detecting those hard objects. This is verified by extensive empirical\nresults on two popular benchmarks, OV-LVIS and OV-COCO, which show that\nSIC-CADS achieves significant and consistent improvement when combined with\ndifferent types of OVOD models. Further, SIC-CADS also improves the\ncross-dataset generalization ability on Objects365 and OpenImages. The code is\navailable at https://github.com/mala-lab/SIC-CADS.\n","authors":["Ruohuan Fang","Guansong Pang","Xiao Bai"],"pdf_url":"https://arxiv.org/pdf/2312.10439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10437v1","updated":"2023-12-16T12:54:28Z","published":"2023-12-16T12:54:28Z","title":"Tender Notice Extraction from E-papers Using Neural Network","summary":"  Tender notices are usually sought by most of the companies at regular\nintervals as a means for obtaining the contracts of various projects. These\nnotices consist of all the required information like description of the work,\nperiod of construction, estimated amount of project, etc. In the context of\nNepal, tender notices are usually published in national as well as local\nnewspapers. The interested bidders should search all the related tender notices\nin newspapers. However, it is very tedious for these companies to manually\nsearch tender notices in every newspaper and figure out which bid is best\nsuited for them. This project is built with the purpose of solving this tedious\ntask of manually searching the tender notices. Initially, the newspapers are\ndownloaded in PDF format using the selenium library of python. After\ndownloading the newspapers, the e-papers are scanned and tender notices are\nautomatically extracted using a neural network. For extraction purposes,\ndifferent architectures of CNN namely ResNet, GoogleNet and Xception are used\nand a model with highest performance has been implemented. Finally, these\nextracted notices are then published on the website and are accessible to the\nusers. This project is helpful for construction companies as well as\ncontractors assuring quality and efficiency. This project has great application\nin the field of competitive bidding as well as managing them in a systematic\nmanner.\n","authors":["Ashmin Bhattarai","Anuj Sedhai","Devraj Neupane","Manish Khadka","Rama Bastola"],"pdf_url":"https://arxiv.org/pdf/2312.10437v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10422v1","updated":"2023-12-16T11:31:34Z","published":"2023-12-16T11:31:34Z","title":"Learning Dense Correspondence for NeRF-Based Face Reenactment","summary":"  Face reenactment is challenging due to the need to establish dense\ncorrespondence between various face representations for motion transfer. Recent\nstudies have utilized Neural Radiance Field (NeRF) as fundamental\nrepresentation, which further enhanced the performance of multi-view face\nreenactment in photo-realism and 3D consistency. However, establishing dense\ncorrespondence between different face NeRFs is non-trivial, because implicit\nrepresentations lack ground-truth correspondence annotations like mesh-based 3D\nparametric models (e.g., 3DMM) with index-aligned vertexes. Although aligning\n3DMM space with NeRF-based face representations can realize motion control, it\nis sub-optimal for their limited face-only modeling and low identity fidelity.\nTherefore, we are inspired to ask: Can we learn the dense correspondence\nbetween different NeRF-based face representations without a 3D parametric model\nprior? To address this challenge, we propose a novel framework, which adopts\ntri-planes as fundamental NeRF representation and decomposes face tri-planes\ninto three components: canonical tri-planes, identity deformations, and motion.\nIn terms of motion control, our key contribution is proposing a Plane\nDictionary (PlaneDict) module, which efficiently maps the motion conditions to\na linear weighted addition of learnable orthogonal plane bases. To the best of\nour knowledge, our framework is the first method that achieves one-shot\nmulti-view face reenactment without a 3D parametric model prior. Extensive\nexperiments demonstrate that we produce better results in fine-grained motion\ncontrol and identity preservation than previous methods.\n","authors":["Songlin Yang","Wei Wang","Yushi Lan","Xiangyu Fan","Bo Peng","Lei Yang","Jing Dong"],"pdf_url":"https://arxiv.org/pdf/2312.10422v1.pdf","comment":"Accepted by Proceedings of the AAAI Conference on Artificial\n  Intelligence, 2024"},{"id":"http://arxiv.org/abs/2305.07490v3","updated":"2023-12-16T10:59:20Z","published":"2023-05-12T14:04:30Z","title":"ArtGPT-4: Towards Artistic-understanding Large Vision-Language Models\n  with Enhanced Adapter","summary":"  In recent years, advancements in large language models have been remarkable,\nwith models such as ChatGPT demonstrating exceptional proficiency in diverse\nlinguistic tasks. The pre-training of large models with billions of parameters,\nposes a formidable challenge, primarily due to the scarcity of datasets of a\ncommensurate scale for effective training. Nevertheless, innovative strategies\nhave emerged, including methods to fine-tune these pre-trained models using\nfewer parameters set, as evidenced by models like MiniGPT-4 and LLaVA. Despite\ntheir potential in various domains, these models remain limited in their\nunderstanding of artistic imagery. They have yet to fully grasp the intricate\nnuances of art images or to provide an objective articulation of the emotions\nthey evoke, in a manner akin to human perception. This work introduces\nArtGPT-4, a pioneering large vision-language model tailored to address the\ndeficiencies of contemporary models in artistic comprehension. ArtGPT-4\nunderwent training on image-text pairs utilizing a Tesla A100 device in a mere\n2 hours, with a dataset comprising approximately 0.52M entries. Impressively,\nthe model can render images with an artistic-understanding and convey the\nemotions they inspire, mirroring human interpretation. Additionally, this work\npresents a unique dataset designed to evaluate the efficacy of vision-language\nmodels. In subsequent evaluations, ArtGPT-4 not only achieved state-of-the-art\nperformance on the ArtEmis and ArtEmis-v2.0 datasets but also exceeded the\nestablished benchmarks introduced in This study, lagging behind professional\nartists' descriptions by a negligible 0.15 points on a 6-point scale. The code\nand the pre-trained model are accessible in\nhttps://huggingface.co/Tyrannosaurus/ArtGPT-4.\n","authors":["Zhengqing Yuan","Xinyi Wang","Kun Wang","Lichao Sun","Yanyang Ye"],"pdf_url":"https://arxiv.org/pdf/2305.07490v3.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2310.18651v4","updated":"2023-12-16T10:50:45Z","published":"2023-10-28T09:35:30Z","title":"PW-Self: Patch-Wise Self-Supervised Visual Representation Learning","summary":"  Self-supervised visual representation learning traditionally focuses on\nimage-level instance discrimination. Our study introduces an innovative\ndimension by integrating patch-level discrimination into these methodologies.\nThis integration allows for the simultaneous analysis of both local and global\nvisual features, thereby enriching the quality of the representations learned.\nInitially, the original images undergo spatial augmentation. Subsequently, we\nemploy a distinctive photometric patch-level augmentation, where each patch is\nindividually augmented, independent from other patches within the same view.\nThis approach generates a diverse training dataset with distinct color\nvariations in each segment. The augmented images are then processed through a\nself-distillation learning framework, utilizing the Vision Transformer (ViT) as\nits backbone. The proposed method minimizes the representation distances across\nboth image and patch levels to capture details from macro to micro\nperspectives. To this end, we present a simple yet effective patch-matching\nalgorithm that can find the corresponding patches across the augmented views.\nThanks to the efficient structure of the patch-matching algorithm, our method\nreduces computational complexity compared to similar approaches. Consequently,\nwe achieve an advanced understanding of the model without adding significant\ncomputational requirements. We have extensively pre-trained our method on\ndatasets of varied scales, such as Cifar10, ImageNet-100, and ImageNet-1K. It\ndemonstrates superior performance over state-of-the-art self-supervised\nrepresentation learning methods in image classification and downstream tasks,\nsuch as copy detection and image retrieval. The implementation of our method is\naccessible on GitHub.\n","authors":["Ali Javidani","Mohammad Amin Sadeghi","Babak Nadjar Araabi"],"pdf_url":"https://arxiv.org/pdf/2310.18651v4.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2204.05980v2","updated":"2023-12-16T10:35:59Z","published":"2022-04-08T06:19:10Z","title":"Optical flow GNSS for navigation in the Indian subcontinent (NavIC)","summary":"  This paper reveals about global navigation satellite system GNSS in the\nindian subcontinent known as the navigation in the indian subcontinent(NavIC)\nWe have tried to model a new technique in GNSS known as the optical flow\ntracking global navigation system (OF GNSS). This method using differential\nequations is very accurate for very small distances on the surface of the earth\nin the 1500km range of the Indian subcontinent satellite coverage. When we talk\nof accuracy of the GPS system it should be very accurate on the surface of the\nearth when used to show changes in coordinate of the moving body with respect\nto the ground by the satellite which is situated on the earths orbit. Optical\nflow is a method which uses movements with respect to x and y axis for\ninfinitesimal changes in its coordinates and then uses this algorithm to use it\nin global positioning system to find accurate position of the body with respect\nto the satellite coordinates with respect to ground positioning. The modern\nmethod of differential frames is also very accurate as it involves\ninfinitesimal frames which are modelled together from the satellite to find\nchanges in the coordinates on the earths surface, so we have designed a new\nalgorithm in this paper on the Optical flow GNSS system which is an alternative\nand can improve the study done in the design of these algorithms in this field\nof applications.\n","authors":["Sunit Shantanu Digamber Fulari"],"pdf_url":"https://arxiv.org/pdf/2204.05980v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10407v1","updated":"2023-12-16T10:17:09Z","published":"2023-12-16T10:17:09Z","title":"DeepArt: A Benchmark to Advance Fidelity Research in AI-Generated\n  Content","summary":"  This paper explores the image synthesis capabilities of GPT-4, a leading\nmulti-modal large language model. We establish a benchmark for evaluating the\nfidelity of texture features in images generated by GPT-4, comprising manually\npainted pictures and their AI-generated counterparts. The contributions of this\nstudy are threefold: First, we provide an in-depth analysis of the fidelity of\nimage synthesis features based on GPT-4, marking the first such study on this\nstate-of-the-art model. Second, the quantitative and qualitative experiments\nfully reveals the limitations of the GPT-4 model in image synthesis. Third, we\nhave compiled a unique benchmark of manual drawings and corresponding\nGPT-4-generated images, introducing a new task to advance fidelity research in\nAI-generated content (AIGC). The dataset will be available after being\naccepted: \\url{https://github.com/rickwang28574/DeepArt}. We hope this study\nwill fuel knowledge, scholarship, and innovation, inspiring uses that transform\nhow we discover and understand the world of art and promote the development of\nAIGC while retaining respect for art.\n","authors":["Wentao Wang","Xuanyao Huang","Swalpa Kumar Roy"],"pdf_url":"https://arxiv.org/pdf/2312.10407v1.pdf","comment":"This is the initial version of this work, and a more comprehensive\n  and improved version will be updated later"},{"id":"http://arxiv.org/abs/2307.03998v2","updated":"2023-12-16T09:17:13Z","published":"2023-07-08T15:43:49Z","title":"Lightweight Improved Residual Network for Efficient Inverse Tone Mapping","summary":"  The display devices like HDR10 televisions are increasingly prevalent in our\ndaily life for visualizing high dynamic range (HDR) images. But the majority of\nmedia images on the internet remain in 8-bit standard dynamic range (SDR)\nformat. Therefore, converting SDR images to HDR ones by inverse tone mapping\n(ITM) is crucial to unlock the full potential of abundant media images.\nHowever, existing ITM methods are usually developed with complex network\narchitectures requiring huge computational costs. In this paper, we propose a\nlightweight Improved Residual Network (IRNet) by enhancing the power of popular\nresidual block for efficient ITM. Specifically, we propose a new Improved\nResidual Block (IRB) to extract and fuse multi-layer features for fine-grained\nHDR image reconstruction. Experiments on three benchmark datasets demonstrate\nthat our IRNet achieves state-of-the-art performance on both the ITM and joint\nSR-ITM tasks. The code, models and data will be publicly available at\nhttps://github.com/ThisisVikki/ITM-baseline.\n","authors":["Liqi Xue","Tianyi Xu","Yongbao Song","Yan Liu","Lei Zhang","Xiantong Zhen","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2307.03998v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10390v1","updated":"2023-12-16T09:08:03Z","published":"2023-12-16T09:08:03Z","title":"Not Every Side Is Equal: Localization Uncertainty Estimation for\n  Semi-Supervised 3D Object Detection","summary":"  Semi-supervised 3D object detection from point cloud aims to train a detector\nwith a small number of labeled data and a large number of unlabeled data. The\ncore of existing methods lies in how to select high-quality pseudo-labels using\nthe designed quality evaluation criterion. However, these methods treat each\npseudo bounding box as a whole and assign equal importance to each side during\ntraining, which is detrimental to model performance due to many sides having\npoor localization quality. Besides, existing methods filter out a large number\nof low-quality pseudo-labels, which also contain some correct regression values\nthat can help with model training. To address the above issues, we propose a\nside-aware framework for semi-supervised 3D object detection consisting of\nthree key designs: a 3D bounding box parameterization method, an uncertainty\nestimation module, and a pseudo-label selection strategy. These modules work\ntogether to explicitly estimate the localization quality of each side and\nassign different levels of importance during the training phase. Extensive\nexperiment results demonstrate that the proposed method can consistently\noutperform baseline models under different scenes and evaluation criteria.\nMoreover, our method achieves state-of-the-art performance on three datasets\nwith different labeled ratios.\n","authors":["ChuXin Wang","Wenfei Yang","Tianzhu Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.10390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10389v1","updated":"2023-12-16T09:04:44Z","published":"2023-12-16T09:04:44Z","title":"ElasticLaneNet: A Geometry-Flexible Approach for Lane Detection","summary":"  The task of lane detection involves identifying the boundaries of driving\nareas. Recognizing lanes with complex and variable geometric structures remains\na challenge. In this paper, we introduce a new lane detection framework named\nElasticLaneNet (Elastic-interaction-energy guided Lane detection Network). A\nnovel and flexible way of representing lanes, namely, implicit representation\nis proposed. The training strategy considers predicted lanes as moving curves\nthat being attracted to the ground truth guided by an elastic interaction\nenergy based loss function (EIE loss). An auxiliary feature refinement (AFR)\nmodule is designed to gather information from different layers. The method\nperforms well in complex lane scenarios, including those with large curvature,\nweak geometric features at intersections, complicated cross lanes, Y-shapes\nlanes, dense lanes, etc. We apply our approach on three datasets: SDLane,\nCULane, and TuSimple. The results demonstrate the exceptional performance of\nour method, with the state-of-the-art results on the structure-diversity\ndataset SDLane, achieving F1-score of 89.51, Recall rate of 87.50, and\nPrecision of 91.61.\n","authors":["Yaxin Feng","Yuan Lan","Luchan Zhang","Yang Xiang"],"pdf_url":"https://arxiv.org/pdf/2312.10389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.11174v4","updated":"2023-12-16T08:34:22Z","published":"2022-11-21T04:36:24Z","title":"The Role of Robust Generalization in Continual Learning: Better Transfer\n  and Less Forgetting","summary":"  This paper considers learning a sequence of tasks continually with the\nobjectives of generalizing over unseen data regardless of its distribution,\naccumulating knowledge and transferring knowledge across tasks. To the best of\nour knowledge, no existing technique can accomplish all of these objectives\nsimultaneously. This paper proposes such a technique by investigating the role\nof robust generalization in Continual Learning (CL). Recent findings show that\nmodels trained to exhibit robust generalization not only generalize better, but\nalso demonstrate improved transferability and tend to find flatter local\nminima. This motivates us to achieve robust generalization in each task in CL,\nfacilitating learning a new task and reducing the risk of forgetting previously\nlearned tasks. To achieve this, we propose a new online shape-texture\nself-distillation (STSD) method that learns both shape and texture\nrepresentations for each task, improving robust generalization. Extensive\nexperiments demonstrate that our approach can be easily combined with existing\nCL methods to improve generalization, encourage knowledge transfer, and reduce\nforgetting. We also show that our approach finds flatter local minima, further\nhighlighting the importance of improving robust generalization in CL. Our\nproposed technique is a significant step forward in achieving the\naforementioned CL objectives simultaneously.\n","authors":["Zenglin Shi","Ying Sun","Joo Hwee Lim","Mengmi Zhang"],"pdf_url":"https://arxiv.org/pdf/2211.11174v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10376v1","updated":"2023-12-16T08:23:43Z","published":"2023-12-16T08:23:43Z","title":"SA$^2$VP: Spatially Aligned-and-Adapted Visual Prompt","summary":"  As a prominent parameter-efficient fine-tuning technique in NLP, prompt\ntuning is being explored its potential in computer vision. Typical methods for\nvisual prompt tuning follow the sequential modeling paradigm stemming from NLP,\nwhich represents an input image as a flattened sequence of token embeddings and\nthen learns a set of unordered parameterized tokens prefixed to the sequence\nrepresentation as the visual prompts for task adaptation of large vision\nmodels. While such sequential modeling paradigm of visual prompt has shown\ngreat promise, there are two potential limitations. First, the learned visual\nprompts cannot model the underlying spatial relations in the input image, which\nis crucial for image encoding. Second, since all prompt tokens play the same\nrole of prompting for all image tokens without distinction, it lacks the\nfine-grained prompting capability, i.e., individual prompting for different\nimage tokens. In this work, we propose the \\mymodel model (\\emph{SA$^2$VP}),\nwhich learns a two-dimensional prompt token map with equal (or scaled) size to\nthe image token map, thereby being able to spatially align with the image map.\nEach prompt token is designated to prompt knowledge only for the spatially\ncorresponding image tokens. As a result, our model can conduct individual\nprompting for different image tokens in a fine-grained manner. Moreover,\nbenefiting from the capability of preserving the spatial structure by the\nlearned prompt token map, our \\emph{SA$^2$VP} is able to model the spatial\nrelations in the input image, leading to more effective prompting. Extensive\nexperiments on three challenging benchmarks for image classification\ndemonstrate the superiority of our model over other state-of-the-art methods\nfor visual prompt tuning. Code is available at\n\\emph{https://github.com/tommy-xq/SA2VP}.\n","authors":["Wenjie Pei","Tongqi Xia","Fanglin Chen","Jinsong Li","Jiandong Tian","Guangming Lu"],"pdf_url":"https://arxiv.org/pdf/2312.10376v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2305.17423v2","updated":"2023-12-16T08:11:19Z","published":"2023-05-27T09:14:03Z","title":"Accelerating Text-to-image Editing via Cache-enabled Sparse Diffusion\n  Inference","summary":"  Due to the recent success of diffusion models, text-to-image generation is\nbecoming increasingly popular and achieves a wide range of applications. Among\nthem, text-to-image editing, or continuous text-to-image generation, attracts\nlots of attention and can potentially improve the quality of generated images.\nIt's common to see that users may want to slightly edit the generated image by\nmaking minor modifications to their input textual descriptions for several\nrounds of diffusion inference. However, such an image editing process suffers\nfrom the low inference efficiency of many existing diffusion models even using\nGPU accelerators. To solve this problem, we introduce Fast Image Semantically\nEdit (FISEdit), a cached-enabled sparse diffusion model inference engine for\nefficient text-to-image editing. The key intuition behind our approach is to\nutilize the semantic mapping between the minor modifications on the input text\nand the affected regions on the output image. For each text editing step,\nFISEdit can automatically identify the affected image regions and utilize the\ncached unchanged regions' feature map to accelerate the inference process.\nExtensive empirical results show that FISEdit can be $3.4\\times$ and\n$4.4\\times$ faster than existing methods on NVIDIA TITAN RTX and A100 GPUs\nrespectively, and even generates more satisfactory images.\n","authors":["Zihao Yu","Haoyang Li","Fangcheng Fu","Xupeng Miao","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2305.17423v2.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2312.10366v1","updated":"2023-12-16T07:49:13Z","published":"2023-12-16T07:49:13Z","title":"Fusing Conditional Submodular GAN and Programmatic Weak Supervision","summary":"  Programmatic Weak Supervision (PWS) and generative models serve as crucial\ntools that enable researchers to maximize the utility of existing datasets\nwithout resorting to laborious data gathering and manual annotation processes.\nPWS uses various weak supervision techniques to estimate the underlying class\nlabels of data, while generative models primarily concentrate on sampling from\nthe underlying distribution of the given dataset. Although these methods have\nthe potential to complement each other, they have mostly been studied\nindependently. Recently, WSGAN proposed a mechanism to fuse these two models.\nTheir approach utilizes the discrete latent factors of InfoGAN to train the\nlabel model and leverages the class-dependent information of the label model to\ngenerate images of specific classes. However, the disentangled latent factors\nlearned by InfoGAN might not necessarily be class-specific and could\npotentially affect the label model's accuracy. Moreover, prediction made by the\nlabel model is often noisy in nature and can have a detrimental impact on the\nquality of images generated by GAN. In our work, we address these challenges by\n(i) implementing a noise-aware classifier using the pseudo labels generated by\nthe label model (ii) utilizing the noise-aware classifier's prediction to train\nthe label model and generate class-conditional images. Additionally, we also\ninvestigate the effect of training the classifier with a subset of the dataset\nwithin a defined uncertainty budget on pseudo labels. We accomplish this by\nformalizing the subset selection problem as a submodular maximization objective\nwith a knapsack constraint on the entropy of pseudo labels. We conduct\nexperiments on multiple datasets and demonstrate the efficacy of our methods on\nseveral tasks vis-a-vis the current state-of-the-art methods.\n","authors":["Kumar Shubham","Pranav Sastry","Prathosh AP"],"pdf_url":"https://arxiv.org/pdf/2312.10366v1.pdf","comment":"Published in AAAI 2024"},{"id":"http://arxiv.org/abs/2311.10522v3","updated":"2023-12-16T07:48:49Z","published":"2023-11-17T13:43:43Z","title":"Enhancing Object Coherence in Layout-to-Image Synthesis","summary":"  Layout-to-image synthesis is an emerging technique in conditional image\ngeneration. It aims to generate complex scenes, where users require fine\ncontrol over the layout of the objects in a scene. However, it remains\nchallenging to control the object coherence, including semantic coherence\n(e.g., the cat looks at the flowers or not) and physical coherence (e.g., the\nhand and the racket should not be misaligned). In this paper, we propose a\nnovel diffusion model with effective global semantic fusion (GSF) and\nself-similarity feature enhancement modules to guide the object coherence for\nthis task. For semantic coherence, we argue that the image caption contains\nrich information for defining the semantic relationship within the objects in\nthe images. Instead of simply employing cross-attention between captions and\ngenerated images, which addresses the highly relevant layout restriction and\nsemantic coherence separately and thus leads to unsatisfying results shown in\nour experiments, we develop GSF to fuse the supervision from the layout\nrestriction and semantic coherence requirement and exploit it to guide the\nimage synthesis process. Moreover, to improve the physical coherence, we\ndevelop a Self-similarity Coherence Attention (SCA) module to explicitly\nintegrate local contextual physical coherence into each pixel's generation\nprocess. Specifically, we adopt a self-similarity map to encode the coherence\nrestrictions and employ it to extract coherent features from text embedding.\nThrough visualization of our self-similarity map, we explore the essence of\nSCA, revealing that its effectiveness is not only in capturing reliable\nphysical coherence patterns but also in enhancing complex texture generation.\nExtensive experiments demonstrate the superiority of our proposed method in\nboth image generation quality and controllability.\n","authors":["Yibin Wang","Weizhong Zhang","Jianwei Zheng","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2311.10522v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08606v2","updated":"2023-12-16T07:45:12Z","published":"2023-12-14T02:16:27Z","title":"VQCNIR: Clearer Night Image Restoration with Vector-Quantized Codebook","summary":"  Night photography often struggles with challenges like low light and\nblurring, stemming from dark environments and prolonged exposures. Current\nmethods either disregard priors and directly fitting end-to-end networks,\nleading to inconsistent illumination, or rely on unreliable handcrafted priors\nto constrain the network, thereby bringing the greater error to the final\nresult. We believe in the strength of data-driven high-quality priors and\nstrive to offer a reliable and consistent prior, circumventing the restrictions\nof manual priors. In this paper, we propose Clearer Night Image Restoration\nwith Vector-Quantized Codebook (VQCNIR) to achieve remarkable and consistent\nrestoration outcomes on real-world and synthetic benchmarks. To ensure the\nfaithful restoration of details and illumination, we propose the incorporation\nof two essential modules: the Adaptive Illumination Enhancement Module (AIEM)\nand the Deformable Bi-directional Cross-Attention (DBCA) module. The AIEM\nleverages the inter-channel correlation of features to dynamically maintain\nillumination consistency between degraded features and high-quality codebook\nfeatures. Meanwhile, the DBCA module effectively integrates texture and\nstructural information through bi-directional cross-attention and deformable\nconvolution, resulting in enhanced fine-grained detail and structural fidelity\nacross parallel decoders. Extensive experiments validate the remarkable\nbenefits of VQCNIR in enhancing image quality under low-light conditions,\nshowcasing its state-of-the-art performance on both synthetic and real-world\ndatasets. The code is available at https://github.com/AlexZou14/VQCNIR.\n","authors":["Wenbin Zou","Hongxia Gao","Tian Ye","Liang Chen","Weipeng Yang","Shasha Huang","Hongsheng Chen","Sixiang Chen"],"pdf_url":"https://arxiv.org/pdf/2312.08606v2.pdf","comment":"This paper is accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2312.10361v1","updated":"2023-12-16T07:40:09Z","published":"2023-12-16T07:40:09Z","title":"Exploring UMAP in hybrid models of entropy-based and representativeness\n  sampling for active learning in biomedical segmentation","summary":"  In this work, we study various hybrid models of entropy-based and\nrepresentativeness sampling techniques in the context of active learning in\nmedical segmentation, in particular examining the role of UMAP (Uniform\nManifold Approximation and Projection) as a technique for capturing\nrepresentativeness. Although UMAP has been shown viable as a general purpose\ndimension reduction method in diverse areas, its role in deep learning-based\nmedical segmentation has yet been extensively explored. Using the cardiac and\nprostate datasets in the Medical Segmentation Decathlon for validation, we\nfound that a novel hybrid combination of Entropy-UMAP sampling technique\nachieved a statistically significant Dice score advantage over the random\nbaseline ($3.2 \\%$ for cardiac, $4.5 \\%$ for prostate), and attained the\nhighest Dice coefficient among the spectrum of 10 distinct active learning\nmethodologies we examined. This provides preliminary evidence that there is an\ninteresting synergy between entropy-based and UMAP methods when the former\nprecedes the latter in a hybrid model of active learning.\n","authors":["H. S. Tan","Kuancheng Wang","Rafe Mcbeth"],"pdf_url":"https://arxiv.org/pdf/2312.10361v1.pdf","comment":"19 pages, 5 figures"},{"id":"http://arxiv.org/abs/2312.04875v2","updated":"2023-12-16T06:54:43Z","published":"2023-12-08T07:16:09Z","title":"MVDD: Multi-View Depth Diffusion Models","summary":"  Denoising diffusion models have demonstrated outstanding results in 2D image\ngeneration, yet it remains a challenge to replicate its success in 3D shape\ngeneration. In this paper, we propose leveraging multi-view depth, which\nrepresents complex 3D shapes in a 2D data format that is easy to denoise. We\npair this representation with a diffusion model, MVDD, that is capable of\ngenerating high-quality dense point clouds with 20K+ points with fine-grained\ndetails. To enforce 3D consistency in multi-view depth, we introduce an\nepipolar line segment attention that conditions the denoising step for a view\non its neighboring views. Additionally, a depth fusion module is incorporated\ninto diffusion steps to further ensure the alignment of depth maps. When\naugmented with surface reconstruction, MVDD can also produce high-quality 3D\nmeshes. Furthermore, MVDD stands out in other tasks such as depth completion,\nand can serve as a 3D prior, significantly boosting many downstream tasks, such\nas GAN inversion. State-of-the-art results from extensive experiments\ndemonstrate MVDD's excellent ability in 3D shape generation, depth completion,\nand its potential as a 3D prior for downstream tasks.\n","authors":["Zhen Wang","Qiangeng Xu","Feitong Tan","Menglei Chai","Shichen Liu","Rohit Pandey","Sean Fanello","Achuta Kadambi","Yinda Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.04875v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10346v1","updated":"2023-12-16T06:28:19Z","published":"2023-12-16T06:28:19Z","title":"MMBaT: A Multi-task Framework for mmWave-based Human Body Reconstruction\n  and Translation Prediction","summary":"  Human body reconstruction with Millimeter Wave (mmWave) radar point clouds\nhas gained significant interest due to its ability to work in adverse\nenvironments and its capacity to mitigate privacy concerns associated with\ntraditional camera-based solutions. Despite pioneering efforts in this field,\ntwo challenges persist. Firstly, raw point clouds contain massive noise points,\nusually caused by the ambient objects and multi-path effects of Radio Frequency\n(RF) signals. Recent approaches typically rely on prior knowledge or elaborate\npreprocessing methods, limiting their applicability. Secondly, even after noise\nremoval, the sparse and inconsistent body-related points pose an obstacle to\naccurate human body reconstruction. To address these challenges, we introduce\nmmBaT, a novel multi-task deep learning framework that concurrently estimates\nthe human body and predicts body translations in subsequent frames to extract\nbody-related point clouds. Our method is evaluated on two public datasets that\nare collected with different radar devices and noise levels. A comprehensive\ncomparison against other state-of-the-art methods demonstrates our method has a\nsuperior reconstruction performance and generalization ability from noisy raw\ndata, even when compared to methods provided with body-related point clouds.\n","authors":["Jiarui Yang","Songpengcheng Xia","Yifan Song","Qi Wu","Ling Pei"],"pdf_url":"https://arxiv.org/pdf/2312.10346v1.pdf","comment":"5 pages, 2 figures, accepted by IEEE ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.10342v1","updated":"2023-12-16T06:21:09Z","published":"2023-12-16T06:21:09Z","title":"Self-supervised Adaptive Weighting for Cooperative Perception in V2V\n  Communications","summary":"  Perception of the driving environment is critical for collision avoidance and\nroute planning to ensure driving safety. Cooperative perception has been widely\nstudied as an effective approach to addressing the shortcomings of\nsingle-vehicle perception. However, the practical limitations of\nvehicle-to-vehicle (V2V) communications have not been adequately investigated.\nIn particular, current cooperative fusion models rely on supervised models and\ndo not address dynamic performance degradation caused by arbitrary channel\nimpairments. In this paper, a self-supervised adaptive weighting model is\nproposed for intermediate fusion to mitigate the adverse effects of channel\ndistortion. The performance of cooperative perception is investigated in\ndifferent system settings. Rician fading and imperfect channel state\ninformation (CSI) are also considered. Numerical results demonstrate that the\nproposed adaptive weighting algorithm significantly outperforms the benchmarks\nwithout weighting. Visualization examples validate that the proposed weighting\nalgorithm can flexibly adapt to various channel conditions. Moreover, the\nadaptive weighting algorithm demonstrates good generalization to untrained\nchannels and test datasets from different domains.\n","authors":["Chenguang Liu","Jianjun Chen","Yunfei Chen","Ryan Payton","Michael Riley","Shuang-Hua Yang"],"pdf_url":"https://arxiv.org/pdf/2312.10342v1.pdf","comment":"accepted by IEEE Transactions on Intelligent Vehicles"},{"id":"http://arxiv.org/abs/2309.06961v2","updated":"2023-12-16T06:14:00Z","published":"2023-09-13T13:54:32Z","title":"Towards Reliable Dermatology Evaluation Benchmarks","summary":"  Benchmark datasets for digital dermatology unwittingly contain inaccuracies\nthat reduce trust in model performance estimates. We propose a\nresource-efficient data-cleaning protocol to identify issues that escaped\nprevious curation. The protocol leverages an existing algorithmic cleaning\nstrategy and is followed by a confirmation process terminated by an intuitive\nstopping criterion. Based on confirmation by multiple dermatologists, we remove\nirrelevant samples and near duplicates and estimate the percentage of label\nerrors in six dermatology image datasets for model evaluation promoted by the\nInternational Skin Imaging Collaboration. Along with this paper, we publish\nrevised file lists for each dataset which should be used for model evaluation.\nOur work paves the way for more trustworthy performance assessment in digital\ndermatology.\n","authors":["Fabian Gröger","Simone Lionetti","Philippe Gottfrois","Alvaro Gonzalez-Jimenez","Matthew Groh","Roxana Daneshjou","Labelling Consortium","Alexander A. Navarini","Marc Pouly"],"pdf_url":"https://arxiv.org/pdf/2309.06961v2.pdf","comment":"Link to the revised file lists:\n  https://github.com/Digital-Dermatology/SelfClean-Revised-Benchmarks"},{"id":"http://arxiv.org/abs/2312.10324v1","updated":"2023-12-16T05:08:02Z","published":"2023-12-16T05:08:02Z","title":"Federated Learning with Instance-Dependent Noisy Labels","summary":"  Federated learning (FL) with noisy labels poses a significant challenge.\nExisting methods designed for handling noisy labels in centralized learning\ntend to lose their effectiveness in the FL setting, mainly due to the small\ndataset size and the heterogeneity of client data. While some attempts have\nbeen made to tackle FL with noisy labels, they primarily focused on scenarios\ninvolving class-conditional noise. In this paper, we study the more challenging\nand practical issue of instance-dependent noise (IDN) in FL. We introduce a\nnovel algorithm called FedBeat (Federated Learning with Bayesian\nEnsemble-Assisted Transition Matrix Estimation). FedBeat aims to build a global\nstatistically consistent classifier using the IDN transition matrix (IDNTM),\nwhich encompasses three synergistic steps: (1) A federated data extraction step\nthat constructs a weak global model and extracts high-confidence data using a\nBayesian model ensemble method. (2) A federated transition matrix estimation\nstep in which clients collaboratively train an IDNTM estimation network based\non the extracted data. (3) A federated classifier correction step that enhances\nthe global model's performance by training it using a loss function tailored\nfor noisy labels, leveraging the IDNTM. Experiments conducted on CIFAR-10 and\nSVHN verify that the proposed method significantly outperforms state-of-the-art\nmethods.\n","authors":["Lei Wang","Jieming Bian","Jie Xu"],"pdf_url":"https://arxiv.org/pdf/2312.10324v1.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.03059v5","updated":"2023-12-16T04:50:37Z","published":"2023-10-04T16:49:36Z","title":"Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models","summary":"  The popularity of pre-trained large models has revolutionized downstream\ntasks across diverse fields, such as language, vision, and multi-modality. To\nminimize the adaption cost for downstream tasks, many Parameter-Efficient\nFine-Tuning (PEFT) techniques are proposed for language and 2D image\npre-trained models. However, the specialized PEFT method for 3D pre-trained\nmodels is still under-explored. To this end, we introduce Point-PEFT, a novel\nframework for adapting point cloud pre-trained models with minimal learnable\nparameters. Specifically, for a pre-trained 3D model, we freeze most of its\nparameters, and only tune the newly added PEFT modules on downstream tasks,\nwhich consist of a Point-prior Prompt and a Geometry-aware Adapter. The\nPoint-prior Prompt adopts a set of learnable prompt tokens, for which we\npropose to construct a memory bank with domain-specific knowledge, and utilize\na parameter-free attention to enhance the prompt tokens. The Geometry-aware\nAdapter aims to aggregate point cloud features within spatial neighborhoods to\ncapture fine-grained geometric information through local interactions.\nExtensive experiments indicate that our Point-PEFT can achieve better\nperformance than the full fine-tuning on various downstream tasks, while using\nonly 5% of the trainable parameters, demonstrating the efficiency and\neffectiveness of our approach. Code is released at\nhttps://github.com/Ivan-Tang-3D/PEFT-3D.\n","authors":["Yiwen Tang","Ray Zhang","Zoey Guo","Xianzheng Ma","Dong Wang","Zhigang Wang","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2310.03059v5.pdf","comment":"10 pages. The specialized PEFT framework for 3D pre-trained models,\n  which achieves competitive performance to full fine-tuning, and significantly\n  reduces the computational resources. Project page:\n  https://github.com/Ivan-Tang-3D/PEFT-3D"},{"id":"http://arxiv.org/abs/2312.10320v1","updated":"2023-12-16T04:50:34Z","published":"2023-12-16T04:50:34Z","title":"Symmetrical Bidirectional Knowledge Alignment for Zero-Shot Sketch-Based\n  Image Retrieval","summary":"  This paper studies the problem of zero-shot sketch-based image retrieval\n(ZS-SBIR), which aims to use sketches from unseen categories as queries to\nmatch the images of the same category. Due to the large cross-modality\ndiscrepancy, ZS-SBIR is still a challenging task and mimics realistic zero-shot\nscenarios. The key is to leverage transferable knowledge from the pre-trained\nmodel to improve generalizability. Existing researchers often utilize the\nsimple fine-tuning training strategy or knowledge distillation from a teacher\nmodel with fixed parameters, lacking efficient bidirectional knowledge\nalignment between student and teacher models simultaneously for better\ngeneralization. In this paper, we propose a novel Symmetrical Bidirectional\nKnowledge Alignment for zero-shot sketch-based image retrieval (SBKA). The\nsymmetrical bidirectional knowledge alignment learning framework is designed to\neffectively learn mutual rich discriminative information between teacher and\nstudent models to achieve the goal of knowledge alignment. Instead of the\nformer one-to-one cross-modality matching in the testing stage, a one-to-many\ncluster cross-modality matching method is proposed to leverage the inherent\nrelationship of intra-class images to reduce the adverse effects of the\nexisting modality gap. Experiments on several representative ZS-SBIR datasets\n(Sketchy Ext dataset, TU-Berlin Ext dataset and QuickDraw Ext dataset) prove\nthe proposed algorithm can achieve superior performance compared with\nstate-of-the-art methods.\n","authors":["Decheng Liu","Xu Luo","Chunlei Peng","Nannan Wang","Ruimin Hu","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2312.10320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.11133v3","updated":"2023-12-16T04:44:25Z","published":"2022-11-21T00:38:59Z","title":"Enhancing Accuracy and Robustness of Steering Angle Prediction with\n  Attention Mechanism","summary":"  In this paper, we investigate the two most popular families of deep neural\narchitectures (i.e., ResNets and InceptionNets) for the autonomous driving task\nof steering angle prediction. To ensure a comprehensive comparison, we\nconducted experiments on the Kaggle SAP dataset and custom dataset and\ncarefully examined a range of different model sizes within both the ResNet and\nInceptionNet families. Our derived models can achieve state-of-the-art results\nin terms of steering angle MSE. In addition to this analysis, we introduced the\nattention mechanism to enhance steering angle prediction. This attention\nmechanism facilitated an in-depth exploration of the model's selective focus on\nessential elements within the input data. Furthermore, recognizing the\nimportance of security and robustness in autonomous driving assessed the\nresilience of our models to adversarial attacks.\n","authors":["Swetha Nadella","Pramiti Barua","Jeremy C. Hagler","David J. Lamb","Qing Tian"],"pdf_url":"https://arxiv.org/pdf/2211.11133v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.05216v4","updated":"2023-12-16T04:39:08Z","published":"2021-10-11T12:32:56Z","title":"High-order Tensor Pooling with Attention for Action Recognition","summary":"  We aim at capturing high-order statistics of feature vectors formed by a\nneural network, and propose end-to-end second- and higher-order pooling to form\na tensor descriptor. Tensor descriptors require a robust similarity measure due\nto low numbers of aggregated vectors and the burstiness phenomenon, when a\ngiven feature appears more/less frequently than statistically expected. The\nHeat Diffusion Process (HDP) on a graph Laplacian is closely related to the\nEigenvalue Power Normalization (EPN) of the covariance/autocorrelation matrix,\nwhose inverse forms a loopy graph Laplacian. We show that the HDP and the EPN\nplay the same role, i.e., to boost or dampen the magnitude of the eigenspectrum\nthus preventing the burstiness. We equip higher-order tensors with EPN which\nacts as a spectral detector of higher-order occurrences to prevent burstiness.\nWe also prove that for a tensor of order r built from d dimensional feature\ndescriptors, such a detector gives the likelihood if at least one higher-order\noccurrence is 'projected' into one of binom(d,r) subspaces represented by the\ntensor; thus forming a tensor power normalization metric endowed with\nbinom(d,r) such 'detectors'. For experimental contributions, we apply several\nsecond- and higher-order pooling variants to action recognition, provide\npreviously not presented comparisons of such pooling variants, and show\nstate-of-the-art results on HMDB-51, YUP++ and MPII Cooking Activities.\n","authors":["Lei Wang","Ke Sun","Piotr Koniusz"],"pdf_url":"https://arxiv.org/pdf/2110.05216v4.pdf","comment":"Accepted by IEEE International Conference on Acoustics, Speech, and\n  Signal Processing (ICASSP 2024)"},{"id":"http://arxiv.org/abs/2310.10059v2","updated":"2023-12-16T04:37:10Z","published":"2023-10-16T04:49:06Z","title":"Flow Dynamics Correction for Action Recognition","summary":"  Various research studies indicate that action recognition performance highly\ndepends on the types of motions being extracted and how accurate the human\nactions are represented. In this paper, we investigate different optical flow,\nand features extracted from these optical flow that capturing both short-term\nand long-term motion dynamics. We perform power normalization on the magnitude\ncomponent of optical flow for flow dynamics correction to boost subtle or\ndampen sudden motions. We show that existing action recognition models which\nrely on optical flow are able to get performance boosted with our corrected\noptical flow. To further improve performance, we integrate our corrected flow\ndynamics into popular models through a simple hallucination step by selecting\nonly the best performing optical flow features, and we show that by\n'translating' the CNN feature maps into these optical flow features with\ndifferent scales of motions leads to the new state-of-the-art performance on\nseveral benchmarks including HMDB-51, YUP++, fine-grained action recognition on\nMPII Cooking Activities, and large-scale Charades.\n","authors":["Lei Wang","Piotr Koniusz"],"pdf_url":"https://arxiv.org/pdf/2310.10059v2.pdf","comment":"Accepted by IEEE International Conference on Acoustics, Speech, and\n  Signal Processing (ICASSP 2024)"},{"id":"http://arxiv.org/abs/2312.10314v1","updated":"2023-12-16T04:23:12Z","published":"2023-12-16T04:23:12Z","title":"DeepCalliFont: Few-shot Chinese Calligraphy Font Synthesis by\n  Integrating Dual-modality Generative Models","summary":"  Few-shot font generation, especially for Chinese calligraphy fonts, is a\nchallenging and ongoing problem. With the help of prior knowledge that is\nmainly based on glyph consistency assumptions, some recently proposed methods\ncan synthesize high-quality Chinese glyph images. However, glyphs in\ncalligraphy font styles often do not meet these assumptions. To address this\nproblem, we propose a novel model, DeepCalliFont, for few-shot Chinese\ncalligraphy font synthesis by integrating dual-modality generative models.\nSpecifically, the proposed model consists of image synthesis and sequence\ngeneration branches, generating consistent results via a dual-modality\nrepresentation learning strategy. The two modalities (i.e., glyph images and\nwriting sequences) are properly integrated using a feature recombination module\nand a rasterization loss function. Furthermore, a new pre-training strategy is\nadopted to improve the performance by exploiting large amounts of uni-modality\ndata. Both qualitative and quantitative experiments have been conducted to\ndemonstrate the superiority of our method to other state-of-the-art approaches\nin the task of few-shot Chinese calligraphy font synthesis. The source code can\nbe found at https://github.com/lsflyt-pku/DeepCalliFont.\n","authors":["Yitian Liu","Zhouhui Lian"],"pdf_url":"https://arxiv.org/pdf/2312.10314v1.pdf","comment":"AAAI2024"},{"id":"http://arxiv.org/abs/2305.17770v2","updated":"2023-12-16T04:04:39Z","published":"2023-05-28T16:33:35Z","title":"Point Cloud Completion Guided by Prior Knowledge via Causal Inference","summary":"  Point cloud completion aims to recover raw point clouds captured by scanners\nfrom partial observations caused by occlusion and limited view angles. This\nmakes it hard to recover details because the global feature is unlikely to\ncapture the full details of all missing parts. In this paper, we propose a\nnovel approach to point cloud completion task called Point-PC, which uses a\nmemory network to retrieve shape priors and designs a causal inference model to\nfilter missing shape information as supplemental geometric information to aid\npoint cloud completion. Specifically, we propose a memory operating mechanism\nwhere the complete shape features and the corresponding shapes are stored in\nthe form of ``key-value'' pairs. To retrieve similar shapes from the partial\ninput, we also apply a contrastive learning-based pre-training scheme to\ntransfer the features of incomplete shapes into the domain of complete shape\nfeatures. Experimental results on the ShapeNet-55, PCN, and KITTI datasets\ndemonstrate that Point-PC outperforms the state-of-the-art methods.\n","authors":["Songxue Gao","Chuanqi Jiao","Ruidong Chen","Weijie Wang","Weizhi Nie"],"pdf_url":"https://arxiv.org/pdf/2305.17770v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10306v1","updated":"2023-12-16T03:49:40Z","published":"2023-12-16T03:49:40Z","title":"Mapping Housing Stock Characteristics from Drone Images for Climate\n  Resilience in the Caribbean","summary":"  Comprehensive information on housing stock is crucial for climate adaptation\ninitiatives aiming to reduce the adverse impacts of climate-extreme hazards in\nhigh-risk regions like the Caribbean. In this study, we propose a workflow for\nrapidly generating critical baseline housing stock data using very\nhigh-resolution drone images and deep learning techniques. Specifically, our\nwork leverages the Segment Anything Model and convolutional neural networks for\nthe automated generation of building footprints and roof classification maps.\nBy strengthening local capacity within government agencies to leverage AI and\nEarth Observation-based solutions, this work seeks to improve the climate\nresilience of the housing sector in small island developing states in the\nCaribbean.\n","authors":["Isabelle Tingzon","Nuala Margaret Cowan","Pierre Chrzanowski"],"pdf_url":"https://arxiv.org/pdf/2312.10306v1.pdf","comment":"Tackling Climate Change with Machine Learning: Workshop at NeurIPS\n  2023"},{"id":"http://arxiv.org/abs/2312.10300v1","updated":"2023-12-16T03:17:30Z","published":"2023-12-16T03:17:30Z","title":"Shot2Story20K: A New Benchmark for Comprehensive Understanding of\n  Multi-shot Videos","summary":"  A short clip of video may contain progression of multiple events and an\ninteresting story line. A human need to capture both the event in every shot\nand associate them together to understand the story behind it. In this work, we\npresent a new multi-shot video understanding benchmark Shot2Story20K with\ndetailed shot-level captions and comprehensive video summaries. To facilitate\nbetter semantic understanding of videos, we provide captions for both visual\nsignals and human narrations. We design several distinct tasks including\nsingle-shot video and narration captioning, multi-shot video summarization, and\nvideo retrieval with shot descriptions. Preliminary experiments show some\nchallenges to generate a long and comprehensive video summary. Nevertheless,\nthe generated imperfect summaries can already significantly boost the\nperformance of existing video understanding tasks such as video\nquestion-answering, promoting an under-explored setting of video understanding\nwith detailed summaries.\n","authors":["Mingfei Han","Xiaojun Chang","Heng Wang","Linjie Yang"],"pdf_url":"https://arxiv.org/pdf/2312.10300v1.pdf","comment":"See https://mingfei.info/shot2story for updates and more information"},{"id":"http://arxiv.org/abs/2203.03005v4","updated":"2023-12-16T03:10:07Z","published":"2022-03-06T16:52:28Z","title":"Self-Supervised Face Image Restoration with a One-Shot Reference","summary":"  For image restoration, methods leveraging priors from generative models have\nbeen proposed and demonstrated a promising capacity to robustly restore\nphotorealistic and high-quality results. However, these methods are susceptible\nto semantic ambiguity, particularly with images that have obviously correct\nsemantics such as facial images. In this paper, we propose a semantic-aware\nlatent space exploration method for image restoration (SAIR). By explicitly\nmodeling semantics information from a given reference image, SAIR is able to\nreliably restore severely degraded images not only to high-resolution and\nhighly realistic looks but also to correct semantics. Quantitative and\nqualitative experiments collectively demonstrate the superior performance of\nthe proposed SAIR. Our code is available at https://github.com/Liamkuo/SAIR.\n","authors":["Yanhui Guo","Fangzhou Luo","Shaoyuan Xu"],"pdf_url":"https://arxiv.org/pdf/2203.03005v4.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.10299v1","updated":"2023-12-16T03:09:28Z","published":"2023-12-16T03:09:28Z","title":"Image Restoration Through Generalized Ornstein-Uhlenbeck Bridge","summary":"  Diffusion models possess powerful generative capabilities enabling the\nmapping of noise to data using reverse stochastic differential equations.\nHowever, in image restoration tasks, the focus is on the mapping relationship\nfrom low-quality images to high-quality images. To address this, we introduced\nthe Generalized Ornstein-Uhlenbeck Bridge (GOUB) model. By leveraging the\nnatural mean-reverting property of the generalized OU process and further\nadjusting the variance of its steady-state distribution through the Doob's\nh-transform, we achieve diffusion mappings from point to point with minimal\ncost. This allows for end-to-end training, enabling the recovery of\nhigh-quality images from low-quality ones. Additionally, we uncovered the\nmathematical essence of some bridge models, all of which are special cases of\nthe GOUB and empirically demonstrated the optimality of our proposed models.\nFurthermore, benefiting from our distinctive parameterization mechanism, we\nproposed the Mean-ODE model that is better at capturing pixel-level information\nand structural perceptions. Experimental results show that both models achieved\nstate-of-the-art results in various tasks, including inpainting, deraining, and\nsuper-resolution. Code is available at https://github.com/Hammour-steak/GOUB.\n","authors":["Conghan Yue","Zhengwei Peng","Junlong Ma","Shiyan Du","Pengxu Wei","Dongyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.10299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.05144v2","updated":"2023-12-16T03:07:56Z","published":"2023-06-08T12:11:16Z","title":"Mesogeos: A multi-purpose dataset for data-driven wildfire modeling in\n  the Mediterranean","summary":"  We introduce Mesogeos, a large-scale multi-purpose dataset for wildfire\nmodeling in the Mediterranean. Mesogeos integrates variables representing\nwildfire drivers (meteorology, vegetation, human activity) and historical\nrecords of wildfire ignitions and burned areas for 17 years (2006-2022). It is\ndesigned as a cloud-friendly spatio-temporal dataset, namely a datacube,\nharmonizing all variables in a grid of 1km x 1km x 1-day resolution. The\ndatacube structure offers opportunities to assess machine learning (ML) usage\nin various wildfire modeling tasks. We extract two ML-ready datasets that\nestablish distinct tracks to demonstrate this potential: (1) short-term\nwildfire danger forecasting and (2) final burned area estimation given the\npoint of ignition. We define appropriate metrics and baselines to evaluate the\nperformance of models in each track. By publishing the datacube, along with the\ncode to create the ML datasets and models, we encourage the community to foster\nthe implementation of additional tracks for mitigating the increasing threat of\nwildfires in the Mediterranean.\n","authors":["Spyros Kondylatos","Ioannis Prapas","Gustau Camps-Valls","Ioannis Papoutsis"],"pdf_url":"https://arxiv.org/pdf/2306.05144v2.pdf","comment":"Accepted at the 37th Conference on Neural Information Processing\n  Systems (NeurIPS 2023) Track on Datasets and Benchmarks (oral presentation)"},{"id":"http://arxiv.org/abs/2211.06663v3","updated":"2023-12-16T02:39:32Z","published":"2022-11-12T13:43:58Z","title":"NeighborTrack: Improving Single Object Tracking by Bipartite Matching\n  with Neighbor Tracklets","summary":"  We propose a post-processor, called NeighborTrack, that leverages neighbor\ninformation of the tracking target to validate and improve single-object\ntracking (SOT) results. It requires no additional data or retraining. Instead,\nit uses the confidence score predicted by the backbone SOT network to\nautomatically derive neighbor information and then uses this information to\nimprove the tracking results. When tracking an occluded target, its appearance\nfeatures are untrustworthy. However, a general siamese network often cannot\ntell whether the tracked object is occluded by reading the confidence score\nalone, because it could be misled by neighbors with high confidence scores. Our\nproposed NeighborTrack takes advantage of unoccluded neighbors' information to\nreconfirm the tracking target and reduces false tracking when the target is\noccluded. It not only reduces the impact caused by occlusion, but also fixes\ntracking problems caused by object appearance changes. NeighborTrack is\nagnostic to SOT networks and post-processing methods. For the VOT challenge\ndataset commonly used in short-term object tracking, we improve three famous\nSOT networks, Ocean, TransT, and OSTrack, by an average of ${1.92\\%}$ EAO and\n${2.11\\%}$ robustness. For the mid- and long-term tracking experiments based on\nOSTrack, we achieve state-of-the-art ${72.25\\%}$ AUC on LaSOT and ${75.7\\%}$ AO\non GOT-10K. Code duplication can be found in\nhttps://github.com/franktpmvu/NeighborTrack.\n","authors":["Yu-Hsi Chen","Chien-Yao Wang","Cheng-Yun Yang","Hung-Shuo Chang","Youn-Long Lin","Yung-Yu Chuang","Hong-Yuan Mark Liao"],"pdf_url":"https://arxiv.org/pdf/2211.06663v3.pdf","comment":"This paper was accepted by 9th International Workshop on Computer\n  Vision in Sports (CVsports) 2023 IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition Workshops (CVPRW)"},{"id":"http://arxiv.org/abs/2312.10282v1","updated":"2023-12-16T01:23:42Z","published":"2023-12-16T01:23:42Z","title":"RetailKLIP : Finetuning OpenCLIP backbone using metric learning on a\n  single GPU for Zero-shot retail product image classification","summary":"  Retail product or packaged grocery goods images need to classified in various\ncomputer vision applications like self checkout stores, supply chain automation\nand retail execution evaluation. Previous works explore ways to finetune deep\nmodels for this purpose. But because of the fact that finetuning a large model\nor even linear layer for a pretrained backbone requires to run at least a few\nepochs of gradient descent for every new retail product added in classification\nrange, frequent retrainings are needed in a real world scenario. In this work,\nwe propose finetuning the vision encoder of a CLIP model in a way that its\nembeddings can be easily used for nearest neighbor based classification, while\nalso getting accuracy close to or exceeding full finetuning. A nearest neighbor\nbased classifier needs no incremental training for new products, thus saving\nresources and wait time.\n","authors":["Muktabh Mayank Srivastava"],"pdf_url":"https://arxiv.org/pdf/2312.10282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10271v1","updated":"2023-12-16T00:23:21Z","published":"2023-12-16T00:23:21Z","title":"Robustness of Deep Learning for Accelerated MRI: Benefits of Diverse\n  Training Data","summary":"  Deep learning based methods for image reconstruction are state-of-the-art for\na variety of imaging tasks. However, neural networks often perform worse if the\ntraining data differs significantly from the data they are applied to. For\nexample, a network trained for accelerated magnetic resonance imaging (MRI) on\none scanner performs worse on another scanner. In this work, we investigate the\nimpact of the training data on the model's performance and robustness for\naccelerated MRI. We find that models trained on the combination of various data\ndistributions, such as those obtained from different MRI scanners and\nanatomies, exhibit robustness equal or superior to models trained on the best\nsingle distribution for a specific target distribution. Thus training on\ndiverse data tends to improve robustness. Furthermore, training on diverse data\ndoes not compromise in-distribution performance, i.e., a model trained on\ndiverse data yields in-distribution performance at least as good as models\ntrained on the more narrow individual distributions. Our results suggest that\ntraining a model for imaging on a variety of distributions tends to yield a\nmore effective and robust model than maintaining separate models for individual\ndistributions.\n","authors":["Kang Lin","Reinhard Heckel"],"pdf_url":"https://arxiv.org/pdf/2312.10271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.08506v2","updated":"2023-12-16T00:05:07Z","published":"2023-07-17T14:08:38Z","title":"Does Visual Pretraining Help End-to-End Reasoning?","summary":"  We aim to investigate whether end-to-end learning of visual reasoning can be\nachieved with general-purpose neural networks, with the help of visual\npretraining. A positive result would refute the common belief that explicit\nvisual abstraction (e.g. object detection) is essential for compositional\ngeneralization on visual reasoning, and confirm the feasibility of a neural\nnetwork \"generalist\" to solve visual recognition and reasoning tasks. We\npropose a simple and general self-supervised framework which \"compresses\" each\nvideo frame into a small set of tokens with a transformer network, and\nreconstructs the remaining frames based on the compressed temporal context. To\nminimize the reconstruction loss, the network must learn a compact\nrepresentation for each image, as well as capture temporal dynamics and object\npermanence from temporal context. We perform evaluation on two visual reasoning\nbenchmarks, CATER and ACRE. We observe that pretraining is essential to achieve\ncompositional generalization for end-to-end visual reasoning. Our proposed\nframework outperforms traditional supervised pretraining, including image\nclassification and explicit object detection, by large margins.\n","authors":["Chen Sun","Calvin Luo","Xingyi Zhou","Anurag Arnab","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2307.08506v2.pdf","comment":"NeurIPS 2023"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2312.10466v1","updated":"2023-12-16T14:47:03Z","published":"2023-12-16T14:47:03Z","title":"RIGHT: Retrieval-augmented Generation for Mainstream Hashtag\n  Recommendation","summary":"  Automatic mainstream hashtag recommendation aims to accurately provide users\nwith concise and popular topical hashtags before publication. Generally,\nmainstream hashtag recommendation faces challenges in the comprehensive\ndifficulty of newly posted tweets in response to new topics, and the accurate\nidentification of mainstream hashtags beyond semantic correctness. However,\nprevious retrieval-based methods based on a fixed predefined mainstream hashtag\nlist excel in producing mainstream hashtags, but fail to understand the\nconstant flow of up-to-date information. Conversely, generation-based methods\ndemonstrate a superior ability to comprehend newly posted tweets, but their\ncapacity is constrained to identifying mainstream hashtags without additional\nfeatures. Inspired by the recent success of the retrieval-augmented technique,\nin this work, we attempt to adopt this framework to combine the advantages of\nboth approaches. Meantime, with the help of the generator component, we could\nrethink how to further improve the quality of the retriever component at a low\ncost. Therefore, we propose RetrIeval-augmented Generative Mainstream HashTag\nRecommender (RIGHT), which consists of three components: 1) a retriever seeks\nrelevant hashtags from the entire tweet-hashtags set; 2) a selector enhances\nmainstream identification by introducing global signals; and 3) a generator\nincorporates input tweets and selected hashtags to directly generate the\ndesired hashtags. The experimental results show that our method achieves\nsignificant improvements over state-of-the-art baselines. Moreover, RIGHT can\nbe easily integrated into large language models, improving the performance of\nChatGPT by more than 10%.\n","authors":["Run-Ze Fan","Yixing Fan","Jiangui Chen","Jiafeng Guo","Ruqing Zhang","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2312.10466v1.pdf","comment":"Accepted by ECIR2024 full paper"},{"id":"http://arxiv.org/abs/2312.10463v1","updated":"2023-12-16T14:42:46Z","published":"2023-12-16T14:42:46Z","title":"RecPrompt: A Prompt Tuning Framework for News Recommendation Using Large\n  Language Models","summary":"  In the evolving field of personalized news recommendation, understanding the\nsemantics of the underlying data is crucial. Large Language Models (LLMs) like\nGPT-4 have shown promising performance in understanding natural language.\nHowever, the extent of their applicability in news recommendation systems\nremains to be validated. This paper introduces RecPrompt, the first framework\nfor news recommendation that leverages the capabilities of LLMs through prompt\nengineering. This system incorporates a prompt optimizer that applies an\niterative bootstrapping process, enhancing the LLM-based recommender's ability\nto align news content with user preferences and interests more effectively.\nMoreover, this study offers insights into the effective use of LLMs in news\nrecommendation, emphasizing both the advantages and the challenges of\nincorporating LLMs into recommendation systems.\n","authors":["Dairui Liu","Boming Yang","Honghui Du","Derek Greene","Aonghus Lawlor","Ruihai Dong","Irene Li"],"pdf_url":"https://arxiv.org/pdf/2312.10463v1.pdf","comment":"8 pages, 3 figures, and 8 tables"},{"id":"http://arxiv.org/abs/2306.15222v2","updated":"2023-12-16T13:26:02Z","published":"2023-06-27T05:48:14Z","title":"Learning to Rank in Generative Retrieval","summary":"  Generative retrieval stands out as a promising new paradigm in text retrieval\nthat aims to generate identifier strings of relevant passages as the retrieval\ntarget. This generative paradigm taps into powerful generative language models,\ndistinct from traditional sparse or dense retrieval methods. However, only\nlearning to generate is insufficient for generative retrieval. Generative\nretrieval learns to generate identifiers of relevant passages as an\nintermediate goal and then converts predicted identifiers into the final\npassage rank list. The disconnect between the learning objective of\nautoregressive models and the desired passage ranking target leads to a\nlearning gap. To bridge this gap, we propose a learning-to-rank framework for\ngenerative retrieval, dubbed LTRGR. LTRGR enables generative retrieval to learn\nto rank passages directly, optimizing the autoregressive model toward the final\npassage ranking target via a rank loss. This framework only requires an\nadditional learning-to-rank training phase to enhance current generative\nretrieval systems and does not add any burden to the inference stage. We\nconducted experiments on three public benchmarks, and the results demonstrate\nthat LTRGR achieves state-of-the-art performance among generative retrieval\nmethods. The code and checkpoints are released at\nhttps://github.com/liyongqi67/LTRGR.\n","authors":["Yongqi Li","Nan Yang","Liang Wang","Furu Wei","Wenjie Li"],"pdf_url":"https://arxiv.org/pdf/2306.15222v2.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2312.10370v1","updated":"2023-12-16T08:08:36Z","published":"2023-12-16T08:08:36Z","title":"Do Similar Entities have Similar Embeddings?","summary":"  Knowledge graph embedding models (KGEMs) developed for link prediction learn\nvector representations for graph entities, known as embeddings. A common tacit\nassumption is the KGE entity similarity assumption, which states that these\nKGEMs retain the graph's structure within their embedding space, i.e., position\nsimilar entities close to one another. This desirable property make KGEMs\nwidely used in downstream tasks such as recommender systems or drug\nrepurposing. Yet, the alignment of graph similarity with embedding space\nsimilarity has rarely been formally evaluated. Typically, KGEMs are assessed\nbased on their sole link prediction capabilities, using ranked-based metrics\nsuch as Hits@K or Mean Rank. This paper challenges the prevailing assumption\nthat entity similarity in the graph is inherently mirrored in the embedding\nspace. Therefore, we conduct extensive experiments to measure the capability of\nKGEMs to cluster similar entities together, and investigate the nature of the\nunderlying factors. Moreover, we study if different KGEMs expose a different\nnotion of similarity. Datasets, pre-trained embeddings and code are available\nat: https://github.com/nicolas-hbt/similar-embeddings.\n","authors":["Nicolas Hubert","Heiko Paulheim","Armelle Brun","Davy Monticolo"],"pdf_url":"https://arxiv.org/pdf/2312.10370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10332v1","updated":"2023-12-16T05:43:11Z","published":"2023-12-16T05:43:11Z","title":"ProTIP: Progressive Tool Retrieval Improves Planning","summary":"  Large language models (LLMs) are increasingly employed for complex multi-step\nplanning tasks, where the tool retrieval (TR) step is crucial for achieving\nsuccessful outcomes. Two prevalent approaches for TR are single-step retrieval,\nwhich utilizes the complete query, and sequential retrieval using task\ndecomposition (TD), where a full query is segmented into discrete atomic\nsubtasks. While single-step retrieval lacks the flexibility to handle\n\"inter-tool dependency,\" the TD approach necessitates maintaining \"subtask-tool\natomicity alignment,\" as the toolbox can evolve dynamically. To address these\nlimitations, we introduce the Progressive Tool retrieval to Improve Planning\n(ProTIP) framework. ProTIP is a lightweight, contrastive learning-based\nframework that implicitly performs TD without the explicit requirement of\nsubtask labels, while simultaneously maintaining subtask-tool atomicity. On the\nToolBench dataset, ProTIP outperforms the ChatGPT task decomposition-based\napproach by a remarkable margin, achieving a 24% improvement in Recall@K=10 for\nTR and a 41% enhancement in tool accuracy for plan generation.\n","authors":["Raviteja Anantha","Bortik Bandyopadhyay","Anirudh Kashi","Sayantan Mahinder","Andrew W Hill","Srinivas Chappidi"],"pdf_url":"https://arxiv.org/pdf/2312.10332v1.pdf","comment":"preprint version"},{"id":"http://arxiv.org/abs/2312.10329v1","updated":"2023-12-16T05:38:39Z","published":"2023-12-16T05:38:39Z","title":"Perturbation-Invariant Adversarial Training for Neural Ranking Models:\n  Improving the Effectiveness-Robustness Trade-Off","summary":"  Neural ranking models (NRMs) have shown great success in information\nretrieval (IR). But their predictions can easily be manipulated using\nadversarial examples, which are crafted by adding imperceptible perturbations\nto legitimate documents. This vulnerability raises significant concerns about\ntheir reliability and hinders the widespread deployment of NRMs. By\nincorporating adversarial examples into training data, adversarial training has\nbecome the de facto defense approach to adversarial attacks against NRMs.\nHowever, this defense mechanism is subject to a trade-off between effectiveness\nand adversarial robustness. In this study, we establish theoretical guarantees\nregarding the effectiveness-robustness trade-off in NRMs. We decompose the\nrobust ranking error into two components, i.e., a natural ranking error for\neffectiveness evaluation and a boundary ranking error for assessing adversarial\nrobustness. Then, we define the perturbation invariance of a ranking model and\nprove it to be a differentiable upper bound on the boundary ranking error for\nattainable computation. Informed by our theoretical analysis, we design a novel\n\\emph{perturbation-invariant adversarial training} (PIAT) method for ranking\nmodels to achieve a better effectiveness-robustness trade-off. We design a\nregularized surrogate loss, in which one term encourages the effectiveness to\nbe maximized while the regularization term encourages the output to be smooth,\nso as to improve adversarial robustness. Experimental results on several\nranking models demonstrate the superiority of PITA compared to existing\nadversarial defenses.\n","authors":["Yu-An Liu","Ruqing Zhang","Mingkun Zhang","Wei Chen","Maarten de Rijke","Jiafeng Guo","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2312.10329v1.pdf","comment":"Accepted by AAAI 24"},{"id":"http://arxiv.org/abs/2312.10325v1","updated":"2023-12-16T05:23:08Z","published":"2023-12-16T05:23:08Z","title":"An Attentive Inductive Bias for Sequential Recommendation Beyond the\n  Self-Attention","summary":"  Sequential recommendation (SR) models based on Transformers have achieved\nremarkable successes. The self-attention mechanism of Transformers for computer\nvision and natural language processing suffers from the oversmoothing problem,\ni.e., hidden representations becoming similar to tokens. In the SR domain, we,\nfor the first time, show that the same problem occurs. We present pioneering\ninvestigations that reveal the low-pass filtering nature of self-attention in\nthe SR, which causes oversmoothing. To this end, we propose a novel method\ncalled Beyond Self-Attention for Sequential Recommendation (BSARec), which\nleverages the Fourier transform to i) inject an inductive bias by considering\nfine-grained sequential patterns and ii) integrate low and high-frequency\ninformation to mitigate oversmoothing. Our discovery shows significant\nadvancements in the SR domain and is expected to bridge the gap for existing\nTransformer-based SR models. We test our proposed approach through extensive\nexperiments on 6 benchmark datasets. The experimental results demonstrate that\nour model outperforms 7 baseline methods in terms of recommendation\nperformance.\n","authors":["Yehjin Shin","Jeongwhan Choi","Hyowon Wi","Noseong Park"],"pdf_url":"https://arxiv.org/pdf/2312.10325v1.pdf","comment":"Accepted by AAAI 2024. Yehjin Shin and Jeongwhan Choi are co-first\n  authors with equal contribution"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2312.10561v1","updated":"2023-12-16T23:31:20Z","published":"2023-12-16T23:31:20Z","title":"Enabling Accelerators for Graph Computing","summary":"  The advent of Graph Neural Networks (GNNs) has revolutionized the field of\nmachine learning, offering a novel paradigm for learning on graph-structured\ndata. Unlike traditional neural networks, GNNs are capable of capturing complex\nrelationships and dependencies inherent in graph data, making them particularly\nsuited for a wide range of applications including social network analysis,\nmolecular chemistry, and network security. The impact of GNNs in these domains\nis profound, enabling more accurate models and predictions, and thereby\ncontributing significantly to advancements in these fields.\n  GNNs, with their unique structure and operation, present new computational\nchallenges compared to conventional neural networks. This requires\ncomprehensive benchmarking and a thorough characterization of GNNs to obtain\ninsight into their computational requirements and to identify potential\nperformance bottlenecks. In this thesis, we aim to develop a better\nunderstanding of how GNNs interact with the underlying hardware and will\nleverage this knowledge as we design specialized accelerators and develop new\noptimizations, leading to more efficient and faster GNN computations.\n  Synthesizing these insights and optimizations, we design a state-of-the-art\nhardware accelerator capable of efficiently handling various GNN workloads. Our\naccelerator architecture is built on our characterization of GNN computational\ndemands, providing clear motivation for our approach. Furthermore, we extend\nour exploration to emerging GNN workloads in the domain of graph neural\nnetworks. This exploration into novel models underlines our comprehensive\napproach, as we strive to enable accelerators that are not just performant, but\nalso versatile, able to adapt to the evolving landscape of graph computing.\n","authors":["Kaustubh Shivdikar"],"pdf_url":"https://arxiv.org/pdf/2312.10561v1.pdf","comment":"Northeastern University Doctoral Dissertation"},{"id":"http://arxiv.org/abs/2312.10560v1","updated":"2023-12-16T23:23:16Z","published":"2023-12-16T23:23:16Z","title":"Optimizing Dense Feed-Forward Neural Networks","summary":"  Deep learning models have been widely used during the last decade due to\ntheir outstanding learning and abstraction capacities. However, one of the main\nchallenges any scientist has to face using deep learning models is to establish\nthe network's architecture. Due to this difficulty, data scientists usually\nbuild over complex models and, as a result, most of them result computationally\nintensive and impose a large memory footprint, generating huge costs,\ncontributing to climate change and hindering their use in computational-limited\ndevices. In this paper, we propose a novel feed-forward neural network\nconstructing method based on pruning and transfer learning. Its performance has\nbeen thoroughly assessed in classification and regression problems. Without any\naccuracy loss, our approach can compress the number of parameters by more than\n70%. Even further, choosing the pruning parameter carefully, most of the\nrefined models outperform original ones. We also evaluate the transfer learning\nlevel comparing the refined model and the original one training from scratch a\nneural network with the same hyper parameters as the optimized model. The\nresults obtained show that our constructing method not only helps in the design\nof more efficient models but also more effective ones.\n","authors":["Luis Balderas","Miguel Lastra","José M. Benítez"],"pdf_url":"https://arxiv.org/pdf/2312.10560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.06764v2","updated":"2023-12-16T23:13:26Z","published":"2023-08-13T13:01:21Z","title":"Few-shot Class-incremental Learning: A Survey","summary":"  Few-shot Class-Incremental Learning (FSCIL) presents a unique challenge in\nMachine Learning (ML), as it necessitates the Incremental Learning (IL) of new\nclasses from sparsely labeled training samples without forgetting previous\nknowledge. While this field has seen recent progress, it remains an active\nexploration area. This paper aims to provide a comprehensive and systematic\nreview of FSCIL. In our in-depth examination, we delve into various facets of\nFSCIL, encompassing the problem definition, the discussion of the primary\nchallenges of unreliable empirical risk minimization and the\nstability-plasticity dilemma, general schemes, and relevant problems of IL and\nFew-shot Learning (FSL). Besides, we offer an overview of benchmark datasets\nand evaluation metrics. Furthermore, we introduce the Few-shot\nClass-incremental Classification (FSCIC) methods from data-based,\nstructure-based, and optimization-based approaches and the Few-shot\nClass-incremental Object Detection (FSCIOD) methods from anchor-free and\nanchor-based approaches. Beyond these, we present several promising research\ndirections within FSCIL that merit further investigation.\n","authors":["Jinghua Zhang","Li Liu","Olli Silvén","Matti Pietikäinen","Dewen Hu"],"pdf_url":"https://arxiv.org/pdf/2308.06764v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10557v1","updated":"2023-12-16T23:11:52Z","published":"2023-12-16T23:11:52Z","title":"Improving Environment Robustness of Deep Reinforcement Learning\n  Approaches for Autonomous Racing Using Bayesian Optimization-based Curriculum\n  Learning","summary":"  Deep reinforcement learning (RL) approaches have been broadly applied to a\nlarge number of robotics tasks, such as robot manipulation and autonomous\ndriving. However, an open problem in deep RL is learning policies that are\nrobust to variations in the environment, which is an important condition for\nsuch systems to be deployed into real-world, unstructured settings. Curriculum\nlearning is one approach that has been applied to improve generalization\nperformance in both supervised and reinforcement learning domains, but\nselecting the appropriate curriculum to achieve robustness can be a\nuser-intensive process. In our work, we show that performing probabilistic\ninference of the underlying curriculum-reward function using Bayesian\nOptimization can be a promising technique for finding a robust curriculum. We\ndemonstrate that a curriculum found with Bayesian optimization can outperform a\nvanilla deep RL agent and a hand-engineered curriculum in the domain of\nautonomous racing with obstacle avoidance. Our code is available at\nhttps://github.com/PRISHIta123/Curriculum_RL_for_Driving.\n","authors":["Rohan Banerjee","Prishita Ray","Mark Campbell"],"pdf_url":"https://arxiv.org/pdf/2312.10557v1.pdf","comment":"Accepted to the IROS 2023 Workshop on Learning Robot Super Autonomy.\n  The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2312.10556v1","updated":"2023-12-16T23:10:09Z","published":"2023-12-16T23:10:09Z","title":"Deep Similarity Learning Loss Functions in Data Transformation for Class\n  Imbalance","summary":"  Improving the classification of multi-class imbalanced data is more difficult\nthan its two-class counterpart. In this paper, we use deep neural networks to\ntrain new representations of tabular multi-class data. Unlike the typically\ndeveloped re-sampling pre-processing methods, our proposal modifies the\ndistribution of features, i.e. the positions of examples in the learned\nembedded representation, and it does not modify the class sizes. To learn such\nembedded representations we introduced various definitions of triplet loss\nfunctions: the simplest one uses weights related to the degree of class\nimbalance, while the next proposals are intended for more complex distributions\nof examples and aim to generate a safe neighborhood of minority examples.\nSimilarly to the resampling approaches, after applying such preprocessing,\ndifferent classifiers can be trained on new representations. Experiments with\npopular multi-class imbalanced benchmark data sets and three classifiers showed\nthe advantage of the proposed approach over popular pre-processing methods as\nwell as basic versions of neural networks with classical loss function\nformulations.\n","authors":["Damian Horna","Lango Mateusz","Jerzy Stefanowski"],"pdf_url":"https://arxiv.org/pdf/2312.10556v1.pdf","comment":"This preprint is an extended version of the paper presented at\n  LIDTA'2023 workshop at ECMLPKDD202"},{"id":"http://arxiv.org/abs/2303.17743v3","updated":"2023-12-16T22:42:42Z","published":"2023-03-30T23:30:42Z","title":"FairGen: Towards Fair Graph Generation","summary":"  There have been tremendous efforts over the past decades dedicated to the\ngeneration of realistic graphs in a variety of domains, ranging from social\nnetworks to computer networks, from gene regulatory networks to online\ntransaction networks. Despite the remarkable success, the vast majority of\nthese works are unsupervised in nature and are typically trained to minimize\nthe expected graph reconstruction loss, which would result in the\nrepresentation disparity issue in the generated graphs, i.e., the protected\ngroups (often minorities) contribute less to the objective and thus suffer from\nsystematically higher errors. In this paper, we aim to tailor graph generation\nto downstream mining tasks by leveraging label information and user-preferred\nparity constraints. In particular, we start from the investigation of\nrepresentation disparity in the context of graph generative models. To mitigate\nthe disparity, we propose a fairness-aware graph generative model named\nFairGen. Our model jointly trains a label-informed graph generation module and\na fair representation learning module by progressively learning the behaviors\nof the protected and unprotected groups, from the `easy' concepts to the `hard'\nones. In addition, we propose a generic context sampling strategy for graph\ngenerative models, which is proven to be capable of fairly capturing the\ncontextual information of each group with a high probability. Experimental\nresults on seven real-world data sets, including web-based graphs, demonstrate\nthat FairGen (1) obtains performance on par with state-of-the-art graph\ngenerative models across nine network properties, (2) mitigates the\nrepresentation disparity issues in the generated graphs, and (3) substantially\nboosts the model performance by up to 17% in downstream tasks via data\naugmentation.\n","authors":["Lecheng Zheng","Dawei Zhou","Hanghang Tong","Jiejun Xu","Yada Zhu","Jingrui He"],"pdf_url":"https://arxiv.org/pdf/2303.17743v3.pdf","comment":"Accepted by ICDE 2024"},{"id":"http://arxiv.org/abs/2312.10553v1","updated":"2023-12-16T22:40:39Z","published":"2023-12-16T22:40:39Z","title":"Machine Learning-Enhanced Prediction of Surface Smoothness for Inertial\n  Confinement Fusion Target Polishing Using Limited Data","summary":"  In Inertial Confinement Fusion (ICF) process, roughly a 2mm spherical shell\nmade of high density carbon is used as target for laser beams, which compress\nand heat it to energy levels needed for high fusion yield. These shells are\npolished meticulously to meet the standards for a fusion shot. However, the\npolishing of these shells involves multiple stages, with each stage taking\nseveral hours. To make sure that the polishing process is advancing in the\nright direction, we are able to measure the shell surface roughness. This\nmeasurement, however, is very labor-intensive, time-consuming, and requires a\nhuman operator. We propose to use machine learning models that can predict\nsurface roughness based on the data collected from a vibration sensor that is\nconnected to the polisher. Such models can generate surface roughness of the\nshells in real-time, allowing the operator to make any necessary changes to the\npolishing for optimal result.\n","authors":["Antonios Alexos","Junze Liu","Akash Tiwari","Kshitij Bhardwaj","Sean Hayes","Pierre Baldi","Satish Bukkapatnam","Suhas Bhandarkar"],"pdf_url":"https://arxiv.org/pdf/2312.10553v1.pdf","comment":"Accepted as Extended Abstract in AIM 2024"},{"id":"http://arxiv.org/abs/2312.10550v1","updated":"2023-12-16T22:27:36Z","published":"2023-12-16T22:27:36Z","title":"Amortized Reparametrization: Efficient and Scalable Variational\n  Inference for Latent SDEs","summary":"  We consider the problem of inferring latent stochastic differential equations\n(SDEs) with a time and memory cost that scales independently with the amount of\ndata, the total length of the time series, and the stiffness of the approximate\ndifferential equations. This is in stark contrast to typical methods for\ninferring latent differential equations which, despite their constant memory\ncost, have a time complexity that is heavily dependent on the stiffness of the\napproximate differential equation. We achieve this computational advancement by\nremoving the need to solve differential equations when approximating gradients\nusing a novel amortization strategy coupled with a recently derived\nreparametrization of expectations under linear SDEs. We show that, in practice,\nthis allows us to achieve similar performance to methods based on adjoint\nsensitivities with more than an order of magnitude fewer evaluations of the\nmodel in training.\n","authors":["Kevin Course","Prasanth B. Nair"],"pdf_url":"https://arxiv.org/pdf/2312.10550v1.pdf","comment":"In Advances in Neural Information Processing Systems. 2023"},{"id":"http://arxiv.org/abs/2312.10549v1","updated":"2023-12-16T22:24:54Z","published":"2023-12-16T22:24:54Z","title":"Catastrophic Forgetting in Deep Learning: A Comprehensive Taxonomy","summary":"  Deep Learning models have achieved remarkable performance in tasks such as\nimage classification or generation, often surpassing human accuracy. However,\nthey can struggle to learn new tasks and update their knowledge without access\nto previous data, leading to a significant loss of accuracy known as\nCatastrophic Forgetting (CF). This phenomenon was first observed by McCloskey\nand Cohen in 1989 and remains an active research topic. Incremental learning\nwithout forgetting is widely recognized as a crucial aspect in building better\nAI systems, as it allows models to adapt to new tasks without losing the\nability to perform previously learned ones. This article surveys recent studies\nthat tackle CF in modern Deep Learning models that use gradient descent as\ntheir learning algorithm. Although several solutions have been proposed, a\ndefinitive solution or consensus on assessing CF is yet to be established. The\narticle provides a comprehensive review of recent solutions, proposes a\ntaxonomy to organize them, and identifies research gaps in this area.\n","authors":["Everton L. Aleixo","Juan G. Colonna","Marco Cristo","Everlandio Fernandes"],"pdf_url":"https://arxiv.org/pdf/2312.10549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10547v1","updated":"2023-12-16T22:09:50Z","published":"2023-12-16T22:09:50Z","title":"Advancing RAN Slicing with Offline Reinforcement Learning","summary":"  Dynamic radio resource management (RRM) in wireless networks presents\nsignificant challenges, particularly in the context of Radio Access Network\n(RAN) slicing. This technology, crucial for catering to varying user\nrequirements, often grapples with complex optimization scenarios. Existing\nReinforcement Learning (RL) approaches, while achieving good performance in RAN\nslicing, typically rely on online algorithms or behavior cloning. These methods\nnecessitate either continuous environmental interactions or access to\nhigh-quality datasets, hindering their practical deployment. Towards addressing\nthese limitations, this paper introduces offline RL to solving the RAN slicing\nproblem, marking a significant shift towards more feasible and adaptive RRM\nmethods. We demonstrate how offline RL can effectively learn near-optimal\npolicies from sub-optimal datasets, a notable advancement over existing\npractices. Our research highlights the inherent flexibility of offline RL,\nshowcasing its ability to adjust policy criteria without the need for\nadditional environmental interactions. Furthermore, we present empirical\nevidence of the efficacy of offline RL in adapting to various service-level\nrequirements, illustrating its potential in diverse RAN slicing scenarios.\n","authors":["Kun Yang","Shu-ping Yeh","Menglei Zhang","Jerry Sydir","Jing Yang","Cong Shen"],"pdf_url":"https://arxiv.org/pdf/2312.10547v1.pdf","comment":"9 pages. 6 figures"},{"id":"http://arxiv.org/abs/2312.10545v1","updated":"2023-12-16T22:02:20Z","published":"2023-12-16T22:02:20Z","title":"Learning graphs and simplicial complexes from data","summary":"  Graphs are widely used to represent complex information and signal domains\nwith irregular support. Typically, the underlying graph topology is unknown and\nmust be estimated from the available data. Common approaches assume pairwise\nnode interactions and infer the graph topology based on this premise. In\ncontrast, our novel method not only unveils the graph topology but also\nidentifies three-node interactions, referred to in the literature as\nsecond-order simplicial complexes (SCs). We model signals using a graph\nautoregressive Volterra framework, enhancing it with structured graph Volterra\nkernels to learn SCs. We propose a mathematical formulation for graph and SC\ninference, solving it through convex optimization involving group norms and\nmask matrices. Experimental results on synthetic and real-world data showcase a\nsuperior performance for our approach compared to existing methods.\n","authors":["Andrei Buciulea","Elvin Isufi","Geert Leus","Antonio G. Marques"],"pdf_url":"https://arxiv.org/pdf/2312.10545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08470v2","updated":"2023-12-16T21:54:59Z","published":"2023-12-13T19:13:11Z","title":"Best practices for machine learning in antibody discovery and\n  development","summary":"  Over the past 40 years, the discovery and development of therapeutic\nantibodies to treat disease has become common practice. However, as therapeutic\nantibody constructs are becoming more sophisticated (e.g., multi-specifics),\nconventional approaches to optimisation are increasingly inefficient. Machine\nlearning (ML) promises to open up an in silico route to antibody discovery and\nhelp accelerate the development of drug products using a reduced number of\nexperiments and hence cost. Over the past few years, we have observed rapid\ndevelopments in the field of ML-guided antibody discovery and development\n(D&D). However, many of the results are difficult to compare or hard to assess\nfor utility by other experts in the field due to the high diversity in the\ndatasets and evaluation techniques and metrics that are across industry and\nacademia. This limitation of the literature curtails the broad adoption of ML\nacross the industry and slows down overall progress in the field, highlighting\nthe need to develop standards and guidelines that may help improve the\nreproducibility of ML models across different research groups. To address these\nchallenges, we set out in this perspective to critically review current\npractices, explain common pitfalls, and clearly define a set of method\ndevelopment and evaluation guidelines that can be applied to different types of\nML-based techniques for therapeutic antibody D&D. Specifically, we address in\nan end-to-end analysis, challenges associated with all aspects of the ML\nprocess and recommend a set of best practices for each stage.\n","authors":["Leonard Wossnig","Norbert Furtmann","Andrew Buchanan","Sandeep Kumar","Victor Greiff"],"pdf_url":"https://arxiv.org/pdf/2312.08470v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02102v2","updated":"2023-12-16T21:40:38Z","published":"2023-12-04T18:26:31Z","title":"Mitigating Data Injection Attacks on Federated Learning","summary":"  Federated learning is a technique that allows multiple entities to\ncollaboratively train models using their data without compromising data\nprivacy. However, despite its advantages, federated learning can be susceptible\nto false data injection attacks. In these scenarios, a malicious entity with\ncontrol over specific agents in the network can manipulate the learning\nprocess, leading to a suboptimal model. Consequently, addressing these data\ninjection attacks presents a significant research challenge in federated\nlearning systems. In this paper, we propose a novel technique to detect and\nmitigate data injection attacks on federated learning systems. Our mitigation\nmethod is a local scheme, performed during a single instance of training by the\ncoordinating node, allowing the mitigation during the convergence of the\nalgorithm. Whenever an agent is suspected to be an attacker, its data will be\nignored for a certain period, this decision will often be re-evaluated. We\nprove that with probability 1, after a finite time, all attackers will be\nignored while the probability of ignoring a trustful agent becomes 0, provided\nthat there is a majority of truthful agents. Simulations show that when the\ncoordinating node detects and isolates all the attackers, the model recovers\nand converges to the truthful model.\n","authors":["Or Shalom","Amir Leshem","Waheed U. Bajwa"],"pdf_url":"https://arxiv.org/pdf/2312.02102v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.05152v7","updated":"2023-12-16T20:35:12Z","published":"2021-06-09T15:51:03Z","title":"Rethinking Transfer Learning for Medical Image Classification","summary":"  Transfer learning (TL) from pretrained deep models is a standard practice in\nmodern medical image classification (MIC). However, what levels of features to\nbe reused are problem-dependent, and uniformly finetuning all layers of\npretrained models may be suboptimal. This insight has partly motivated the\nrecent differential TL strategies, such as TransFusion (TF) and layer-wise\nfinetuning (LWFT), which treat the layers in the pretrained models\ndifferentially. In this paper, we add one more strategy into this family,\ncalled TruncatedTL, which reuses and finetunes appropriate bottom layers and\ndirectly discards the remaining layers. This yields not only superior MIC\nperformance but also compact models for efficient inference, compared to other\ndifferential TL methods. Our code is available at:\nhttps://github.com/sun-umn/TTL\n","authors":["Le Peng","Hengyue Liang","Gaoxiang Luo","Taihui Li","Ju Sun"],"pdf_url":"https://arxiv.org/pdf/2106.05152v7.pdf","comment":"Accepted to BMVC2023 (oral)"},{"id":"http://arxiv.org/abs/2305.15747v2","updated":"2023-12-16T20:29:49Z","published":"2023-05-25T05:52:43Z","title":"Union Subgraph Neural Networks","summary":"  Graph Neural Networks (GNNs) are widely used for graph representation\nlearning in many application domains. The expressiveness of vanilla GNNs is\nupper-bounded by 1-dimensional Weisfeiler-Leman (1-WL) test as they operate on\nrooted subtrees through iterative message passing. In this paper, we empower\nGNNs by injecting neighbor-connectivity information extracted from a new type\nof substructure. We first investigate different kinds of connectivities\nexisting in a local neighborhood and identify a substructure called union\nsubgraph, which is able to capture the complete picture of the 1-hop\nneighborhood of an edge. We then design a shortest-path-based substructure\ndescriptor that possesses three nice properties and can effectively encode the\nhigh-order connectivities in union subgraphs. By infusing the encoded neighbor\nconnectivities, we propose a novel model, namely Union Subgraph Neural Network\n(UnionSNN), which is proven to be strictly more powerful than 1-WL in\ndistinguishing non-isomorphic graphs. Additionally, the local encoding from\nunion subgraphs can also be injected into arbitrary message-passing neural\nnetworks (MPNNs) and Transformer-based models as a plugin. Extensive\nexperiments on 18 benchmarks of both graph-level and node-level tasks\ndemonstrate that UnionSNN outperforms state-of-the-art baseline models, with\ncompetitive computational efficiency. The injection of our local encoding to\nexisting models is able to boost the performance by up to 11.09\\%. Our code is\navailable at https://github.com/AngusMonroe/UnionSNN.\n","authors":["Jiaxing Xu","Aihu Zhang","Qingtian Bian","Vijay Prakash Dwivedi","Yiping Ke"],"pdf_url":"https://arxiv.org/pdf/2305.15747v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10534v1","updated":"2023-12-16T20:20:38Z","published":"2023-12-16T20:20:38Z","title":"Rethinking Robustness of Model Attributions","summary":"  For machine learning models to be reliable and trustworthy, their decisions\nmust be interpretable. As these models find increasing use in safety-critical\napplications, it is important that not just the model predictions but also\ntheir explanations (as feature attributions) be robust to small\nhuman-imperceptible input perturbations. Recent works have shown that many\nattribution methods are fragile and have proposed improvements in either these\nmethods or the model training. We observe two main causes for fragile\nattributions: first, the existing metrics of robustness (e.g., top-k\nintersection) over-penalize even reasonable local shifts in attribution,\nthereby making random perturbations to appear as a strong attack, and second,\nthe attribution can be concentrated in a small region even when there are\nmultiple important parts in an image. To rectify this, we propose simple ways\nto strengthen existing metrics and attribution methods that incorporate\nlocality of pixels in robustness metrics and diversity of pixel locations in\nattributions. Towards the role of model training in attributional robustness,\nwe empirically observe that adversarially trained models have more robust\nattributions on smaller datasets, however, this advantage disappears in larger\ndatasets. Code is available at https://github.com/ksandeshk/LENS.\n","authors":["Sandesh Kamath","Sankalp Mittal","Amit Deshpande","Vineeth N Balasubramanian"],"pdf_url":"https://arxiv.org/pdf/2312.10534v1.pdf","comment":"Accepted AAAI 2024"},{"id":"http://arxiv.org/abs/2312.10528v1","updated":"2023-12-16T19:59:07Z","published":"2023-12-16T19:59:07Z","title":"Cross-Linguistic Offensive Language Detection: BERT-Based Analysis of\n  Bengali, Assamese, & Bodo Conversational Hateful Content from Social Media","summary":"  In today's age, social media reigns as the paramount communication platform,\nproviding individuals with the avenue to express their conjectures,\nintellectual propositions, and reflections. Unfortunately, this freedom often\ncomes with a downside as it facilitates the widespread proliferation of hate\nspeech and offensive content, leaving a deleterious impact on our world. Thus,\nit becomes essential to discern and eradicate such offensive material from the\nrealm of social media. This article delves into the comprehensive results and\nkey revelations from the HASOC-2023 offensive language identification result.\nThe primary emphasis is placed on the meticulous detection of hate speech\nwithin the linguistic domains of Bengali, Assamese, and Bodo, forming the\nframework for Task 4: Annihilate Hates. In this work, we used BERT models,\nincluding XML-Roberta, L3-cube, IndicBERT, BenglaBERT, and BanglaHateBERT. The\nresearch outcomes were promising and showed that XML-Roberta-lagre performed\nbetter than monolingual models in most cases. Our team 'TeamBD' achieved rank\n3rd for Task 4 - Assamese, & 5th for Bengali.\n","authors":["Jhuma Kabir Mim","Mourad Oussalah","Akash Singhal"],"pdf_url":"https://arxiv.org/pdf/2312.10528v1.pdf","comment":"9 pages, 1 figure, 5 tables"},{"id":"http://arxiv.org/abs/2312.10527v1","updated":"2023-12-16T19:56:10Z","published":"2023-12-16T19:56:10Z","title":"CoCoGen: Physically-Consistent and Conditioned Score-based Generative\n  Models for Forward and Inverse Problems","summary":"  Recent advances in generative artificial intelligence have had a significant\nimpact on diverse domains spanning computer vision, natural language\nprocessing, and drug discovery. This work extends the reach of generative\nmodels into physical problem domains, particularly addressing the efficient\nenforcement of physical laws and conditioning for forward and inverse problems\ninvolving partial differential equations (PDEs). Our work introduces two key\ncontributions: firstly, we present an efficient approach to promote consistency\nwith the underlying PDE. By incorporating discretized information into\nscore-based generative models, our method generates samples closely aligned\nwith the true data distribution, showcasing residuals comparable to data\ngenerated through conventional PDE solvers, significantly enhancing fidelity.\nSecondly, we showcase the potential and versatility of score-based generative\nmodels in various physics tasks, specifically highlighting surrogate modeling\nas well as probabilistic field reconstruction and inversion from sparse\nmeasurements. A robust foundation is laid by designing unconditional\nscore-based generative models that utilize reversible probability flow ordinary\ndifferential equations. Leveraging conditional models that require minimal\ntraining, we illustrate their flexibility when combined with a frozen\nunconditional model. These conditional models generate PDE solutions by\nincorporating parameters, macroscopic quantities, or partial field measurements\nas guidance. The results illustrate the inherent flexibility of score-based\ngenerative models and explore the synergy between unconditional score-based\ngenerative models and the present physically-consistent sampling approach,\nemphasizing the power and flexibility in solving for and inverting physical\nfields governed by differential equations, and in other scientific machine\nlearning tasks.\n","authors":["Christian Jacobsen","Yilin Zhuang","Karthik Duraisamy"],"pdf_url":"https://arxiv.org/pdf/2312.10527v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.19496v2","updated":"2023-12-16T19:35:31Z","published":"2023-05-31T02:10:27Z","title":"Is Learning in Games Good for the Learners?","summary":"  We consider a number of questions related to tradeoffs between reward and\nregret in repeated gameplay between two agents. To facilitate this, we\nintroduce a notion of $\\textit{generalized equilibrium}$ which allows for\nasymmetric regret constraints, and yields polytopes of feasible values for each\nagent and pair of regret constraints, where we show that any such equilibrium\nis reachable by a pair of algorithms which maintain their regret guarantees\nagainst arbitrary opponents. As a central example, we highlight the case one\nagent is no-swap and the other's regret is unconstrained. We show that this\ncaptures an extension of $\\textit{Stackelberg}$ equilibria with a matching\noptimal value, and that there exists a wide class of games where a player can\nsignificantly increase their utility by deviating from a no-swap-regret\nalgorithm against a no-swap learner (in fact, almost any game without pure Nash\nequilibria is of this form). Additionally, we make use of generalized\nequilibria to consider tradeoffs in terms of the opponent's algorithm choice.\nWe give a tight characterization for the maximal reward obtainable against\n$\\textit{some}$ no-regret learner, yet we also show a class of games in which\nthis is bounded away from the value obtainable against the class of common\n\"mean-based\" no-regret algorithms. Finally, we consider the question of\nlearning reward-optimal strategies via repeated play with a no-regret agent\nwhen the game is initially unknown. Again we show tradeoffs depending on the\nopponent's learning algorithm: the Stackelberg strategy is learnable in\nexponential time with any no-regret agent (and in polynomial time with any\nno-$\\textit{adaptive}$-regret agent) for any game where it is learnable via\nqueries, and there are games where it is learnable in polynomial time against\nany no-swap-regret agent but requires exponential time against a mean-based\nno-regret agent.\n","authors":["William Brown","Jon Schneider","Kiran Vodrahalli"],"pdf_url":"https://arxiv.org/pdf/2305.19496v2.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2312.10523v1","updated":"2023-12-16T19:12:45Z","published":"2023-12-16T19:12:45Z","title":"Paloma: A Benchmark for Evaluating Language Model Fit","summary":"  Language models (LMs) commonly report perplexity on monolithic data held out\nfrom training. Implicitly or explicitly, this data is composed of\ndomains$\\unicode{x2013}$varying distributions of language. Rather than assuming\nperplexity on one distribution extrapolates to others, Perplexity Analysis for\nLanguage Model Assessment (Paloma), measures LM fit to 585 text domains,\nranging from nytimes.com to r/depression on Reddit. We invite submissions to\nour benchmark and organize results by comparability based on compliance with\nguidelines such as removal of benchmark contamination from pretraining.\nSubmissions can also record parameter and training token count to make\ncomparisons of Pareto efficiency for performance as a function of these\nmeasures of cost. We populate our benchmark with results from 6 baselines\npretrained on popular corpora. In case studies, we demonstrate analyses that\nare possible with Paloma, such as finding that pretraining without data beyond\nCommon Crawl leads to inconsistent fit to many domains.\n","authors":["Ian Magnusson","Akshita Bhagia","Valentin Hofmann","Luca Soldaini","Ananya Harsh Jha","Oyvind Tafjord","Dustin Schwenk","Evan Pete Walsh","Yanai Elazar","Kyle Lo","Dirk Groeneveld","Iz Beltagy","Hannaneh Hajishirzi","Noah A. Smith","Kyle Richardson","Jesse Dodge"],"pdf_url":"https://arxiv.org/pdf/2312.10523v1.pdf","comment":"Project Page: https://paloma.allen.ai/"},{"id":"http://arxiv.org/abs/2308.13616v2","updated":"2023-12-16T18:36:19Z","published":"2023-08-25T18:18:47Z","title":"Channel Estimation in RIS-Enabled mmWave Wireless Systems: A Variational\n  Inference Approach","summary":"  Channel estimation in reconfigurable intelligent surfaces (RIS)-aided systems\nis crucial for optimal configuration of the RIS and various downstream tasks\nsuch as user localization. In RIS-aided systems, channel estimation involves\nestimating two channels for the user-RIS (UE-RIS) and RIS-base station (RIS-BS)\nlinks. In the literature, two approaches are proposed: (i) cascaded channel\nestimation where the two channels are collapsed into a single one and estimated\nusing training signals at the BS, and (ii) separate channel estimation that\nestimates each channel separately either in a passive or semi-passive RIS\nsetting. In this work, we study the separate channel estimation problem in a\nfully passive RIS-aided millimeter-wave (mmWave) single-user single-input\nmultiple-output (SIMO) communication system. First, we adopt a\nvariational-inference (VI) approach to jointly estimate the UE-RIS and RIS-BS\ninstantaneous channel state information (I-CSI). In particular, auxiliary\nposterior distributions of the I-CSI are learned through the maximization of\nthe evidence lower bound. However, estimating the I-CSI for both links in every\ncoherence block results in a high signaling overhead to control the RIS in\nscenarios with highly mobile users. Thus, we extend our first approach to\nestimate the slow-varying statistical CSI of the UE-RIS link overcoming the\nhighly variant I-CSI. Precisely, our second method estimates the I-CSI of\nRIS-BS channel and the UE-RIS channel covariance matrix (CCM) directly from the\nuplink training signals in a fully passive RIS-aided system. The simulation\nresults demonstrate that using maximum a posteriori channel estimation using\nthe auxiliary posteriors can provide a capacity that approaches the capacity\nwith perfect CSI.\n","authors":["Firas Fredj","Amal Feriani","Amine Mezghani","Ekram Hossain"],"pdf_url":"https://arxiv.org/pdf/2308.13616v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10512v1","updated":"2023-12-16T17:51:22Z","published":"2023-12-16T17:51:22Z","title":"Value of Information and Timing-aware Scheduling for Federated Learning","summary":"  Data possesses significant value as it fuels advancements in AI. However,\nprotecting the privacy of the data generated by end-user devices has become\ncrucial. Federated Learning (FL) offers a solution by preserving data privacy\nduring training. FL brings the model directly to User Equipments (UEs) for\nlocal training by an access point (AP). The AP periodically aggregates trained\nparameters from UEs, enhancing the model and sending it back to them. However,\ndue to communication constraints, only a subset of UEs can update parameters\nduring each global aggregation. Consequently, developing innovative scheduling\nalgorithms is vital to enable complete FL implementation and enhance FL\nconvergence. In this paper, we present a scheduling policy combining Age of\nUpdate (AoU) concepts and data Shapley metrics. This policy considers the\nfreshness and value of received parameter updates from individual data sources\nand real-time channel conditions to enhance FL's operational efficiency. The\nproposed algorithm is simple, and its effectiveness is demonstrated through\nsimulations.\n","authors":["Muhammad Azeem Khan","Howard H. Yang","Zihan Chen","Antonio Iera","Nikolaos Pappas"],"pdf_url":"https://arxiv.org/pdf/2312.10512v1.pdf","comment":"IEEE Conference on Standards for Communications and Networking 2023"},{"id":"http://arxiv.org/abs/2312.10508v1","updated":"2023-12-16T17:36:23Z","published":"2023-12-16T17:36:23Z","title":"TrojFair: Trojan Fairness Attacks","summary":"  Deep learning models have been incorporated into high-stakes sectors,\nincluding healthcare diagnosis, loan approvals, and candidate recruitment,\namong others. Consequently, any bias or unfairness in these models can harm\nthose who depend on such models. In response, many algorithms have emerged to\nensure fairness in deep learning. However, while the potential for harm is\nsubstantial, the resilience of these fair deep learning models against\nmalicious attacks has never been thoroughly explored, especially in the context\nof emerging Trojan attacks. Moving beyond prior research, we aim to fill this\nvoid by introducing \\textit{TrojFair}, a Trojan fairness attack. Unlike\nexisting attacks, TrojFair is model-agnostic and crafts a Trojaned model that\nfunctions accurately and equitably for clean inputs. However, it displays\ndiscriminatory behaviors \\text{-} producing both incorrect and unfair results\n\\text{-} for specific groups with tainted inputs containing a trigger. TrojFair\nis a stealthy Fairness attack that is resilient to existing model fairness\naudition detectors since the model for clean inputs is fair. TrojFair achieves\na target group attack success rate exceeding $88.77\\%$, with an average\naccuracy loss less than $0.44\\%$. It also maintains a high discriminative score\nbetween the target and non-target groups across various datasets and models.\n","authors":["Mengxin Zheng","Jiaqi Xue","Yi Sheng","Lei Yang","Qian Lou","Lei Jiang"],"pdf_url":"https://arxiv.org/pdf/2312.10508v1.pdf","comment":"12 pages, 2 figures"},{"id":"http://arxiv.org/abs/2312.01479v3","updated":"2023-12-16T17:22:45Z","published":"2023-12-03T18:41:54Z","title":"OpenVoice: Versatile Instant Voice Cloning","summary":"  We introduce OpenVoice, a versatile voice cloning approach that requires only\na short audio clip from the reference speaker to replicate their voice and\ngenerate speech in multiple languages. OpenVoice represents a significant\nadvancement in addressing the following open challenges in the field: 1)\nFlexible Voice Style Control. OpenVoice enables granular control over voice\nstyles, including emotion, accent, rhythm, pauses, and intonation, in addition\nto replicating the tone color of the reference speaker. The voice styles are\nnot directly copied from and constrained by the style of the reference speaker.\nPrevious approaches lacked the ability to flexibly manipulate voice styles\nafter cloning. 2) Zero-Shot Cross-Lingual Voice Cloning. OpenVoice achieves\nzero-shot cross-lingual voice cloning for languages not included in the\nmassive-speaker training set. Unlike previous approaches, which typically\nrequire extensive massive-speaker multi-lingual (MSML) dataset for all\nlanguages, OpenVoice can clone voices into a new language without any\nmassive-speaker training data for that language. OpenVoice is also\ncomputationally efficient, costing tens of times less than commercially\navailable APIs that offer even inferior performance. To foster further research\nin the field, we have made the source code and trained model publicly\naccessible. We also provide qualitative results in our demo website. Prior to\nits public release, our internal version of OpenVoice was used tens of millions\nof times by users worldwide between May and October 2023, serving as the\nbackend of MyShell.\n","authors":["Zengyi Qin","Wenliang Zhao","Xumin Yu","Xin Sun"],"pdf_url":"https://arxiv.org/pdf/2312.01479v3.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2312.07547v2","updated":"2023-12-16T17:15:36Z","published":"2023-12-06T09:38:35Z","title":"Active Inference and Intentional Behaviour","summary":"  Recent advances in theoretical biology suggest that basal cognition and\nsentient behaviour are emergent properties of in vitro cell cultures and\nneuronal networks, respectively. Such neuronal networks spontaneously learn\nstructured behaviours in the absence of reward or reinforcement. In this paper,\nwe characterise this kind of self-organisation through the lens of the free\nenergy principle, i.e., as self-evidencing. We do this by first discussing the\ndefinitions of reactive and sentient behaviour in the setting of active\ninference, which describes the behaviour of agents that model the consequences\nof their actions. We then introduce a formal account of intentional behaviour,\nthat describes agents as driven by a preferred endpoint or goal in latent\nstate-spaces. We then investigate these forms of (reactive, sentient, and\nintentional) behaviour using simulations. First, we simulate the aforementioned\nin vitro experiments, in which neuronal cultures spontaneously learn to play\nPong, by implementing nested, free energy minimising processes. The simulations\nare then used to deconstruct the ensuing predictive behaviour, leading to the\ndistinction between merely reactive, sentient, and intentional behaviour, with\nthe latter formalised in terms of inductive planning. This distinction is\nfurther studied using simple machine learning benchmarks (navigation in a grid\nworld and the Tower of Hanoi problem), that show how quickly and efficiently\nadaptive behaviour emerges under an inductive form of active inference.\n","authors":["Karl J. Friston","Tommaso Salvatori","Takuya Isomura","Alexander Tschantz","Alex Kiefer","Tim Verbelen","Magnus Koudahl","Aswin Paul","Thomas Parr","Adeel Razi","Brett Kagan","Christopher L. Buckley","Maxwell J. D. Ramstead"],"pdf_url":"https://arxiv.org/pdf/2312.07547v2.pdf","comment":"33 pages, 9 figures"},{"id":"http://arxiv.org/abs/2312.10494v1","updated":"2023-12-16T16:16:28Z","published":"2023-12-16T16:16:28Z","title":"Do Bayesian Neural Networks Weapon System Improve Predictive\n  Maintenance?","summary":"  We implement a Bayesian inference process for Neural Networks to model the\ntime to failure of highly reliable weapon systems with interval-censored data\nand time-varying covariates. We analyze and benchmark our approach, LaplaceNN,\non synthetic and real datasets with standard classification metrics such as\nReceiver Operating Characteristic (ROC) Area Under Curve (AUC) Precision-Recall\n(PR) AUC, and reliability curve visualizations.\n","authors":["Michael Potter","Miru Jun"],"pdf_url":"https://arxiv.org/pdf/2312.10494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10490v1","updated":"2023-12-16T15:52:13Z","published":"2023-12-16T15:52:13Z","title":"Spatial Deep Learning for Site-Specific Movement Optimization of Aerial\n  Base Stations","summary":"  Unmanned aerial vehicles (UAVs) can be utilized as aerial base stations\n(ABSs) to provide wireless connectivity for ground users (GUs) in various\nemergency scenarios. However, it is a NP-hard problem with exponential\ncomplexity in $M$ and $N$, in order to maximize the coverage rate of $M$ GUs by\njointly placing $N$ ABSs with limited coverage range. The problem is further\ncomplicated when the coverage range becomes irregular due to site-specific\nblockages (e.g., buildings) on the air-ground channel, and/or when the GUs are\nmoving. To address the above challenges, we study a multi-ABS movement\noptimization problem to maximize the average coverage rate of mobile GUs in a\nsite-specific environment. The Spatial Deep Learning with Multi-dimensional\nArchive of Phenotypic Elites (SDL-ME) algorithm is proposed to tackle this\nchallenging problem by 1) partitioning the complicated ABS movement problem\ninto ABS placement sub-problems each spanning finite time horizon; 2) using an\nencoder-decoder deep neural network (DNN) as the emulator to capture the\nspatial correlation of ABSs/GUs and thereby reducing the cost of interaction\nwith the actual environment; 3) employing the emulator to speed up a\nquality-diversity search for the optimal placement solution; and 4) proposing a\nplanning-exploration-serving scheme for multi-ABS movement coordination.\nNumerical results demonstrate that the proposed approach significantly\noutperforms the benchmark Deep Reinforcement Learning (DRL)-based method and\nother two baselines in terms of average coverage rate, training time and/or\nsample efficiency. Moreover, with one-time training, our proposed method can be\napplied in scenarios where the number of ABSs/GUs dynamically changes on site\nand/or with different/varying GU speeds, which is thus more robust and flexible\ncompared with conventional DRL-based methods.\n","authors":["Jiangbin Lyu","Xu Chen","Jiefeng Zhang","Liqun Fu"],"pdf_url":"https://arxiv.org/pdf/2312.10490v1.pdf","comment":"Manuscript submitted to IEEE Trans. Wireless Communications on 15\n  Jan. 2023; revised 11 Sep. 2023; accepted 5 Dec. 2023"},{"id":"http://arxiv.org/abs/2206.06004v4","updated":"2023-12-16T15:36:26Z","published":"2022-06-13T09:48:38Z","title":"A novel multi-layer modular approach for real-time fuzzy-identification\n  of gravitational-wave signals","summary":"  Advanced LIGO and Advanced Virgo ground-based interferometers are instruments\ncapable to detect gravitational wave signals exploiting advanced laser\ninterferometry techniques. The underlying data analysis task consists in\nidentifying specific patterns in noisy timeseries, but it is made extremely\ncomplex by the incredibly small amplitude of the target signals. In this\nscenario, the development of effective gravitational wave detection algorithms\nis crucial. We propose a novel layered framework for real-time detection of\ngravitational waves inspired by speech processing techniques and, in the\npresent implementation, based on a state-of-the-art machine learning approach\ninvolving a hybridization of genetic programming and neural networks. The key\naspects of the newly proposed framework are: the well structured, layered\napproach, and the low computational complexity. The paper describes the basic\nconcepts of the framework and the derivation of the first three layers. Even if\nthe layers are based on models derived using a machine learning approach, the\nproposed layered structure has a universal nature. Compared to more complex\napproaches, such as convolutional neural networks, which comprise a parameter\nset of several tens of MB and were tested exclusively for fixed length data\nsamples, our framework has lower accuracy (e.g., it identifies 45% of low\nsignal-to-noise-ration gravitational wave signals, against 65% of the\nstate-of-the-art, at a false alarm probability of $10^{-2}$), but has a much\nlower computational complexity and a higher degree of modularity. Furthermore,\nthe exploitation of short-term features makes the results of the new framework\nvirtually independent against time-position of gravitational wave signals,\nsimplifying its future exploitation in real-time multi-layer pipelines for\ngravitational-wave detection with new generation interferometers.\n","authors":["Francesco Pio Barone","Daniele Dell'Aquila","Marco Russo"],"pdf_url":"https://arxiv.org/pdf/2206.06004v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10472v1","updated":"2023-12-16T15:06:29Z","published":"2023-12-16T15:06:29Z","title":"Generalization Analysis of Policy Networks: An Example of\n  Double-Integrator","summary":"  Extensive utilization of deep reinforcement learning (DRL) policy networks in\ndiverse continuous control tasks has raised questions regarding performance\ndegradation in expansive state spaces where the input state norm is larger than\nthat in the training environment. This paper aims to uncover the underlying\nfactors contributing to such performance deterioration when dealing with\nexpanded state spaces, using a novel analysis technique known as state\ndivision. In contrast to prior approaches that employ state division merely as\na post-hoc explanatory tool, our methodology delves into the intrinsic\ncharacteristics of DRL policy networks. Specifically, we demonstrate that the\nexpansion of state space induces the activation function $\\tanh$ to exhibit\nsaturability, resulting in the transformation of the state division boundary\nfrom nonlinear to linear. Our analysis centers on the paradigm of the\ndouble-integrator system, revealing that this gradual shift towards linearity\nimparts a control behavior reminiscent of bang-bang control. However, the\ninherent linearity of the division boundary prevents the attainment of an ideal\nbang-bang control, thereby introducing unavoidable overshooting. Our\nexperimental investigations, employing diverse RL algorithms, establish that\nthis performance phenomenon stems from inherent attributes of the DRL policy\nnetwork, remaining consistent across various optimization algorithms.\n","authors":["Ruining Zhang","Haoran Han","Maolong Lv","Qisong Yang","Jian Cheng"],"pdf_url":"https://arxiv.org/pdf/2312.10472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10469v1","updated":"2023-12-16T14:59:11Z","published":"2023-12-16T14:59:11Z","title":"One step closer to unbiased aleatoric uncertainty estimation","summary":"  Neural networks are powerful tools in various applications, and quantifying\ntheir uncertainty is crucial for reliable decision-making. In the deep learning\nfield, the uncertainties are usually categorized into aleatoric (data) and\nepistemic (model) uncertainty. In this paper, we point out that the existing\npopular variance attenuation method highly overestimates aleatoric uncertainty.\nTo address this issue, we propose a new estimation method by actively\nde-noising the observed data \\footnote{Source code available at\n\\url{https://github.com/wz16/DVA}.}. By conducting a broad range of\nexperiments, we demonstrate that our proposed approach provides a much closer\napproximation to the actual data uncertainty than the standard method.\n","authors":["Wang Zhang","Ziwen Ma","Subhro Das","Tsui-Wei Weng","Alexandre Megretski","Luca Daniel","Lam M. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2312.10469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10467v1","updated":"2023-12-16T14:49:36Z","published":"2023-12-16T14:49:36Z","title":"TrojFSP: Trojan Insertion in Few-shot Prompt Tuning","summary":"  Prompt tuning is one of the most effective solutions to adapting a fixed\npre-trained language model (PLM) for various downstream tasks, especially with\nonly a few input samples. However, the security issues, e.g., Trojan attacks,\nof prompt tuning on a few data samples are not well-studied. Transferring\nestablished data poisoning attacks directly to few-shot prompt tuning presents\nmultiple challenges. One significant issue is the \\textit{poisoned imbalance\nissue}, where non-target class samples are added to the target class, resulting\nin a greater number of target-class samples compared to non-target class. While\nthis issue is not critical in regular tuning, it significantly hampers the\nfew-shot prompt tuning, making it difficult to simultaneously achieve a high\nattack success rate (ASR) and maintain clean data accuracy (CDA). Additionally,\nfew-shot prompting is prone to overfitting in terms of both ASR and CDA. In\nthis paper, we introduce \\textit{TrojFSP}, a method designed to address the\nchallenges. To solve the poisoned imbalance issue, we develop a\n\\textit{Target-Class Shrink (TC-Shrink)} technique, which aims to equalize the\nnumber of poisoning samples. To combat overfitting, we employ a\n\\textit{Selective Token Poisoning} technique to boost attack performance.\nFurthermore, we introduce a \\textit{Trojan-Trigger Attention} objective\nfunction to amplify the attention of the poisoned trojan prompt on triggers.\nExperiments show that our TrojFSP achieves an ASR of over 99\\% while\nmaintaining negligible decreases in CDA across various PLMs and datasets.\n","authors":["Mengxin Zheng","Jiaqi Xue","Xun Chen","YanShan Wang","Qian Lou","Lei Jiang"],"pdf_url":"https://arxiv.org/pdf/2312.10467v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2312.10464v1","updated":"2023-12-16T14:46:24Z","published":"2023-12-16T14:46:24Z","title":"Unveiling Empirical Pathologies of Laplace Approximation for Uncertainty\n  Estimation","summary":"  In this paper, we critically evaluate Bayesian methods for uncertainty\nestimation in deep learning, focusing on the widely applied Laplace\napproximation and its variants. Our findings reveal that the conventional\nmethod of fitting the Hessian matrix negatively impacts out-of-distribution\n(OOD) detection efficiency. We propose a different point of view, asserting\nthat focusing solely on optimizing prior precision can yield more accurate\nuncertainty estimates in OOD detection while preserving adequate calibration\nmetrics. Moreover, we demonstrate that this property is not connected to the\ntraining stage of a model but rather to its intrinsic properties. Through\nextensive experimental evaluation, we establish the superiority of our\nsimplified approach over traditional methods in the out-of-distribution domain.\n","authors":["Maksim Zhdanov","Stanislav Dereka","Sergey Kolesnikov"],"pdf_url":"https://arxiv.org/pdf/2312.10464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10458v1","updated":"2023-12-16T14:09:23Z","published":"2023-12-16T14:09:23Z","title":"Degree-based stratification of nodes in Graph Neural Networks","summary":"  Despite much research, Graph Neural Networks (GNNs) still do not display the\nfavorable scaling properties of other deep neural networks such as\nConvolutional Neural Networks and Transformers. Previous work has identified\nissues such as oversmoothing of the latent representation and have suggested\nsolutions such as skip connections and sophisticated normalization schemes.\nHere, we propose a different approach that is based on a stratification of the\ngraph nodes. We provide motivation that the nodes in a graph can be stratified\ninto those with a low degree and those with a high degree and that the two\ngroups are likely to behave differently. Based on this motivation, we modify\nthe Graph Neural Network (GNN) architecture so that the weight matrices are\nlearned, separately, for the nodes in each group. This simple-to-implement\nmodification seems to improve performance across datasets and GNN methods. To\nverify that this increase in performance is not only due to the added capacity,\nwe also perform the same modification for random splits of the nodes, which\ndoes not lead to any improvement.\n","authors":["Ameen Ali","Hakan Cevikalp","Lior Wolf"],"pdf_url":"https://arxiv.org/pdf/2312.10458v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06290v2","updated":"2023-12-16T13:37:35Z","published":"2023-12-11T10:44:52Z","title":"Exploiting Label Skews in Federated Learning with Model Concatenation","summary":"  Federated Learning (FL) has emerged as a promising solution to perform deep\nlearning on different data owners without exchanging raw data. However, non-IID\ndata has been a key challenge in FL, which could significantly degrade the\naccuracy of the final model. Among different non-IID types, label skews have\nbeen challenging and common in image classification and other tasks. Instead of\naveraging the local models in most previous studies, we propose FedConcat, a\nsimple and effective approach that concatenates these local models as the base\nof the global model to effectively aggregate the local knowledge. To reduce the\nsize of the global model, we adopt the clustering technique to group the\nclients by their label distributions and collaboratively train a model inside\neach cluster. We theoretically analyze the advantage of concatenation over\naveraging by analyzing the information bottleneck of deep neural networks.\nExperimental results demonstrate that FedConcat achieves significantly higher\naccuracy than previous state-of-the-art FL methods in various heterogeneous\nlabel skew distribution settings and meanwhile has lower communication costs.\nOur code is publicly available at https://github.com/sjtudyq/FedConcat.\n","authors":["Yiqun Diao","Qinbin Li","Bingsheng He"],"pdf_url":"https://arxiv.org/pdf/2312.06290v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10440v1","updated":"2023-12-16T13:15:44Z","published":"2023-12-16T13:15:44Z","title":"Weight-Entanglement Meets Gradient-Based Neural Architecture Search","summary":"  Weight sharing is a fundamental concept in neural architecture search (NAS),\nenabling gradient-based methods to explore cell-based architecture spaces\nsignificantly faster than traditional blackbox approaches. In parallel, weight\n\\emph{entanglement} has emerged as a technique for intricate parameter sharing\namong architectures within macro-level search spaces. %However, the macro\nstructure of such spaces poses compatibility challenges for gradient-based NAS\nmethods. %As a result, blackbox optimization methods have been commonly\nemployed, particularly in conjunction with supernet training, to maintain\nsearch efficiency. %Due to the inherent differences in the structure of these\nsearch spaces, these Since weight-entanglement poses compatibility challenges\nfor gradient-based NAS methods, these two paradigms have largely developed\nindependently in parallel sub-communities. This paper aims to bridge the gap\nbetween these sub-communities by proposing a novel scheme to adapt\ngradient-based methods for weight-entangled spaces. This enables us to conduct\nan in-depth comparative assessment and analysis of the performance of\ngradient-based NAS in weight-entangled search spaces. Our findings reveal that\nthis integration of weight-entanglement and gradient-based NAS brings forth the\nvarious benefits of gradient-based methods (enhanced performance, improved\nsupernet training properties and superior any-time performance), while\npreserving the memory efficiency of weight-entangled spaces. The code for our\nwork is openly accessible\n\\href{https://anonymous.4open.science/r/TangleNAS-527C}{here}\n","authors":["Rhea Sanjay Sukthanker","Arjun Krishnakumar","Mahmoud Safari","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2312.10440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10431v1","updated":"2023-12-16T12:21:03Z","published":"2023-12-16T12:21:03Z","title":"Continuous Diffusion for Mixed-Type Tabular Data","summary":"  Score-based generative models (or diffusion models for short) have proven\nsuccessful across many domains in generating text and image data. However, the\nconsideration of mixed-type tabular data with this model family has fallen\nshort so far. Existing research mainly combines different diffusion processes\nwithout explicitly accounting for the feature heterogeneity inherent to tabular\ndata. In this paper, we combine score matching and score interpolation to\nensure a common type of continuous noise distribution that affects both\ncontinuous and categorical features alike. Further, we investigate the impact\nof distinct noise schedules per feature or per data type. We allow for\nadaptive, learnable noise schedules to ensure optimally allocated model\ncapacity and balanced generative capability. Results show that our model\nconsistently outperforms state-of-the-art benchmark models and that accounting\nfor heterogeneity within the noise schedule design boosts the sample quality.\n","authors":["Markus Mueller","Kathrin Gruber","Dennis Fok"],"pdf_url":"https://arxiv.org/pdf/2312.10431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.03281v2","updated":"2023-12-16T12:20:16Z","published":"2022-01-10T10:54:11Z","title":"IoTGAN: GAN Powered Camouflage Against Machine Learning Based IoT Device\n  Identification","summary":"  With the proliferation of IoT devices, researchers have developed a variety\nof IoT device identification methods with the assistance of machine learning.\nNevertheless, the security of these identification methods mostly depends on\ncollected training data. In this research, we propose a novel attack strategy\nnamed IoTGAN to manipulate an IoT device's traffic such that it can evade\nmachine learning based IoT device identification. In the development of IoTGAN,\nwe have two major technical challenges: (i) How to obtain the discriminative\nmodel in a black-box setting, and (ii) How to add perturbations to IoT traffic\nthrough the manipulative model, so as to evade the identification while not\ninfluencing the functionality of IoT devices. To address these challenges, a\nneural network based substitute model is used to fit the target model in\nblack-box settings, it works as a discriminative model in IoTGAN. A\nmanipulative model is trained to add adversarial perturbations into the IoT\ndevice's traffic to evade the substitute model. Experimental results show that\nIoTGAN can successfully achieve the attack goals. We also develop efficient\ncountermeasures to protect machine learning based IoT device identification\nfrom been undermined by IoTGAN.\n","authors":["Tao Hou","Tao Wang","Zhuo Lu","Yao Liu","Yalin Sagduyu"],"pdf_url":"https://arxiv.org/pdf/2201.03281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.13709v4","updated":"2023-12-16T12:01:57Z","published":"2023-07-24T20:56:42Z","title":"Neural Bradley-Terry Rating: Quantifying Properties from Comparisons","summary":"  Many properties in the real world doesn't have metrics and can't be\nnumerically observed, making them difficult to learn. To deal with this\nchallenging problem, prior works have primarily focused on estimating those\nproperties by using graded human scores as the target label in the training.\nMeanwhile, rating algorithms based on the Bradley-Terry model are extensively\nstudied to evaluate the competitiveness of players based on their match\nhistory. In this paper, we introduce the Neural Bradley-Terry Rating (NBTR), a\nnovel machine learning framework designed to quantify and evaluate properties\nof unknown items. Our method seamlessly integrates the Bradley-Terry model into\nthe neural network structure. Moreover, we generalize this architecture further\nto asymmetric environments with unfairness, a condition more commonly\nencountered in real-world settings. Through experimental analysis, we\ndemonstrate that NBTR successfully learns to quantify and estimate desired\nproperties.\n","authors":["Satoru Fujii"],"pdf_url":"https://arxiv.org/pdf/2307.13709v4.pdf","comment":"Appearing on ICAART 2024"},{"id":"http://arxiv.org/abs/2312.10425v1","updated":"2023-12-16T11:40:49Z","published":"2023-12-16T11:40:49Z","title":"Take History as a Mirror in Heterogeneous Federated Learning","summary":"  Federated Learning (FL) allows several clients to cooperatively train machine\nlearning models without disclosing the raw data. In practice, due to the system\nand statistical heterogeneity among devices, synchronous FL often encounters\nthe straggler effect. In contrast, asynchronous FL can mitigate this problem,\nmaking it suitable for scenarios involving numerous participants. However,\nNon-IID data and stale models present significant challenges to asynchronous\nFL, as they would diminish the practicality of the global model and even lead\nto training failures. In this work, we propose a novel asynchronous FL\nframework called Federated Historical Learning (FedHist), which effectively\naddresses the challenges posed by both Non-IID data and gradient staleness.\nFedHist enhances the stability of local gradients by performing weighted fusion\nwith historical global gradients cached on the server. Relying on hindsight, it\nassigns aggregation weights to each participant in a multi-dimensional manner\nduring each communication round. To further enhance the efficiency and\nstability of the training process, we introduce an intelligent $\\ell_2$-norm\namplification scheme, which dynamically regulates the learning progress based\non the $\\ell_2$-norms of the submitted gradients. Extensive experiments\ndemonstrate that FedHist outperforms state-of-the-art methods in terms of\nconvergence performance and test accuracy.\n","authors":["Xiaorui Jiang","Hengwei Xu","Yu Gao","Yong Liao","Pengyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2312.10425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10424v1","updated":"2023-12-16T11:33:12Z","published":"2023-12-16T11:33:12Z","title":"A Concentration Bound for TD(0) with Function Approximation","summary":"  We derive a concentration bound of the type `for all $n \\geq n_0$ for some\n$n_0$' for TD(0) with linear function approximation. We work with online TD\nlearning with samples from a single sample path of the underlying Markov chain.\nThis makes our analysis significantly different from offline TD learning or TD\nlearning with access to independent samples from the stationary distribution of\nthe Markov chain. We treat TD(0) as a contractive stochastic approximation\nalgorithm, with both martingale and Markov noises. Markov noise is handled\nusing the Poisson equation and the lack of almost sure guarantees on\nboundedness of iterates is handled using the concept of relaxed concentration\ninequalities.\n","authors":["Siddharth Chandak","Vivek S. Borkar"],"pdf_url":"https://arxiv.org/pdf/2312.10424v1.pdf","comment":"Submitted to Stochastic Systems"},{"id":"http://arxiv.org/abs/2312.10423v1","updated":"2023-12-16T11:32:28Z","published":"2023-12-16T11:32:28Z","title":"Stochastic Bayesian Optimization with Unknown Continuous Context\n  Distribution via Kernel Density Estimation","summary":"  Bayesian optimization (BO) is a sample-efficient method and has been widely\nused for optimizing expensive black-box functions. Recently, there has been a\nconsiderable interest in BO literature in optimizing functions that are\naffected by context variable in the environment, which is uncontrollable by\ndecision makers. In this paper, we focus on the optimization of functions'\nexpectations over continuous context variable, subject to an unknown\ndistribution. To address this problem, we propose two algorithms that employ\nkernel density estimation to learn the probability density function (PDF) of\ncontinuous context variable online. The first algorithm is simpler, which\ndirectly optimizes the expectation under the estimated PDF. Considering that\nthe estimated PDF may have high estimation error when the true distribution is\ncomplicated, we further propose the second algorithm that optimizes the\ndistributionally robust objective. Theoretical results demonstrate that both\nalgorithms have sub-linear Bayesian cumulative regret on the expectation\nobjective. Furthermore, we conduct numerical experiments to empirically\ndemonstrate the effectiveness of our algorithms.\n","authors":["Xiaobin Huang","Lei Song","Ke Xue","Chao Qian"],"pdf_url":"https://arxiv.org/pdf/2312.10423v1.pdf","comment":"AAAI 2024 Accept"},{"id":"http://arxiv.org/abs/2204.01682v3","updated":"2023-12-16T11:17:51Z","published":"2022-04-04T17:51:49Z","title":"Deep Feature Screening: Feature Selection for Ultra High-Dimensional\n  Data via Deep Neural Networks","summary":"  The applications of traditional statistical feature selection methods to\nhigh-dimension, low sample-size data often struggle and encounter challenging\nproblems, such as overfitting, curse of dimensionality, computational\ninfeasibility, and strong model assumption. In this paper, we propose a novel\ntwo-step nonparametric approach called Deep Feature Screening (DeepFS) that can\novercome these problems and identify significant features with high precision\nfor ultra high-dimensional, low-sample-size data. This approach first extracts\na low-dimensional representation of input data and then applies feature\nscreening based on multivariate rank distance correlation recently developed by\nDeb and Sen (2021). This approach combines the strengths of both deep neural\nnetworks and feature screening, and thereby has the following appealing\nfeatures in addition to its ability of handling ultra high-dimensional data\nwith small number of samples: (1) it is model free and distribution free; (2)\nit can be used for both supervised and unsupervised feature selection; and (3)\nit is capable of recovering the original input data. The superiority of DeepFS\nis demonstrated via extensive simulation studies and real data analyses.\n","authors":["Kexuan Li","Fangfang Wang","Lingli Yang","Ruiqi Liu"],"pdf_url":"https://arxiv.org/pdf/2204.01682v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.03747v2","updated":"2023-12-16T11:15:29Z","published":"2023-01-10T01:55:55Z","title":"Semiparametric Regression for Spatial Data via Deep Learning","summary":"  In this work, we propose a deep learning-based method to perform\nsemiparametric regression analysis for spatially dependent data. To be\nspecific, we use a sparsely connected deep neural network with rectified linear\nunit (ReLU) activation function to estimate the unknown regression function\nthat describes the relationship between response and covariates in the presence\nof spatial dependence. Under some mild conditions, the estimator is proven to\nbe consistent, and the rate of convergence is determined by three factors: (1)\nthe architecture of neural network class, (2) the smoothness and (intrinsic)\ndimension of true mean function, and (3) the magnitude of spatial dependence.\nOur method can handle well large data set owing to the stochastic gradient\ndescent optimization algorithm. Simulation studies on synthetic data are\nconducted to assess the finite sample performance, the results of which\nindicate that the proposed method is capable of picking up the intricate\nrelationship between response and covariates. Finally, a real data analysis is\nprovided to demonstrate the validity and effectiveness of the proposed method.\n","authors":["Kexuan Li","Jun Zhu","Anthony R. Ives","Volker C. Radeloff","Fangfang Wang"],"pdf_url":"https://arxiv.org/pdf/2301.03747v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10418v1","updated":"2023-12-16T11:13:40Z","published":"2023-12-16T11:13:40Z","title":"Fractional Deep Reinforcement Learning for Age-Minimal Mobile Edge\n  Computing","summary":"  Mobile edge computing (MEC) is a promising paradigm for real-time\napplications with intensive computational needs (e.g., autonomous driving), as\nit can reduce the processing delay. In this work, we focus on the timeliness of\ncomputational-intensive updates, measured by Age-ofInformation (AoI), and study\nhow to jointly optimize the task updating and offloading policies for AoI with\nfractional form. Specifically, we consider edge load dynamics and formulate a\ntask scheduling problem to minimize the expected time-average AoI. The\nuncertain edge load dynamics, the nature of the fractional objective, and\nhybrid continuous-discrete action space (due to the joint optimization) make\nthis problem challenging and existing approaches not directly applicable. To\nthis end, we propose a fractional reinforcement learning(RL) framework and\nprove its convergence. We further design a model-free fractional deep RL (DRL)\nalgorithm, where each device makes scheduling decisions with the hybrid action\nspace without knowing the system dynamics and decisions of other devices.\nExperimental results show that our proposed algorithms reduce the average AoI\nby up to 57.6% compared with several non-fractional benchmarks.\n","authors":["Ming Tang","Lyudong Jin","Meng Zhang","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2312.10418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08763v2","updated":"2023-12-16T10:46:48Z","published":"2023-12-14T09:16:01Z","title":"Learning from Polar Representation: An Extreme-Adaptive Model for\n  Long-Term Time Series Forecasting","summary":"  In the hydrology field, time series forecasting is crucial for efficient\nwater resource management, improving flood and drought control and increasing\nthe safety and quality of life for the general population. However, predicting\nlong-term streamflow is a complex task due to the presence of extreme events.\nIt requires the capture of long-range dependencies and the modeling of rare but\nimportant extreme values. Existing approaches often struggle to tackle these\ndual challenges simultaneously. In this paper, we specifically delve into these\nissues and propose Distance-weighted Auto-regularized Neural network (DAN), a\nnovel extreme-adaptive model for long-range forecasting of stremflow enhanced\nby polar representation learning. DAN utilizes a distance-weighted multi-loss\nmechanism and stackable blocks to dynamically refine indicator sequences from\nexogenous data, while also being able to handle uni-variate time-series by\nemploying Gaussian Mixture probability modeling to improve robustness to severe\nevents. We also introduce Kruskal-Wallis sampling and gate control vectors to\nhandle imbalanced extreme data. On four real-life hydrologic streamflow\ndatasets, we demonstrate that DAN significantly outperforms both\nstate-of-the-art hydrologic time series prediction methods and general methods\ndesigned for long-term time series prediction.\n","authors":["Yanhong Li","Jack Xu","David C. Anastasiu"],"pdf_url":"https://arxiv.org/pdf/2312.08763v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10411v1","updated":"2023-12-16T10:35:06Z","published":"2023-12-16T10:35:06Z","title":"Towards Reliable Participation in UAV-Enabled Federated Edge Learning on\n  Non-IID Data","summary":"  Federated Learning (FL) is a decentralized machine learning (ML) technique\nthat allows a number of participants to train an ML model collaboratively\nwithout having to share their private local datasets with others. When\nparticipants are unmanned aerial vehicles (UAVs), UAV-enabled FL would\nexperience heterogeneity due to the majorly skewed (non-independent and\nidentically distributed -IID) collected data. In addition, UAVs may demonstrate\nunintentional misbehavior in which the latter may fail to send updates to the\nFL server due, for instance, to UAVs' disconnectivity from the FL system caused\nby high mobility, unavailability, or battery depletion. Such challenges may\nsignificantly affect the convergence of the FL model. A recent way to tackle\nthese challenges is client selection, based on customized criteria that\nconsider UAV computing power and energy consumption. However, most existing\nclient selection schemes neglected the participants' reliability. Indeed, FL\ncan be targeted by poisoning attacks, in which malicious UAVs upload poisonous\nlocal models to the FL server, by either providing targeted false predictions\nfor specifically chosen inputs or by compromising the global model's accuracy\nthrough tampering with the local model. Hence, we propose in this paper a novel\nclient selection scheme that enhances convergence by prioritizing fast UAVs\nwith high-reliability scores, while eliminating malicious UAVs from training.\nThrough experiments, we assess the effectiveness of our scheme in resisting\ndifferent attack scenarios, in terms of convergence and achieved model\naccuracy. Finally, we demonstrate the performance superiority of the proposed\napproach compared to baseline methods.\n","authors":["Youssra Cheriguene","Wael Jaafar","Halim Yanikomeroglu","Chaker Abdelaziz Kerrache"],"pdf_url":"https://arxiv.org/pdf/2312.10411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10407v1","updated":"2023-12-16T10:17:09Z","published":"2023-12-16T10:17:09Z","title":"DeepArt: A Benchmark to Advance Fidelity Research in AI-Generated\n  Content","summary":"  This paper explores the image synthesis capabilities of GPT-4, a leading\nmulti-modal large language model. We establish a benchmark for evaluating the\nfidelity of texture features in images generated by GPT-4, comprising manually\npainted pictures and their AI-generated counterparts. The contributions of this\nstudy are threefold: First, we provide an in-depth analysis of the fidelity of\nimage synthesis features based on GPT-4, marking the first such study on this\nstate-of-the-art model. Second, the quantitative and qualitative experiments\nfully reveals the limitations of the GPT-4 model in image synthesis. Third, we\nhave compiled a unique benchmark of manual drawings and corresponding\nGPT-4-generated images, introducing a new task to advance fidelity research in\nAI-generated content (AIGC). The dataset will be available after being\naccepted: \\url{https://github.com/rickwang28574/DeepArt}. We hope this study\nwill fuel knowledge, scholarship, and innovation, inspiring uses that transform\nhow we discover and understand the world of art and promote the development of\nAIGC while retaining respect for art.\n","authors":["Wentao Wang","Xuanyao Huang","Swalpa Kumar Roy"],"pdf_url":"https://arxiv.org/pdf/2312.10407v1.pdf","comment":"This is the initial version of this work, and a more comprehensive\n  and improved version will be updated later"},{"id":"http://arxiv.org/abs/2312.10401v1","updated":"2023-12-16T10:05:18Z","published":"2023-12-16T10:05:18Z","title":"Rethinking Dimensional Rationale in Graph Contrastive Learning from\n  Causal Perspective","summary":"  Graph contrastive learning is a general learning paradigm excelling at\ncapturing invariant information from diverse perturbations in graphs. Recent\nworks focus on exploring the structural rationale from graphs, thereby\nincreasing the discriminability of the invariant information. However, such\nmethods may incur in the mis-learning of graph models towards the\ninterpretability of graphs, and thus the learned noisy and task-agnostic\ninformation interferes with the prediction of graphs. To this end, with the\npurpose of exploring the intrinsic rationale of graphs, we accordingly propose\nto capture the dimensional rationale from graphs, which has not received\nsufficient attention in the literature. The conducted exploratory experiments\nattest to the feasibility of the aforementioned roadmap. To elucidate the\ninnate mechanism behind the performance improvement arising from the\ndimensional rationale, we rethink the dimensional rationale in graph\ncontrastive learning from a causal perspective and further formalize the\ncausality among the variables in the pre-training stage to build the\ncorresponding structural causal model. On the basis of the understanding of the\nstructural causal model, we propose the dimensional rationale-aware graph\ncontrastive learning approach, which introduces a learnable dimensional\nrationale acquiring network and a redundancy reduction constraint. The\nlearnable dimensional rationale acquiring network is updated by leveraging a\nbi-level meta-learning technique, and the redundancy reduction constraint\ndisentangles the redundant features through a decorrelation process during\nlearning. Empirically, compared with state-of-the-art methods, our method can\nyield significant performance boosts on various benchmarks with respect to\ndiscriminability and transferability. The code implementation of our method is\navailable at https://github.com/ByronJi/DRGCL.\n","authors":["Qirui Ji","Jiangmeng Li","Jie Hu","Rui Wang","Changwen Zheng","Fanjiang Xu"],"pdf_url":"https://arxiv.org/pdf/2312.10401v1.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2312.10396v1","updated":"2023-12-16T09:49:31Z","published":"2023-12-16T09:49:31Z","title":"How Far Can Fairness Constraints Help Recover From Biased Data?","summary":"  Blum & Stangl (2019) propose a data bias model to simulate\nunder-representation and label bias in underprivileged population. For a\nstylized data distribution with i.i.d. label noise, under certain simple\nconditions on the bias parameters, they show that fair classification with\nequal opportunity constraints even on extremely biased distribution can recover\nan optimally accurate and fair classifier on the original distribution.\nAlthough their distribution is stylized, their result is interesting because it\ndemonstrates that fairness constraints can implicitly rectify data bias and\nsimultaneously overcome a perceived fairness-accuracy trade-off. In this paper,\nwe give an alternate proof of their result using threshold-based\ncharacterization of optimal fair classifiers. Moreover, we show that their\nconditions on the bias parameters are both necessary and sufficient for their\nrecovery result. Our technique is arguably more flexible, as it readily extends\nto more general distributions, e.g., when the labels in the original\ndistribution have Massart noise instead of i.i.d. noise. Finally, we prove that\nfor any data distribution, if the optimally accurate classifier in a hypothesis\nclass is fair and robust, then it can be recovered through fair classification\non the biased distribution, whenever the bias parameters satisfy certain simple\nconditions.\n","authors":["Mohit Sharma","Amit Deshpande"],"pdf_url":"https://arxiv.org/pdf/2312.10396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.00369v2","updated":"2023-12-16T09:38:26Z","published":"2022-11-01T10:34:27Z","title":"A General Search-based Framework for Generating Textual Counterfactual\n  Explanations","summary":"  One of the prominent methods for explaining the decision of a\nmachine-learning classifier is by a counterfactual example. Most current\nalgorithms for generating such examples in the textual domain are based on\ngenerative language models. Generative models, however, are trained to minimize\na specific loss function in order to fulfill certain requirements for the\ngenerated texts. Any change in the requirements may necessitate costly\nretraining, thus potentially limiting their applicability. In this paper, we\npresent a general search-based framework for generating counterfactual\nexplanations in the textual domain. Our framework is model-agnostic,\ndomain-agnostic, anytime, and does not require retraining in order to adapt to\nchanges in the user requirements. We model the task as a search problem in a\nspace where the initial state is the classified text, and the goal state is a\ntext in a given target class. Our framework includes domain-independent\nmodification operators, but can also exploit domain-specific knowledge through\nspecialized operators. The search algorithm attempts to find a text from the\ntarget class with minimal user-specified distance from the original classified\nobject.\n","authors":["Daniel Gilo","Shaul Markovitch"],"pdf_url":"https://arxiv.org/pdf/2211.00369v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10393v1","updated":"2023-12-16T09:36:54Z","published":"2023-12-16T09:36:54Z","title":"Lecture Notes in Probabilistic Diffusion Models","summary":"  Diffusion models are loosely modelled based on non-equilibrium\nthermodynamics, where \\textit{diffusion} refers to particles flowing from\nhigh-concentration regions towards low-concentration regions. In statistics,\nthe meaning is quite similar, namely the process of transforming a complex\ndistribution $p_{\\text{complex}}$ on $\\mathbb{R}^d$ to a simple distribution\n$p_{\\text{prior}}$ on the same domain. This constitutes a Markov chain of\ndiffusion steps of slowly adding random noise to data, followed by a reverse\ndiffusion process in which the data is reconstructed from the noise. The\ndiffusion model learns the data manifold to which the original and thus the\nreconstructed data samples belong, by training on a large number of data\npoints. While the diffusion process pushes a data sample off the data manifold,\nthe reverse process finds a trajectory back to the data manifold. Diffusion\nmodels have -- unlike variational autoencoder and flow models -- latent\nvariables with the same dimensionality as the original data, and they are\ncurrently\\footnote{At the time of writing, 2023.} outperforming other\napproaches -- including Generative Adversarial Networks (GANs) -- to modelling\nthe distribution of, e.g., natural images.\n","authors":["Inga Strümke","Helge Langseth"],"pdf_url":"https://arxiv.org/pdf/2312.10393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10386v1","updated":"2023-12-16T08:51:52Z","published":"2023-12-16T08:51:52Z","title":"RedCore: Relative Advantage Aware Cross-modal Representation Learning\n  for Missing Modalities with Imbalanced Missing Rates","summary":"  Multimodal learning is susceptible to modality missing, which poses a major\nobstacle for its practical applications and, thus, invigorates increasing\nresearch interest. In this paper, we investigate two challenging problems: 1)\nwhen modality missing exists in the training data, how to exploit the\nincomplete samples while guaranteeing that they are properly supervised? 2)\nwhen the missing rates of different modalities vary, causing or exacerbating\nthe imbalance among modalities, how to address the imbalance and ensure all\nmodalities are well-trained? To tackle these two challenges, we first introduce\nthe variational information bottleneck (VIB) method for the cross-modal\nrepresentation learning of missing modalities, which capitalizes on the\navailable modalities and the labels as supervision. Then, accounting for the\nimbalanced missing rates, we define relative advantage to quantify the\nadvantage of each modality over others. Accordingly, a bi-level optimization\nproblem is formulated to adaptively regulate the supervision of all modalities\nduring training. As a whole, the proposed approach features \\textbf{Re}lative\na\\textbf{d}vantage aware \\textbf{C}ross-m\\textbf{o}dal \\textbf{r}epresentation\nl\\textbf{e}arning (abbreviated as \\textbf{RedCore}) for missing modalities with\nimbalanced missing rates. Extensive empirical results demonstrate that RedCore\noutperforms competing models in that it exhibits superior robustness against\neither large or imbalanced missing rates.\n","authors":["Jun Sun","Xinxin Zhang","Shoukang Han","Yu-ping Ruan","Taihao Li"],"pdf_url":"https://arxiv.org/pdf/2312.10386v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10385v1","updated":"2023-12-16T08:48:46Z","published":"2023-12-16T08:48:46Z","title":"Imitate the Good and Avoid the Bad: An Incremental Approach to Safe\n  Reinforcement Learning","summary":"  A popular framework for enforcing safe actions in Reinforcement Learning (RL)\nis Constrained RL, where trajectory based constraints on expected cost (or\nother cost measures) are employed to enforce safety and more importantly these\nconstraints are enforced while maximizing expected reward. Most recent\napproaches for solving Constrained RL convert the trajectory based cost\nconstraint into a surrogate problem that can be solved using minor\nmodifications to RL methods. A key drawback with such approaches is an over or\nunderestimation of the cost constraint at each state. Therefore, we provide an\napproach that does not modify the trajectory based cost constraint and instead\nimitates ``good'' trajectories and avoids ``bad'' trajectories generated from\nincrementally improving policies. We employ an oracle that utilizes a reward\nthreshold (which is varied with learning) and the overall cost constraint to\nlabel trajectories as ``good'' or ``bad''. A key advantage of our approach is\nthat we are able to work from any starting policy or set of trajectories and\nimprove on it. In an exhaustive set of experiments, we demonstrate that our\napproach is able to outperform top benchmark approaches for solving Constrained\nRL problems, with respect to expected cost, CVaR cost, or even unknown cost\nconstraints.\n","authors":["Huy Hoang","Tien Mai Pradeep Varakantham"],"pdf_url":"https://arxiv.org/pdf/2312.10385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10380v1","updated":"2023-12-16T08:32:29Z","published":"2023-12-16T08:32:29Z","title":"PPIDSG: A Privacy-Preserving Image Distribution Sharing Scheme with GAN\n  in Federated Learning","summary":"  Federated learning (FL) has attracted growing attention since it allows for\nprivacy-preserving collaborative training on decentralized clients without\nexplicitly uploading sensitive data to the central server. However, recent\nworks have revealed that it still has the risk of exposing private data to\nadversaries. In this paper, we conduct reconstruction attacks and enhance\ninference attacks on various datasets to better understand that sharing trained\nclassification model parameters to a central server is the main problem of\nprivacy leakage in FL. To tackle this problem, a privacy-preserving image\ndistribution sharing scheme with GAN (PPIDSG) is proposed, which consists of a\nblock scrambling-based encryption algorithm, an image distribution sharing\nmethod, and local classification training. Specifically, our method can capture\nthe distribution of a target image domain which is transformed by the block\nencryption algorithm, and upload generator parameters to avoid classifier\nsharing with negligible influence on model performance. Furthermore, we apply a\nfeature extractor to motivate model utility and train it separately from the\nclassifier. The extensive experimental results and security analyses\ndemonstrate the superiority of our proposed scheme compared to other\nstate-of-the-art defense methods. The code is available at\nhttps://github.com/ytingma/PPIDSG.\n","authors":["Yuting Ma","Yuanzhi Yao","Xiaohua Xu"],"pdf_url":"https://arxiv.org/pdf/2312.10380v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.10375v1","updated":"2023-12-16T08:23:12Z","published":"2023-12-16T08:23:12Z","title":"Collect and Connect Data Leaves to Feature Concepts: Interactive Graph\n  Generation Toward Well-being","summary":"  Feature concepts and data leaves have been invented using datasets to foster\ncreative thoughts for creating well-being in daily life. The idea, simply put,\nis to attach selected and collected data leaves that are summaries of event\nflows to be discovered from corresponding datasets, on the target feature\nconcept representing the well-being aimed. A graph of existing or expected\ndatasets to be attached to a feature concept is generated semi-automatically.\nRather than sheer automated generative AI, our work addresses the process of\ngenerative artificial and natural intelligence to create the basis for data use\nand reuse.\n","authors":["Yukio Ohsawa","Tomohide Maekawa","Hiroki Yamaguchi","Hiro Yoshida","Kaira Sekiguchi"],"pdf_url":"https://arxiv.org/pdf/2312.10375v1.pdf","comment":"2 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2312.10374v1","updated":"2023-12-16T08:18:39Z","published":"2023-12-16T08:18:39Z","title":"Neural Operators for Boundary Stabilization of Stop-and-go Traffic","summary":"  This paper introduces a novel approach to PDE boundary control design using\nneural operators to alleviate stop-and-go instabilities in congested traffic\nflow. Our framework leverages neural operators to design control strategies for\ntraffic flow systems. The traffic dynamics are described by the Aw-Rascle-Zhang\n(ARZ) model, which comprises a set of second-order coupled hyperbolic partial\ndifferential equations (PDEs). Backstepping method is widely used for boundary\ncontrol of such PDE systems. The PDE model-based control design can be\ntime-consuming and require intensive depth of expertise since it involves\nconstructing and solving backstepping control kernels. To overcome these\nchallenges, we present two distinct neural operator (NO) learning schemes aimed\nat stabilizing the traffic PDE system. The first scheme embeds NO-approximated\ngain kernels within a predefined backstepping controller, while the second one\ndirectly learns a boundary control law. The Lyapunov analysis is conducted to\nevaluate the stability of the NO-approximated gain kernels and control law. It\nis proved that the NO-based closed-loop system is practical stable under\ncertain approximation accuracy conditions in NO-learning. To validate the\nefficacy of the proposed approach, simulations are conducted to compare the\nperformance of the two neural operator controllers with a PDE backstepping\ncontroller and a Proportional Integral (PI) controller. While the\nNO-approximated methods exhibit higher errors compared to the backstepping\ncontroller, they consistently outperform the PI controller, demonstrating\nfaster computation speeds across all scenarios. This result suggests that\nneural operators can significantly expedite and simplify the process of\nobtaining boundary controllers in traffic PDE systems.\n","authors":["Yihuai Zhang","Ruiguo Zhong","Huan Yu"],"pdf_url":"https://arxiv.org/pdf/2312.10374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10370v1","updated":"2023-12-16T08:08:36Z","published":"2023-12-16T08:08:36Z","title":"Do Similar Entities have Similar Embeddings?","summary":"  Knowledge graph embedding models (KGEMs) developed for link prediction learn\nvector representations for graph entities, known as embeddings. A common tacit\nassumption is the KGE entity similarity assumption, which states that these\nKGEMs retain the graph's structure within their embedding space, i.e., position\nsimilar entities close to one another. This desirable property make KGEMs\nwidely used in downstream tasks such as recommender systems or drug\nrepurposing. Yet, the alignment of graph similarity with embedding space\nsimilarity has rarely been formally evaluated. Typically, KGEMs are assessed\nbased on their sole link prediction capabilities, using ranked-based metrics\nsuch as Hits@K or Mean Rank. This paper challenges the prevailing assumption\nthat entity similarity in the graph is inherently mirrored in the embedding\nspace. Therefore, we conduct extensive experiments to measure the capability of\nKGEMs to cluster similar entities together, and investigate the nature of the\nunderlying factors. Moreover, we study if different KGEMs expose a different\nnotion of similarity. Datasets, pre-trained embeddings and code are available\nat: https://github.com/nicolas-hbt/similar-embeddings.\n","authors":["Nicolas Hubert","Heiko Paulheim","Armelle Brun","Davy Monticolo"],"pdf_url":"https://arxiv.org/pdf/2312.10370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05705v2","updated":"2023-12-16T07:37:37Z","published":"2023-12-09T23:13:32Z","title":"Structured Inverse-Free Natural Gradient: Memory-Efficient &\n  Numerically-Stable KFAC for Large Neural Nets","summary":"  Second-order methods for deep learning -- such as KFAC -- can be useful for\nneural net training. However, they are often memory-inefficient and numerically\nunstable for low-precision training since their preconditioning Kronecker\nfactors are dense, and require high-precision matrix inversion or\ndecomposition. Consequently, such methods are not widely used for training\nlarge neural networks such as transformer-based models. We address these two\nissues by (i) formulating an inverse-free update of KFAC and (ii) imposing\nstructures in each of the Kronecker factors, resulting in a method we term\nstructured inverse-free natural gradient descent (SINGD). On large modern\nneural networks, we show that, in contrast to KFAC, SINGD is memory efficient\nand numerically robust, and often outperforms AdamW even in half precision.\nHence, our work closes a gap between first-order and second-order methods in\nmodern low precision training for large neural nets.\n","authors":["Wu Lin","Felix Dangel","Runa Eschenhagen","Kirill Neklyudov","Agustinus Kristiadi","Richard E. Turner","Alireza Makhzani"],"pdf_url":"https://arxiv.org/pdf/2312.05705v2.pdf","comment":"updated Sec 3.2 to include more discussion about challenges of\n  proposing a structured and inverse-free update rule"}],"Multimedia":[{"id":"http://arxiv.org/abs/2312.10493v1","updated":"2023-12-16T16:14:50Z","published":"2023-12-16T16:14:50Z","title":"Debiasing Multimodal Sarcasm Detection with Contrastive Learning","summary":"  Despite commendable achievements made by existing work, prevailing multimodal\nsarcasm detection studies rely more on textual content over visual information.\nIt unavoidably induces spurious correlations between textual words and labels,\nthereby significantly hindering the models' generalization capability. To\naddress this problem, we define the task of out-of-distribution (OOD)\nmultimodal sarcasm detection, which aims to evaluate models' generalizability\nwhen the word distribution is different in training and testing settings.\nMoreover, we propose a novel debiasing multimodal sarcasm detection framework\nwith contrastive learning, which aims to mitigate the harmful effect of biased\ntextual factors for robust OOD generalization. In particular, we first design\ncounterfactual data augmentation to construct the positive samples with\ndissimilar word biases and negative samples with similar word biases.\nSubsequently, we devise an adapted debiasing contrastive learning mechanism to\nempower the model to learn robust task-relevant features and alleviate the\nadverse effect of biased words. Extensive experiments show the superiority of\nthe proposed framework.\n","authors":["Mengzhao Jia","Can Xie","Liqiang Jing"],"pdf_url":"https://arxiv.org/pdf/2312.10493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10407v1","updated":"2023-12-16T10:17:09Z","published":"2023-12-16T10:17:09Z","title":"DeepArt: A Benchmark to Advance Fidelity Research in AI-Generated\n  Content","summary":"  This paper explores the image synthesis capabilities of GPT-4, a leading\nmulti-modal large language model. We establish a benchmark for evaluating the\nfidelity of texture features in images generated by GPT-4, comprising manually\npainted pictures and their AI-generated counterparts. The contributions of this\nstudy are threefold: First, we provide an in-depth analysis of the fidelity of\nimage synthesis features based on GPT-4, marking the first such study on this\nstate-of-the-art model. Second, the quantitative and qualitative experiments\nfully reveals the limitations of the GPT-4 model in image synthesis. Third, we\nhave compiled a unique benchmark of manual drawings and corresponding\nGPT-4-generated images, introducing a new task to advance fidelity research in\nAI-generated content (AIGC). The dataset will be available after being\naccepted: \\url{https://github.com/rickwang28574/DeepArt}. We hope this study\nwill fuel knowledge, scholarship, and innovation, inspiring uses that transform\nhow we discover and understand the world of art and promote the development of\nAIGC while retaining respect for art.\n","authors":["Wentao Wang","Xuanyao Huang","Swalpa Kumar Roy"],"pdf_url":"https://arxiv.org/pdf/2312.10407v1.pdf","comment":"This is the initial version of this work, and a more comprehensive\n  and improved version will be updated later"},{"id":"http://arxiv.org/abs/2312.10406v1","updated":"2023-12-16T10:14:16Z","published":"2023-12-16T10:14:16Z","title":"Statistical Analysis of Inter Coding in VVC Test Model (VTM)","summary":"  The promising improvement in compression efficiency of Versatile Video Coding\n(VVC) compared to High Efficiency Video Coding (HEVC) comes at the cost of a\nnon-negligible encoder side complexity. The largely increased complexity\noverhead is a possible obstacle towards its industrial implementation. Many\npapers have proposed acceleration methods for VVC. Still, a better\nunderstanding of VVC complexity, especially related to new partitions and\ncoding tools, is desirable to help the design of new and better acceleration\nmethods. For this purpose, statistical analyses have been conducted, with a\nfocus on Coding Unit (CU) sizes and inter coding modes.\n","authors":["Yiqun Liu","Mohsen Abdoli","Thomas Guionnet","Christine Guillemot","Aline Roumy"],"pdf_url":"https://arxiv.org/pdf/2312.10406v1.pdf","comment":"Accepted by ICIP 2022"},{"id":"http://arxiv.org/abs/2312.10307v1","updated":"2023-12-16T03:50:13Z","published":"2023-12-16T03:50:13Z","title":"MusER: Musical Element-Based Regularization for Generating Symbolic\n  Music with Emotion","summary":"  Generating music with emotion is an important task in automatic music\ngeneration, in which emotion is evoked through a variety of musical elements\n(such as pitch and duration) that change over time and collaborate with each\nother. However, prior research on deep learning-based emotional music\ngeneration has rarely explored the contribution of different musical elements\nto emotions, let alone the deliberate manipulation of these elements to alter\nthe emotion of music, which is not conducive to fine-grained element-level\ncontrol over emotions. To address this gap, we present a novel approach\nemploying musical element-based regularization in the latent space to\ndisentangle distinct elements, investigate their roles in distinguishing\nemotions, and further manipulate elements to alter musical emotions.\nSpecifically, we propose a novel VQ-VAE-based model named MusER. MusER\nincorporates a regularization loss to enforce the correspondence between the\nmusical element sequences and the specific dimensions of latent variable\nsequences, providing a new solution for disentangling discrete sequences.\nTaking advantage of the disentangled latent vectors, a two-level decoding\nstrategy that includes multiple decoders attending to latent vectors with\ndifferent semantics is devised to better predict the elements. By visualizing\nlatent space, we conclude that MusER yields a disentangled and interpretable\nlatent space and gain insights into the contribution of distinct elements to\nthe emotional dimensions (i.e., arousal and valence). Experimental results\ndemonstrate that MusER outperforms the state-of-the-art models for generating\nemotional music in both objective and subjective evaluation. Besides, we\nrearrange music through element transfer and attempt to alter the emotion of\nmusic by transferring emotion-distinguishable elements.\n","authors":["Shulei Ji","Xinyu Yang"],"pdf_url":"https://arxiv.org/pdf/2312.10307v1.pdf","comment":"Accepted by AAAI 2024"}]},"2023-12-19T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2312.12436v1","updated":"2023-12-19T18:59:22Z","published":"2023-12-19T18:59:22Z","title":"A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise","summary":"  The surge of interest towards Multi-modal Large Language Models (MLLMs),\ne.g., GPT-4V(ision) from OpenAI, has marked a significant trend in both\nacademia and industry. They endow Large Language Models (LLMs) with powerful\ncapabilities in visual understanding, enabling them to tackle diverse\nmulti-modal tasks. Very recently, Google released Gemini, its newest and most\ncapable MLLM built from the ground up for multi-modality. In light of the\nsuperior reasoning capabilities, can Gemini challenge GPT-4V's leading position\nin multi-modal learning? In this paper, we present a preliminary exploration of\nGemini Pro's visual understanding proficiency, which comprehensively covers\nfour domains: fundamental perception, advanced cognition, challenging vision\ntasks, and various expert capacities. We compare Gemini Pro with the\nstate-of-the-art GPT-4V to evaluate its upper limits, along with the latest\nopen-sourced MLLM, Sphinx, which reveals the gap between manual efforts and\nblack-box systems. The qualitative samples indicate that, while GPT-4V and\nGemini showcase different answering styles and preferences, they can exhibit\ncomparable visual reasoning capabilities, and Sphinx still trails behind them\nconcerning domain generalizability. Specifically, GPT-4V tends to elaborate\ndetailed explanations and intermediate steps, and Gemini prefers to output a\ndirect and concise answer. The quantitative evaluation on the popular MME\nbenchmark also demonstrates the potential of Gemini to be a strong challenger\nto GPT-4V. Our early investigation of Gemini also observes some common issues\nof MLLMs, indicating that there still remains a considerable distance towards\nartificial general intelligence. Our project for tracking the progress of MLLM\nis released at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\n","authors":["Chaoyou Fu","Renrui Zhang","Haojia Lin","Zihan Wang","Timin Gao","Yongdong Luo","Yubo Huang","Zhengye Zhang","Longtian Qiu","Gaoxiang Ye","Yunhang Shen","Mengdan Zhang","Peixian Chen","Sirui Zhao","Xiawu Zheng","Shaohui Lin","Deqiang Jiang","Di Yin","Peng Gao","Ke Li","Xing Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2312.12436v1.pdf","comment":"Total 120 pages. See our project at\n  https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models"},{"id":"http://arxiv.org/abs/2312.12430v1","updated":"2023-12-19T18:56:52Z","published":"2023-12-19T18:56:52Z","title":"Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP","summary":"  We introduce Efficient Title Reranker via Broadcasting Query Encoder, a novel\ntitle reranking technique to achieve efficient title reranking 20x-40x faster\nthan vanilla passage reranker. However, one of the challenges with the training\nof Efficient Title Reranker is the instability. Analyzing the issue, we found\nsome very difficult ground truths might act as noisy labels causing accuracy to\ndrop as well as some extreme values in model probability output causing nan. To\naddress these issues, we introduce the Sigmoid Trick, a novel technique that\nreduces the gradient update of both cases resulting in better retrieval\nefficacy. Experiments showed the effectiveness of ETR and sigmoid trick as we\nachieved four state-of-the-art positions on the kilt knowledge benchmark.\n","authors":["Ziyi Chen","Heyi Tao","Daqian Zuo","Jize Jiang","Yang Jun","Yuxiang Wei"],"pdf_url":"https://arxiv.org/pdf/2312.12430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12364v1","updated":"2023-12-19T17:48:26Z","published":"2023-12-19T17:48:26Z","title":"SpokesBiz -- an Open Corpus of Conversational Polish","summary":"  This paper announces the early release of SpokesBiz, a freely available\ncorpus of conversational Polish developed within the CLARIN-BIZ project and\ncomprising over 650 hours of recordings. The transcribed recordings have been\ndiarized and manually annotated for punctuation and casing. We outline the\ngeneral structure and content of the corpus, showcasing selected applications\nin linguistic research, evaluation and improvement of automatic speech\nrecognition (ASR) systems\n","authors":["Piotr Pęzik","Sylwia Karasińska","Anna Cichosz","Łukasz Jałowiecki","Konrad Kaczyński","Małgorzata Krawentek","Karolina Walkusz","Paweł Wilk","Mariusz Kleć","Krzysztof Szklanny","Szymon Marszałkowski"],"pdf_url":"https://arxiv.org/pdf/2312.12364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.08456v3","updated":"2023-12-19T17:42:02Z","published":"2023-06-14T11:57:31Z","title":"PoetryDiffusion: Towards Joint Semantic and Metrical Manipulation in\n  Poetry Generation","summary":"  Controllable text generation is a challenging and meaningful field in natural\nlanguage generation (NLG). Especially, poetry generation is a typical one with\nwell-defined and strict conditions for text generation which is an ideal\nplayground for the assessment of current methodologies. While prior works\nsucceeded in controlling either semantic or metrical aspects of poetry\ngeneration, simultaneously addressing both remains a challenge. In this paper,\nwe pioneer the use of the Diffusion model for generating sonnets and Chinese\nSongCi poetry to tackle such challenges. In terms of semantics, our\nPoetryDiffusion model, built upon the Diffusion model, generates entire\nsentences or poetry by comprehensively considering the entirety of sentence\ninformation. This approach enhances semantic expression, distinguishing it from\nautoregressive and large language models (LLMs). For metrical control, the\nseparation feature of diffusion generation and its constraint control module\nenable us to flexibly incorporate a novel metrical controller to manipulate and\nevaluate metrics (format and rhythm). The denoising process in PoetryDiffusion\nallows for gradual enhancement of semantics and flexible integration of the\nmetrical controller which can calculate and impose penalties on states that\nstray significantly from the target control distribution. Experimental results\non two datasets demonstrate that our model outperforms existing models in\nautomatic evaluation of semantic, metrical, and overall performance as well as\nhuman evaluation.\n","authors":["Zhiyuan Hu","Chumin Liu","Yue Feng","Anh Tuan Luu","Bryan Hooi"],"pdf_url":"https://arxiv.org/pdf/2306.08456v3.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2312.12343v1","updated":"2023-12-19T17:16:43Z","published":"2023-12-19T17:16:43Z","title":"Avoiding Data Contamination in Language Model Evaluation: Dynamic Test\n  Construction with Latest Materials","summary":"  Data contamination in evaluation is getting increasingly prevalent with the\nemerge of language models pre-trained on super large, automatically-crawled\ncorpora. This problem leads to significant challenges in accurate assessment of\nmodel capabilities and generalisations. In this paper, we propose LatestEval,\nan automatic method leverages the most recent texts to create uncontaminated\nreading comprehension evaluations. LatestEval avoids data contamination by only\nusing texts published within a recent time window, ensuring no overlap with the\ntraining corpora of pre-trained language models. We develop LatestEval\nautomated pipeline to 1) gather latest texts; 2) identify key information, and\n3) construct questions targeting the information while removing the existing\nanswers from the context. This encourages models to infer the answers\nthemselves based on the remaining context, rather than just copy-paste. Our\nexperiments demonstrate that language models exhibit negligible memorisation\nbehaviours on LatestEval as opposed to previous benchmarks, suggesting a\nsignificantly reduced risk of data contamination and leading to a more robust\nevaluation. Data and code are publicly available at:\nhttps://github.com/liyucheng09/LatestEval.\n","authors":["Yucheng Li","Frank Geurin","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2312.12343v1.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2312.12334v1","updated":"2023-12-19T17:01:58Z","published":"2023-12-19T17:01:58Z","title":"PowMix: A Versatile Regularizer for Multimodal Sentiment Analysis","summary":"  Multimodal sentiment analysis (MSA) leverages heterogeneous data sources to\ninterpret the complex nature of human sentiments. Despite significant progress\nin multimodal architecture design, the field lacks comprehensive regularization\nmethods. This paper introduces PowMix, a versatile embedding space regularizer\nthat builds upon the strengths of unimodal mixing-based regularization\napproaches and introduces novel algorithmic components that are specifically\ntailored to multimodal tasks. PowMix is integrated before the fusion stage of\nmultimodal architectures and facilitates intra-modal mixing, such as mixing\ntext with text, to act as a regularizer. PowMix consists of five components: 1)\na varying number of generated mixed examples, 2) mixing factor reweighting, 3)\nanisotropic mixing, 4) dynamic mixing, and 5) cross-modal label mixing.\nExtensive experimentation across benchmark MSA datasets and a broad spectrum of\ndiverse architectural designs demonstrate the efficacy of PowMix, as evidenced\nby consistent performance improvements over baselines and existing mixing\nmethods. An in-depth ablation study highlights the critical contribution of\neach PowMix component and how they synergistically enhance performance.\nFurthermore, algorithmic analysis demonstrates how PowMix behaves in different\nscenarios, particularly comparing early versus late fusion architectures.\nNotably, PowMix enhances overall performance without sacrificing model\nrobustness or magnifying text dominance. It also retains its strong performance\nin situations of limited data. Our findings position PowMix as a promising\nversatile regularization strategy for MSA. Code will be made available.\n","authors":["Efthymios Georgiou","Yannis Avrithis","Alexandros Potamianos"],"pdf_url":"https://arxiv.org/pdf/2312.12334v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2312.12321v1","updated":"2023-12-19T16:47:12Z","published":"2023-12-19T16:47:12Z","title":"Bypassing the Safety Training of Open-Source LLMs with Priming Attacks","summary":"  With the recent surge in popularity of LLMs has come an ever-increasing need\nfor LLM safety training. In this paper, we show that SOTA open-source LLMs are\nvulnerable to simple, optimization-free attacks we refer to as $\\textit{priming\nattacks}$, which are easy to execute and effectively bypass alignment from\nsafety training. Our proposed attack improves the Attack Success Rate on\nHarmful Behaviors, as measured by Llama Guard, by up to $3.3\\times$ compared to\nbaselines. Source code and data are available at\nhttps://github.com/uiuc-focal-lab/llm-priming-attacks .\n","authors":["Jason Vega","Isha Chaudhary","Changming Xu","Gagandeep Singh"],"pdf_url":"https://arxiv.org/pdf/2312.12321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12299v1","updated":"2023-12-19T16:20:49Z","published":"2023-12-19T16:20:49Z","title":"Instruct-SCTG: Guiding Sequential Controlled Text Generation through\n  Instructions","summary":"  Instruction-tuned large language models have shown remarkable performance in\naligning generated text with user intentions across various tasks. However,\nmaintaining human-like discourse structure in the generated text remains a\nchallenging research question. In this paper, we propose Instruct-SCTG, a\nflexible and effective sequential framework that harnesses instruction-tuned\nlanguage models to generate structurally coherent text in both fine-tuned and\nzero-shot setups. Our framework generates articles in a section-by-section\nmanner, aligned with the desired human structure using natural language\ninstructions. Furthermore, we introduce a new automatic metric that measures\ndiscourse divergence in a fuzzy manner. Extensive experiments on three datasets\nfrom representative domains of news and recipes demonstrate the\nstate-of-the-art performance of our framework in imposing discourse structure\nduring text generation, as verified by both automatic and human evaluation. Our\ncode will be available on Github.\n","authors":["Yinhong Liu","Yixuan Su","Ehsan Shareghi","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2312.12299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14743v5","updated":"2023-12-19T16:05:51Z","published":"2023-11-21T18:41:26Z","title":"A Baseline Analysis of Reward Models' Ability To Accurately Analyze\n  Foundation Models Under Distribution Shift","summary":"  Foundation models, specifically Large Language Models (LLM's), have lately\ngained wide-spread attention and adoption. Reinforcement Learning with Human\nFeedback (RLHF) involves training a reward model to capture desired behaviors,\nwhich is then used to align LLM's. These reward models are additionally used at\ninference-time to estimate LLM responses' adherence to those desired behaviors.\nHowever, there is little work measuring how robust these reward models are to\ndistribution shifts. In this work, we evaluate how reward model performance -\nmeasured via accuracy and calibration (i.e. alignment between accuracy and\nconfidence) - is affected by distribution shift. We show novel calibration\npatterns and accuracy drops due to OOD prompts and responses, and that the\nreward model is more sensitive to shifts in responses than prompts.\nAdditionally, we adapt an OOD detection technique commonly used in\nclassification to the reward model setting to detect these distribution shifts\nin prompts and responses.\n","authors":["Will LeVine","Ben Pikus","Tony Chen","Sean Hendryx"],"pdf_url":"https://arxiv.org/pdf/2311.14743v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10493v2","updated":"2023-12-19T15:55:23Z","published":"2023-12-16T16:14:50Z","title":"Debiasing Multimodal Sarcasm Detection with Contrastive Learning","summary":"  Despite commendable achievements made by existing work, prevailing multimodal\nsarcasm detection studies rely more on textual content over visual information.\nIt unavoidably induces spurious correlations between textual words and labels,\nthereby significantly hindering the models' generalization capability. To\naddress this problem, we define the task of out-of-distribution (OOD)\nmultimodal sarcasm detection, which aims to evaluate models' generalizability\nwhen the word distribution is different in training and testing settings.\nMoreover, we propose a novel debiasing multimodal sarcasm detection framework\nwith contrastive learning, which aims to mitigate the harmful effect of biased\ntextual factors for robust OOD generalization. In particular, we first design\ncounterfactual data augmentation to construct the positive samples with\ndissimilar word biases and negative samples with similar word biases.\nSubsequently, we devise an adapted debiasing contrastive learning mechanism to\nempower the model to learn robust task-relevant features and alleviate the\nadverse effect of biased words. Extensive experiments show the superiority of\nthe proposed framework.\n","authors":["Mengzhao Jia","Can Xie","Liqiang Jing"],"pdf_url":"https://arxiv.org/pdf/2312.10493v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12269v1","updated":"2023-12-19T15:51:49Z","published":"2023-12-19T15:51:49Z","title":"Automated speech audiometry: Can it work using open-source pre-trained\n  Kaldi-NL automatic speech recognition?","summary":"  A practical speech audiometry tool is the digits-in-noise (DIN) test for\nhearing screening of populations of varying ages and hearing status. The test\nis usually conducted by a human supervisor (e.g., clinician), who scores the\nresponses spoken by the listener, or online, where a software scores the\nresponses entered by the listener. The test has 24 digit-triplets presented in\nan adaptive staircase procedure, resulting in a speech reception threshold\n(SRT). We propose an alternative automated DIN test setup that can evaluate\nspoken responses whilst conducted without a human supervisor, using the\nopen-source automatic speech recognition toolkit, Kaldi-NL. Thirty\nself-reported normal-hearing Dutch adults (19-64 years) completed one\nDIN+Kaldi-NL test. Their spoken responses were recorded, and used for\nevaluating the transcript of decoded responses by Kaldi-NL. Study 1 evaluated\nthe Kaldi-NL performance through its word error rate (WER), percentage of\nsummed decoding errors regarding only digits found in the transcript compared\nto the total number of digits present in the spoken responses. Average WER\nacross participants was 5.0% (range 0 - 48%, SD = 8.8%), with average decoding\nerrors in three triplets per participant. Study 2 analysed the effect that\ntriplets with decoding errors from Kaldi-NL had on the DIN test output (SRT),\nusing bootstrapping simulations. Previous research indicated 0.70 dB as the\ntypical within-subject SRT variability for normal-hearing adults. Study 2\nshowed that up to four triplets with decoding errors produce SRT variations\nwithin this range, suggesting that our proposed setup could be feasible for\nclinical applications.\n","authors":["Gloria Araiza-Illan","Luke Meyer","Khiet P. Truong","Deniz Baskent"],"pdf_url":"https://arxiv.org/pdf/2312.12269v1.pdf","comment":"25 pages (double spaced), 5 figures, 3 tables, 54 references"},{"id":"http://arxiv.org/abs/2312.12253v1","updated":"2023-12-19T15:37:27Z","published":"2023-12-19T15:37:27Z","title":"Geo-located Aspect Based Sentiment Analysis (ABSA) for Crowdsourced\n  Evaluation of Urban Environments","summary":"  Sentiment analysis methods are rapidly being adopted by the field of Urban\nDesign and Planning, for the crowdsourced evaluation of urban environments.\nHowever, most models used within this domain are able to identify positive or\nnegative sentiment associated with a textual appraisal as a whole, without\ninferring information about specific urban aspects contained within it, or the\nsentiment associated with them. While Aspect Based Sentiment Analysis (ABSA) is\nbecoming increasingly popular, most existing ABSA models are trained on\nnon-urban themes such as restaurants, electronics, consumer goods and the like.\nThis body of research develops an ABSA model capable of extracting urban\naspects contained within geo-located textual urban appraisals, along with\ncorresponding aspect sentiment classification. We annotate a dataset of 2500\ncrowdsourced reviews of public parks, and train a Bidirectional Encoder\nRepresentations from Transformers (BERT) model with Local Context Focus (LCF)\non this data. Our model achieves significant improvement in prediction accuracy\non urban reviews, for both Aspect Term Extraction (ATE) and Aspect Sentiment\nClassification (ASC) tasks. For demonstrative analysis, positive and negative\nurban aspects across Boston are spatially visualized. We hope that this model\nis useful for designers and planners for fine-grained urban sentiment\nevaluation.\n","authors":["Demircan Tas","Rohit Priyadarshi Sanatani"],"pdf_url":"https://arxiv.org/pdf/2312.12253v1.pdf","comment":"Created for 6.8610, Quantitative Methods for Natural Language\n  Processing at MIT Fall 2022. 5 pages, 4 figures"},{"id":"http://arxiv.org/abs/2312.12241v1","updated":"2023-12-19T15:25:39Z","published":"2023-12-19T15:25:39Z","title":"GeomVerse: A Systematic Evaluation of Large Models for Geometric\n  Reasoning","summary":"  Large language models have shown impressive results for multi-hop\nmathematical reasoning when the input question is only textual. Many\nmathematical reasoning problems, however, contain both text and image. With the\never-increasing adoption of vision language models (VLMs), understanding their\nreasoning abilities for such problems is crucial. In this paper, we evaluate\nthe reasoning capabilities of VLMs along various axes through the lens of\ngeometry problems. We procedurally create a synthetic dataset of geometry\nquestions with controllable difficulty levels along multiple axes, thus\nenabling a systematic evaluation. The empirical results obtained using our\nbenchmark for state-of-the-art VLMs indicate that these models are not as\ncapable in subjects like geometry (and, by generalization, other topics\nrequiring similar reasoning) as suggested by previous benchmarks. This is made\nespecially clear by the construction of our benchmark at various depth levels,\nsince solving higher-depth problems requires long chains of reasoning rather\nthan additional memorized knowledge. We release the dataset for further\nresearch in this area.\n","authors":["Mehran Kazemi","Hamidreza Alvari","Ankit Anand","Jialin Wu","Xi Chen","Radu Soricut"],"pdf_url":"https://arxiv.org/pdf/2312.12241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14160v4","updated":"2023-12-19T15:13:52Z","published":"2023-05-23T15:26:20Z","title":"Label Words are Anchors: An Information Flow Perspective for\n  Understanding In-Context Learning","summary":"  In-context learning (ICL) emerges as a promising capability of large language\nmodels (LLMs) by providing them with demonstration examples to perform diverse\ntasks. However, the underlying mechanism of how LLMs learn from the provided\ncontext remains under-explored. In this paper, we investigate the working\nmechanism of ICL through an information flow lens. Our findings reveal that\nlabel words in the demonstration examples function as anchors: (1) semantic\ninformation aggregates into label word representations during the shallow\ncomputation layers' processing; (2) the consolidated information in label words\nserves as a reference for LLMs' final predictions. Based on these insights, we\nintroduce an anchor re-weighting method to improve ICL performance, a\ndemonstration compression technique to expedite inference, and an analysis\nframework for diagnosing ICL errors in GPT2-XL. The promising applications of\nour findings again validate the uncovered ICL working mechanism and pave the\nway for future studies.\n","authors":["Lean Wang","Lei Li","Damai Dai","Deli Chen","Hao Zhou","Fandong Meng","Jie Zhou","Xu Sun"],"pdf_url":"https://arxiv.org/pdf/2305.14160v4.pdf","comment":"Accepted by EMNLP 2023"},{"id":"http://arxiv.org/abs/2305.14901v2","updated":"2023-12-19T14:12:04Z","published":"2023-05-24T08:55:08Z","title":"Chain-of-Questions Training with Latent Answers for Robust Multistep\n  Question Answering","summary":"  We train a language model (LM) to robustly answer multistep questions by\ngenerating and answering sub-questions. We propose Chain-of-Questions, a\nframework that trains a model to generate sub-questions and sub-answers one at\na time by leveraging human annotated question decomposition meaning\nrepresentation (QDMR). The key technical challenge is that QDMR only contains\nsub-questions but not answers to those sub-questions, so we treat sub-answers\nas latent variables and optimize them using a novel dynamic mixture of Hard-EM\nand MAPO. Chain-of-Questions greatly outperforms strong neuro-symbolic methods\nby 9.0 F1 on DROP contrast set, and outperforms GPT-3.5 by 24.3 F1 on HOTPOTQA\nadversarial set, thus demonstrating the effectiveness and robustness of our\nframework.\n","authors":["Wang Zhu","Jesse Thomason","Robin Jia"],"pdf_url":"https://arxiv.org/pdf/2305.14901v2.pdf","comment":"Accepted by the EMNLP 2023"},{"id":"http://arxiv.org/abs/2311.17280v3","updated":"2023-12-19T14:04:33Z","published":"2023-11-28T23:40:13Z","title":"Does VLN Pretraining Work with Nonsensical or Irrelevant Instructions?","summary":"  Data augmentation via back-translation is common when pretraining\nVision-and-Language Navigation (VLN) models, even though the generated\ninstructions are noisy. But: does that noise matter? We find that nonsensical\nor irrelevant language instructions during pretraining can have little effect\non downstream performance for both HAMT and VLN-BERT on R2R, and is still\nbetter than only using clean, human data. To underscore these results, we\nconcoct an efficient augmentation method, Unigram + Object, which generates\nnonsensical instructions that nonetheless improve downstream performance. Our\nfindings suggest that what matters for VLN R2R pretraining is the quantity of\nvisual trajectories, not the quality of instructions.\n","authors":["Wang Zhu","Ishika Singh","Yuan Huang","Robin Jia","Jesse Thomason"],"pdf_url":"https://arxiv.org/pdf/2311.17280v3.pdf","comment":"Accepted by O-DRUM @ CVPR 2023"},{"id":"http://arxiv.org/abs/2312.12148v1","updated":"2023-12-19T13:31:24Z","published":"2023-12-19T13:31:24Z","title":"Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models:\n  A Critical Review and Assessment","summary":"  With the continuous growth in the number of parameters of transformer-based\npretrained language models (PLMs), particularly the emergence of large language\nmodels (LLMs) with billions of parameters, many natural language processing\n(NLP) tasks have demonstrated remarkable success. However, the enormous size\nand computational demands of these models pose significant challenges for\nadapting them to specific downstream tasks, especially in environments with\nlimited computational resources. Parameter Efficient Fine-Tuning (PEFT) offers\nan effective solution by reducing the number of fine-tuning parameters and\nmemory usage while achieving comparable performance to full fine-tuning. The\ndemands for fine-tuning PLMs, especially LLMs, have led to a surge in the\ndevelopment of PEFT methods, as depicted in Fig. 1. In this paper, we present a\ncomprehensive and systematic review of PEFT methods for PLMs. We summarize\nthese PEFT methods, discuss their applications, and outline future directions.\nFurthermore, we conduct experiments using several representative PEFT methods\nto better understand their effectiveness in parameter efficiency and memory\nefficiency. By offering insights into the latest advancements and practical\napplications, this survey serves as an invaluable resource for researchers and\npractitioners seeking to navigate the challenges and opportunities presented by\nPEFT in the context of PLMs.\n","authors":["Lingling Xu","Haoran Xie","Si-Zhao Joe Qin","Xiaohui Tao","Fu Lee Wang"],"pdf_url":"https://arxiv.org/pdf/2312.12148v1.pdf","comment":"20 pages, 4 figures"},{"id":"http://arxiv.org/abs/2312.11193v2","updated":"2023-12-19T13:24:26Z","published":"2023-12-18T13:40:16Z","title":"\"Paraphrasing The Original Text\" Makes High Accuracy Long-Context QA","summary":"  Although LLMs continue to iterate and improve, most open-source models still\nhave a context window of no more than 4k, limiting their ability to handle\nlong-context problems. Most existing open-source models for long-context chat\nstill lack satisfactory accuracy. To address this issue, I approach it from the\nperspective of training data and theoretically prove that training the\ncapability to handle long contexts requires \"effective\" rather than \"long\"\ndata. Based on this, I propose using the \"original text paraphrase\" task, and\nsuccessfully extend the context window of the existing model to 32k by a\nlow-cost and effective method, achieving extremely high accuracy in\nmulti-document-QA and surpassing all existing open-source models of the same\nscale. The model and training data have been open-sourced on\nHuggingFace(https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k) and\nWiseModel(https://wisemodel.cn/models/yuyijiong/Qwen-14b-chat-yarn-32k).\n","authors":["Yijiong Yu"],"pdf_url":"https://arxiv.org/pdf/2312.11193v2.pdf","comment":"Chinese version of this paper can be downloaded from\n  (https://cloud.tsinghua.edu.cn/d/5894ec4442e54a6aac96/)"},{"id":"http://arxiv.org/abs/2310.13023v2","updated":"2023-12-19T13:23:56Z","published":"2023-10-19T06:17:46Z","title":"GraphGPT: Graph Instruction Tuning for Large Language Models","summary":"  Graph Neural Networks (GNNs) have advanced graph structure understanding via\nrecursive information exchange and aggregation among graph nodes. To improve\nmodel robustness, self-supervised learning (SSL) has emerged as a promising\napproach for data augmentation. However, existing methods for generating\npre-trained graph embeddings often rely on fine-tuning with specific downstream\ntask labels, which limits their usability in scenarios where labeled data is\nscarce or unavailable. To address this, our research focuses on advancing the\ngeneralization capabilities of graph models in challenging zero-shot learning\nscenarios. Inspired by the success of large language models (LLMs), we aim to\ndevelop a graph-oriented LLM that can achieve high generalization across\ndiverse downstream datasets and tasks, even without any information available\nfrom the downstream graph data. In this work, we present the GraphGPT framework\nthat aligns LLMs with graph structural knowledge with a graph instruction\ntuning paradigm. Our framework incorporates a text-graph grounding component to\nestablish a connection between textual information and graph structures.\nAdditionally, we propose a dual-stage instruction tuning paradigm, accompanied\nby a lightweight graph-text alignment projector. This paradigm explores\nself-supervised graph structural signals and task-specific graph instructions,\nto guide LLMs in understanding complex graph structures and improving their\nadaptability across different downstream tasks. Our framework is evaluated on\nsupervised and zero-shot graph learning tasks, demonstrating superior\ngeneralization and outperforming state-of-the-art baselines.\n","authors":["Jiabin Tang","Yuhao Yang","Wei Wei","Lei Shi","Lixin Su","Suqi Cheng","Dawei Yin","Chao Huang"],"pdf_url":"https://arxiv.org/pdf/2310.13023v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12141v1","updated":"2023-12-19T13:23:18Z","published":"2023-12-19T13:23:18Z","title":"Exploring the Residual Stream of Transformers","summary":"  Transformer-based models have achieved great breakthroughs in recent years.\nHowever, there are many significant questions that have not been answered in\nthe field of explaining the reason why the models have powerful outputs. We do\nnot know how to locate the models' important parameters storing the knowledge\nfor predicting the next word, and whether these parameters are stored on the\nsame layer/module or different ones. Moreover, we do not understand the\nmechanism to merge the knowledge into the final embedding for next word\nprediction. In this paper, we explore the residual stream of transformers to\nincrease the interpretability. We find the mechanism behind residual connection\nis a direct addition function on before-softmax values, so the probabilities of\ntokens with larger before-softmax values will increase. Moreover, we prove that\nusing log probability increase as contribution scores is reasonable, and based\non this we can locate important parameters. Besides, we propose a method to\nanalyze how previous layers affect upper layers by comparing the inner\nproducts. The experimental results and case study show that our research can\nincrease the interpretability of transformer-based models. We will release our\ncode on https://github.com/zepingyu0512/residualstream.\n","authors":["Zeping Yu","Kailai Yang","Zhiwei Liu","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2312.12141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.09897v2","updated":"2023-12-19T13:05:12Z","published":"2022-12-19T22:37:46Z","title":"Inducing Character-level Structure in Subword-based Language Models with\n  Type-level Interchange Intervention Training","summary":"  Language tasks involving character-level manipulations (e.g., spelling\ncorrections, arithmetic operations, word games) are challenging for models\noperating on subword units. To address this, we develop a causal intervention\nframework to learn robust and interpretable character representations inside\nsubword-based language models. Our method treats each character as a typed\nvariable in a causal model and learns such causal structures by adapting the\ninterchange intervention training method of Geiger et al. (2021). We\nadditionally introduce a suite of character-level tasks that systematically\nvary in their dependence on meaning and sequence-level context. While\ncharacter-level models still perform best on purely form-based tasks like\nstring reversal, our method outperforms character-level models on more complex\ntasks that blend form, meaning, and context, such as spelling correction in\ncontext and word search games. Compared with standard subword-based models, our\napproach also significantly improves robustness on unseen token sequences and\nleads to human-interpretable internal representations of characters.\n","authors":["Jing Huang","Zhengxuan Wu","Kyle Mahowald","Christopher Potts"],"pdf_url":"https://arxiv.org/pdf/2212.09897v2.pdf","comment":"Findings of the Association for Computational Linguistics: ACL 2023"},{"id":"http://arxiv.org/abs/2310.09767v2","updated":"2023-12-19T13:01:50Z","published":"2023-10-15T07:58:52Z","title":"VLIS: Unimodal Language Models Guide Multimodal Language Generation","summary":"  Multimodal language generation, which leverages the synergy of language and\nvision, is a rapidly expanding field. However, existing vision-language models\nface challenges in tasks that require complex linguistic understanding. To\naddress this issue, we introduce Visual-Language models as Importance Sampling\nweights (VLIS), a novel framework that combines the visual conditioning\ncapability of vision-language models with the language understanding of\nunimodal text-only language models without further training. It extracts\npointwise mutual information of each image and text from a visual-language\nmodel and uses the value as an importance sampling weight to adjust the token\nlikelihood from a text-only model. VLIS improves vision-language models on\ndiverse tasks, including commonsense understanding (WHOOPS, OK-VQA, and\nScienceQA) and complex text generation (Concadia, Image Paragraph Captioning,\nand ROCStories). Our results suggest that VLIS represents a promising new\ndirection for multimodal language generation.\n","authors":["Jiwan Chung","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2310.09767v2.pdf","comment":"Accepted as main paper in EMNLP 2023"},{"id":"http://arxiv.org/abs/2101.00153v3","updated":"2023-12-19T12:57:23Z","published":"2021-01-01T03:29:21Z","title":"Graphmax for Text Generation","summary":"  In text generation, a large language model (LM) makes a choice of each new\nword based only on the former selection of its context using the softmax\nfunction. Nevertheless, the link statistics information of concurrent words\nbased on a scene-specific corpus is valuable in choosing the next word, which\ncan help to ensure the topic of the generated text to be aligned with the\ncurrent task. To fully explore the co-occurrence information,we propose a\ngraphmax function for task-specific text generation. Using the graph-based\nregularization, graphmax enables the final word choice to be determined by both\nthe global knowledge from the LM and the local knowledge from the\nscene-specific corpus. The traditional softmax function is regularized with a\ngraph total variation (GTV) term, which incorporates the local knowledge into\nthe LM and encourages the model to consider the statistical relationships\nbetween words in a scene-specific corpus. The proposed graphmax is versatile\nand can be readily plugged into any large pre-trained LM for text generation\nand machine translation. Through extensive experiments, we demonstrate that the\nnew GTV-based regularization can improve performances in various natural\nlanguage processing tasks in comparison with existing methods. Moreover,\nthrough human experiments, we observe that participants can easily distinguish\nthe text generated by graphmax or softmax.\n","authors":["Liu Bin","Yin Guosheng"],"pdf_url":"https://arxiv.org/pdf/2101.00153v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.07924v4","updated":"2023-12-19T12:56:13Z","published":"2023-07-16T02:11:34Z","title":"Communicative Agents for Software Development","summary":"  Software engineering is a domain characterized by intricate decision-making\nprocesses, often relying on nuanced intuition and consultation. Recent\nadvancements in deep learning have started to revolutionize software\nengineering practices through elaborate designs implemented at various stages\nof software development. In this paper, we present an innovative paradigm that\nleverages large language models (LLMs) throughout the entire software\ndevelopment process, streamlining and unifying key processes through natural\nlanguage communication, thereby eliminating the need for specialized models at\neach phase. At the core of this paradigm lies ChatDev, a virtual chat-powered\nsoftware development company that mirrors the established waterfall model,\nmeticulously dividing the development process into four distinct chronological\nstages: designing, coding, testing, and documenting. Each stage engages a team\nof \"software agents\", such as programmers, code reviewers, and test engineers,\nfostering collaborative dialogue and facilitating a seamless workflow. The chat\nchain acts as a facilitator, breaking down each stage into atomic subtasks.\nThis enables dual roles, allowing for proposing and validating solutions\nthrough context-aware communication, leading to efficient resolution of\nspecific subtasks. The instrumental analysis of ChatDev highlights its\nremarkable efficacy in software generation, enabling the completion of the\nentire software development process in under seven minutes at a cost of less\nthan one dollar. It not only identifies and alleviates potential\nvulnerabilities but also rectifies potential hallucinations while maintaining\ncommendable efficiency and cost-effectiveness. The potential of ChatDev unveils\nfresh possibilities for integrating LLMs into the realm of software\ndevelopment. Our code is available at https://github.com/OpenBMB/ChatDev.\n","authors":["Chen Qian","Xin Cong","Wei Liu","Cheng Yang","Weize Chen","Yusheng Su","Yufan Dang","Jiahao Li","Juyuan Xu","Dahai Li","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2307.07924v4.pdf","comment":"https://github.com/OpenBMB/ChatDev"},{"id":"http://arxiv.org/abs/2312.12108v1","updated":"2023-12-19T12:32:27Z","published":"2023-12-19T12:32:27Z","title":"Knowledge Graph Error Detection with Contrastive Confidence Adaption","summary":"  Knowledge graphs (KGs) often contain various errors. Previous works on\ndetecting errors in KGs mainly rely on triplet embedding from graph structure.\nWe conduct an empirical study and find that these works struggle to\ndiscriminate noise from semantically-similar correct triplets. In this paper,\nwe propose a KG error detection model CCA to integrate both textual and graph\nstructural information from triplet reconstruction for better distinguishing\nsemantics. We design interactive contrastive learning to capture the\ndifferences between textual and structural patterns. Furthermore, we construct\nrealistic datasets with semantically-similar noise and adversarial noise.\nExperimental results demonstrate that CCA outperforms state-of-the-art\nbaselines, especially in detecting semantically-similar noise and adversarial\nnoise.\n","authors":["Xiangyu Liu","Yang Liu","Wei Hu"],"pdf_url":"https://arxiv.org/pdf/2312.12108v1.pdf","comment":"Accepted in the 38th AAAI Conference on Artificial Intelligence (AAAI\n  2024)"},{"id":"http://arxiv.org/abs/2310.18313v2","updated":"2023-12-19T12:27:58Z","published":"2023-10-27T17:59:51Z","title":"FP8-LM: Training FP8 Large Language Models","summary":"  In this paper, we explore FP8 low-bit data formats for efficient training of\nlarge language models (LLMs). Our key insight is that most variables, such as\ngradients and optimizer states, in LLM training can employ low-precision data\nformats without compromising model accuracy and requiring no changes to\nhyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision\nframework for training LLMs. This framework offers three levels of FP8\nutilization to streamline mixed-precision and distributed parallel training for\nLLMs. It gradually incorporates 8-bit gradients, optimizer states, and\ndistributed learning in an incremental manner. Experiment results show that,\nduring the training of GPT-175B model on H100 GPU platform, our FP8\nmixed-precision training framework not only achieved a remarkable 39% reduction\nin real memory usage but also ran 75% faster than the widely adopted BF16\nframework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer\nEngine by 37%. This largely reduces the training costs for large foundation\nmodels. Furthermore, our FP8 mixed-precision training methodology is generic.\nIt can be seamlessly applied to other tasks such as LLM instruction tuning and\nreinforcement learning with human feedback, offering savings in fine-tuning\nexpenses. Our FP8 low-precision training framework is open-sourced at\n{https://github.com/Azure/MS-AMP}{aka.ms/MS.AMP}.\n","authors":["Houwen Peng","Kan Wu","Yixuan Wei","Guoshuai Zhao","Yuxiang Yang","Ze Liu","Yifan Xiong","Ziyue Yang","Bolin Ni","Jingcheng Hu","Ruihang Li","Miaosen Zhang","Chen Li","Jia Ning","Ruizhe Wang","Zheng Zhang","Shuguang Liu","Joe Chau","Han Hu","Peng Cheng"],"pdf_url":"https://arxiv.org/pdf/2310.18313v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06453v2","updated":"2023-12-19T12:13:25Z","published":"2023-09-12T08:16:58Z","title":"Narrowing the Gap between Supervised and Unsupervised Sentence\n  Representation Learning with Large Language Model","summary":"  Sentence Representation Learning (SRL) is a fundamental task in Natural\nLanguage Processing (NLP), with the Contrastive Learning of Sentence Embeddings\n(CSE) being the mainstream technique due to its superior performance. An\nintriguing phenomenon in CSE is the significant performance gap between\nsupervised and unsupervised methods, with their only difference lying in the\ntraining data. Previous works attribute this performance gap to differences in\ntwo representation properties (alignment and uniformity). However, since\nalignment and uniformity only measure the results, they fail to answer \"What\naspects of the training data contribute to the performance gap?\" and \"How can\nthe performance gap be narrowed?\", In this paper, we conduct empirical\nexperiments to answer these \"What\" and \"How\" questions. We first answer the\n\"What\" question by thoroughly comparing the behavior of supervised and\nunsupervised CSE during their respective training processes. From the\ncomparison, we identify the similarity pattern as a key factor to the\nperformance gap, and introduce a metric, called Relative Fitting Difficulty\n(RFD), to measure the complexity of the similarity pattern. Then, based on the\ninsights gained from the \"What\" question, we tackle the \"How\" question by\nincreasing the pattern complexity of the training data. We achieve this by\nleveraging the In-Context Learning (ICL) capability of the Large Language Model\n(LLM) to generate data that simulates complex patterns. By utilizing the\nhierarchical patterns in the LLM-generated data, we effectively narrow the gap\nbetween supervised and unsupervised CSE. We release our codes and appendix at\nhttps://github.com/BDBC-KG-NLP/NGCSE.\n","authors":["Mingxin Li","Richong Zhang","Zhijie Nie","Yongyi Mao"],"pdf_url":"https://arxiv.org/pdf/2309.06453v2.pdf","comment":"Accepted at AAAI24"},{"id":"http://arxiv.org/abs/2312.12037v1","updated":"2023-12-19T10:46:13Z","published":"2023-12-19T10:46:13Z","title":"Founder-GPT: Self-play to evaluate the Founder-Idea fit","summary":"  This research introduces an innovative evaluation method for the\n\"founder-idea\" fit in early-stage startups, utilizing advanced large language\nmodel techniques to assess founders' profiles against their startup ideas to\nenhance decision-making. Embeddings, self-play, tree-of-thought, and\ncritique-based refinement techniques show early promising results that each\nidea's success patterns are unique and they should be evaluated based on the\ncontext of the founder's background.\n","authors":["Sichao Xiong","Yigit Ihlamur"],"pdf_url":"https://arxiv.org/pdf/2312.12037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12021v1","updated":"2023-12-19T10:16:24Z","published":"2023-12-19T10:16:24Z","title":"Synergistic Anchored Contrastive Pre-training for Few-Shot Relation\n  Extraction","summary":"  Few-shot Relation Extraction (FSRE) aims to extract relational facts from a\nsparse set of labeled corpora. Recent studies have shown promising results in\nFSRE by employing Pre-trained Language Models (PLMs) within the framework of\nsupervised contrastive learning, which considers both instances and label\nfacts. However, how to effectively harness massive instance-label pairs to\nencompass the learned representation with semantic richness in this learning\nparadigm is not fully explored. To address this gap, we introduce a novel\nsynergistic anchored contrastive pre-training framework. This framework is\nmotivated by the insight that the diverse viewpoints conveyed through\ninstance-label pairs capture incomplete yet complementary intrinsic textual\nsemantics. Specifically, our framework involves a symmetrical contrastive\nobjective that encompasses both sentence-anchored and label-anchored\ncontrastive losses. By combining these two losses, the model establishes a\nrobust and uniform representation space. This space effectively captures the\nreciprocal alignment of feature distributions among instances and relational\nfacts, simultaneously enhancing the maximization of mutual information across\ndiverse perspectives within the same relation. Experimental results demonstrate\nthat our framework achieves significant performance enhancements compared to\nbaseline models in downstream FSRE tasks. Furthermore, our approach exhibits\nsuperior adaptability to handle the challenges of domain shift and zero-shot\nrelation extraction. Our code is available online at\nhttps://github.com/AONE-NLP/FSRE-SaCon.\n","authors":[" DaLuo","Yanglei Gan","Rui Hou","Run Lin","Qiao Liu","Yuxiang Cai","Wannian Gao"],"pdf_url":"https://arxiv.org/pdf/2312.12021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05161v4","updated":"2023-12-19T10:13:33Z","published":"2023-10-08T13:36:05Z","title":"Recurrent Neural Language Models as Probabilistic Finite-state Automata","summary":"  Studying language models (LMs) in terms of well-understood formalisms allows\nus to precisely characterize their abilities and limitations. Previous work has\ninvestigated the representational capacity of recurrent neural network (RNN)\nLMs in terms of their capacity to recognize unweighted formal languages.\nHowever, LMs do not describe unweighted formal languages -- rather, they define\n\\emph{probability distributions} over strings. In this work, we study what\nclasses of such probability distributions RNN LMs can represent, which allows\nus to make more direct statements about their capabilities. We show that simple\nRNNs are equivalent to a subclass of probabilistic finite-state automata, and\ncan thus model a strict subset of probability distributions expressible by\nfinite-state models. Furthermore, we study the space complexity of representing\nfinite-state LMs with RNNs. We show that, to represent an arbitrary\ndeterministic finite-state LM with $N$ states over an alphabet $\\alphabet$, an\nRNN requires $\\Omega\\left(N |\\Sigma|\\right)$ neurons. These results present a\nfirst step towards characterizing the classes of distributions RNN LMs can\nrepresent and thus help us understand their capabilities and limitations.\n","authors":["Anej Svete","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2310.05161v4.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2312.12009v1","updated":"2023-12-19T09:58:54Z","published":"2023-12-19T09:58:54Z","title":"Active Preference Inference using Language Models and Probabilistic\n  Reasoning","summary":"  Actively inferring user preferences, for example by asking good questions, is\nimportant for any human-facing decision-making system. Active inference allows\nsuch systems to adapt and personalize themselves to nuanced individual\npreferences. To enable this ability for instruction-tuned large language models\n(LLMs), one may prompt them to ask users questions to infer their preferences,\ntransforming the language models into more robust, interactive systems.\nHowever, out of the box, these models are not efficient at extracting\npreferences: the questions they generate are not informative, requiring a high\nnumber of user interactions and impeding the usability of the downstream\nsystem. In this work, we introduce an inference-time algorithm that helps LLMs\nquickly infer preferences by using more informative questions. Our algorithm\nuses a probabilistic model whose conditional distributions are defined by\nprompting an LLM, and returns questions that optimize expected entropy and\nexpected model change. Results in a simplified interactive web shopping setting\nwith real product items show that an LLM equipped with our entropy reduction\nalgorithm outperforms baselines with the same underlying LLM on task\nperformance while using fewer user interactions.\n","authors":["Top Piriyakulkij","Volodymyr Kuleshov","Kevin Ellis"],"pdf_url":"https://arxiv.org/pdf/2312.12009v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12006v1","updated":"2023-12-19T09:54:27Z","published":"2023-12-19T09:54:27Z","title":"Can ChatGPT be Your Personal Medical Assistant?","summary":"  The advanced large language model (LLM) ChatGPT has shown its potential in\ndifferent domains and remains unbeaten due to its characteristics compared to\nother LLMs. This study aims to evaluate the potential of using a fine-tuned\nChatGPT model as a personal medical assistant in the Arabic language. To do so,\nthis study uses publicly available online questions and answering datasets in\nArabic language. There are almost 430K questions and answers for 20\ndisease-specific categories. GPT-3.5-turbo model was fine-tuned with a portion\nof this dataset. The performance of this fine-tuned model was evaluated through\nautomated and human evaluation. The automated evaluations include perplexity,\ncoherence, similarity, and token count. Native Arabic speakers with medical\nknowledge evaluated the generated text by calculating relevance, accuracy,\nprecision, logic, and originality. The overall result shows that ChatGPT has a\nbright future in medical assistance.\n","authors":["Md. Rafiul Biswas","Ashhadul Islam","Zubair Shah","Wajdi Zaghouani","Samir Brahim Belhaouari"],"pdf_url":"https://arxiv.org/pdf/2312.12006v1.pdf","comment":"5 pages, 7 figures, two tables, Accepted on The International\n  Symposium on Foundation and Large Language Models (FLLM2023)"},{"id":"http://arxiv.org/abs/2312.11997v1","updated":"2023-12-19T09:39:27Z","published":"2023-12-19T09:39:27Z","title":"Coreference Graph Guidance for Mind-Map Generation","summary":"  Mind-map generation aims to process a document into a hierarchical structure\nto show its central idea and branches. Such a manner is more conducive to\nunderstanding the logic and semantics of the document than plain text.\nRecently, a state-of-the-art method encodes the sentences of a document\nsequentially and converts them to a relation graph via sequence-to-graph.\nThough this method is efficient to generate mind-maps in parallel, its\nmechanism focuses more on sequential features while hardly capturing structural\ninformation. Moreover, it's difficult to model long-range semantic relations.\nIn this work, we propose a coreference-guided mind-map generation network\n(CMGN) to incorporate external structure knowledge. Specifically, we construct\na coreference graph based on the coreference semantic relationship to introduce\nthe graph structure information. Then we employ a coreference graph encoder to\nmine the potential governing relations between sentences. In order to exclude\nnoise and better utilize the information of the coreference graph, we adopt a\ngraph enhancement module in a contrastive learning manner. Experimental results\ndemonstrate that our model outperforms all the existing methods. The case study\nfurther proves that our model can more accurately and concisely reveal the\nstructure and semantics of a document. Code and data are available at\nhttps://github.com/Cyno2232/CMGN.\n","authors":["Zhuowei Zhang","Mengting Hu","Yinhao Bai","Zhen Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.11997v1.pdf","comment":"9 pages, 6 figures. Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.11985v1","updated":"2023-12-19T09:26:46Z","published":"2023-12-19T09:26:46Z","title":"Climate Change from Large Language Models","summary":"  Climate change presents significant challenges to the global community, and\nit is imperative to raise widespread awareness of the climate crisis and\neducate users about low-carbon living. Artificial intelligence, particularly\nlarge language models (LLMs), have emerged as powerful tools in mitigating the\nclimate crisis, leveraging their extensive knowledge, broad user base, and\nnatural language interaction capabilities. However, despite the growing body of\nresearch on climate change, there is a lack of comprehensive assessments of\nclimate crisis knowledge within LLMs. This paper aims to resolve this gap by\nproposing an automatic evaluation framework. We employ a hybrid approach to\ndata acquisition that combines data synthesis and manual collection to compile\na diverse set of questions related to the climate crisis. These questions cover\nvarious aspects of climate change, including its causes, impacts, mitigation\nstrategies, and adaptation measures. We then evaluate the model knowledge\nthrough prompt engineering based on the collected questions and generated\nanswers. We propose a set of comprehensive metrics to evaluate the climate\ncrisis knowledge, incorporating indicators from 10 different perspectives.\nExperimental results show that our method is effective in evaluating the\nknowledge of LLMs regarding the climate crisis. We evaluate several\nstate-of-the-art LLMs and find that their knowledge falls short in terms of\ntimeliness.\n","authors":["Hongyin Zhu","Prayag Tiwari"],"pdf_url":"https://arxiv.org/pdf/2312.11985v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11983v1","updated":"2023-12-19T09:23:48Z","published":"2023-12-19T09:23:48Z","title":"Fluctuation-based Adaptive Structured Pruning for Large Language Models","summary":"  Network Pruning is a promising way to address the huge computing resource\ndemands of the deployment and inference of Large Language Models (LLMs).\nRetraining-free is important for LLMs' pruning methods. However, almost all of\nthe existing retraining-free pruning approaches for LLMs focus on unstructured\npruning, which requires specific hardware support for acceleration. In this\npaper, we propose a novel retraining-free structured pruning framework for\nLLMs, named FLAP (FLuctuation-based Adaptive Structured Pruning). It is\nhardware-friendly by effectively reducing storage and enhancing inference\nspeed. For effective structured pruning of LLMs, we highlight three critical\nelements that demand the utmost attention: formulating structured importance\nmetrics, adaptively searching the global compressed model, and implementing\ncompensation mechanisms to mitigate performance loss. First, FLAP determines\nwhether the output feature map is easily recoverable when a column of weight is\nremoved, based on the fluctuation pruning metric. Then it standardizes the\nimportance scores to adaptively determine the global compressed model\nstructure. At last, FLAP adds additional bias terms to recover the output\nfeature maps using the baseline values. We thoroughly evaluate our approach on\na variety of language benchmarks. Without any retraining, our method\nsignificantly outperforms the state-of-the-art methods, including LLM-Pruner\nand the extension of Wanda in structured pruning. The code is released at\nhttps://github.com/CASIA-IVA-Lab/FLAP.\n","authors":["Yongqi An","Xu Zhao","Tao Yu","Ming Tang","Jinqiao Wang"],"pdf_url":"https://arxiv.org/pdf/2312.11983v1.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2301.04312v5","updated":"2023-12-19T09:12:01Z","published":"2023-01-11T05:21:00Z","title":"Word-Graph2vec: An efficient word embedding approach on word\n  co-occurrence graph using random walk sampling","summary":"  Word embedding has become ubiquitous and is widely used in various text\nmining and natural language processing (NLP) tasks, such as information\nretrieval, semantic analysis, and machine translation, among many others.\nUnfortunately, it is prohibitively expensive to train the word embedding in a\nrelatively large corpus. We propose a graph-based word embedding algorithm,\ncalled Word-Graph2vec, which converts the large corpus into a word\nco-occurrence graph, then takes the word sequence samples from this graph by\nrandomly traveling and trains the word embedding on this sampling corpus in the\nend. We posit that because of the stable vocabulary, relative idioms, and fixed\nexpressions in English, the size and density of the word co-occurrence graph\nchange slightly with the increase in the training corpus. So that\nWord-Graph2vec has stable runtime on the large scale data set, and its\nperformance advantage becomes more and more obvious with the growth of the\ntraining corpus. Extensive experiments conducted on real-world datasets show\nthat the proposed algorithm outperforms traditional Skip-Gram by four-five\ntimes in terms of efficiency, while the error generated by the random walk\nsampling is small.\n","authors":["Wenting Li","Jiahong Xue","Xi Zhang","Huacan Chen","Zeyu Chen","Yuanzhe Cai"],"pdf_url":"https://arxiv.org/pdf/2301.04312v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11970v1","updated":"2023-12-19T09:06:45Z","published":"2023-12-19T09:06:45Z","title":"Large Language Models Empowered Agent-based Modeling and Simulation: A\n  Survey and Perspectives","summary":"  Agent-based modeling and simulation has evolved as a powerful tool for\nmodeling complex systems, offering insights into emergent behaviors and\ninteractions among diverse agents. Integrating large language models into\nagent-based modeling and simulation presents a promising avenue for enhancing\nsimulation capabilities. This paper surveys the landscape of utilizing large\nlanguage models in agent-based modeling and simulation, examining their\nchallenges and promising future directions. In this survey, since this is an\ninterdisciplinary field, we first introduce the background of agent-based\nmodeling and simulation and large language model-empowered agents. We then\ndiscuss the motivation for applying large language models to agent-based\nsimulation and systematically analyze the challenges in environment perception,\nhuman alignment, action generation, and evaluation. Most importantly, we\nprovide a comprehensive overview of the recent works of large language\nmodel-empowered agent-based modeling and simulation in multiple scenarios,\nwhich can be divided into four domains: cyber, physical, social, and hybrid,\ncovering simulation of both real-world and virtual environments. Finally, since\nthis area is new and quickly evolving, we discuss the open problems and\npromising future directions.\n","authors":["Chen Gao","Xiaochong Lan","Nian Li","Yuan Yuan","Jingtao Ding","Zhilun Zhou","Fengli Xu","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2312.11970v1.pdf","comment":"37 pages"},{"id":"http://arxiv.org/abs/2207.08012v5","updated":"2023-12-19T09:05:55Z","published":"2022-07-16T20:37:46Z","title":"Meta-Referential Games to Learn Compositional Learning Behaviours","summary":"  Human beings use compositionality to generalise from past experiences to\nnovel experiences. We assume a separation of our experiences into fundamental\natomic components that can be recombined in novel ways to support our ability\nto engage with novel experiences. We frame this as the ability to learn to\ngeneralise compositionally, and we will refer to behaviours making use of this\nability as compositional learning behaviours (CLBs). A central problem to\nlearning CLBs is the resolution of a binding problem (BP). While it is another\nfeat of intelligence that human beings perform with ease, it is not the case\nfor state-of-the-art artificial agents. Thus, in order to build artificial\nagents able to collaborate with human beings, we propose to develop a novel\nbenchmark to investigate agents' abilities to exhibit CLBs by solving a\ndomain-agnostic version of the BP. We take inspiration from the language\nemergence and grounding framework of referential games and propose a\nmeta-learning extension of referential games, entitled Meta-Referential Games,\nand use this framework to build our benchmark, the Symbolic Behaviour Benchmark\n(S2B). We provide baseline results and error analysis showing that our\nbenchmark is a compelling challenge that we hope will spur the research\ncommunity towards developing more capable artificial agents.\n","authors":["Kevin Denamganaï","Sondess Missaoui","James Alfred Walker"],"pdf_url":"https://arxiv.org/pdf/2207.08012v5.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2312.04877v2","updated":"2023-12-19T08:58:19Z","published":"2023-12-08T07:27:26Z","title":"Generating Explanations to Understand and Repair Embedding-based Entity\n  Alignment","summary":"  Entity alignment (EA) seeks identical entities in different knowledge graphs,\nwhich is a long-standing task in the database research. Recent work leverages\ndeep learning to embed entities in vector space and align them via nearest\nneighbor search. Although embedding-based EA has gained marked success in\nrecent years, it lacks explanations for alignment decisions. In this paper, we\npresent the first framework that can generate explanations for understanding\nand repairing embedding-based EA results. Given an EA pair produced by an\nembedding model, we first compare its neighbor entities and relations to build\na matching subgraph as a local explanation. We then construct an alignment\ndependency graph to understand the pair from an abstract perspective. Finally,\nwe repair the pair by resolving three types of alignment conflicts based on\ndependency graphs. Experiments on a variety of EA datasets demonstrate the\neffectiveness, generalization, and robustness of our framework in explaining\nand repairing embedding-based EA results.\n","authors":["Xiaobin Tian","Zequn Sun","Wei Hu"],"pdf_url":"https://arxiv.org/pdf/2312.04877v2.pdf","comment":"Accepted in the 40th IEEE International Conference on Data\n  Engineering (ICDE 2024)"},{"id":"http://arxiv.org/abs/2312.11947v1","updated":"2023-12-19T08:47:50Z","published":"2023-12-19T08:47:50Z","title":"Emotion Rendering for Conversational Speech Synthesis with Heterogeneous\n  Graph-Based Context Modeling","summary":"  Conversational Speech Synthesis (CSS) aims to accurately express an utterance\nwith the appropriate prosody and emotional inflection within a conversational\nsetting. While recognising the significance of CSS task, the prior studies have\nnot thoroughly investigated the emotional expressiveness problems due to the\nscarcity of emotional conversational datasets and the difficulty of stateful\nemotion modeling. In this paper, we propose a novel emotional CSS model, termed\nECSS, that includes two main components: 1) to enhance emotion understanding,\nwe introduce a heterogeneous graph-based emotional context modeling mechanism,\nwhich takes the multi-source dialogue history as input to model the dialogue\ncontext and learn the emotion cues from the context; 2) to achieve emotion\nrendering, we employ a contrastive learning-based emotion renderer module to\ninfer the accurate emotion style for the target utterance. To address the issue\nof data scarcity, we meticulously create emotional labels in terms of category\nand intensity, and annotate additional emotional information on the existing\nconversational dataset (DailyTalk). Both objective and subjective evaluations\nsuggest that our model outperforms the baseline models in understanding and\nrendering emotions. These evaluations also underscore the importance of\ncomprehensive emotional annotations. Code and audio samples can be found at:\nhttps://github.com/walker-hyf/ECSS.\n","authors":["Rui Liu","Yifan Hu","Yi Ren","Xiang Yin","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2312.11947v1.pdf","comment":"9 pages, 4 figures, Accepted by AAAI'2024, Code and audio samples:\n  https://github.com/walker-hyf/ECSS"},{"id":"http://arxiv.org/abs/2312.11945v1","updated":"2023-12-19T08:43:02Z","published":"2023-12-19T08:43:02Z","title":"Multi-Granularity Information Interaction Framework for Incomplete\n  Utterance Rewriting","summary":"  Recent approaches in Incomplete Utterance Rewriting (IUR) fail to capture the\nsource of important words, which is crucial to edit the incomplete utterance,\nand introduce words from irrelevant utterances. We propose a novel and\neffective multi-task information interaction framework including context\nselection, edit matrix construction, and relevance merging to capture the\nmulti-granularity of semantic information. Benefiting from fetching the\nrelevant utterance and figuring out the important words, our approach\noutperforms existing state-of-the-art models on two benchmark datasets\nRestoration-200K and CANAND in this field. Code will be provided on\n\\url{https://github.com/yanmenxue/QR}.\n","authors":["Haowei Du","Dingyu Zhang","Chen Li","Yang Li","Dongyan Zhao"],"pdf_url":"https://arxiv.org/pdf/2312.11945v1.pdf","comment":"Findings of EMNLP2023 (short)"},{"id":"http://arxiv.org/abs/2309.04766v2","updated":"2023-12-19T08:25:22Z","published":"2023-09-09T11:42:22Z","title":"SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment\n  to Cultural Reasoning","summary":"  We present SeaEval, a benchmark for multilingual foundation models. In\naddition to characterizing how these models understand and reason with natural\nlanguage, we also investigate how well they comprehend cultural practices,\nnuances, and values. Alongside standard accuracy metrics, we investigate the\nbrittleness of foundation models in the dimensions of semantics and\nmultilinguality. Our analyses span both open-sourced and closed models, leading\nto empirical results across classic NLP tasks, reasoning, and cultural\ncomprehension. Key findings indicate (1) Most models exhibit varied behavior\nwhen given paraphrased instructions. (2) Many models still suffer from exposure\nbias (e.g., positional bias, majority label bias). (3) For questions rooted in\nfactual, scientific, and commonsense knowledge, consistent responses are\nexpected across multilingual queries that are semantically equivalent. Yet,\nmost models surprisingly demonstrate inconsistent performance on these queries.\n(4) Multilingually-trained models have not attained \"balanced multilingual\"\ncapabilities. Our endeavors underscore the need for more generalizable semantic\nrepresentations and enhanced multilingual contextualization. SeaEval can serve\nas a launchpad for more thorough investigations and evaluations for\nmultilingual and multicultural scenarios.\n","authors":["Bin Wang","Zhengyuan Liu","Xin Huang","Fangkai Jiao","Yang Ding","Ai Ti Aw","Nancy F. Chen"],"pdf_url":"https://arxiv.org/pdf/2309.04766v2.pdf","comment":"20 pages. More datasets (2 on Cross-Lingual Consistency and 4 on\n  Cultural Understanding) and more supported languages. Code:\n  https://github.com/SeaEval/SeaEval"},{"id":"http://arxiv.org/abs/2307.10156v2","updated":"2023-12-19T08:02:03Z","published":"2023-07-19T17:37:03Z","title":"Exploring Transformer Extrapolation","summary":"  Length extrapolation has attracted considerable attention recently since it\nallows transformers to be tested on longer sequences than those used in\ntraining. Previous research has shown that this property can be attained by\nusing carefully designed Relative Positional Encodings (RPEs). While these\nmethods perform well on a variety of corpora, the conditions for length\nextrapolation have yet to be investigated. This paper attempts to determine\nwhat types of RPEs allow for length extrapolation through a thorough\nmathematical and empirical analysis. We discover that a transformer is certain\nto possess this property as long as the series that corresponds to the RPE's\nexponential converges. Two practices are derived from the conditions and\nexamined in language modeling tasks on a variety of corpora. As a bonus from\nthe conditions, we derive a new Theoretical Receptive Field (TRF) to measure\nthe receptive field of RPEs without taking any training steps. Extensive\nexperiments are conducted on the Wikitext-103, Books, Github, and WikiBook\ndatasets to demonstrate the viability of our discovered conditions. We also\ncompare TRF to Empirical Receptive Field (ERF) across different models, showing\nconsistently matched trends on the aforementioned datasets. The code is\navailable at https://github.com/OpenNLPLab/Rpe.\n","authors":["Zhen Qin","Yiran Zhong","Hui Deng"],"pdf_url":"https://arxiv.org/pdf/2307.10156v2.pdf","comment":"AAAI Camera Ready. Zhen Qin and Yiran Zhong contribute equally to\n  this paper; Yiran Zhong is the corresponding author. The code is available at\n  https://github.com/OpenNLPLab/Rpe"},{"id":"http://arxiv.org/abs/2312.11922v1","updated":"2023-12-19T08:01:48Z","published":"2023-12-19T08:01:48Z","title":"Relation-Aware Question Answering for Heterogeneous Knowledge Graphs","summary":"  Multi-hop Knowledge Base Question Answering(KBQA) aims to find the answer\nentity in a knowledge graph (KG), which requires multiple steps of reasoning.\nExisting retrieval-based approaches solve this task by concentrating on the\nspecific relation at different hops and predicting the intermediate entity\nwithin the reasoning path. During the reasoning process of these methods, the\nrepresentation of relations are fixed but the initial relation representation\nmay not be optimal. We claim they fail to utilize information from head-tail\nentities and the semantic connection between relations to enhance the current\nrelation representation, which undermines the ability to capture information of\nrelations in KGs. To address this issue, we construct a \\textbf{dual relation\ngraph} where each node denotes a relation in the original KG (\\textbf{primal\nentity graph}) and edges are constructed between relations sharing same head or\ntail entities. Then we iteratively do primal entity graph reasoning, dual\nrelation graph information propagation, and interaction between these two\ngraphs. In this way, the interaction between entity and relation is enhanced,\nand we derive better entity and relation representations. Experiments on two\npublic datasets, WebQSP and CWQ, show that our approach achieves a significant\nperformance gain over the prior state-of-the-art. Our code is available on\n\\url{https://github.com/yanmenxue/RAH-KBQA}.\n","authors":["Haowei Du","Quzhe Huang","Chen Li","Chen Zhang","Yang Li","Dongyan Zhao"],"pdf_url":"https://arxiv.org/pdf/2312.11922v1.pdf","comment":"Findings of EMNLP2023 (Long)"},{"id":"http://arxiv.org/abs/2312.11920v1","updated":"2023-12-19T08:00:10Z","published":"2023-12-19T08:00:10Z","title":"External Knowledge Augmented Polyphone Disambiguation Using Large\n  Language Model","summary":"  One of the key issues in Mandarin Chinese text-to-speech (TTS) systems is\npolyphone disambiguation when doing grapheme-to-phoneme (G2P) conversion. In\nthis paper, we introduce a novel method to solve the problem as a generation\ntask. Following the trending research of large language models (LLM) and prompt\nlearning, the proposed method consists of three modules. Retrieval module\nincorporates external knowledge which is a multi-level semantic dictionary of\nChinese polyphonic characters to format the sentence into a prompt. Generation\nmodule adopts the decoder-only Transformer architecture to induce the target\ntext. Postprocess module corrects the generated text into a valid result if\nneeded. Experimental results show that our method outperforms the existing\nmethods on a public dataset called CPP. We also empirically study the impacts\nof different templates of the prompt, different sizes of training data, and\nwhether to incorporate external knowledge.\n","authors":["Chen Li"],"pdf_url":"https://arxiv.org/pdf/2312.11920v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11075v2","updated":"2023-12-19T07:47:09Z","published":"2023-12-18T10:16:37Z","title":"Split and Rephrase with Large Language Models","summary":"  The Split and Rephrase task, which consists in splitting complex sentences\ninto a sequence of shorter grammatical sentences, while preserving the original\nmeaning, can facilitate the processing of complex texts for humans and machines\nalike. In this work, we describe an approach based on large language models,\nwhich improves over the state of the art by large margins on all the major\nmetrics for the task, on publicly available datasets. We also describe results\nfrom two human evaluations that further establish the significant improvements\nobtained with large language models and the viability of the approach. We\nevaluate different strategies, including fine-tuning pretrained language models\nof varying parameter size, and applying both zero-shot and few-shot in-context\nlearning on instruction-tuned language models. Although the latter were\nmarkedly outperformed by fine-tuned models, they still achieved promising\nresults overall. Our results thus demonstrate the strong potential of different\nvariants of large language models for the Split and Rephrase task, using\nrelatively small amounts of training samples and model parameters overall.\n","authors":["David Ponce","Thierry Etchegoyhen","Jesús Calleja Pérez","Harritxu Gete"],"pdf_url":"https://arxiv.org/pdf/2312.11075v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16583v5","updated":"2023-12-19T07:41:50Z","published":"2023-09-28T16:43:35Z","title":"GPT-Fathom: Benchmarking Large Language Models to Decipher the\n  Evolutionary Path towards GPT-4 and Beyond","summary":"  With the rapid advancement of large language models (LLMs), there is a\npressing need for a comprehensive evaluation suite to assess their capabilities\nand limitations. Existing LLM leaderboards often reference scores reported in\nother papers without consistent settings and prompts, which may inadvertently\nencourage cherry-picking favored settings and prompts for better results. In\nthis work, we introduce GPT-Fathom, an open-source and reproducible LLM\nevaluation suite built on top of OpenAI Evals. We systematically evaluate 10+\nleading LLMs as well as OpenAI's legacy models on 20+ curated benchmarks across\n7 capability categories, all under aligned settings. Our retrospective study on\nOpenAI's earlier models offers valuable insights into the evolutionary path\nfrom GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3\nprogressively improves to GPT-4, including technical details like whether\nadding code data improves LLM's reasoning capability, which aspects of LLM\ncapability can be improved by SFT and RLHF, how much is the alignment tax, etc.\nOur analysis sheds light on many of these questions, aiming to improve the\ntransparency of advanced LLMs.\n","authors":["Shen Zheng","Yuyu Zhang","Yijie Zhu","Chenguang Xi","Pengyang Gao","Xun Zhou","Kevin Chen-Chuan Chang"],"pdf_url":"https://arxiv.org/pdf/2309.16583v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11608v2","updated":"2023-12-19T07:18:24Z","published":"2023-11-20T08:51:30Z","title":"Taiyi: A Bilingual Fine-Tuned Large Language Model for Diverse\n  Biomedical Tasks","summary":"  Objective: Most existing fine-tuned biomedical large language models (LLMs)\nfocus on enhancing performance in monolingual biomedical question answering and\nconversation tasks. To investigate the effectiveness of the fine-tuned LLMs on\ndiverse biomedical NLP tasks in different languages, We present Taiyi, a\nbilingual fine-tuned LLM for diverse biomedical tasks. Materials and Methods:\nWe first curated a comprehensive collection of 140 existing biomedical text\nmining datasets (102 English and 38 Chinese datasets) across over 10 task\ntypes. Subsequently, a two-stage strategy is proposed for supervised\nfine-tuning to optimize the model performance across varied tasks. Results:\nExperimental results on 13 test sets covering named entity recognition,\nrelation extraction, text classification, question answering tasks demonstrate\nthat Taiyi achieves superior performance compared to general LLMs. The case\nstudy involving additional biomedical NLP tasks further shows Taiyi's\nconsiderable potential for bilingual biomedical multi-tasking. Conclusion:\nLeveraging rich high-quality biomedical corpora and developing effective\nfine-tuning strategies can significantly improve the performance of LLMs within\nthe biomedical domain. Taiyi shows the bilingual multi-tasking capability\nthrough supervised fine-tuning. However, those tasks such as information\nextraction that are not generation tasks in nature remain challenging for\nLLM-based generative approaches, and they still underperform the conventional\ndiscriminative approaches of smaller language models.\n","authors":["Ling Luo","Jinzhong Ning","Yingwen Zhao","Zhijun Wang","Zeyuan Ding","Peng Chen","Weiru Fu","Qinyu Han","Guangtao Xu","Yunzhi Qiu","Dinghao Pan","Jiru Li","Hao Li","Wenduo Feng","Senbo Tu","Yuqi Liu","Zhihao Yang","Jian Wang","Yuanyuan Sun","Hongfei Lin"],"pdf_url":"https://arxiv.org/pdf/2311.11608v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11895v1","updated":"2023-12-19T06:39:38Z","published":"2023-12-19T06:39:38Z","title":"Analyzing Public Reactions, Perceptions, and Attitudes during the MPox\n  Outbreak: Findings from Topic Modeling of Tweets","summary":"  The recent outbreak of the MPox virus has resulted in a tremendous increase\nin the usage of Twitter. Prior works in this area of research have primarily\nfocused on the sentiment analysis and content analysis of these Tweets, and the\nfew works that have focused on topic modeling have multiple limitations. This\npaper aims to address this research gap and makes two scientific contributions\nto this field. First, it presents the results of performing Topic Modeling on\n601,432 Tweets about the 2022 Mpox outbreak that were posted on Twitter between\n7 May 2022 and 3 March 2023. The results indicate that the conversations on\nTwitter related to Mpox during this time range may be broadly categorized into\nfour distinct themes - Views and Perspectives about Mpox, Updates on Cases and\nInvestigations about Mpox, Mpox and the LGBTQIA+ Community, and Mpox and\nCOVID-19. Second, the paper presents the findings from the analysis of these\nTweets. The results show that the theme that was most popular on Twitter (in\nterms of the number of Tweets posted) during this time range was Views and\nPerspectives about Mpox. This was followed by the theme of Mpox and the\nLGBTQIA+ Community, which was followed by the themes of Mpox and COVID-19 and\nUpdates on Cases and Investigations about Mpox, respectively. Finally, a\ncomparison with related studies in this area of research is also presented to\nhighlight the novelty and significance of this research work.\n","authors":["Nirmalya Thakur","Yuvraj Nihal Duggal","Zihui Liu"],"pdf_url":"https://arxiv.org/pdf/2312.11895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.07490v4","updated":"2023-12-19T06:27:45Z","published":"2023-05-12T14:04:30Z","title":"ArtGPT-4: Towards Artistic-understanding Large Vision-Language Models\n  with Enhanced Adapter","summary":"  In recent years, advancements in large language models have been remarkable,\nwith models such as ChatGPT demonstrating exceptional proficiency in diverse\nlinguistic tasks. The pre-training of large models with billions of parameters,\nposes a formidable challenge, primarily due to the scarcity of datasets of a\ncommensurate scale for effective training. Nevertheless, innovative strategies\nhave emerged, including methods to fine-tune these pre-trained models using\nfewer parameters set, as evidenced by models like MiniGPT-4 and LLaVA. Despite\ntheir potential in various domains, these models remain limited in their\nunderstanding of artistic imagery. They have yet to fully grasp the intricate\nnuances of art images or to provide an objective articulation of the emotions\nthey evoke, in a manner akin to human perception. This work introduces\nArtGPT-4, a pioneering large vision-language model tailored to address the\ndeficiencies of contemporary models in artistic comprehension. ArtGPT-4\nunderwent training on image-text pairs utilizing a Tesla A100 device in a mere\n2 hours, with a dataset comprising approximately 0.52M entries. Impressively,\nthe model can render images with an artistic-understanding and convey the\nemotions they inspire, mirroring human interpretation. Additionally, this work\npresents a unique dataset designed to evaluate the efficacy of vision-language\nmodels. In subsequent evaluations, ArtGPT-4 not only achieved state-of-the-art\nperformance on the ArtEmis and ArtEmis-v2.0 datasets but also exceeded the\nestablished benchmarks introduced in This study, lagging behind professional\nartists' descriptions by a negligible 0.15 points on a 6-point scale. The code\nand the pre-trained model are accessible in\nhttps://huggingface.co/Tyrannosaurus/ArtGPT-4.\n","authors":["Zhengqing Yuan","Xinyi Wang","Kun Wang","Lichao Sun","Yanfang Ye"],"pdf_url":"https://arxiv.org/pdf/2305.07490v4.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2312.11890v1","updated":"2023-12-19T06:26:25Z","published":"2023-12-19T06:26:25Z","title":"Difficulty-Focused Contrastive Learning for Knowledge Tracing with a\n  Large Language Model-Based Difficulty Prediction","summary":"  This paper presents novel techniques for enhancing the performance of\nknowledge tracing (KT) models by focusing on the crucial factor of question and\nconcept difficulty level. Despite the acknowledged significance of difficulty,\nprevious KT research has yet to exploit its potential for model optimization\nand has struggled to predict difficulty from unseen data. To address these\nproblems, we propose a difficulty-centered contrastive learning method for KT\nmodels and a Large Language Model (LLM)-based framework for difficulty\nprediction. These innovative methods seek to improve the performance of KT\nmodels and provide accurate difficulty estimates for unseen data. Our ablation\nstudy demonstrates the efficacy of these techniques by demonstrating enhanced\nKT model performance. Nonetheless, the complex relationship between language\nand difficulty merits further investigation.\n","authors":["Unggi Lee","Sungjun Yoon","Joon Seo Yun","Kyoungsoo Park","YoungHoon Jung","Damji Stratton","Hyeoncheol Kim"],"pdf_url":"https://arxiv.org/pdf/2312.11890v1.pdf","comment":"10 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2312.11882v1","updated":"2023-12-19T06:16:13Z","published":"2023-12-19T06:16:13Z","title":"ConsistentEE: A Consistent and Hardness-Guided Early Exiting Method for\n  Accelerating Language Models Inference","summary":"  Early Exiting is one of the most popular methods to achieve efficient\ninference. Current early exiting methods adopt the (weighted) sum of the cross\nentropy loss of all internal classifiers during training, imposing all these\nclassifiers to predict all instances correctly. However, during inference, as\nlong as one internal classifier predicts an instance correctly, it can\naccelerate without losing accuracy. Thus, there is a notable gap between\ntraining and inference. We propose ConsistentEE, an early exiting method that\nis consistent in training and inference. ConsistentEE formulates the early\nexiting process as a reinforcement learning problem. A policy network is added\nto decide whether an instance should exit or continue. The training objective\nof ConsistentEE only require each instance to be predicted correctly by one\ninternal classifier. Additionally, we introduce the concept Memorize Layer to\nmeasure the hardness of an instance. We incorporate memorized layer into reward\nfunction design, which allows ``easy'' instances to focus more on acceleration\nwhile ``hard'' instances to focus more on accuracy. Experimental results show\nthat our method outperforms other baselines on various natural language\nunderstanding and generation tasks.\n","authors":["Ziqian Zeng","Yihuai Hong","Hongliang Dai","Huiping Zhuang","Cen Chen"],"pdf_url":"https://arxiv.org/pdf/2312.11882v1.pdf","comment":"Accepted in AAAI24"},{"id":"http://arxiv.org/abs/2312.11881v1","updated":"2023-12-19T06:15:52Z","published":"2023-12-19T06:15:52Z","title":"Punctuation restoration Model and Spacing Model for Korean Ancient\n  Document","summary":"  In Korean ancient documents, there is no spacing or punctuation, and they are\nwritten in classical Chinese characters. This makes it challenging for modern\nindividuals and translation models to accurately interpret and translate them.\nWhile China has models predicting punctuation and spacing, applying them\ndirectly to Korean texts is problematic due to data differences. Therefore, we\ndeveloped the first models which predict punctuation and spacing for Korean\nhistorical texts and evaluated their performance. Our punctuation restoration\nmodel achieved an F1 score of 0.84, and Spacing model achieved a score of 0.96.\nIt has the advantage of enabling inference on low-performance GPUs with less\nVRAM while maintaining quite high accuracy.\n","authors":["Taehong Jang","Joonmo Ahn","Sojung Lucia Kim"],"pdf_url":"https://arxiv.org/pdf/2312.11881v1.pdf","comment":"5 Pages, 2 Figures"},{"id":"http://arxiv.org/abs/2312.11875v1","updated":"2023-12-19T06:06:30Z","published":"2023-12-19T06:06:30Z","title":"Sparse is Enough in Fine-tuning Pre-trained Large Language Model","summary":"  With the prevalence of pre-training-fine-tuning paradigm, how to efficiently\nadapt the pre-trained model to the downstream tasks has been an intriguing\nissue. Parameter-Efficient Fine-Tuning (PEFT) methods have been proposed for\nlow-cost adaptation, including Adapters, Bia-only, and the recently widely used\nLow-Rank Adaptation. Although these methods have demonstrated their\neffectiveness to some extent and have been widely applied, the underlying\nprinciples are still unclear. In this paper, we reveal the transition of loss\nlandscape in the downstream domain from random initialization to pre-trained\ninitialization, that is, from low-amplitude oscillation to high-amplitude\noscillation. The parameter gradients exhibit a property akin to sparsity, where\na small fraction of components dominate the total gradient norm, for instance,\n1% of the components account for 99% of the gradient. This property ensures\nthat the pre-trained model can easily find a flat minimizer which guarantees\nthe model's ability to generalize even with a low number of trainable\nparameters. Based on this, we propose a gradient-based sparse fine-tuning\nalgorithm, named Sparse Increment Fine-Tuning (SIFT), and validate its\neffectiveness on a range of tasks including the GLUE Benchmark and\nInstruction-tuning. The code is accessible at https://github.com/song-wx/SIFT/.\n","authors":["Weixi Song","Zuchao Li","Lefei Zhang","Hai Zhao","Bo Du"],"pdf_url":"https://arxiv.org/pdf/2312.11875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11870v1","updated":"2023-12-19T05:46:11Z","published":"2023-12-19T05:46:11Z","title":"A Revisit of Fake News Dataset with Augmented Fact-checking by ChatGPT","summary":"  The proliferation of fake news has emerged as a critical issue in recent\nyears, requiring significant efforts to detect it. However, the existing fake\nnews detection datasets are sourced from human journalists, which are likely to\nhave inherent bias limitations due to the highly subjective nature of this\ntask. In this paper, we revisit the existing fake news dataset verified by\nhuman journalists with augmented fact-checking by large language models\n(ChatGPT), and we name the augmented fake news dataset ChatGPT-FC. We\nquantitatively analyze the distinctions and resemblances between human\njournalists and LLM in assessing news subject credibility, news creator\ncredibility, time-sensitive, and political framing. Our findings highlight\nLLM's potential to serve as a preliminary screening method, offering a\npromising avenue to mitigate the inherent biases of human journalists and\nenhance fake news detection.\n","authors":["Zizhong Li","Haopeng Zhang","Jiawei Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.11870v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11276v2","updated":"2023-12-19T05:29:28Z","published":"2023-12-18T15:18:57Z","title":"Compositional Generalization for Multi-label Text Classification: A\n  Data-Augmentation Approach","summary":"  Despite significant advancements in multi-label text classification, the\nability of existing models to generalize to novel and seldom-encountered\ncomplex concepts, which are compositions of elementary ones, remains\nunderexplored. This research addresses this gap. By creating unique data splits\nacross three benchmarks, we assess the compositional generalization ability of\nexisting multi-label text classification models. Our results show that these\nmodels often fail to generalize to compositional concepts encountered\ninfrequently during training, leading to inferior performance on tests with\nthese new combinations. To address this, we introduce a data augmentation\nmethod that leverages two innovative text generation models designed to enhance\nthe classification models' capacity for compositional generalization. Our\nexperiments show that this data augmentation approach significantly improves\nthe compositional generalization capabilities of classification models on our\nbenchmarks, with both generation models surpassing other text generation\nbaselines.\n","authors":["Yuyang Chai","Zhuang Li","Jiahui Liu","Lei Chen","Fei Li","Donghong Ji","Chong Teng"],"pdf_url":"https://arxiv.org/pdf/2312.11276v2.pdf","comment":"Accepted by AAAI'24"},{"id":"http://arxiv.org/abs/2312.10793v2","updated":"2023-12-19T04:52:40Z","published":"2023-12-17T18:44:26Z","title":"Understanding the Instruction Mixture for Large Language Model\n  Fine-tuning","summary":"  While instructions fine-tuning of large language models (LLMs) has been\nproven to enhance performance across various applications, the influence of the\ninstruction dataset mixture on LLMs has not been thoroughly explored. In this\nstudy, we classify instructions into three main types: NLP downstream tasks,\ncoding, and general chatting, and investigate their impact on LLMs. Our\nfindings reveal that specific types of instructions are more beneficial for\nparticular uses, while it may cause harms to other aspects, emphasizing the\nimportance of meticulously designing the instruction mixture to maximize model\nperformance. This study sheds light on the instruction mixture and paves the\nway for future research.\n","authors":["Renxi Wang","Minghao Wu","Yuxia Wang","Xudong Han","Chiyu Zhang","Haonan Li"],"pdf_url":"https://arxiv.org/pdf/2312.10793v2.pdf","comment":"Instruction Tuning, Large Language Model, Alignment"},{"id":"http://arxiv.org/abs/2312.11852v1","updated":"2023-12-19T04:42:56Z","published":"2023-12-19T04:42:56Z","title":"Predicting Human Translation Difficulty with Neural Machine Translation","summary":"  Human translators linger on some words and phrases more than others, and\npredicting this variation is a step towards explaining the underlying cognitive\nprocesses. Using data from the CRITT Translation Process Research Database, we\nevaluate the extent to which surprisal and attentional features derived from a\nNeural Machine Translation (NMT) model account for reading and production times\nof human translators. We find that surprisal and attention are complementary\npredictors of translation difficulty, and that surprisal derived from a NMT\nmodel is the single most successful predictor of production duration. Our\nanalyses draw on data from hundreds of translators operating across 13 language\npairs, and represent the most comprehensive investigation of human translation\ndifficulty to date.\n","authors":["Zheng Wei Lim","Ekaterina Vylomova","Charles Kemp","Trevor Cohn"],"pdf_url":"https://arxiv.org/pdf/2312.11852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11111v2","updated":"2023-12-19T04:27:47Z","published":"2023-12-18T11:19:45Z","title":"The Good, The Bad, and Why: Unveiling Emotions in Generative AI","summary":"  Emotion significantly impacts our daily behaviors and interactions. While\nrecent generative AI models, such as large language models, have shown\nimpressive performance in various tasks, it remains unclear whether they truly\ncomprehend emotions. This paper aims to address this gap by incorporating\npsychological theories to gain a holistic understanding of emotions in\ngenerative AI models. Specifically, we propose three approaches: 1)\nEmotionPrompt to enhance AI model performance, 2) EmotionAttack to impair AI\nmodel performance, and 3) EmotionDecode to explain the effects of emotional\nstimuli, both benign and malignant. Through extensive experiments involving\nlanguage and multi-modal models on semantic understanding, logical reasoning,\nand generation tasks, we demonstrate that both textual and visual EmotionPrompt\ncan boost the performance of AI models while EmotionAttack can hinder it.\nAdditionally, EmotionDecode reveals that AI models can comprehend emotional\nstimuli akin to the mechanism of dopamine in the human brain. Our work heralds\na novel avenue for exploring psychology to enhance our understanding of\ngenerative AI models. This paper is an extended version of our previous work\nEmotionPrompt (arXiv:2307.11760).\n","authors":["Cheng Li","Jindong Wang","Yixuan Zhang","Kaijie Zhu","Xinyi Wang","Wenxin Hou","Jianxun Lian","Fang Luo","Qiang Yang","Xing Xie"],"pdf_url":"https://arxiv.org/pdf/2312.11111v2.pdf","comment":"Technical report; an extension to EmotionPrompt (arXiv:2307.11760);\n  34 pages"},{"id":"http://arxiv.org/abs/2312.10302v2","updated":"2023-12-19T03:48:21Z","published":"2023-12-16T03:33:12Z","title":"One Shot Learning as Instruction Data Prospector for Large Language\n  Models","summary":"  Aligning large language models(LLMs) with human is a critical step in\neffectively utilizing their pre-trained capabilities across a wide array of\nlanguage tasks. Current instruction tuning practices often rely on expanding\ndataset size without a clear strategy for ensuring data quality, which can\ninadvertently introduce noise and degrade model performance. To address this\nchallenge, we introduce Nuggets, a novel and efficient methodology that employs\none shot learning to select high-quality instruction data from expansive\ndatasets. Nuggets assesses the potential of individual instruction examples to\nact as effective one shot examples, thereby identifying those that can\nsignificantly enhance diverse task performance. Nuggets utilizes a scoring\nsystem based on the impact of candidate examples on the perplexity of a diverse\nanchor set, facilitating the selection of the most beneficial data for\ninstruction tuning. Through rigorous testing on two benchmarks, including\nMT-Bench and Alpaca-Eval, we demonstrate that instruction tuning with the top\n1% of Nuggets-curated examples substantially outperforms conventional methods\nthat use the full dataset. These findings advocate for a data selection\nparadigm that prioritizes quality, offering a more efficient pathway to align\nLLMs with humans.\n","authors":["Yunshui Li","Binyuan Hui","Xiaobo Xia","Jiaxi Yang","Min Yang","Lei Zhang","Shuzheng Si","Junhao Liu","Tongliang Liu","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2312.10302v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07594v2","updated":"2023-12-19T03:44:25Z","published":"2023-11-10T09:51:24Z","title":"How to Bridge the Gap between Modalities: A Comprehensive Survey on\n  Multimodal Large Language Model","summary":"  This review paper explores Multimodal Large Language Models (MLLMs), which\nintegrate Large Language Models (LLMs) like GPT-4 to handle multimodal data\nsuch as text and vision. MLLMs demonstrate capabilities like generating image\nnarratives and answering image-based questions, bridging the gap towards\nreal-world human-computer interactions and hinting at a potential pathway to\nartificial general intelligence. However, MLLMs still face challenges in\nprocessing the semantic gap in multimodality, which may lead to erroneous\ngeneration, posing potential risks to society. Choosing the appropriate\nmodality alignment method is crucial, as improper methods might require more\nparameters with limited performance improvement. This paper aims to explore\nmodality alignment methods for LLMs and their existing capabilities.\nImplementing modality alignment allows LLMs to address environmental issues and\nenhance accessibility. The study surveys existing modal alignment methods in\nMLLMs into four groups: (1) Multimodal Converters that change data into\nsomething LLMs can understand; (2) Multimodal Perceivers to improve how LLMs\nperceive different types of data; (3) Tools Assistance for changing data into\none common format, usually text; and (4) Data-Driven methods that teach LLMs to\nunderstand specific types of data in a dataset. This field is still in a phase\nof exploration and experimentation, and we will organize and update various\nexisting research methods for multimodal information alignment.\n","authors":["Shezheng Song","Xiaopeng Li","Shasha Li","Shan Zhao","Jie Yu","Jun Ma","Xiaoguang Mao","Weimin Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.07594v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11828v1","updated":"2023-12-19T03:39:23Z","published":"2023-12-19T03:39:23Z","title":"TESS: A Multi-intent Parser for Conversational Multi-Agent Systems with\n  Decentralized Natural Language Understanding Models","summary":"  Chatbots have become one of the main pathways for the delivery of business\nautomation tools. Multi-agent systems offer a framework for designing chatbots\nat scale, making it easier to support complex conversations that span across\nmultiple domains as well as enabling developers to maintain and expand their\ncapabilities incrementally over time. However, multi-agent systems complicate\nthe natural language understanding (NLU) of user intents, especially when they\nrely on decentralized NLU models: some utterances (termed single intent) may\ninvoke a single agent while others (termed multi-intent) may explicitly invoke\nmultiple agents. Without correctly parsing multi-intent inputs, decentralized\nNLU approaches will not achieve high prediction accuracy. In this paper, we\npropose an efficient parsing and orchestration pipeline algorithm to service\nmulti-intent utterances from the user in the context of a multi-agent system.\nOur proposed approach achieved comparable performance to competitive deep\nlearning models on three different datasets while being up to 48 times faster.\n","authors":["Burak Aksar","Yara Rizk","Tathagata Chakraborti"],"pdf_url":"https://arxiv.org/pdf/2312.11828v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2208.11790v2","updated":"2023-12-19T03:32:26Z","published":"2022-08-24T22:44:09Z","title":"Addressing Token Uniformity in Transformers via Singular Value\n  Transformation","summary":"  Token uniformity is commonly observed in transformer-based models, in which\ndifferent tokens share a large proportion of similar information after going\nthrough stacked multiple self-attention layers in a transformer. In this paper,\nwe propose to use the distribution of singular values of outputs of each\ntransformer layer to characterise the phenomenon of token uniformity and\nempirically illustrate that a less skewed singular value distribution can\nalleviate the `token uniformity' problem. Base on our observations, we define\nseveral desirable properties of singular value distributions and propose a\nnovel transformation function for updating the singular values. We show that\napart from alleviating token uniformity, the transformation function should\npreserve the local neighbourhood structure in the original embedding space. Our\nproposed singular value transformation function is applied to a range of\ntransformer-based language models such as BERT, ALBERT, RoBERTa and DistilBERT,\nand improved performance is observed in semantic textual similarity evaluation\nand a range of GLUE tasks. Our source code is available at\nhttps://github.com/hanqi-qi/tokenUni.git.\n","authors":["Hanqi Yan","Lin Gui","Wenjie Li","Yulan He"],"pdf_url":"https://arxiv.org/pdf/2208.11790v2.pdf","comment":"UAI2022 Main Conference, Spotlight, combined with supplementary files"},{"id":"http://arxiv.org/abs/2312.11819v1","updated":"2023-12-19T03:24:55Z","published":"2023-12-19T03:24:55Z","title":"An Adaptive Placement and Parallelism Framework for Accelerating RLHF\n  Training","summary":"  Recently, ChatGPT or InstructGPT like large language models (LLM) has made a\nsignificant impact in the AI world. These models are incredibly versatile,\ncapable of performing language tasks on par or even exceeding the capabilities\nof human experts. Many works have attempted to reproduce the complex\nInstructGPT's RLHF (Reinforcement Learning with Human Feedback) training\npipeline. However, the mainstream distributed RLHF training methods typically\nadopt a fixed model placement strategy, referred to as the Flattening strategy.\nThis strategy treats all four models involved in RLHF as a single entity and\nplaces them on all devices, regardless of their differences. Unfortunately,\nthis strategy exacerbates the generation bottlenecks in the RLHF training and\ndegrades the overall training efficiency. To address these issues, we propose\nan adaptive model placement framework that offers two flexible model placement\nstrategies. These strategies allow for the agile allocation of models across\ndevices in a fine-grained manner. The Interleaving strategy helps reduce memory\nredundancy and communication costs during RLHF training. On the other hand, the\nSeparation strategy improves the throughput of model training by separating the\ntraining and generation stages of the RLHF pipeline. Notably, this framework\nseamlessly integrates with other mainstream techniques for acceleration and\nenables automatic hyperparameter search. Extensive experiments have\ndemonstrated that our Interleaving and Separation strategies can achieve\nnotable improvements up to 11x, compared to the current state-of-the-art (SOTA)\napproaches. These experiments encompassed a wide range of training scenarios,\ninvolving models of varying sizes and devices of different scales. The results\nhighlight the effectiveness and superiority of our approaches in accelerating\nthe training of distributed RLHF.\n","authors":["Youshao Xiao","Weichang Wu","Zhenglei Zhou","Fagui Mao","Shangchun Zhao","Lin Ju","Lei Liang","Xiaolu Zhang","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2312.11819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.03518v3","updated":"2023-12-19T02:57:31Z","published":"2021-06-07T11:14:58Z","title":"Position Bias Mitigation: A Knowledge-Aware Graph Model for Emotion\n  Cause Extraction","summary":"  The Emotion Cause Extraction (ECE)} task aims to identify clauses which\ncontain emotion-evoking information for a particular emotion expressed in text.\nWe observe that a widely-used ECE dataset exhibits a bias that the majority of\nannotated cause clauses are either directly before their associated emotion\nclauses or are the emotion clauses themselves. Existing models for ECE tend to\nexplore such relative position information and suffer from the dataset bias. To\ninvestigate the degree of reliance of existing ECE models on clause relative\npositions, we propose a novel strategy to generate adversarial examples in\nwhich the relative position information is no longer the indicative feature of\ncause clauses. We test the performance of existing models on such adversarial\nexamples and observe a significant performance drop. To address the dataset\nbias, we propose a novel graph-based method to explicitly model the emotion\ntriggering paths by leveraging the commonsense knowledge to enhance the\nsemantic dependencies between a candidate clause and an emotion clause.\nExperimental results show that our proposed approach performs on par with the\nexisting state-of-the-art methods on the original ECE dataset, and is more\nrobust against adversarial attacks compared to existing models.\n","authors":["Hanqi Yan","Lin Gui","Gabriele Pergola","Yulan He"],"pdf_url":"https://arxiv.org/pdf/2106.03518v3.pdf","comment":"ACL2021 Main Conference, Oral paper"},{"id":"http://arxiv.org/abs/2312.11805v1","updated":"2023-12-19T02:39:27Z","published":"2023-12-19T02:39:27Z","title":"Gemini: A Family of Highly Capable Multimodal Models","summary":"  This report introduces a new family of multimodal models, Gemini, that\nexhibit remarkable capabilities across image, audio, video, and text\nunderstanding. The Gemini family consists of Ultra, Pro, and Nano sizes,\nsuitable for applications ranging from complex reasoning tasks to on-device\nmemory-constrained use-cases. Evaluation on a broad range of benchmarks shows\nthat our most-capable Gemini Ultra model advances the state of the art in 30 of\n32 of these benchmarks - notably being the first model to achieve human-expert\nperformance on the well-studied exam benchmark MMLU, and improving the state of\nthe art in every one of the 20 multimodal benchmarks we examined. We believe\nthat the new capabilities of Gemini models in cross-modal reasoning and\nlanguage understanding will enable a wide variety of use cases and we discuss\nour approach toward deploying them responsibly to users.\n","authors":[" Gemini Team","Rohan Anil","Sebastian Borgeaud","Yonghui Wu","Jean-Baptiste Alayrac","Jiahui Yu","Radu Soricut","Johan Schalkwyk","Andrew M. Dai","Anja Hauth","Katie Millican","David Silver","Slav Petrov","Melvin Johnson","Ioannis Antonoglou","Julian Schrittwieser","Amelia Glaese","Jilin Chen","Emily Pitler","Timothy Lillicrap","Angeliki Lazaridou","Orhan Firat","James Molloy","Michael Isard","Paul R. Barham","Tom Hennigan","Benjamin Lee","Fabio Viola","Malcolm Reynolds","Yuanzhong Xu","Ryan Doherty","Eli Collins","Clemens Meyer","Eliza Rutherford","Erica Moreira","Kareem Ayoub","Megha Goel","George Tucker","Enrique Piqueras","Maxim Krikun","Iain Barr","Nikolay Savinov","Ivo Danihelka","Becca Roelofs","Anaïs White","Anders Andreassen","Tamara von Glehn","Lakshman Yagati","Mehran Kazemi","Lucas Gonzalez","Misha Khalman","Jakub Sygnowski","Alexandre Frechette","Charlotte Smith","Laura Culp","Lev Proleev","Yi Luan","Xi Chen","James Lottes","Nathan Schucher","Federico Lebron","Alban Rrustemi","Natalie Clay","Phil Crone","Tomas Kocisky","Jeffrey Zhao","Bartek Perz","Dian Yu","Heidi Howard","Adam Bloniarz","Jack W. Rae","Han Lu","Laurent Sifre","Marcello Maggioni","Fred Alcober","Dan Garrette","Megan Barnes","Shantanu Thakoor","Jacob Austin","Gabriel Barth-Maron","William Wong","Rishabh Joshi","Rahma Chaabouni","Deeni Fatiha","Arun Ahuja","Ruibo Liu","Yunxuan Li","Sarah Cogan","Jeremy Chen","Chao Jia","Chenjie Gu","Qiao Zhang","Jordan Grimstad","Ale Jakse Hartman","Martin Chadwick","Gaurav Singh Tomar","Xavier Garcia","Evan Senter","Emanuel Taropa","Thanumalayan Sankaranarayana Pillai","Jacob Devlin","Michael Laskin","Diego de Las Casas","Dasha Valter","Connie Tao","Lorenzo Blanco","Adrià Puigdomènech Badia","David Reitter","Mianna Chen","Jenny Brennan","Clara Rivera","Sergey Brin","Shariq Iqbal","Gabriela Surita","Jane Labanowski","Abhi Rao","Stephanie Winkler","Emilio Parisotto","Yiming Gu","Kate Olszewska","Yujing Zhang","Ravi Addanki","Antoine Miech","Annie Louis","Laurent El Shafey","Denis Teplyashin","Geoff Brown","Elliot Catt","Nithya Attaluri","Jan Balaguer","Jackie Xiang","Pidong Wang","Zoe Ashwood","Anton Briukhov","Albert Webson","Sanjay Ganapathy","Smit Sanghavi","Ajay Kannan","Ming-Wei Chang","Axel Stjerngren","Josip Djolonga","Yuting Sun","Ankur Bapna","Matthew Aitchison","Pedram Pejman","Henryk Michalewski","Tianhe Yu","Cindy Wang","Juliette Love","Junwhan Ahn","Dawn Bloxwich","Kehang Han","Peter Humphreys","Thibault Sellam","James Bradbury","Varun Godbole","Sina Samangooei","Bogdan Damoc","Alex Kaskasoli","Sébastien M. R. Arnold","Vijay Vasudevan","Shubham Agrawal","Jason Riesa","Dmitry Lepikhin","Richard Tanburn","Srivatsan Srinivasan","Hyeontaek Lim","Sarah Hodkinson","Pranav Shyam","Johan Ferret","Steven Hand","Ankush Garg","Tom Le Paine","Jian Li","Yujia Li","Minh Giang","Alexander Neitz","Zaheer Abbas","Sarah York","Machel Reid","Elizabeth Cole","Aakanksha Chowdhery","Dipanjan Das","Dominika Rogozińska","Vitaly Nikolaev","Pablo Sprechmann","Zachary Nado","Lukas Zilka","Flavien Prost","Luheng He","Marianne Monteiro","Gaurav Mishra","Chris Welty","Josh Newlan","Dawei Jia","Miltiadis Allamanis","Clara Huiyi Hu","Raoul de Liedekerke","Justin Gilmer","Carl Saroufim","Shruti Rijhwani","Shaobo Hou","Disha Shrivastava","Anirudh Baddepudi","Alex Goldin","Adnan Ozturel","Albin Cassirer","Yunhan Xu","Daniel Sohn","Devendra Sachan","Reinald Kim Amplayo","Craig Swanson","Dessie Petrova","Shashi Narayan","Arthur Guez","Siddhartha Brahma","Jessica Landon","Miteyan Patel","Ruizhe Zhao","Kevin Villela","Luyu Wang","Wenhao Jia","Matthew Rahtz","Mai Giménez","Legg Yeung","Hanzhao Lin","James Keeling","Petko Georgiev","Diana Mincu","Boxi Wu","Salem Haykal","Rachel Saputro","Kiran Vodrahalli","James Qin","Zeynep Cankara","Abhanshu Sharma","Nick Fernando","Will Hawkins","Behnam Neyshabur","Solomon Kim","Adrian Hutter","Priyanka Agrawal","Alex Castro-Ros","George van den Driessche","Tao Wang","Fan Yang","Shuo-yiin Chang","Paul Komarek","Ross McIlroy","Mario Lučić","Guodong Zhang","Wael Farhan","Michael Sharman","Paul Natsev","Paul Michel","Yong Cheng","Yamini Bansal","Siyuan Qiao","Kris Cao","Siamak Shakeri","Christina Butterfield","Justin Chung","Paul Kishan Rubenstein","Shivani Agrawal","Arthur Mensch","Kedar Soparkar","Karel Lenc","Timothy Chung","Aedan Pope","Loren Maggiore","Jackie Kay","Priya Jhakra","Shibo Wang","Joshua Maynez","Mary Phuong","Taylor Tobin","Andrea Tacchetti","Maja Trebacz","Kevin Robinson","Yash Katariya","Sebastian Riedel","Paige Bailey","Kefan Xiao","Nimesh Ghelani","Lora Aroyo","Ambrose Slone","Neil Houlsby","Xuehan Xiong","Zhen Yang","Elena Gribovskaya","Jonas Adler","Mateo Wirth","Lisa Lee","Music Li","Thais Kagohara","Jay Pavagadhi","Sophie Bridgers","Anna Bortsova","Sanjay Ghemawat","Zafarali Ahmed","Tianqi Liu","Richard Powell","Vijay Bolina","Mariko Iinuma","Polina Zablotskaia","James Besley","Da-Woon Chung","Timothy Dozat","Ramona Comanescu","Xiance Si","Jeremy Greer","Guolong Su","Martin Polacek","Raphaël Lopez Kaufman","Simon Tokumine","Hexiang Hu","Elena Buchatskaya","Yingjie Miao","Mohamed Elhawaty","Aditya Siddhant","Nenad Tomasev","Jinwei Xing","Christina Greer","Helen Miller","Shereen Ashraf","Aurko Roy","Zizhao Zhang","Ada Ma","Angelos Filos","Milos Besta","Rory Blevins","Ted Klimenko","Chih-Kuan Yeh","Soravit Changpinyo","Jiaqi Mu","Oscar Chang","Mantas Pajarskas","Carrie Muir","Vered Cohen","Charline Le Lan","Krishna Haridasan","Amit Marathe","Steven Hansen","Sholto Douglas","Rajkumar Samuel","Mingqiu Wang","Sophia Austin","Chang Lan","Jiepu Jiang","Justin Chiu","Jaime Alonso Lorenzo","Lars Lowe Sjösund","Sébastien Cevey","Zach Gleicher","Thi Avrahami","Anudhyan Boral","Hansa Srinivasan","Vittorio Selo","Rhys May","Konstantinos Aisopos","Léonard Hussenot","Livio Baldini Soares","Kate Baumli","Michael B. Chang","Adrià Recasens","Ben Caine","Alexander Pritzel","Filip Pavetic","Fabio Pardo","Anita Gergely","Justin Frye","Vinay Ramasesh","Dan Horgan","Kartikeya Badola","Nora Kassner","Subhrajit Roy","Ethan Dyer","Víctor Campos","Alex Tomala","Yunhao Tang","Dalia El Badawy","Elspeth White","Basil Mustafa","Oran Lang","Abhishek Jindal","Sharad Vikram","Zhitao Gong","Sergi Caelles","Ross Hemsley","Gregory Thornton","Fangxiaoyu Feng","Wojciech Stokowiec","Ce Zheng","Phoebe Thacker","Çağlar Ünlü","Zhishuai Zhang","Mohammad Saleh","James Svensson","Max Bileschi","Piyush Patil","Ankesh Anand","Roman Ring","Katerina Tsihlas","Arpi Vezer","Marco Selvi","Toby Shevlane","Mikel Rodriguez","Tom Kwiatkowski","Samira Daruki","Keran Rong","Allan Dafoe","Nicholas FitzGerald","Keren Gu-Lemberg","Mina Khan","Lisa Anne Hendricks","Marie Pellat","Vladimir Feinberg","James Cobon-Kerr","Tara Sainath","Maribeth Rauh","Sayed Hadi Hashemi","Richard Ives","Yana Hasson","YaGuang Li","Eric Noland","Yuan Cao","Nathan Byrd","Le Hou","Qingze Wang","Thibault Sottiaux","Michela Paganini","Jean-Baptiste Lespiau","Alexandre Moufarek","Samer Hassan","Kaushik Shivakumar","Joost van Amersfoort","Amol Mandhane","Pratik Joshi","Anirudh Goyal","Matthew Tung","Andrew Brock","Hannah Sheahan","Vedant Misra","Cheng Li","Nemanja Rakićević","Mostafa Dehghani","Fangyu Liu","Sid Mittal","Junhyuk Oh","Seb Noury","Eren Sezener","Fantine Huot","Matthew Lamm","Nicola De Cao","Charlie Chen","Gamaleldin Elsayed","Ed Chi","Mahdis Mahdieh","Ian Tenney","Nan Hua","Ivan Petrychenko","Patrick Kane","Dylan Scandinaro","Rishub Jain","Jonathan Uesato","Romina Datta","Adam Sadovsky","Oskar Bunyan","Dominik Rabiej","Shimu Wu","John Zhang","Gautam Vasudevan","Edouard Leurent","Mahmoud Alnahlawi","Ionut Georgescu","Nan Wei","Ivy Zheng","Betty Chan","Pam G Rabinovitch","Piotr Stanczyk","Ye Zhang","David Steiner","Subhajit Naskar","Michael Azzam","Matthew Johnson","Adam Paszke","Chung-Cheng Chiu","Jaume Sanchez Elias","Afroz Mohiuddin","Faizan Muhammad","Jin Miao","Andrew Lee","Nino Vieillard","Sahitya Potluri","Jane Park","Elnaz Davoodi","Jiageng Zhang","Jeff Stanway","Drew Garmon","Abhijit Karmarkar","Zhe Dong","Jong Lee","Aviral Kumar","Luowei Zhou","Jonathan Evens","William Isaac","Zhe Chen","Johnson Jia","Anselm Levskaya","Zhenkai Zhu","Chris Gorgolewski","Peter Grabowski","Yu Mao","Alberto Magni","Kaisheng Yao","Javier Snaider","Norman Casagrande","Paul Suganthan","Evan Palmer","Geoffrey Irving","Edward Loper","Manaal Faruqui","Isha Arkatkar","Nanxin Chen","Izhak Shafran","Michael Fink","Alfonso Castaño","Irene Giannoumis","Wooyeol Kim","Mikołaj Rybiński","Ashwin Sreevatsa","Jennifer Prendki","David Soergel","Adrian Goedeckemeyer","Willi Gierke","Mohsen Jafari","Meenu Gaba","Jeremy Wiesner","Diana Gage Wright","Yawen Wei","Harsha Vashisht","Yana Kulizhskaya","Jay Hoover","Maigo Le","Lu Li","Chimezie Iwuanyanwu","Lu Liu","Kevin Ramirez","Andrey Khorlin","Albert Cui","Tian LIN","Marin Georgiev","Marcus Wu","Ricardo Aguilar","Keith Pallo","Abhishek Chakladar","Alena Repina","Xihui Wu","Tom van der Weide","Priya Ponnapalli","Caroline Kaplan","Jiri Simsa","Shuangfeng Li","Olivier Dousse","Fan Yang","Jeff Piper","Nathan Ie","Minnie Lui","Rama Pasumarthi","Nathan Lintz","Anitha Vijayakumar","Lam Nguyen Thiet","Daniel Andor","Pedro Valenzuela","Cosmin Paduraru","Daiyi Peng","Katherine Lee","Shuyuan Zhang","Somer Greene","Duc Dung Nguyen","Paula Kurylowicz","Sarmishta Velury","Sebastian Krause","Cassidy Hardin","Lucas Dixon","Lili Janzer","Kiam Choo","Ziqiang Feng","Biao Zhang","Achintya Singhal","Tejasi Latkar","Mingyang Zhang","Quoc Le","Elena Allica Abellan","Dayou Du","Dan McKinnon","Natasha Antropova","Tolga Bolukbasi","Orgad Keller","David Reid","Daniel Finchelstein","Maria Abi Raad","Remi Crocker","Peter Hawkins","Robert Dadashi","Colin Gaffney","Sid Lall","Ken Franko","Egor Filonov","Anna Bulanova","Rémi Leblond","Vikas Yadav","Shirley Chung","Harry Askham","Luis C. Cobo","Kelvin Xu","Felix Fischer","Jun Xu","Christina Sorokin","Chris Alberti","Chu-Cheng Lin","Colin Evans","Hao Zhou","Alek Dimitriev","Hannah Forbes","Dylan Banarse","Zora Tung","Jeremiah Liu","Mark Omernick","Colton Bishop","Chintu Kumar","Rachel Sterneck","Ryan Foley","Rohan Jain","Swaroop Mishra","Jiawei Xia","Taylor Bos","Geoffrey Cideron","Ehsan Amid","Francesco Piccinno","Xingyu Wang","Praseem Banzal","Petru Gurita","Hila Noga","Premal Shah","Daniel J. Mankowitz","Alex Polozov","Nate Kushman","Victoria Krakovna","Sasha Brown","MohammadHossein Bateni","Dennis Duan","Vlad Firoiu","Meghana Thotakuri","Tom Natan","Anhad Mohananey","Matthieu Geist","Sidharth Mudgal","Sertan Girgin","Hui Li","Jiayu Ye","Ofir Roval","Reiko Tojo","Michael Kwong","James Lee-Thorp","Christopher Yew","Quan Yuan","Sumit Bagri","Danila Sinopalnikov","Sabela Ramos","John Mellor","Abhishek Sharma","Aliaksei Severyn","Jonathan Lai","Kathy Wu","Heng-Tze Cheng","David Miller","Nicolas Sonnerat","Denis Vnukov","Rory Greig","Jennifer Beattie","Emily Caveness","Libin Bai","Julian Eisenschlos","Alex Korchemniy","Tomy Tsai","Mimi Jasarevic","Weize Kong","Phuong Dao","Zeyu Zheng","Frederick Liu","Fan Yang","Rui Zhu","Mark Geller","Tian Huey Teh","Jason Sanmiya","Evgeny Gladchenko","Nejc Trdin","Andrei Sozanschi","Daniel Toyama","Evan Rosen","Sasan Tavakkol","Linting Xue","Chen Elkind","Oliver Woodman","John Carpenter","George Papamakarios","Rupert Kemp","Sushant Kafle","Tanya Grunina","Rishika Sinha","Alice Talbert","Abhimanyu Goyal","Diane Wu","Denese Owusu-Afriyie","Cosmo Du","Chloe Thornton","Jordi Pont-Tuset","Pradyumna Narayana","Jing Li","Sabaer Fatehi","John Wieting","Omar Ajmeri","Benigno Uria","Tao Zhu","Yeongil Ko","Laura Knight","Amélie Héliou","Ning Niu","Shane Gu","Chenxi Pang","Dustin Tran","Yeqing Li","Nir Levine","Ariel Stolovich","Norbert Kalb","Rebeca Santamaria-Fernandez","Sonam Goenka","Wenny Yustalim","Robin Strudel","Ali Elqursh","Balaji Lakshminarayanan","Charlie Deck","Shyam Upadhyay","Hyo Lee","Mike Dusenberry","Zonglin Li","Xuezhi Wang","Kyle Levin","Raphael Hoffmann","Dan Holtmann-Rice","Olivier Bachem","Summer Yue","Sho Arora","Eric Malmi","Daniil Mirylenka","Qijun Tan","Christy Koh","Soheil Hassas Yeganeh","Siim Põder","Steven Zheng","Francesco Pongetti","Mukarram Tariq","Yanhua Sun","Lucian Ionita","Mojtaba Seyedhosseini","Pouya Tafti","Ragha Kotikalapudi","Zhiyu Liu","Anmol Gulati","Jasmine Liu","Xinyu Ye","Bart Chrzaszcz","Lily Wang","Nikhil Sethi","Tianrun Li","Ben Brown","Shreya Singh","Wei Fan","Aaron Parisi","Joe Stanton","Chenkai Kuang","Vinod Koverkathu","Christopher A. Choquette-Choo","Yunjie Li","TJ Lu","Abe Ittycheriah","Prakash Shroff","Pei Sun","Mani Varadarajan","Sanaz Bahargam","Rob Willoughby","David Gaddy","Ishita Dasgupta","Guillaume Desjardins","Marco Cornero","Brona Robenek","Bhavishya Mittal","Ben Albrecht","Ashish Shenoy","Fedor Moiseev","Henrik Jacobsson","Alireza Ghaffarkhah","Morgane Rivière","Alanna Walton","Clément Crepy","Alicia Parrish","Yuan Liu","Zongwei Zhou","Clement Farabet","Carey Radebaugh","Praveen Srinivasan","Claudia van der Salm","Andreas Fidjeland","Salvatore Scellato","Eri Latorre-Chimoto","Hanna Klimczak-Plucińska","David Bridson","Dario de Cesare","Tom Hudson","Piermaria Mendolicchio","Lexi Walker","Alex Morris","Ivo Penchev","Matthew Mauger","Alexey Guseynov","Alison Reid","Seth Odoom","Lucia Loher","Victor Cotruta","Madhavi Yenugula","Dominik Grewe","Anastasia Petrushkina","Tom Duerig","Antonio Sanchez","Steve Yadlowsky","Amy Shen","Amir Globerson","Adam Kurzrok","Lynette Webb","Sahil Dua","Dong Li","Preethi Lahoti","Surya Bhupatiraju","Dan Hurt","Haroon Qureshi","Ananth Agarwal","Tomer Shani","Matan Eyal","Anuj Khare","Shreyas Rammohan Belle","Lei Wang","Chetan Tekur","Mihir Sanjay Kale","Jinliang Wei","Ruoxin Sang","Brennan Saeta","Tyler Liechty","Yi Sun","Yao Zhao","Stephan Lee","Pandu Nayak","Doug Fritz","Manish Reddy Vuyyuru","John Aslanides","Nidhi Vyas","Martin Wicke","Xiao Ma","Taylan Bilal","Evgenii Eltyshev","Daniel Balle","Nina Martin","Hardie Cate","James Manyika","Keyvan Amiri","Yelin Kim","Xi Xiong","Kai Kang","Florian Luisier","Nilesh Tripuraneni","David Madras","Mandy Guo","Austin Waters","Oliver Wang","Joshua Ainslie","Jason Baldridge","Han Zhang","Garima Pruthi","Jakob Bauer","Feng Yang","Riham Mansour","Jason Gelman","Yang Xu","George Polovets","Ji Liu","Honglong Cai","Warren Chen","XiangHai Sheng","Emily Xue","Sherjil Ozair","Adams Yu","Christof Angermueller","Xiaowei Li","Weiren Wang","Julia Wiesinger","Emmanouil Koukoumidis","Yuan Tian","Anand Iyer","Madhu Gurumurthy","Mark Goldenson","Parashar Shah","MK Blake","Hongkun Yu","Anthony Urbanowicz","Jennimaria Palomaki","Chrisantha Fernando","Kevin Brooks","Ken Durden","Harsh Mehta","Nikola Momchev","Elahe Rahimtoroghi","Maria Georgaki","Amit Raul","Sebastian Ruder","Morgan Redshaw","Jinhyuk Lee","Komal Jalan","Dinghua Li","Ginger Perng","Blake Hechtman","Parker Schuh","Milad Nasr","Mia Chen","Kieran Milan","Vladimir Mikulik","Trevor Strohman","Juliana Franco","Tim Green","Demis Hassabis","Koray Kavukcuoglu","Jeffrey Dean","Oriol Vinyals"],"pdf_url":"https://arxiv.org/pdf/2312.11805v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11803v1","updated":"2023-12-19T02:35:13Z","published":"2023-12-19T02:35:13Z","title":"Designing Guiding Principles for NLP for Healthcare: A Case Study of\n  Maternal Health","summary":"  Objective: An ethical framework for the use of large language models (LLMs)\nis urgently needed to shape how natural language processing (NLP) tools are\nused for healthcare applications. Drawing directly from the voices of those\nmost affected, we propose a set of guiding principles for the use of NLP in\nhealthcare, with examples based on applications in maternal health.\n  Materials and Methods: We led an interactive session centered on an LLM-based\nchatbot demonstration during a full-day workshop with 39 participants, and\nadditionally surveyed 30 healthcare workers and 30 birthing people about their\nvalues, needs, and perceptions of AI and LLMs. We conducted quantitative and\nqualitative analyses of the interactive discussions to consolidate our findings\ninto a set of guiding principles.\n  Results: Using the case study of maternal health, we propose nine principles\nfor ethical use of LLMs, grouped into three categories: (i) contextual\nsignificance, (ii) measurements, and (iii) who/what is valued. We describe\nrationales underlying these principles and provide practical advice.\n  Discussion: Healthcare faces existing challenges including the balance of\npower in clinician-patient relationships, systemic health disparities,\nhistorical injustices, and economic constraints. Our principles serve as a\nframework for surfacing key considerations when deploying LLMs in medicine, as\nwell as providing a methodological pattern for other researchers to follow.\n  Conclusion: This set of principles can serve as a resource to practitioners\nworking on maternal health and other healthcare fields to emphasize the\nimportance of technical nuance, historical context, and inclusive design when\ndeveloping LLMs for use in clinical settings.\n","authors":["Maria Antoniak","Aakanksha Naik","Carla S. Alvarado","Lucy Lu Wang","Irene Y. Chen"],"pdf_url":"https://arxiv.org/pdf/2312.11803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11795v1","updated":"2023-12-19T02:11:01Z","published":"2023-12-19T02:11:01Z","title":"MELO: Enhancing Model Editing with Neuron-Indexed Dynamic LoRA","summary":"  Large language models (LLMs) have shown great success in various Natural\nLanguage Processing (NLP) tasks, whist they still need updates after deployment\nto fix errors or keep pace with the changing knowledge in the world.\nResearchers formulate such problem as Model Editing and have developed various\neditors focusing on different axes of editing properties. However, current\neditors can hardly support all properties and rely on heavy computational\nresources. In this paper, we propose a plug-in Model Editing method based on\nneuron-indexed dynamic LoRA (MELO), which alters the behavior of language\nmodels by dynamically activating certain LoRA blocks according to the index\nbuilt in an inner vector database. Our method satisfies various editing\nproperties with high efficiency and can be easily integrated into multiple LLM\nbackbones. Experimental results show that our proposed MELO achieves\nstate-of-the-art editing performance on three sequential editing tasks\n(document classification, question answering and hallucination correction),\nwhile requires the least trainable parameters and computational cost.\n","authors":["Lang Yu","Qin Chen","Jie Zhou","Liang He"],"pdf_url":"https://arxiv.org/pdf/2312.11795v1.pdf","comment":"In Proceedings of The 38th Annual AAAI Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2312.11792v1","updated":"2023-12-19T02:07:42Z","published":"2023-12-19T02:07:42Z","title":"COOPER: Coordinating Specialized Agents towards a Complex Dialogue Goal","summary":"  In recent years, there has been a growing interest in exploring dialogues\nwith more complex goals, such as negotiation, persuasion, and emotional\nsupport, which go beyond traditional service-focused dialogue systems. Apart\nfrom the requirement for much more sophisticated strategic reasoning and\ncommunication skills, a significant challenge of these tasks lies in the\ndifficulty of objectively measuring the achievement of their goals in a\nquantifiable way, making it difficult for existing research to directly\noptimize the dialogue procedure towards them. In our work, we emphasize the\nmultifaceted nature of complex dialogue goals and argue that it is more\nfeasible to accomplish them by comprehensively considering and jointly\npromoting their different aspects. To this end, we propose a novel dialogue\nframework, Cooper, which coordinates multiple specialized agents, each\ndedicated to a specific dialogue goal aspect separately, to approach the\ncomplex objective. Through this divide-and-conquer manner, we make complex\ndialogue goals more approachable and elicit greater intelligence via the\ncollaboration of individual agents. Experiments on persuasion and emotional\nsupport dialogues demonstrate the superiority of our method over a set of\ncompetitive baselines.\n","authors":["Yi Cheng","Wenge Liu","Jian Wang","Chak Tou Leong","Yi Ouyang","Wenjie Li","Xian Wu","Yefeng Zheng"],"pdf_url":"https://arxiv.org/pdf/2312.11792v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2309.12276v2","updated":"2023-12-19T01:49:45Z","published":"2023-09-21T17:37:01Z","title":"LLMR: Real-time Prompting of Interactive Worlds using Large Language\n  Models","summary":"  We present Large Language Model for Mixed Reality (LLMR), a framework for the\nreal-time creation and modification of interactive Mixed Reality experiences\nusing LLMs. LLMR leverages novel strategies to tackle difficult cases where\nideal training data is scarce, or where the design goal requires the synthesis\nof internal dynamics, intuitive analysis, or advanced interactivity. Our\nframework relies on text interaction and the Unity game engine. By\nincorporating techniques for scene understanding, task planning,\nself-debugging, and memory management, LLMR outperforms the standard GPT-4 by\n4x in average error rate. We demonstrate LLMR's cross-platform interoperability\nwith several example worlds, and evaluate it on a variety of creation and\nmodification tasks to show that it can produce and edit diverse objects, tools,\nand scenes. Finally, we conducted a usability study (N=11) with a diverse set\nthat revealed participants had positive experiences with the system and would\nuse it again.\n","authors":["Fernanda De La Torre","Cathy Mengying Fang","Han Huang","Andrzej Banburski-Fahey","Judith Amores Fernandez","Jaron Lanier"],"pdf_url":"https://arxiv.org/pdf/2309.12276v2.pdf","comment":"60 pages, 18 figures; Expanded discussion of experiments and the\n  influence of various modules"},{"id":"http://arxiv.org/abs/2312.11785v1","updated":"2023-12-19T01:48:31Z","published":"2023-12-19T01:48:31Z","title":"Zero-Shot Fact-Checking with Semantic Triples and Knowledge Graphs","summary":"  Despite progress in automated fact-checking, most systems require a\nsignificant amount of labeled training data, which is expensive. In this paper,\nwe propose a novel zero-shot method, which instead of operating directly on the\nclaim and evidence sentences, decomposes them into semantic triples augmented\nusing external knowledge graphs, and uses large language models trained for\nnatural language inference. This allows it to generalize to adversarial\ndatasets and domains that supervised models require specific training data for.\nOur empirical results show that our approach outperforms previous zero-shot\napproaches on FEVER, FEVER-Symmetric, FEVER 2.0, and Climate-FEVER, while being\ncomparable or better than supervised models on the adversarial and the\nout-of-domain datasets.\n","authors":["Zhangdie Yuan","Andreas Vlachos"],"pdf_url":"https://arxiv.org/pdf/2312.11785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11779v1","updated":"2023-12-19T01:28:46Z","published":"2023-12-19T01:28:46Z","title":"Are you talking to ['xem'] or ['x', 'em']? On Tokenization and\n  Addressing Misgendering in LLMs with Pronoun Tokenization Parity","summary":"  A large body of NLP research has documented the ways gender biases manifest\nand amplify within large language models (LLMs), though this research has\npredominantly operated within a gender binary-centric context. A growing body\nof work has identified the harmful limitations of this gender-exclusive\nframing; many LLMs cannot correctly and consistently refer to persons outside\nthe gender binary, especially if they use neopronouns. While data scarcity has\nbeen identified as a possible culprit, the precise mechanisms through which it\ninfluences LLM misgendering remain underexplored. Our work addresses this gap\nby studying data scarcity's role in subword tokenization and, consequently, the\nformation of LLM word representations. We uncover how the Byte-Pair Encoding\n(BPE) tokenizer, a backbone for many popular LLMs, contributes to neopronoun\nmisgendering through out-of-vocabulary behavior. We introduce pronoun\ntokenization parity (PTP), a novel approach to reduce LLM neopronoun\nmisgendering by preserving a token's functional structure. We evaluate PTP's\nefficacy using pronoun consistency-based metrics and a novel syntax-based\nmetric. Through several controlled experiments, finetuning LLMs with PTP\nimproves neopronoun consistency from 14.5% to 58.4%, highlighting the\nsignificant role tokenization plays in LLM pronoun consistency.\n","authors":["Anaelia Ovalle","Ninareh Mehrabi","Palash Goyal","Jwala Dhamala","Kai-Wei Chang","Richard Zemel","Aram Galstyan","Yuval Pinter","Rahul Gupta"],"pdf_url":"https://arxiv.org/pdf/2312.11779v1.pdf","comment":"Accepted to 2023 Neurips Queer in AI workshop"},{"id":"http://arxiv.org/abs/2303.08774v4","updated":"2023-12-19T00:34:40Z","published":"2023-03-15T17:15:04Z","title":"GPT-4 Technical Report","summary":"  We report the development of GPT-4, a large-scale, multimodal model which can\naccept image and text inputs and produce text outputs. While less capable than\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance on\nvarious professional and academic benchmarks, including passing a simulated bar\nexam with a score around the top 10% of test takers. GPT-4 is a\nTransformer-based model pre-trained to predict the next token in a document.\nThe post-training alignment process results in improved performance on measures\nof factuality and adherence to desired behavior. A core component of this\nproject was developing infrastructure and optimization methods that behave\npredictably across a wide range of scales. This allowed us to accurately\npredict some aspects of GPT-4's performance based on models trained with no\nmore than 1/1,000th the compute of GPT-4.\n","authors":[" OpenAI"," :","Josh Achiam","Steven Adler","Sandhini Agarwal","Lama Ahmad","Ilge Akkaya","Florencia Leoni Aleman","Diogo Almeida","Janko Altenschmidt","Sam Altman","Shyamal Anadkat","Red Avila","Igor Babuschkin","Suchir Balaji","Valerie Balcom","Paul Baltescu","Haiming Bao","Mo Bavarian","Jeff Belgum","Irwan Bello","Jake Berdine","Gabriel Bernadett-Shapiro","Christopher Berner","Lenny Bogdonoff","Oleg Boiko","Madelaine Boyd","Anna-Luisa Brakman","Greg Brockman","Tim Brooks","Miles Brundage","Kevin Button","Trevor Cai","Rosie Campbell","Andrew Cann","Brittany Carey","Chelsea Carlson","Rory Carmichael","Brooke Chan","Che Chang","Fotis Chantzis","Derek Chen","Sully Chen","Ruby Chen","Jason Chen","Mark Chen","Ben Chess","Chester Cho","Casey Chu","Hyung Won Chung","Dave Cummings","Jeremiah Currier","Yunxing Dai","Cory Decareaux","Thomas Degry","Noah Deutsch","Damien Deville","Arka Dhar","David Dohan","Steve Dowling","Sheila Dunning","Adrien Ecoffet","Atty Eleti","Tyna Eloundou","David Farhi","Liam Fedus","Niko Felix","Simón Posada Fishman","Juston Forte","Isabella Fulford","Leo Gao","Elie Georges","Christian Gibson","Vik Goel","Tarun Gogineni","Gabriel Goh","Rapha Gontijo-Lopes","Jonathan Gordon","Morgan Grafstein","Scott Gray","Ryan Greene","Joshua Gross","Shixiang Shane Gu","Yufei Guo","Chris Hallacy","Jesse Han","Jeff Harris","Yuchen He","Mike Heaton","Johannes Heidecke","Chris Hesse","Alan Hickey","Wade Hickey","Peter Hoeschele","Brandon Houghton","Kenny Hsu","Shengli Hu","Xin Hu","Joost Huizinga","Shantanu Jain","Shawn Jain","Joanne Jang","Angela Jiang","Roger Jiang","Haozhun Jin","Denny Jin","Shino Jomoto","Billie Jonn","Heewoo Jun","Tomer Kaftan","Łukasz Kaiser","Ali Kamali","Ingmar Kanitscheider","Nitish Shirish Keskar","Tabarak Khan","Logan Kilpatrick","Jong Wook Kim","Christina Kim","Yongjik Kim","Hendrik Kirchner","Jamie Kiros","Matt Knight","Daniel Kokotajlo","Łukasz Kondraciuk","Andrew Kondrich","Aris Konstantinidis","Kyle Kosic","Gretchen Krueger","Vishal Kuo","Michael Lampe","Ikai Lan","Teddy Lee","Jan Leike","Jade Leung","Daniel Levy","Chak Ming Li","Rachel Lim","Molly Lin","Stephanie Lin","Mateusz Litwin","Theresa Lopez","Ryan Lowe","Patricia Lue","Anna Makanju","Kim Malfacini","Sam Manning","Todor Markov","Yaniv Markovski","Bianca Martin","Katie Mayer","Andrew Mayne","Bob McGrew","Scott Mayer McKinney","Christine McLeavey","Paul McMillan","Jake McNeil","David Medina","Aalok Mehta","Jacob Menick","Luke Metz","Andrey Mishchenko","Pamela Mishkin","Vinnie Monaco","Evan Morikawa","Daniel Mossing","Tong Mu","Mira Murati","Oleg Murk","David Mély","Ashvin Nair","Reiichiro Nakano","Rajeev Nayak","Arvind Neelakantan","Richard Ngo","Hyeonwoo Noh","Long Ouyang","Cullen O'Keefe","Jakub Pachocki","Alex Paino","Joe Palermo","Ashley Pantuliano","Giambattista Parascandolo","Joel Parish","Emy Parparita","Alex Passos","Mikhail Pavlov","Andrew Peng","Adam Perelman","Filipe de Avila Belbute Peres","Michael Petrov","Henrique Ponde de Oliveira Pinto"," Michael"," Pokorny","Michelle Pokrass","Vitchyr Pong","Tolly Powell","Alethea Power","Boris Power","Elizabeth Proehl","Raul Puri","Alec Radford","Jack Rae","Aditya Ramesh","Cameron Raymond","Francis Real","Kendra Rimbach","Carl Ross","Bob Rotsted","Henri Roussez","Nick Ryder","Mario Saltarelli","Ted Sanders","Shibani Santurkar","Girish Sastry","Heather Schmidt","David Schnurr","John Schulman","Daniel Selsam","Kyla Sheppard","Toki Sherbakov","Jessica Shieh","Sarah Shoker","Pranav Shyam","Szymon Sidor","Eric Sigler","Maddie Simens","Jordan Sitkin","Katarina Slama","Ian Sohl","Benjamin Sokolowsky","Yang Song","Natalie Staudacher","Felipe Petroski Such","Natalie Summers","Ilya Sutskever","Jie Tang","Nikolas Tezak","Madeleine Thompson","Phil Tillet","Amin Tootoonchian","Elizabeth Tseng","Preston Tuggle","Nick Turley","Jerry Tworek","Juan Felipe Cerón Uribe","Andrea Vallone","Arun Vijayvergiya","Chelsea Voss","Carroll Wainwright","Justin Jay Wang","Alvin Wang","Ben Wang","Jonathan Ward","Jason Wei","CJ Weinmann","Akila Welihinda","Peter Welinder","Jiayi Weng","Lilian Weng","Matt Wiethoff","Dave Willner","Clemens Winter","Samuel Wolrich","Hannah Wong","Lauren Workman","Sherwin Wu","Jeff Wu","Michael Wu","Kai Xiao","Tao Xu","Sarah Yoo","Kevin Yu","Qiming Yuan","Wojciech Zaremba","Rowan Zellers","Chong Zhang","Marvin Zhang","Shengjia Zhao","Tianhao Zheng","Juntang Zhuang","William Zhuk","Barret Zoph"],"pdf_url":"https://arxiv.org/pdf/2303.08774v4.pdf","comment":"100 pages; updated authors list"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2312.12437v1","updated":"2023-12-19T18:59:53Z","published":"2023-12-19T18:59:53Z","title":"Weakly Supervised Open-Vocabulary Object Detection","summary":"  Despite weakly supervised object detection (WSOD) being a promising step\ntoward evading strong instance-level annotations, its capability is confined to\nclosed-set categories within a single training dataset. In this paper, we\npropose a novel weakly supervised open-vocabulary object detection framework,\nnamely WSOVOD, to extend traditional WSOD to detect novel concepts and utilize\ndiverse datasets with only image-level annotations. To achieve this, we explore\nthree vital strategies, including dataset-level feature adaptation, image-level\nsalient object localization, and region-level vision-language alignment. First,\nwe perform data-aware feature extraction to produce an input-conditional\ncoefficient, which is leveraged into dataset attribute prototypes to identify\ndataset bias and help achieve cross-dataset generalization. Second, a\ncustomized location-oriented weakly supervised region proposal network is\nproposed to utilize high-level semantic layouts from the category-agnostic\nsegment anything model to distinguish object boundaries. Lastly, we introduce a\nproposal-concept synchronized multiple-instance network, i.e., object mining\nand refinement with visual-semantic alignment, to discover objects matched to\nthe text embeddings of concepts. Extensive experiments on Pascal VOC and MS\nCOCO demonstrate that the proposed WSOVOD achieves new state-of-the-art\ncompared with previous WSOD methods in both close-set object localization and\ndetection tasks. Meanwhile, WSOVOD enables cross-dataset and open-vocabulary\nlearning to achieve on-par or even better performance than well-established\nfully-supervised open-vocabulary object detection (FSOVOD).\n","authors":["Jianghang Lin","Yunhang Shen","Bingquan Wang","Shaohui Lin","Ke Li","Liujuan Cao"],"pdf_url":"https://arxiv.org/pdf/2312.12437v1.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2312.12436v1","updated":"2023-12-19T18:59:22Z","published":"2023-12-19T18:59:22Z","title":"A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise","summary":"  The surge of interest towards Multi-modal Large Language Models (MLLMs),\ne.g., GPT-4V(ision) from OpenAI, has marked a significant trend in both\nacademia and industry. They endow Large Language Models (LLMs) with powerful\ncapabilities in visual understanding, enabling them to tackle diverse\nmulti-modal tasks. Very recently, Google released Gemini, its newest and most\ncapable MLLM built from the ground up for multi-modality. In light of the\nsuperior reasoning capabilities, can Gemini challenge GPT-4V's leading position\nin multi-modal learning? In this paper, we present a preliminary exploration of\nGemini Pro's visual understanding proficiency, which comprehensively covers\nfour domains: fundamental perception, advanced cognition, challenging vision\ntasks, and various expert capacities. We compare Gemini Pro with the\nstate-of-the-art GPT-4V to evaluate its upper limits, along with the latest\nopen-sourced MLLM, Sphinx, which reveals the gap between manual efforts and\nblack-box systems. The qualitative samples indicate that, while GPT-4V and\nGemini showcase different answering styles and preferences, they can exhibit\ncomparable visual reasoning capabilities, and Sphinx still trails behind them\nconcerning domain generalizability. Specifically, GPT-4V tends to elaborate\ndetailed explanations and intermediate steps, and Gemini prefers to output a\ndirect and concise answer. The quantitative evaluation on the popular MME\nbenchmark also demonstrates the potential of Gemini to be a strong challenger\nto GPT-4V. Our early investigation of Gemini also observes some common issues\nof MLLMs, indicating that there still remains a considerable distance towards\nartificial general intelligence. Our project for tracking the progress of MLLM\nis released at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\n","authors":["Chaoyou Fu","Renrui Zhang","Haojia Lin","Zihan Wang","Timin Gao","Yongdong Luo","Yubo Huang","Zhengye Zhang","Longtian Qiu","Gaoxiang Ye","Yunhang Shen","Mengdan Zhang","Peixian Chen","Sirui Zhao","Xiawu Zheng","Shaohui Lin","Deqiang Jiang","Di Yin","Peng Gao","Ke Li","Xing Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2312.12436v1.pdf","comment":"Total 120 pages. See our project at\n  https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models"},{"id":"http://arxiv.org/abs/2312.12433v1","updated":"2023-12-19T18:58:40Z","published":"2023-12-19T18:58:40Z","title":"Tracking Any Object Amodally","summary":"  Amodal perception, the ability to comprehend complete object structures from\npartial visibility, is a fundamental skill, even for infants. Its significance\nextends to applications like autonomous driving, where a clear understanding of\nheavily occluded objects is essential. However, modern detection and tracking\nalgorithms often overlook this critical capability, perhaps due to the\nprevalence of modal annotations in most datasets. To address the scarcity of\namodal data, we introduce the TAO-Amodal benchmark, featuring 880 diverse\ncategories in thousands of video sequences. Our dataset includes amodal and\nmodal bounding boxes for visible and occluded objects, including objects that\nare partially out-of-frame. To enhance amodal tracking with object permanence,\nwe leverage a lightweight plug-in module, the amodal expander, to transform\nstandard, modal trackers into amodal ones through fine-tuning on a few hundred\nvideo sequences with data augmentation. We achieve a 3.3\\% and 1.6\\%\nimprovement on the detection and tracking of occluded objects on TAO-Amodal.\nWhen evaluated on people, our method produces dramatic improvements of 2x\ncompared to state-of-the-art modal baselines.\n","authors":["Cheng-Yen Hsieh","Tarasha Khurana","Achal Dave","Deva Ramanan"],"pdf_url":"https://arxiv.org/pdf/2312.12433v1.pdf","comment":"Project Page: https://tao-amodal.github.io"},{"id":"http://arxiv.org/abs/2312.12431v1","updated":"2023-12-19T18:57:34Z","published":"2023-12-19T18:57:34Z","title":"On Inference Stability for Diffusion Models","summary":"  Denoising Probabilistic Models (DPMs) represent an emerging domain of\ngenerative models that excel in generating diverse and high-quality images.\nHowever, most current training methods for DPMs often neglect the correlation\nbetween timesteps, limiting the model's performance in generating images\neffectively. Notably, we theoretically point out that this issue can be caused\nby the cumulative estimation gap between the predicted and the actual\ntrajectory. To minimize that gap, we propose a novel \\textit{sequence-aware}\nloss that aims to reduce the estimation gap to enhance the sampling quality.\nFurthermore, we theoretically show that our proposed loss function is a tighter\nupper bound of the estimation loss in comparison with the conventional loss in\nDPMs. Experimental results on several benchmark datasets including CIFAR10,\nCelebA, and CelebA-HQ consistently show a remarkable improvement of our\nproposed method regarding the image generalization quality measured by FID and\nInception Score compared to several DPM baselines. Our code and pre-trained\ncheckpoints are available at \\url{https://github.com/viettmab/SA-DPM}.\n","authors":["Viet Nguyen","Giang Vu","Tung Nguyen Thanh","Khoat Than","Toan Tran"],"pdf_url":"https://arxiv.org/pdf/2312.12431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12429v1","updated":"2023-12-19T18:56:44Z","published":"2023-12-19T18:56:44Z","title":"The Endoscapes Dataset for Surgical Scene Segmentation, Object\n  Detection, and Critical View of Safety Assessment: Official Splits and\n  Benchmark","summary":"  This technical report provides a detailed overview of Endoscapes, a dataset\nof laparoscopic cholecystectomy (LC) videos with highly intricate annotations\ntargeted at automated assessment of the Critical View of Safety (CVS).\nEndoscapes comprises 201 LC videos with frames annotated sparsely but regularly\nwith segmentation masks, bounding boxes, and CVS assessment by three different\nclinical experts. Altogether, there are 11090 frames annotated with CVS and\n1933 frames annotated with tool and anatomy bounding boxes from the 201 videos,\nas well as an additional 422 frames from 50 of the 201 videos annotated with\ntool and anatomy segmentation masks. In this report, we provide detailed\ndataset statistics (size, class distribution, dataset splits, etc.) and a\ncomprehensive performance benchmark for instance segmentation, object\ndetection, and CVS prediction. The dataset and model checkpoints are publically\navailable at https://github.com/CAMMA-public/Endoscapes.\n","authors":["Aditya Murali","Deepak Alapatt","Pietro Mascagni","Armine Vardazaryan","Alain Garcia","Nariaki Okamoto","Guido Costamagna","Didier Mutter","Jacques Marescaux","Bernard Dallemagne","Nicolas Padoy"],"pdf_url":"https://arxiv.org/pdf/2312.12429v1.pdf","comment":"7 pages; 3 figures"},{"id":"http://arxiv.org/abs/2312.12425v1","updated":"2023-12-19T18:53:47Z","published":"2023-12-19T18:53:47Z","title":"SegRefiner: Towards Model-Agnostic Segmentation Refinement with Discrete\n  Diffusion Process","summary":"  In this paper, we explore a principal way to enhance the quality of object\nmasks produced by different segmentation models. We propose a model-agnostic\nsolution called SegRefiner, which offers a novel perspective on this problem by\ninterpreting segmentation refinement as a data generation process. As a result,\nthe refinement process can be smoothly implemented through a series of\ndenoising diffusion steps. Specifically, SegRefiner takes coarse masks as\ninputs and refines them using a discrete diffusion process. By predicting the\nlabel and corresponding states-transition probabilities for each pixel,\nSegRefiner progressively refines the noisy masks in a conditional denoising\nmanner. To assess the effectiveness of SegRefiner, we conduct comprehensive\nexperiments on various segmentation tasks, including semantic segmentation,\ninstance segmentation, and dichotomous image segmentation. The results\ndemonstrate the superiority of our SegRefiner from multiple aspects. Firstly,\nit consistently improves both the segmentation metrics and boundary metrics\nacross different types of coarse masks. Secondly, it outperforms previous\nmodel-agnostic refinement methods by a significant margin. Lastly, it exhibits\na strong capability to capture extremely fine details when refining\nhigh-resolution images. The source code and trained models are available at\nhttps://github.com/MengyuWang826/SegRefiner.\n","authors":["Mengyu Wang","Henghui Ding","Jun Hao Liew","Jiajun Liu","Yao Zhao","Yunchao Wei"],"pdf_url":"https://arxiv.org/pdf/2312.12425v1.pdf","comment":"NeurIPS 2023, Code: https://github.com/MengyuWang826/SegRefiner"},{"id":"http://arxiv.org/abs/2312.12423v1","updated":"2023-12-19T18:53:01Z","published":"2023-12-19T18:53:01Z","title":"Jack of All Tasks, Master of Many: Designing General-purpose\n  Coarse-to-Fine Vision-Language Model","summary":"  The ability of large language models (LLMs) to process visual inputs has\ngiven rise to general-purpose vision systems, unifying various vision-language\n(VL) tasks by instruction tuning. However, due to the enormous diversity in\ninput-output formats in the vision domain, existing general-purpose models fail\nto successfully integrate segmentation and multi-image inputs with coarse-level\ntasks into a single framework. In this work, we introduce VistaLLM, a powerful\nvisual system that addresses coarse- and fine-grained VL tasks over single and\nmultiple input images using a unified framework. VistaLLM utilizes an\ninstruction-guided image tokenizer that filters global embeddings using task\ndescriptions to extract compressed and refined features from numerous images.\nMoreover, VistaLLM employs a gradient-aware adaptive sampling technique to\nrepresent binary segmentation masks as sequences, significantly improving over\npreviously used uniform sampling. To bolster the desired capability of\nVistaLLM, we curate CoinIt, a comprehensive coarse-to-fine instruction tuning\ndataset with 6.8M samples. We also address the lack of multi-image grounding\ndatasets by introducing a novel task, AttCoSeg (Attribute-level\nCo-Segmentation), which boosts the model's reasoning and grounding capability\nover multiple input images. Extensive experiments on a wide range of V- and VL\ntasks demonstrate the effectiveness of VistaLLM by achieving consistent\nstate-of-the-art performance over strong baselines across all downstream tasks.\nOur project page can be found at https://shramanpramanick.github.io/VistaLLM/.\n","authors":["Shraman Pramanick","Guangxing Han","Rui Hou","Sayan Nag","Ser-Nam Lim","Nicolas Ballas","Qifan Wang","Rama Chellappa","Amjad Almahairi"],"pdf_url":"https://arxiv.org/pdf/2312.12423v1.pdf","comment":"24 pages including references and supplementary"},{"id":"http://arxiv.org/abs/2312.12419v1","updated":"2023-12-19T18:50:33Z","published":"2023-12-19T18:50:33Z","title":"Scene-Conditional 3D Object Stylization and Composition","summary":"  Recently, 3D generative models have made impressive progress, enabling the\ngeneration of almost arbitrary 3D assets from text or image inputs. However,\nthese approaches generate objects in isolation without any consideration for\nthe scene where they will eventually be placed. In this paper, we propose a\nframework that allows for the stylization of an existing 3D asset to fit into a\ngiven 2D scene, and additionally produce a photorealistic composition as if the\nasset was placed within the environment. This not only opens up a new level of\ncontrol for object stylization, for example, the same assets can be stylized to\nreflect changes in the environment, such as summer to winter or fantasy versus\nfuturistic settings-but also makes the object-scene composition more\ncontrollable. We achieve this by combining modeling and optimizing the object's\ntexture and environmental lighting through differentiable ray tracing with\nimage priors from pre-trained text-to-image diffusion models. We demonstrate\nthat our method is applicable to a wide variety of indoor and outdoor scenes\nand arbitrary objects.\n","authors":["Jinghao Zhou","Tomas Jakab","Philip Torr","Christian Rupprecht"],"pdf_url":"https://arxiv.org/pdf/2312.12419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12418v1","updated":"2023-12-19T18:50:10Z","published":"2023-12-19T18:50:10Z","title":"LASA: Instance Reconstruction from Real Scans using A Large-scale\n  Aligned Shape Annotation Dataset","summary":"  Instance shape reconstruction from a 3D scene involves recovering the full\ngeometries of multiple objects at the semantic instance level. Many methods\nleverage data-driven learning due to the intricacies of scene complexity and\nsignificant indoor occlusions. Training these methods often requires a\nlarge-scale, high-quality dataset with aligned and paired shape annotations\nwith real-world scans. Existing datasets are either synthetic or misaligned,\nrestricting the performance of data-driven methods on real data. To this end,\nwe introduce LASA, a Large-scale Aligned Shape Annotation Dataset comprising\n10,412 high-quality CAD annotations aligned with 920 real-world scene scans\nfrom ArkitScenes, created manually by professional artists. On this top, we\npropose a novel Diffusion-based Cross-Modal Shape Reconstruction (DisCo)\nmethod. It is empowered by a hybrid feature aggregation design to fuse\nmulti-modal inputs and recover high-fidelity object geometries. Besides, we\npresent an Occupancy-Guided 3D Object Detection (OccGOD) method and demonstrate\nthat our shape annotations provide scene occupancy clues that can further\nimprove 3D object detection. Supported by LASA, extensive experiments show that\nour methods achieve state-of-the-art performance in both instance-level scene\nreconstruction and 3D object detection tasks.\n","authors":["Haolin Liu","Chongjie Ye","Yinyu Nie","Yingfan He","Xiaoguang Han"],"pdf_url":"https://arxiv.org/pdf/2312.12418v1.pdf","comment":"homepage: https://gap-lab-cuhk-sz.github.io/LASA/"},{"id":"http://arxiv.org/abs/2312.12416v1","updated":"2023-12-19T18:47:30Z","published":"2023-12-19T18:47:30Z","title":"Prompting Hard or Hardly Prompting: Prompt Inversion for Text-to-Image\n  Diffusion Models","summary":"  The quality of the prompts provided to text-to-image diffusion models\ndetermines how faithful the generated content is to the user's intent, often\nrequiring `prompt engineering'. To harness visual concepts from target images\nwithout prompt engineering, current approaches largely rely on embedding\ninversion by optimizing and then mapping them to pseudo-tokens. However,\nworking with such high-dimensional vector representations is challenging\nbecause they lack semantics and interpretability, and only allow simple vector\noperations when using them. Instead, this work focuses on inverting the\ndiffusion model to obtain interpretable language prompts directly. The\nchallenge of doing this lies in the fact that the resulting optimization\nproblem is fundamentally discrete and the space of prompts is exponentially\nlarge; this makes using standard optimization techniques, such as stochastic\ngradient descent, difficult. To this end, we utilize a delayed projection\nscheme to optimize for prompts representative of the vocabulary space in the\nmodel. Further, we leverage the findings that different timesteps of the\ndiffusion process cater to different levels of detail in an image. The later,\nnoisy, timesteps of the forward diffusion process correspond to the semantic\ninformation, and therefore, prompt inversion in this range provides tokens\nrepresentative of the image semantics. We show that our approach can identify\nsemantically interpretable and meaningful prompts for a target image which can\nbe used to synthesize diverse images with similar content. We further\nillustrate the application of the optimized prompts in evolutionary image\ngeneration and concept removal.\n","authors":["Shweta Mahajan","Tanzila Rahman","Kwang Moo Yi","Leonid Sigal"],"pdf_url":"https://arxiv.org/pdf/2312.12416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.13304v2","updated":"2023-12-19T18:45:10Z","published":"2023-08-25T11:04:35Z","title":"Rapid Artefact Removal and H&E-Stained Tissue Segmentation","summary":"  We present an innovative method for rapidly segmenting hematoxylin and eosin\n(H&E)-stained tissue in whole-slide images (WSIs) that eliminates a wide range\nof undesirable artefacts such as pen marks and scanning artefacts. Our method\ninvolves taking a single-channel representation of a lowmagnification RGB\noverview of the WSI in which the pixel values are bimodally distributed such\nthat H&E-stained tissue is easily distinguished from both background and a wide\nvariety of artefacts. We demonstrate our method on 30 WSIs prepared from a wide\nrange of institutions and WSI digital scanners, each containing substantial\nartefacts, and compare it to segmentations provided by Otsu thresholding and\nHistolab tissue segmentation and pen filtering tools. We found that our method\nsegmented the tissue and fully removed all artefacts in 29 out of 30 WSIs,\nwhereas Otsu thresholding failed to remove any artefacts, and the Histolab pen\nfiltering tools only partially removed the pen marks. The beauty of our\napproach lies in its simplicity: manipulating RGB colour space and using Otsu\nthresholding allows for the segmentation of H&E-stained tissue and the rapid\nremoval of artefacts without the need for machine learning or parameter tuning.\n","authors":["B. A. Schreiber","J. Denholm","F. Jaeckle","M. J. Arends","K. M. Branson","C. -B. Schönlieb","E. J. Soilleux"],"pdf_url":"https://arxiv.org/pdf/2308.13304v2.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2312.12379v1","updated":"2023-12-19T18:11:19Z","published":"2023-12-19T18:11:19Z","title":"Mixture of Cluster-conditional LoRA Experts for Vision-language\n  Instruction Tuning","summary":"  Instruction tuning of the Large Vision-language Models (LVLMs) has\nrevolutionized the development of versatile models with zero-shot\ngeneralization across a wide range of downstream vision-language tasks.\nHowever, diversity of training tasks of different sources and formats would\nlead to inevitable task conflicts, where different tasks conflicts for the same\nset of model parameters, resulting in sub-optimal instruction-following\nabilities. To address that, we propose the Mixture of Cluster-conditional LoRA\nExperts (MoCLE), a novel Mixture of Experts (MoE) architecture designed to\nactivate the task-customized model parameters based on the instruction\nclusters. A separate universal expert is further incorporated to improve the\ngeneralization capabilities of MoCLE for novel instructions. Extensive\nexperiments on 10 zero-shot tasks demonstrate the effectiveness of MoCLE.\n","authors":["Yunhao Gou","Zhili Liu","Kai Chen","Lanqing Hong","Hang Xu","Aoxue Li","Dit-Yan Yeung","James T. Kwok","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.12379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.15677v4","updated":"2023-12-19T17:42:05Z","published":"2022-05-31T10:35:55Z","title":"Augmentation-Aware Self-Supervision for Data-Efficient GAN Training","summary":"  Training generative adversarial networks (GANs) with limited data is\nchallenging because the discriminator is prone to overfitting. Previously\nproposed differentiable augmentation demonstrates improved data efficiency of\ntraining GANs. However, the augmentation implicitly introduces undesired\ninvariance to augmentation for the discriminator since it ignores the change of\nsemantics in the label space caused by data transformation, which may limit the\nrepresentation learning ability of the discriminator and ultimately affect the\ngenerative modeling performance of the generator. To mitigate the negative\nimpact of invariance while inheriting the benefits of data augmentation, we\npropose a novel augmentation-aware self-supervised discriminator that predicts\nthe augmentation parameter of the augmented data. Particularly, the prediction\ntargets of real data and generated data are required to be distinguished since\nthey are different during training. We further encourage the generator to\nadversarially learn from the self-supervised discriminator by generating\naugmentation-predictable real and not fake data. This formulation connects the\nlearning objective of the generator and the arithmetic $-$ harmonic mean\ndivergence under certain assumptions. We compare our method with\nstate-of-the-art (SOTA) methods using the class-conditional BigGAN and\nunconditional StyleGAN2 architectures on data-limited CIFAR-10, CIFAR-100,\nFFHQ, LSUN-Cat, and five low-shot datasets. Experimental results demonstrate\nsignificant improvements of our method over SOTA methods in training\ndata-efficient GANs.\n","authors":["Liang Hou","Qi Cao","Yige Yuan","Songtao Zhao","Chongyang Ma","Siyuan Pan","Pengfei Wan","Zhongyuan Wang","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2205.15677v4.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2312.12359v1","updated":"2023-12-19T17:40:27Z","published":"2023-12-19T17:40:27Z","title":"CLIP-DINOiser: Teaching CLIP a few DINO tricks","summary":"  The popular CLIP model displays impressive zero-shot capabilities thanks to\nits seamless interaction with arbitrary text prompts. However, its lack of\nspatial awareness makes it unsuitable for dense computer vision tasks, e.g.,\nsemantic segmentation, without an additional fine-tuning step that often uses\nannotations and can potentially suppress its original open-vocabulary\nproperties. Meanwhile, self-supervised representation methods have demonstrated\ngood localization properties without human-made annotations nor explicit\nsupervision. In this work, we take the best of both worlds and propose a\nzero-shot open-vocabulary semantic segmentation method, which does not require\nany annotations. We propose to locally improve dense MaskCLIP features,\ncomputed with a simple modification of CLIP's last pooling layer, by\nintegrating localization priors extracted from self-supervised features. By\ndoing so, we greatly improve the performance of MaskCLIP and produce smooth\noutputs. Moreover, we show that the used self-supervised feature properties can\ndirectly be learnt from CLIP features therefore allowing us to obtain the best\nresults with a single pass through CLIP model. Our method CLIP-DINOiser needs\nonly a single forward pass of CLIP and two light convolutional layers at\ninference, no extra supervision nor extra memory and reaches state-of-the-art\nresults on challenging and fine-grained benchmarks such as COCO, Pascal\nContext, Cityscapes and ADE20k. The code to reproduce our results is available\nat https://github.com/wysoczanska/clip_dinoiser.\n","authors":["Monika Wysoczańska","Oriane Siméoni","Michaël Ramamonjisoa","Andrei Bursuc","Tomasz Trzciński","Patrick Pérez"],"pdf_url":"https://arxiv.org/pdf/2312.12359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12347v1","updated":"2023-12-19T17:26:44Z","published":"2023-12-19T17:26:44Z","title":"SMC-NCA: Semantic-guided Multi-level Contrast for Semi-supervised Action\n  Segmentation","summary":"  Semi-supervised action segmentation aims to perform frame-wise classification\nin long untrimmed videos, where only a fraction of videos in the training set\nhave labels. Recent studies have shown the potential of contrastive learning in\nunsupervised representation learning using unlabelled data. However, learning\nthe representation of each frame by unsupervised contrastive learning for\naction segmentation remains an open and challenging problem. In this paper, we\npropose a novel Semantic-guided Multi-level Contrast scheme with a\nNeighbourhood-Consistency-Aware unit (SMC-NCA) to extract strong frame-wise\nrepresentations for semi-supervised action segmentation. Specifically, for\nrepresentation learning, SMC is firstly used to explore intra- and\ninter-information variations in a unified and contrastive way, based on dynamic\nclustering process of the original input, encoded semantic and temporal\nfeatures. Then, the NCA module, which is responsible for enforcing spatial\nconsistency between neighbourhoods centered at different frames to alleviate\nover-segmentation issues, works alongside SMC for semi-supervised learning. Our\nSMC outperforms the other state-of-the-art methods on three benchmarks,\noffering improvements of up to 17.8% and 12.6% in terms of edit distance and\naccuracy, respectively. Additionally, the NCA unit results in significant\nbetter segmentation performance against the others in the presence of only 5%\nlabelled videos. We also demonstrate the effectiveness of the proposed method\non our Parkinson's Disease Mouse Behaviour (PDMB) dataset. The code and\ndatasets will be made publicly available.\n","authors":["Feixiang Zhou","Zheheng Jiang","Huiyu Zhou","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2312.12347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12340v1","updated":"2023-12-19T17:13:51Z","published":"2023-12-19T17:13:51Z","title":"Scalable Geometric Fracture Assembly via Co-creation Space among\n  Assemblers","summary":"  Geometric fracture assembly presents a challenging practical task in\narchaeology and 3D computer vision. Previous methods have focused solely on\nassembling fragments based on semantic information, which has limited the\nquantity of objects that can be effectively assembled. Therefore, there is a\nneed to develop a scalable framework for geometric fracture assembly without\nrelying on semantic information. To improve the effectiveness of assembling\ngeometric fractures without semantic information, we propose a co-creation\nspace comprising several assemblers capable of gradually and unambiguously\nassembling fractures. Additionally, we introduce a novel loss function, i.e.,\nthe geometric-based collision loss, to address collision issues during the\nfracture assembly process and enhance the results. Our framework exhibits\nbetter performance on both PartNet and Breaking Bad datasets compared to\nexisting state-of-the-art frameworks. Extensive experiments and quantitative\ncomparisons demonstrate the effectiveness of our proposed framework, which\nfeatures linear computational complexity, enhanced abstraction, and improved\ngeneralization. Our code is publicly available at\nhttps://github.com/Ruiyuan-Zhang/CCS.\n","authors":["Ruiyuan Zhang","Jiaxiang Liu","Zexi Li","Hao Dong","Jie Fu","Chao Wu"],"pdf_url":"https://arxiv.org/pdf/2312.12340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.11145v2","updated":"2023-12-19T17:09:04Z","published":"2023-01-26T14:52:30Z","title":"Learning from Mistakes: Self-Regularizing Hierarchical Representations\n  in Point Cloud Semantic Segmentation","summary":"  Recent advances in autonomous robotic technologies have highlighted the\ngrowing need for precise environmental analysis. LiDAR semantic segmentation\nhas gained attention to accomplish fine-grained scene understanding by acting\ndirectly on raw content provided by sensors. Recent solutions showed how\ndifferent learning techniques can be used to improve the performance of the\nmodel, without any architectural or dataset change. Following this trend, we\npresent a coarse-to-fine setup that LEArns from classification mistaKes (LEAK)\nderived from a standard model. First, classes are clustered into macro groups\naccording to mutual prediction errors; then, the learning process is\nregularized by: (1) aligning class-conditional prototypical feature\nrepresentation for both fine and coarse classes, (2) weighting instances with a\nper-class fairness index. Our LEAK approach is very general and can be\nseamlessly applied on top of any segmentation architecture; indeed,\nexperimental results showed that it enables state-of-the-art performances on\ndifferent architectures, datasets and tasks, while ensuring more balanced\nclass-wise results and faster convergence.\n","authors":["Elena Camuffo","Umberto Michieli","Simone Milani"],"pdf_url":"https://arxiv.org/pdf/2301.11145v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12337v1","updated":"2023-12-19T17:03:50Z","published":"2023-12-19T17:03:50Z","title":"pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable\n  Generalizable 3D Reconstruction","summary":"  We introduce pixelSplat, a feed-forward model that learns to reconstruct 3D\nradiance fields parameterized by 3D Gaussian primitives from pairs of images.\nOur model features real-time and memory-efficient rendering for scalable\ntraining as well as fast 3D reconstruction at inference time. To overcome local\nminima inherent to sparse and locally supported representations, we predict a\ndense probability distribution over 3D and sample Gaussian means from that\nprobability distribution. We make this sampling operation differentiable via a\nreparameterization trick, allowing us to back-propagate gradients through the\nGaussian splatting representation. We benchmark our method on wide-baseline\nnovel view synthesis on the real-world RealEstate10k and ACID datasets, where\nwe outperform state-of-the-art light field transformers and accelerate\nrendering by 2.5 orders of magnitude while reconstructing an interpretable and\neditable 3D radiance field.\n","authors":["David Charatan","Sizhe Li","Andrea Tagliasacchi","Vincent Sitzmann"],"pdf_url":"https://arxiv.org/pdf/2312.12337v1.pdf","comment":"Project page: https://pixelsplat.github.io/"},{"id":"http://arxiv.org/abs/2310.10198v3","updated":"2023-12-19T16:44:46Z","published":"2023-10-16T09:09:02Z","title":"MoConVQ: Unified Physics-Based Motion Control via Scalable Discrete\n  Representations","summary":"  In this work, we present MoConVQ, a novel unified framework for physics-based\nmotion control leveraging scalable discrete representations. Building upon\nvector quantized variational autoencoders (VQ-VAE) and model-based\nreinforcement learning, our approach effectively learns motion embeddings from\na large, unstructured dataset spanning tens of hours of motion examples. The\nresultant motion representation not only captures diverse motion skills but\nalso offers a robust and intuitive interface for various applications. We\ndemonstrate the versatility of MoConVQ through several applications: universal\ntracking control from various motion sources, interactive character control\nwith latent motion representations using supervised learning, physics-based\nmotion generation from natural language descriptions using the GPT framework,\nand, most interestingly, seamless integration with large language models (LLMs)\nwith in-context learning to tackle complex and abstract tasks.\n","authors":["Heyuan Yao","Zhenhua Song","Yuyang Zhou","Tenglong Ao","Baoquan Chen","Libin Liu"],"pdf_url":"https://arxiv.org/pdf/2310.10198v3.pdf","comment":"Project page: MoConVQ.github.io"},{"id":"http://arxiv.org/abs/2312.12314v1","updated":"2023-12-19T16:39:02Z","published":"2023-12-19T16:39:02Z","title":"First qualitative observations on deep learning vision model YOLO and\n  DETR for automated driving in Austria","summary":"  This study investigates the application of single and two-stage 2D-object\ndetection algorithms like You Only Look Once (YOLO), Real-Time DEtection\nTRansformer (RT-DETR) algorithm for automated object detection to enhance road\nsafety for autonomous driving on Austrian roads. The YOLO algorithm is a\nstate-of-the-art real-time object detection system known for its efficiency and\naccuracy. In the context of driving, its potential to rapidly identify and\ntrack objects is crucial for advanced driver assistance systems (ADAS) and\nautonomous vehicles. The research focuses on the unique challenges posed by the\nroad conditions and traffic scenarios in Austria. The country's diverse\nlandscape, varying weather conditions, and specific traffic regulations\nnecessitate a tailored approach for reliable object detection. The study\nutilizes a selective dataset comprising images and videos captured on Austrian\nroads, encompassing urban, rural, and alpine environments.\n","authors":["Stefan Schoder"],"pdf_url":"https://arxiv.org/pdf/2312.12314v1.pdf","comment":"draft"},{"id":"http://arxiv.org/abs/2312.12274v1","updated":"2023-12-19T15:56:19Z","published":"2023-12-19T15:56:19Z","title":"Intrinsic Image Diffusion for Single-view Material Estimation","summary":"  We present Intrinsic Image Diffusion, a generative model for appearance\ndecomposition of indoor scenes. Given a single input view, we sample multiple\npossible material explanations represented as albedo, roughness, and metallic\nmaps. Appearance decomposition poses a considerable challenge in computer\nvision due to the inherent ambiguity between lighting and material properties\nand the lack of real datasets. To address this issue, we advocate for a\nprobabilistic formulation, where instead of attempting to directly predict the\ntrue material properties, we employ a conditional generative model to sample\nfrom the solution space. Furthermore, we show that utilizing the strong learned\nprior of recent diffusion models trained on large-scale real-world images can\nbe adapted to material estimation and highly improves the generalization to\nreal images. Our method produces significantly sharper, more consistent, and\nmore detailed materials, outperforming state-of-the-art methods by $1.5dB$ on\nPSNR and by $45\\%$ better FID score on albedo prediction. We demonstrate the\neffectiveness of our approach through experiments on both synthetic and\nreal-world datasets.\n","authors":["Peter Kocsis","Vincent Sitzmann","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2312.12274v1.pdf","comment":"Project page: https://peter-kocsis.github.io/IntrinsicImageDiffusion/\n  Video: https://youtu.be/lz0meJlj5cA"},{"id":"http://arxiv.org/abs/2312.12273v1","updated":"2023-12-19T15:56:08Z","published":"2023-12-19T15:56:08Z","title":"VQA4CIR: Boosting Composed Image Retrieval with Visual Question\n  Answering","summary":"  Albeit progress has been made in Composed Image Retrieval (CIR), we\nempirically find that a certain percentage of failure retrieval results are not\nconsistent with their relative captions. To address this issue, this work\nprovides a Visual Question Answering (VQA) perspective to boost the performance\nof CIR. The resulting VQA4CIR is a post-processing approach and can be directly\nplugged into existing CIR methods. Given the top-C retrieved images by a CIR\nmethod, VQA4CIR aims to decrease the adverse effect of the failure retrieval\nresults being inconsistent with the relative caption. To find the retrieved\nimages inconsistent with the relative caption, we resort to the \"QA generation\nto VQA\" self-verification pipeline. For QA generation, we suggest fine-tuning\nLLM (e.g., LLaMA) to generate several pairs of questions and answers from each\nrelative caption. We then fine-tune LVLM (e.g., LLaVA) to obtain the VQA model.\nBy feeding the retrieved image and question to the VQA model, one can find the\nimages inconsistent with relative caption when the answer by VQA is\ninconsistent with the answer in the QA pair. Consequently, the CIR performance\ncan be boosted by modifying the ranks of inconsistently retrieved images.\nExperimental results show that our proposed method outperforms state-of-the-art\nCIR methods on the CIRR and Fashion-IQ datasets.\n","authors":["Chun-Mei Feng","Yang Bai","Tao Luo","Zhen Li","Salman Khan","Wangmeng Zuo","Xinxing Xu","Rick Siow Mong Goh","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2312.12273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12263v1","updated":"2023-12-19T15:46:47Z","published":"2023-12-19T15:46:47Z","title":"FedDiv: Collaborative Noise Filtering for Federated Learning with Noisy\n  Labels","summary":"  Federated learning with noisy labels (F-LNL) aims at seeking an optimal\nserver model via collaborative distributed learning by aggregating multiple\nclient models trained with local noisy or clean samples. On the basis of a\nfederated learning framework, recent advances primarily adopt label noise\nfiltering to separate clean samples from noisy ones on each client, thereby\nmitigating the negative impact of label noise. However, these prior methods do\nnot learn noise filters by exploiting knowledge across all clients, leading to\nsub-optimal and inferior noise filtering performance and thus damaging training\nstability. In this paper, we present FedDiv to tackle the challenges of F-LNL.\nSpecifically, we propose a global noise filter called Federated Noise Filter\nfor effectively identifying samples with noisy labels on every client, thereby\nraising stability during local training sessions. Without sacrificing data\nprivacy, this is achieved by modeling the global distribution of label noise\nacross all clients. Then, in an effort to make the global model achieve higher\nperformance, we introduce a Predictive Consistency based Sampler to identify\nmore credible local data for local model training, thus preventing noise\nmemorization and further boosting the training stability. Extensive experiments\non CIFAR-10, CIFAR-100, and Clothing1M demonstrate that \\texttt{FedDiv}\nachieves superior performance over state-of-the-art F-LNL methods under\ndifferent label noise settings for both IID and non-IID data partitions. Source\ncode is publicly available at https://github.com/lijichang/FLNL-FedDiv.\n","authors":["Jichang Li","Guanbin Li","Hui Cheng","Zicheng Liao","Yizhou Yu"],"pdf_url":"https://arxiv.org/pdf/2312.12263v1.pdf","comment":"To appear in AAAI-2024"},{"id":"http://arxiv.org/abs/2312.10237v2","updated":"2023-12-19T15:44:40Z","published":"2023-12-15T22:09:04Z","title":"Vertical Federated Alzheimer's Detection on Multimodal Data","summary":"  In the era of rapidly advancing medical technologies, the segmentation of\nmedical data has become inevitable, necessitating the development of privacy\npreserving machine learning algorithms that can train on distributed data.\nConsolidating sensitive medical data is not always an option particularly due\nto the stringent privacy regulations imposed by the Health Insurance\nPortability and Accountability Act (HIPAA). In this paper, we introduce a HIPAA\ncompliant framework that can train from distributed data. We then propose a\nmultimodal vertical federated model for Alzheimer's Disease (AD) detection, a\nserious neurodegenerative condition that can cause dementia, severely impairing\nbrain function and hindering simple tasks, especially without preventative\ncare. This vertical federated model offers a distributed architecture that\nenables collaborative learning across diverse sources of medical data while\nrespecting privacy constraints imposed by HIPAA. It is also able to leverage\nmultiple modalities of data, enhancing the robustness and accuracy of AD\ndetection. Our proposed model not only contributes to the advancement of\nfederated learning techniques but also holds promise for overcoming the hurdles\nposed by data segmentation in medical research. By using vertical federated\nlearning, this research strives to provide a framework that enables healthcare\ninstitutions to harness the collective intelligence embedded in their\ndistributed datasets without compromising patient privacy.\n","authors":["Paul K. Mandal"],"pdf_url":"https://arxiv.org/pdf/2312.10237v2.pdf","comment":"14 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2312.09754v2","updated":"2023-12-19T15:44:15Z","published":"2023-12-15T12:49:08Z","title":"PPFM: Image denoising in photon-counting CT using single-step posterior\n  sampling Poisson flow generative models","summary":"  Diffusion and Poisson flow models have shown impressive performance in a wide\nrange of generative tasks, including low-dose CT image denoising. However, one\nlimitation in general, and for clinical applications in particular, is slow\nsampling. Due to their iterative nature, the number of function evaluations\n(NFE) required is usually on the order of $10-10^3$, both for conditional and\nunconditional generation. In this paper, we present posterior sampling Poisson\nflow generative models (PPFM), a novel image denoising technique for low-dose\nand photon-counting CT that produces excellent image quality whilst keeping\nNFE=1. Updating the training and sampling processes of Poisson flow generative\nmodels (PFGM)++, we learn a conditional generator which defines a trajectory\nbetween the prior noise distribution and the posterior distribution of\ninterest. We additionally hijack and regularize the sampling process to achieve\nNFE=1. Our results shed light on the benefits of the PFGM++ framework compared\nto diffusion models. In addition, PPFM is shown to perform favorably compared\nto current state-of-the-art diffusion-style models with NFE=1, consistency\nmodels, as well as popular deep learning and non-deep learning-based image\ndenoising techniques, on clinical low-dose CT images and clinical images from a\nprototype photon-counting CT system.\n","authors":["Dennis Hein","Staffan Holmin","Timothy Szczykutowicz","Jonathan S Maltz","Mats Danielsson","Ge Wang","Mats Persson"],"pdf_url":"https://arxiv.org/pdf/2312.09754v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12250v1","updated":"2023-12-19T15:33:57Z","published":"2023-12-19T15:33:57Z","title":"ST(OR)2: Spatio-Temporal Object Level Reasoning for Activity Recognition\n  in the Operating Room","summary":"  Surgical robotics holds much promise for improving patient safety and\nclinician experience in the Operating Room (OR). However, it also comes with\nnew challenges, requiring strong team coordination and effective OR management.\nAutomatic detection of surgical activities is a key requirement for developing\nAI-based intelligent tools to tackle these challenges. The current\nstate-of-the-art surgical activity recognition methods however operate on\nimage-based representations and depend on large-scale labeled datasets whose\ncollection is time-consuming and resource-expensive. This work proposes a new\nsample-efficient and object-based approach for surgical activity recognition in\nthe OR. Our method focuses on the geometric arrangements between clinicians and\nsurgical devices, thus utilizing the significant object interaction dynamics in\nthe OR. We conduct experiments in a low-data regime study for long video\nactivity recognition. We also benchmark our method againstother object-centric\napproaches on clip-level action classification and show superior performance.\n","authors":["Idris Hamoud","Muhammad Abdullah Jamal","Vinkle Srivastav","Didier Mutter","Nicolas Padoy","Omid Mohareri"],"pdf_url":"https://arxiv.org/pdf/2312.12250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12246v1","updated":"2023-12-19T15:30:10Z","published":"2023-12-19T15:30:10Z","title":"MDD-UNet: Domain Adaptation for Medical Image Segmentation with\n  Theoretical Guarantees, a Proof of Concept","summary":"  The current state-of-the art techniques for image segmentation are often\nbased on U-Net architectures, a U-shaped encoder-decoder networks with skip\nconnections. Despite the powerful performance, the architecture often does not\nperform well when used on data which has different characteristics than the\ndata it was trained on. Many techniques for improving performance in the\npresence of domain shift have been developed, however typically only have loose\nconnections to the theory of domain adaption. In this work, we propose an\nunsupervised domain adaptation framework for U-Nets with theoretical guarantees\nbased on the Margin Disparity Discrepancy [1] called the MDD-UNet. We evaluate\nthe proposed technique on the task of hippocampus segmentation, and find that\nthe MDD-UNet is able to learn features which are domain-invariant with no\nknowledge about the labels in the target domain. The MDD-UNet improves\nperformance over the standard U-Net on 11 out of 12 combinations of datasets.\nThis work serves as a proof of concept by demonstrating an improvement on the\nU-Net in it's standard form without modern enhancements, which opens up a new\navenue of studying domain adaptation for models with very large hypothesis\nspaces from both methodological and practical perspectives. Code is available\nat https://github.com/asbjrnmunk/mdd-unet.\n","authors":["Asbjørn Munk","Ao Ma","Mads Nielsen"],"pdf_url":"https://arxiv.org/pdf/2312.12246v1.pdf","comment":"Published at NLDL 2024"},{"id":"http://arxiv.org/abs/2312.12241v1","updated":"2023-12-19T15:25:39Z","published":"2023-12-19T15:25:39Z","title":"GeomVerse: A Systematic Evaluation of Large Models for Geometric\n  Reasoning","summary":"  Large language models have shown impressive results for multi-hop\nmathematical reasoning when the input question is only textual. Many\nmathematical reasoning problems, however, contain both text and image. With the\never-increasing adoption of vision language models (VLMs), understanding their\nreasoning abilities for such problems is crucial. In this paper, we evaluate\nthe reasoning capabilities of VLMs along various axes through the lens of\ngeometry problems. We procedurally create a synthetic dataset of geometry\nquestions with controllable difficulty levels along multiple axes, thus\nenabling a systematic evaluation. The empirical results obtained using our\nbenchmark for state-of-the-art VLMs indicate that these models are not as\ncapable in subjects like geometry (and, by generalization, other topics\nrequiring similar reasoning) as suggested by previous benchmarks. This is made\nespecially clear by the construction of our benchmark at various depth levels,\nsince solving higher-depth problems requires long chains of reasoning rather\nthan additional memorized knowledge. We release the dataset for further\nresearch in this area.\n","authors":["Mehran Kazemi","Hamidreza Alvari","Ankit Anand","Jialin Wu","Xi Chen","Radu Soricut"],"pdf_url":"https://arxiv.org/pdf/2312.12241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12232v1","updated":"2023-12-19T15:18:40Z","published":"2023-12-19T15:18:40Z","title":"Brush Your Text: Synthesize Any Scene Text on Images via Diffusion Model","summary":"  Recently, diffusion-based image generation methods are credited for their\nremarkable text-to-image generation capabilities, while still facing challenges\nin accurately generating multilingual scene text images. To tackle this\nproblem, we propose Diff-Text, which is a training-free scene text generation\nframework for any language. Our model outputs a photo-realistic image given a\ntext of any language along with a textual description of a scene. The model\nleverages rendered sketch images as priors, thus arousing the potential\nmultilingual-generation ability of the pre-trained Stable Diffusion. Based on\nthe observation from the influence of the cross-attention map on object\nplacement in generated images, we propose a localized attention constraint into\nthe cross-attention layer to address the unreasonable positioning problem of\nscene text. Additionally, we introduce contrastive image-level prompts to\nfurther refine the position of the textual region and achieve more accurate\nscene text generation. Experiments demonstrate that our method outperforms the\nexisting method in both the accuracy of text recognition and the naturalness of\nforeground-background blending.\n","authors":["Lingjun Zhang","Xinyuan Chen","Yaohui Wang","Yue Lu","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2312.12232v1.pdf","comment":"Accepted to AAAI 2024. Code:\n  https://github.com/ecnuljzhang/brush-your-text"},{"id":"http://arxiv.org/abs/2303.14027v3","updated":"2023-12-19T15:15:50Z","published":"2023-03-24T14:37:07Z","title":"Poincaré ResNet","summary":"  This paper introduces an end-to-end residual network that operates entirely\non the Poincar\\'e ball model of hyperbolic space. Hyperbolic learning has\nrecently shown great potential for visual understanding, but is currently only\nperformed in the penultimate layer(s) of deep networks. All visual\nrepresentations are still learned through standard Euclidean networks. In this\npaper we investigate how to learn hyperbolic representations of visual data\ndirectly from the pixel-level. We propose Poincar\\'e ResNet, a hyperbolic\ncounterpart of the celebrated residual network, starting from Poincar\\'e 2D\nconvolutions up to Poincar\\'e residual connections. We identify three\nroadblocks for training convolutional networks entirely in hyperbolic space and\npropose a solution for each: (i) Current hyperbolic network initializations\ncollapse to the origin, limiting their applicability in deeper networks. We\nprovide an identity-based initialization that preserves norms over many layers.\n(ii) Residual networks rely heavily on batch normalization, which comes with\nexpensive Fr\\'echet mean calculations in hyperbolic space. We introduce\nPoincar\\'e midpoint batch normalization as a faster and equally effective\nalternative. (iii) Due to the many intermediate operations in Poincar\\'e\nlayers, we lastly find that the computation graphs of deep learning libraries\nblow up, limiting our ability to train on deep hyperbolic networks. We provide\nmanual backward derivations of core hyperbolic operations to maintain\nmanageable computation graphs.\n","authors":["Max van Spengler","Erwin Berkhout","Pascal Mettes"],"pdf_url":"https://arxiv.org/pdf/2303.14027v3.pdf","comment":"International Conference on Computer Vision 2023"},{"id":"http://arxiv.org/abs/2312.12227v1","updated":"2023-12-19T15:13:08Z","published":"2023-12-19T15:13:08Z","title":"HuTuMotion: Human-Tuned Navigation of Latent Motion Diffusion Models\n  with Minimal Feedback","summary":"  We introduce HuTuMotion, an innovative approach for generating natural human\nmotions that navigates latent motion diffusion models by leveraging few-shot\nhuman feedback. Unlike existing approaches that sample latent variables from a\nstandard normal prior distribution, our method adapts the prior distribution to\nbetter suit the characteristics of the data, as indicated by human feedback,\nthus enhancing the quality of motion generation. Furthermore, our findings\nreveal that utilizing few-shot feedback can yield performance levels on par\nwith those attained through extensive human feedback. This discovery emphasizes\nthe potential and efficiency of incorporating few-shot human-guided\noptimization within latent diffusion models for personalized and style-aware\nhuman motion generation applications. The experimental results show the\nsignificantly superior performance of our method over existing state-of-the-art\napproaches.\n","authors":["Gaoge Han","Shaoli Huang","Mingming Gong","Jinglei Tang"],"pdf_url":"https://arxiv.org/pdf/2312.12227v1.pdf","comment":"Accepted by AAAI 2024 Main Track"},{"id":"http://arxiv.org/abs/2312.12223v1","updated":"2023-12-19T15:11:46Z","published":"2023-12-19T15:11:46Z","title":"Self-Supervised Detection of Perfect and Partial Input-Dependent\n  Symmetries","summary":"  Group equivariance ensures consistent responses to group transformations of\nthe input, leading to more robust models and enhanced generalization\ncapabilities. However, this property can lead to overly constrained models if\nthe symmetries considered in the group differ from those observed in data.\nWhile common methods address this by determining the appropriate level of\nsymmetry at the dataset level, they are limited to supervised settings and\nignore scenarios in which multiple levels of symmetry co-exist in the same\ndataset. For instance, pictures of cars and planes exhibit different levels of\nrotation, yet both are included in the CIFAR-10 dataset. In this paper, we\npropose a method able to detect the level of symmetry of each input without the\nneed for labels. To this end, we derive a sufficient and necessary condition to\nlearn the distribution of symmetries in the data. Using the learned\ndistribution, we generate pseudo-labels that allow us to learn the levels of\nsymmetry of each input in a self-supervised manner. We validate the\neffectiveness of our approach on synthetic datasets with different per-class\nlevels of symmetries e.g. MNISTMultiple, in which digits are uniformly rotated\nwithin a class-dependent interval. We demonstrate that our method can be used\nfor practical applications such as the generation of standardized datasets in\nwhich the symmetries are not present, as well as the detection of\nout-of-distribution symmetries during inference. By doing so, both the\ngeneralization and robustness of non-equivariant models can be improved. Our\ncode is publicly available at https://github.com/aurban0/ssl-sym.\n","authors":["Alonso Urbano","David W. Romero"],"pdf_url":"https://arxiv.org/pdf/2312.12223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12222v1","updated":"2023-12-19T15:11:32Z","published":"2023-12-19T15:11:32Z","title":"EarthVQA: Towards Queryable Earth via Relational Reasoning-Based Remote\n  Sensing Visual Question Answering","summary":"  Earth vision research typically focuses on extracting geospatial object\nlocations and categories but neglects the exploration of relations between\nobjects and comprehensive reasoning. Based on city planning needs, we develop a\nmulti-modal multi-task VQA dataset (EarthVQA) to advance relational\nreasoning-based judging, counting, and comprehensive analysis. The EarthVQA\ndataset contains 6000 images, corresponding semantic masks, and 208,593 QA\npairs with urban and rural governance requirements embedded. As objects are the\nbasis for complex relational reasoning, we propose a Semantic OBject Awareness\nframework (SOBA) to advance VQA in an object-centric way. To preserve refined\nspatial locations and semantics, SOBA leverages a segmentation network for\nobject semantics generation. The object-guided attention aggregates object\ninterior features via pseudo masks, and bidirectional cross-attention further\nmodels object external relations hierarchically. To optimize object counting,\nwe propose a numerical difference loss that dynamically adds difference\npenalties, unifying the classification and regression tasks. Experimental\nresults show that SOBA outperforms both advanced general and remote sensing\nmethods. We believe this dataset and framework provide a strong benchmark for\nEarth vision's complex analysis. The project page is at\nhttps://Junjue-Wang.github.io/homepage/EarthVQA.\n","authors":["Junjue Wang","Zhuo Zheng","Zihang Chen","Ailong Ma","Yanfei Zhong"],"pdf_url":"https://arxiv.org/pdf/2312.12222v1.pdf","comment":"Accepted By AAAI 2024"},{"id":"http://arxiv.org/abs/2312.12198v1","updated":"2023-12-19T14:34:36Z","published":"2023-12-19T14:34:36Z","title":"Mask Grounding for Referring Image Segmentation","summary":"  Referring Image Segmentation (RIS) is a challenging task that requires an\nalgorithm to segment objects referred by free-form language expressions.\nDespite significant progress in recent years, most state-of-the-art (SOTA)\nmethods still suffer from considerable language-image modality gap at the pixel\nand word level. These methods generally 1) rely on sentence-level language\nfeatures for language-image alignment and 2) lack explicit training supervision\nfor fine-grained visual grounding. Consequently, they exhibit weak object-level\ncorrespondence between visual and language features. Without well-grounded\nfeatures, prior methods struggle to understand complex expressions that require\nstrong reasoning over relationships among multiple objects, especially when\ndealing with rarely used or ambiguous clauses. To tackle this challenge, we\nintroduce a novel Mask Grounding auxiliary task that significantly improves\nvisual grounding within language features, by explicitly teaching the model to\nlearn fine-grained correspondence between masked textual tokens and their\nmatching visual objects. Mask Grounding can be directly used on prior RIS\nmethods and consistently bring improvements. Furthermore, to holistically\naddress the modality gap, we also design a cross-modal alignment loss and an\naccompanying alignment module. These additions work synergistically with Mask\nGrounding. With all these techniques, our comprehensive approach culminates in\nMagNet Mask-grounded Network), an architecture that significantly outperforms\nprior arts on three key benchmarks (RefCOCO, RefCOCO+ and G-Ref), demonstrating\nour method's effectiveness in addressing current limitations of RIS algorithms.\nOur code and pre-trained weights will be released.\n","authors":["Yong Xien Chng","Henry Zheng","Yizeng Han","Xuchong Qiu","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2312.12198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.06962v2","updated":"2023-12-19T14:29:58Z","published":"2023-08-14T06:32:54Z","title":"Color-NeuS: Reconstructing Neural Implicit Surfaces with Color","summary":"  The reconstruction of object surfaces from multi-view images or monocular\nvideo is a fundamental issue in computer vision. However, much of the recent\nresearch concentrates on reconstructing geometry through implicit or explicit\nmethods. In this paper, we shift our focus towards reconstructing mesh in\nconjunction with color. We remove the view-dependent color from neural volume\nrendering while retaining volume rendering performance through a relighting\nnetwork. Mesh is extracted from the signed distance function (SDF) network for\nthe surface, and color for each surface vertex is drawn from the global color\nnetwork. To evaluate our approach, we conceived a in hand object scanning task\nfeaturing numerous occlusions and dramatic shifts in lighting conditions. We've\ngathered several videos for this task, and the results surpass those of any\nexisting methods capable of reconstructing mesh alongside color. Additionally,\nour method's performance was assessed using public datasets, including DTU,\nBlendedMVS, and OmniObject3D. The results indicated that our method performs\nwell across all these datasets. Project page:\nhttps://colmar-zlicheng.github.io/color_neus.\n","authors":["Licheng Zhong","Lixin Yang","Kailin Li","Haoyu Zhen","Mei Han","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2308.06962v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12189v1","updated":"2023-12-19T14:23:47Z","published":"2023-12-19T14:23:47Z","title":"Teeth Localization and Lesion Segmentation in CBCT Images using\n  SpatialConfiguration-Net and U-Net","summary":"  The localization of teeth and segmentation of periapical lesions in cone-beam\ncomputed tomography (CBCT) images are crucial tasks for clinical diagnosis and\ntreatment planning, which are often time-consuming and require a high level of\nexpertise. However, automating these tasks is challenging due to variations in\nshape, size, and orientation of lesions, as well as similar topologies among\nteeth. Moreover, the small volumes occupied by lesions in CBCT images pose a\nclass imbalance problem that needs to be addressed. In this study, we propose a\ndeep learning-based method utilizing two convolutional neural networks: the\nSpatialConfiguration-Net (SCN) and a modified version of the U-Net. The SCN\naccurately predicts the coordinates of all teeth present in an image, enabling\nprecise cropping of teeth volumes that are then fed into the U-Net which\ndetects lesions via segmentation. To address class imbalance, we compare the\nperformance of three reweighting loss functions. After evaluation on 144 CBCT\nimages, our method achieves a 97.3% accuracy for teeth localization, along with\na promising sensitivity and specificity of 0.97 and 0.88, respectively, for\nsubsequent lesion detection.\n","authors":["Arnela Hadzic","Barbara Kirnbauer","Darko Stern","Martin Urschler"],"pdf_url":"https://arxiv.org/pdf/2312.12189v1.pdf","comment":"Accepted for VISIGRAPP 2024 (Track: VISAPP), 8 pages"},{"id":"http://arxiv.org/abs/2312.10447v2","updated":"2023-12-19T14:22:43Z","published":"2023-12-16T13:39:52Z","title":"Finger Biometric Recognition With Feature Selection","summary":"  Biometrics is indispensable in this modern digital era for secure automated\nhuman authentication in various fields of machine learning and pattern\nrecognition. Hand geometry is a promising physiological biometric trait with\nample deployed application areas for identity verification. Due to the\nintricate anatomic foundation of the thumb and substantial inter-finger posture\nvariation, satisfactory performances cannot be achieved while the thumb is\nincluded in the contact-free environment. To overcome the hindrances associated\nwith the thumb, four finger-based (excluding the thumb) biometric approaches\nhave been devised. In this chapter, a four-finger based biometric method has\nbeen presented. Again, selection of salient features is essential to reduce the\nfeature dimensionality by eliminating the insignificant features. Weights are\nassigned according to the discriminative efficiency of the features to\nemphasize on the essential features. Two different strategies namely, the\nglobal and local feature selection methods are adopted based on the adaptive\nforward-selection and backward-elimination (FoBa) algorithm. The identification\nperformances are evaluated using the weighted k-nearest neighbor (wk-NN) and\nrandom forest (RF) classifiers. The experiments are conducted using the\nselected feature subsets over the 300 subjects of the Bosphorus hand database.\nThe best identification accuracy of 98.67%, and equal error rate (EER) of 4.6%\nhave been achieved using the subset of 25 features which are selected by the\nrank-based local FoBa algorithm.\n","authors":["Asish Bera","Debotosh Bhattacharjee","Mita Nasipuri"],"pdf_url":"https://arxiv.org/pdf/2312.10447v2.pdf","comment":"34 pages. The Biometric Computing: Recognition and Registration, 2019"},{"id":"http://arxiv.org/abs/2305.18072v2","updated":"2023-12-19T14:17:57Z","published":"2023-05-29T13:18:59Z","title":"Image Captioning with Multi-Context Synthetic Data","summary":"  Image captioning requires numerous annotated image-text pairs, resulting in\nsubstantial annotation costs. Recently, large models (e.g. diffusion models and\nlarge language models) have excelled in producing high-quality images and text.\nThis potential can be harnessed to create synthetic image-text pairs for\ntraining captioning models. Synthetic data can improve cost and time efficiency\nin data collection, allow for customization to specific domains, bootstrap\ngeneralization capability for zero-shot performance, and circumvent privacy\nconcerns associated with real-world data. However, existing methods struggle to\nattain satisfactory performance solely through synthetic data. We identify the\nissue as generated images from simple descriptions mostly capture a solitary\nperspective with limited context, failing to align with the intricate scenes\nprevalent in real-world imagery. To tackle this, we present an innovative\npipeline that introduces multi-context data generation. Beginning with an\ninitial text corpus, our approach employs a large language model to extract\nmultiple sentences portraying the same scene from diverse viewpoints. These\nsentences are then condensed into a single sentence with multiple contexts.\nSubsequently, we generate intricate images using the condensed captions through\ndiffusion models. Our model is exclusively trained on synthetic image-text\npairs crafted through this process. The effectiveness of our pipeline is\nvalidated through experimental results in both the in-domain and cross-domain\nsettings, where it achieves state-of-the-art performance on well-known datasets\nsuch as MSCOCO, Flickr30k, and NoCaps.\n","authors":["Feipeng Ma","Yizhou Zhou","Fengyun Rao","Yueyi Zhang","Xiaoyan Sun"],"pdf_url":"https://arxiv.org/pdf/2305.18072v2.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2309.15289v4","updated":"2023-12-19T14:14:22Z","published":"2023-09-26T21:56:03Z","title":"SEPT: Towards Efficient Scene Representation Learning for Motion\n  Prediction","summary":"  Motion prediction is crucial for autonomous vehicles to operate safely in\ncomplex traffic environments. Extracting effective spatiotemporal relationships\namong traffic elements is key to accurate forecasting. Inspired by the\nsuccessful practice of pretrained large language models, this paper presents\nSEPT, a modeling framework that leverages self-supervised learning to develop\npowerful spatiotemporal understanding for complex traffic scenes. Specifically,\nour approach involves three masking-reconstruction modeling tasks on scene\ninputs including agents' trajectories and road network, pretraining the scene\nencoder to capture kinematics within trajectory, spatial structure of road\nnetwork, and interactions among roads and agents. The pretrained encoder is\nthen finetuned on the downstream forecasting task. Extensive experiments\ndemonstrate that SEPT, without elaborate architectural design or manual feature\nengineering, achieves state-of-the-art performance on the Argoverse 1 and\nArgoverse 2 motion forecasting benchmarks, outperforming previous methods on\nall main metrics by a large margin.\n","authors":["Zhiqian Lan","Yuxuan Jiang","Yao Mu","Chen Chen","Shengbo Eben Li"],"pdf_url":"https://arxiv.org/pdf/2309.15289v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12176v1","updated":"2023-12-19T14:09:12Z","published":"2023-12-19T14:09:12Z","title":"All for One, and One for All: UrbanSyn Dataset, the third Musketeer of\n  Synthetic Driving Scenes","summary":"  We introduce UrbanSyn, a photorealistic dataset acquired through\nsemi-procedurally generated synthetic urban driving scenarios. Developed using\nhigh-quality geometry and materials, UrbanSyn provides pixel-level ground\ntruth, including depth, semantic segmentation, and instance segmentation with\nobject bounding boxes and occlusion degree. It complements GTAV and Synscapes\ndatasets to form what we coin as the 'Three Musketeers'. We demonstrate the\nvalue of the Three Musketeers in unsupervised domain adaptation for image\nsemantic segmentation. Results on real-world datasets, Cityscapes, Mapillary\nVistas, and BDD100K, establish new benchmarks, largely attributed to UrbanSyn.\nWe make UrbanSyn openly and freely accessible (www.urbansyn.org).\n","authors":["Jose L. Gómez","Manuel Silva","Antonio Seoane","Agnès Borrás","Mario Noriega","Germán Ros","Jose A. Iglesias-Guitian","Antonio M. López"],"pdf_url":"https://arxiv.org/pdf/2312.12176v1.pdf","comment":"The UrbanSyn Dataset is available in http://urbansyn.org/"},{"id":"http://arxiv.org/abs/2311.17280v3","updated":"2023-12-19T14:04:33Z","published":"2023-11-28T23:40:13Z","title":"Does VLN Pretraining Work with Nonsensical or Irrelevant Instructions?","summary":"  Data augmentation via back-translation is common when pretraining\nVision-and-Language Navigation (VLN) models, even though the generated\ninstructions are noisy. But: does that noise matter? We find that nonsensical\nor irrelevant language instructions during pretraining can have little effect\non downstream performance for both HAMT and VLN-BERT on R2R, and is still\nbetter than only using clean, human data. To underscore these results, we\nconcoct an efficient augmentation method, Unigram + Object, which generates\nnonsensical instructions that nonetheless improve downstream performance. Our\nfindings suggest that what matters for VLN R2R pretraining is the quantity of\nvisual trajectories, not the quality of instructions.\n","authors":["Wang Zhu","Ishika Singh","Yuan Huang","Robin Jia","Jesse Thomason"],"pdf_url":"https://arxiv.org/pdf/2311.17280v3.pdf","comment":"Accepted by O-DRUM @ CVPR 2023"},{"id":"http://arxiv.org/abs/2312.10656v2","updated":"2023-12-19T13:54:15Z","published":"2023-12-17T09:05:56Z","title":"VidToMe: Video Token Merging for Zero-Shot Video Editing","summary":"  Diffusion models have made significant advances in generating high-quality\nimages, but their application to video generation has remained challenging due\nto the complexity of temporal motion. Zero-shot video editing offers a solution\nby utilizing pre-trained image diffusion models to translate source videos into\nnew ones. Nevertheless, existing methods struggle to maintain strict temporal\nconsistency and efficient memory consumption. In this work, we propose a novel\napproach to enhance temporal consistency in generated videos by merging\nself-attention tokens across frames. By aligning and compressing temporally\nredundant tokens across frames, our method improves temporal coherence and\nreduces memory consumption in self-attention computations. The merging strategy\nmatches and aligns tokens according to the temporal correspondence between\nframes, facilitating natural temporal consistency in generated video frames. To\nmanage the complexity of video processing, we divide videos into chunks and\ndevelop intra-chunk local token merging and inter-chunk global token merging,\nensuring both short-term video continuity and long-term content consistency.\nOur video editing approach seamlessly extends the advancements in image editing\nto video editing, rendering favorable results in temporal consistency over\nstate-of-the-art methods.\n","authors":["Xirui Li","Chao Ma","Xiaokang Yang","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2312.10656v2.pdf","comment":"Project page: https://vidtome-diffusion.github.io"},{"id":"http://arxiv.org/abs/2312.12155v1","updated":"2023-12-19T13:38:48Z","published":"2023-12-19T13:38:48Z","title":"Towards Balanced Alignment: Modal-Enhanced Semantic Modeling for Video\n  Moment Retrieval","summary":"  Video Moment Retrieval (VMR) aims to retrieve temporal segments in untrimmed\nvideos corresponding to a given language query by constructing cross-modal\nalignment strategies. However, these existing strategies are often sub-optimal\nsince they ignore the modality imbalance problem, \\textit{i.e.}, the semantic\nrichness inherent in videos far exceeds that of a given limited-length\nsentence. Therefore, in pursuit of better alignment, a natural idea is\nenhancing the video modality to filter out query-irrelevant semantics, and\nenhancing the text modality to capture more segment-relevant knowledge. In this\npaper, we introduce Modal-Enhanced Semantic Modeling (MESM), a novel framework\nfor more balanced alignment through enhancing features at two levels. First, we\nenhance the video modality at the frame-word level through word reconstruction.\nThis strategy emphasizes the portions associated with query words in\nframe-level features while suppressing irrelevant parts. Therefore, the\nenhanced video contains less redundant semantics and is more balanced with the\ntextual modality. Second, we enhance the textual modality at the\nsegment-sentence level by learning complementary knowledge from context\nsentences and ground-truth segments. With the knowledge added to the query, the\ntextual modality thus maintains more meaningful semantics and is more balanced\nwith the video modality. By implementing two levels of MESM, the semantic\ninformation from both modalities is more balanced to align, thereby bridging\nthe modality gap. Experiments on three widely used benchmarks, including the\nout-of-distribution settings, show that the proposed framework achieves a new\nstart-of-the-art performance with notable generalization ability (e.g., 4.42%\nand 7.69% average gains of R1@0.7 on Charades-STA and Charades-CG). The code\nwill be available at https://github.com/lntzm/MESM.\n","authors":["Zhihang Liu","Jun Li","Hongtao Xie","Pandeng Li","Jiannan Ge","Sun-Ao Liu","Guoqing Jin"],"pdf_url":"https://arxiv.org/pdf/2312.12155v1.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2312.12151v1","updated":"2023-12-19T13:33:59Z","published":"2023-12-19T13:33:59Z","title":"SoftCTM: Cell detection by soft instance segmentation and consideration\n  of cell-tissue interaction","summary":"  Detecting and classifying cells in histopathology H\\&E stained whole-slide\nimages is a core task in computational pathology, as it provides valuable\ninsight into the tumor microenvironment. In this work we investigate the impact\nof ground truth formats on the models performance. Additionally, cell-tissue\ninteractions are considered by providing tissue segmentation predictions as\ninput to the cell detection model. We find that a \"soft\", probability-map\ninstance segmentation ground truth leads to best model performance. Combined\nwith cell-tissue interaction and test-time augmentation our Soft\nCell-Tissue-Model (SoftCTM) achieves 0.7172 mean F1-Score on the Overlapped\nCell On Tissue (OCELOT) test set, achieving the third best overall score in the\nOCELOT 2023 Challenge. The source code for our approach is made publicly\navailable at https://github.com/lely475/ocelot23algo.\n","authors":["Lydia A. Schoenpflug","Viktor H. Koelzer"],"pdf_url":"https://arxiv.org/pdf/2312.12151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12144v1","updated":"2023-12-19T13:25:45Z","published":"2023-12-19T13:25:45Z","title":"M-BEV: Masked BEV Perception for Robust Autonomous Driving","summary":"  3D perception is a critical problem in autonomous driving. Recently, the\nBird-Eye-View (BEV) approach has attracted extensive attention, due to low-cost\ndeployment and desirable vision detection capacity. However, the existing\nmodels ignore a realistic scenario during the driving procedure, i.e., one or\nmore view cameras may be failed, which largely deteriorates the performance. To\ntackle this problem, we propose a generic Masked BEV (M-BEV) perception\nframework, which can effectively improve robustness to this challenging\nscenario, by random masking and reconstructing camera views in the end-to-end\ntraining. More specifically, we develop a novel Masked View Reconstruction\n(MVR) module for M-BEV. It mimics various missing cases by randomly masking\nfeatures of different camera views, then leverages the original features of\nthese views as self-supervision, and reconstructs the masked ones with the\ndistinct spatio-temporal context across views. Via such a plug-and-play MVR,\nour M-BEV is capable of learning the missing views from the resting ones, and\nthus well generalized for robust view recovery and accurate perception in the\ntesting. We perform extensive experiments on the popular NuScenes benchmark,\nwhere our framework can significantly boost 3D perception performance of the\nstate-of-the-art models on various missing view cases, e.g., for the absence of\nback view, our M-BEV promotes the PETRv2 model with 10.3% mAP gain.\n","authors":["Siran Chen","Yue Ma","Yu Qiao","Yali Wang"],"pdf_url":"https://arxiv.org/pdf/2312.12144v1.pdf","comment":"Github repository: https://github.com/Sranc3/M-BEV"},{"id":"http://arxiv.org/abs/2312.12143v1","updated":"2023-12-19T13:23:49Z","published":"2023-12-19T13:23:49Z","title":"Integrating Human Vision Perception in Vision Transformers for\n  Classifying Waste Items","summary":"  In this paper, we propose an novel methodology aimed at simulating the\nlearning phenomenon of nystagmus through the application of differential\nblurring on datasets. Nystagmus is a biological phenomenon that influences\nhuman vision throughout life, notably by diminishing head shake from infancy to\nadulthood. Leveraging this concept, we address the issue of waste\nclassification, a pressing global concern. The proposed framework comprises two\nmodules, with the second module closely resembling the original Vision\nTransformer, a state of the art model model in classification tasks. The\nprimary motivation behind our approach is to enhance the model's precision and\nadaptability, mirroring the real world conditions that the human visual system\nundergoes. This novel methodology surpasses the standard Vision Transformer\nmodel in waste classification tasks, exhibiting an improvement with a margin of\n2%. This improvement underscores the potential of our methodology in improving\nmodel precision by drawing inspiration from human vision perception. Further\nresearch in the proposed methodology could yield greater performance results,\nand can extrapolated to other global tasks.\n","authors":["Akshat Kishore Shrivastava","Tapan Kumar Gandhi"],"pdf_url":"https://arxiv.org/pdf/2312.12143v1.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2312.12142v1","updated":"2023-12-19T13:23:20Z","published":"2023-12-19T13:23:20Z","title":"FontDiffuser: One-Shot Font Generation via Denoising Diffusion with\n  Multi-Scale Content Aggregation and Style Contrastive Learning","summary":"  Automatic font generation is an imitation task, which aims to create a font\nlibrary that mimics the style of reference images while preserving the content\nfrom source images. Although existing font generation methods have achieved\nsatisfactory performance, they still struggle with complex characters and large\nstyle variations. To address these issues, we propose FontDiffuser, a\ndiffusion-based image-to-image one-shot font generation method, which\ninnovatively models the font imitation task as a noise-to-denoise paradigm. In\nour method, we introduce a Multi-scale Content Aggregation (MCA) block, which\neffectively combines global and local content cues across different scales,\nleading to enhanced preservation of intricate strokes of complex characters.\nMoreover, to better manage the large variations in style transfer, we propose a\nStyle Contrastive Refinement (SCR) module, which is a novel structure for style\nrepresentation learning. It utilizes a style extractor to disentangle styles\nfrom images, subsequently supervising the diffusion model via a meticulously\ndesigned style contrastive loss. Extensive experiments demonstrate\nFontDiffuser's state-of-the-art performance in generating diverse characters\nand styles. It consistently excels on complex characters and large style\nchanges compared to previous methods. The code is available at\nhttps://github.com/yeungchenwa/FontDiffuser.\n","authors":["Zhenhua Yang","Dezhi Peng","Yuxin Kong","Yuyi Zhang","Cong Yao","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2312.12142v1.pdf","comment":"Accepted to AAAI 2024; Github Page:\n  https://github.com/yeungchenwa/FontDiffuser"},{"id":"http://arxiv.org/abs/2203.03005v5","updated":"2023-12-19T13:15:00Z","published":"2022-03-06T16:52:28Z","title":"Self-Supervised Face Image Restoration with a One-Shot Reference","summary":"  For image restoration, methods leveraging priors from generative models have\nbeen proposed and demonstrated a promising capacity to robustly restore\nphotorealistic and high-quality results. However, these methods are susceptible\nto semantic ambiguity, particularly with images that have obviously correct\nsemantics such as facial images. In this paper, we propose a semantic-aware\nlatent space exploration method for image restoration (SAIR). By explicitly\nmodeling semantics information from a given reference image, SAIR is able to\nreliably restore severely degraded images not only to high-resolution and\nhighly realistic looks but also to correct semantics. Quantitative and\nqualitative experiments collectively demonstrate the superior performance of\nthe proposed SAIR. Our code is available at https://github.com/Liamkuo/SAIR.\n","authors":["Yanhui Guo","Fangzhou Luo","Shaoyuan Xu"],"pdf_url":"https://arxiv.org/pdf/2203.03005v5.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.12135v1","updated":"2023-12-19T13:14:52Z","published":"2023-12-19T13:14:52Z","title":"Object Detection for Automated Coronary Artery Using Deep Learning","summary":"  In the era of digital medicine, medical imaging serves as a widespread\ntechnique for early disease detection, with a substantial volume of images\nbeing generated and stored daily in electronic patient records. X-ray\nangiography imaging is a standard and one of the most common methods for\nrapidly diagnosing coronary artery diseases. The notable achievements of recent\ndeep learning algorithms align with the increased use of electronic health\nrecords and diagnostic imaging. Deep neural networks, leveraging abundant data,\nadvanced algorithms, and powerful computational capabilities, prove highly\neffective in the analysis and interpretation of images. In this context, Object\ndetection methods have become a promising approach, particularly through\nconvolutional neural networks (CNN), streamlining medical image analysis by\neliminating manual feature extraction. This allows for direct feature\nextraction from images, ensuring high accuracy in results. Therefore, in our\npaper, we utilized the object detection method on X-ray angiography images to\nprecisely identify the location of coronary artery stenosis. As a result, this\nmodel enables automatic and real-time detection of stenosis locations,\nassisting in the crucial and sensitive decision-making process for healthcare\nprofessionals.\n","authors":["Hadis Keshavarz","Hossein Sadr"],"pdf_url":"https://arxiv.org/pdf/2312.12135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12133v1","updated":"2023-12-19T13:11:35Z","published":"2023-12-19T13:11:35Z","title":"Object-Aware Domain Generalization for Object Detection","summary":"  Single-domain generalization (S-DG) aims to generalize a model to unseen\nenvironments with a single-source domain. However, most S-DG approaches have\nbeen conducted in the field of classification. When these approaches are\napplied to object detection, the semantic features of some objects can be\ndamaged, which can lead to imprecise object localization and misclassification.\nTo address these problems, we propose an object-aware domain generalization\n(OA-DG) method for single-domain generalization in object detection. Our method\nconsists of data augmentation and training strategy, which are called OA-Mix\nand OA-Loss, respectively. OA-Mix generates multi-domain data with multi-level\ntransformation and object-aware mixing strategy. OA-Loss enables models to\nlearn domain-invariant representations for objects and backgrounds from the\noriginal and OA-Mixed images. Our proposed method outperforms state-of-the-art\nworks on standard benchmarks. Our code is available at\nhttps://github.com/WoojuLee24/OA-DG.\n","authors":["Wooju Lee","Dasol Hong","Hyungtae Lim","Hyun Myung"],"pdf_url":"https://arxiv.org/pdf/2312.12133v1.pdf","comment":"Accepted by AAAI-24. The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2310.00757v2","updated":"2023-12-19T13:10:23Z","published":"2023-10-01T18:27:59Z","title":"Mind the Gap: Federated Learning Broadens Domain Generalization in\n  Diagnostic AI Models","summary":"  Developing robust artificial intelligence (AI) models that generalize well to\nunseen datasets is challenging and usually requires large and variable\ndatasets, preferably from multiple institutions. In federated learning (FL), a\nmodel is trained collaboratively at numerous sites that hold local datasets\nwithout exchanging them. So far, the impact of training strategy, i.e., local\nversus collaborative, on the diagnostic on-domain and off-domain performance of\nAI models interpreting chest radiographs has not been assessed. Consequently,\nusing 610,000 chest radiographs from five institutions across the globe, we\nassessed diagnostic performance as a function of training strategy (i.e., local\nvs. collaborative), network architecture (i.e., convolutional vs.\ntransformer-based), generalization performance (i.e., on-domain vs.\noff-domain), imaging finding (i.e., cardiomegaly, pleural effusion, pneumonia,\natelectasis, consolidation, pneumothorax, and no abnormality), dataset size\n(i.e., from n=18,000 to 213,921 radiographs), and dataset diversity. Large\ndatasets not only showed minimal performance gains with FL but, in some\ninstances, even exhibited decreases. In contrast, smaller datasets revealed\nmarked improvements. Thus, on-domain performance was mainly driven by training\ndata size. However, off-domain performance leaned more on training diversity.\nWhen trained collaboratively across diverse external institutions, AI models\nconsistently surpassed models trained locally for off-domain tasks, emphasizing\nFL's potential in leveraging data diversity. In conclusion, FL can bolster\ndiagnostic privacy, reproducibility, and off-domain reliability of AI models\nand, potentially, optimize healthcare outcomes.\n","authors":["Soroosh Tayebi Arasteh","Christiane Kuhl","Marwin-Jonathan Saehn","Peter Isfort","Daniel Truhn","Sven Nebelung"],"pdf_url":"https://arxiv.org/pdf/2310.00757v2.pdf","comment":"Published in Nature Scientific Reports"},{"id":"http://arxiv.org/abs/2302.04977v3","updated":"2023-12-19T13:05:06Z","published":"2023-02-09T23:34:17Z","title":"Mithridates: Auditing and Boosting Backdoor Resistance of Machine\n  Learning Pipelines","summary":"  Machine learning (ML) models trained on data from potentially untrusted\nsources are vulnerable to poisoning. A small, maliciously crafted subset of the\ntraining inputs can cause the model to learn a \"backdoor\" task (e.g.,\nmisclassify inputs with a certain feature) in addition to its main task. Recent\nresearch proposed many hypothetical backdoor attacks whose efficacy heavily\ndepends on the configuration and training hyperparameters of the target model.\n  Given the variety of potential backdoor attacks, ML engineers who are not\nsecurity experts have no way to measure how vulnerable their current training\npipelines are, nor do they have a practical way to compare training\nconfigurations so as to pick the more resistant ones. Deploying a defense\nrequires evaluating and choosing from among dozens of research papers and\nre-engineering the training pipeline.\n  In this paper, we aim to provide ML engineers with pragmatic tools to audit\nthe backdoor resistance of their training pipelines and to compare different\ntraining configurations, to help choose one that best balances accuracy and\nsecurity.\n  First, we propose a universal, attack-agnostic resistance metric based on the\nminimum number of training inputs that must be compromised before the model\nlearns any backdoor.\n  Second, we design, implement, and evaluate Mithridates a multi-stage approach\nthat integrates backdoor resistance into the training-configuration search. ML\ndevelopers already rely on hyperparameter search to find configurations that\nmaximize the model's accuracy. Mithridates extends this standard tool to\nbalance accuracy and resistance without disruptive changes to the training\npipeline. We show that hyperparameters found by Mithridates increase resistance\nto multiple types of backdoor attacks by 3-5x with only a slight impact on\naccuracy. We also discuss extensions to AutoML and federated learning.\n","authors":["Eugene Bagdasaryan","Vitaly Shmatikov"],"pdf_url":"https://arxiv.org/pdf/2302.04977v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.16557v2","updated":"2023-12-19T12:58:02Z","published":"2022-03-30T18:00:07Z","title":"COSMOS: Cross-Modality Unsupervised Domain Adaptation for 3D Medical\n  Image Segmentation based on Target-aware Domain Translation and Iterative\n  Self-Training","summary":"  Recent advances in deep learning-based medical image segmentation studies\nachieve nearly human-level performance when in fully supervised condition.\nHowever, acquiring pixel-level expert annotations is extremely expensive and\nlaborious in medical imaging fields. Unsupervised domain adaptation can\nalleviate this problem, which makes it possible to use annotated data in one\nimaging modality to train a network that can successfully perform segmentation\non target imaging modality with no labels. In this work, we propose a\nself-training based unsupervised domain adaptation framework for 3D medical\nimage segmentation named COSMOS and validate it with automatic segmentation of\nVestibular Schwannoma (VS) and cochlea on high-resolution T2 Magnetic Resonance\nImages (MRI). Our target-aware contrast conversion network translates source\ndomain annotated T1 MRI to pseudo T2 MRI to enable segmentation training on\ntarget domain, while preserving important anatomical features of interest in\nthe converted images. Iterative self-training is followed to incorporate\nunlabeled data to training and incrementally improve the quality of\npseudo-labels, thereby leading to improved performance of segmentation. COSMOS\nwon the 1\\textsuperscript{st} place in the Cross-Modality Domain Adaptation\n(crossMoDA) challenge held in conjunction with the 24th International\nConference on Medical Image Computing and Computer Assisted Intervention\n(MICCAI 2021). It achieves mean Dice score and Average Symmetric Surface\nDistance of 0.871(0.063) and 0.437(0.270) for VS, and 0.842(0.020) and\n0.152(0.030) for cochlea.\n","authors":["Hyungseob Shin","Hyeongyu Kim","Sewon Kim","Yohan Jun","Taejoon Eo","Dosik Hwang"],"pdf_url":"https://arxiv.org/pdf/2203.16557v2.pdf","comment":"10 pages, 6 figures, MICCAI 2021 Cross-Modality Domain Adaptation\n  (crossMoDA) Challenge"},{"id":"http://arxiv.org/abs/2312.12122v1","updated":"2023-12-19T12:54:54Z","published":"2023-12-19T12:54:54Z","title":"ZS-SRT: An Efficient Zero-Shot Super-Resolution Training Method for\n  Neural Radiance Fields","summary":"  Neural Radiance Fields (NeRF) have achieved great success in the task of\nsynthesizing novel views that preserve the same resolution as the training\nviews. However, it is challenging for NeRF to synthesize high-quality\nhigh-resolution novel views with low-resolution training data. To solve this\nproblem, we propose a zero-shot super-resolution training framework for NeRF.\nThis framework aims to guide the NeRF model to synthesize high-resolution novel\nviews via single-scene internal learning rather than requiring any external\nhigh-resolution training data. Our approach consists of two stages. First, we\nlearn a scene-specific degradation mapping by performing internal learning on a\npretrained low-resolution coarse NeRF. Second, we optimize a super-resolution\nfine NeRF by conducting inverse rendering with our mapping function so as to\nbackpropagate the gradients from low-resolution 2D space into the\nsuper-resolution 3D sampling space. Then, we further introduce a temporal\nensemble strategy in the inference phase to compensate for the scene estimation\nerrors. Our method is featured on two points: (1) it does not consume\nhigh-resolution views or additional scene data to train super-resolution NeRF;\n(2) it can speed up the training process by adopting a coarse-to-fine strategy.\nBy conducting extensive experiments on public datasets, we have qualitatively\nand quantitatively demonstrated the effectiveness of our method.\n","authors":["Xiang Feng","Yongbo He","Yubo Wang","Chengkai Wang","Zhenzhong Kuang","Jiajun Ding","Feiwei Qin","Jun Yu","Jianping Fan"],"pdf_url":"https://arxiv.org/pdf/2312.12122v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06171v2","updated":"2023-12-19T12:36:47Z","published":"2023-12-11T07:20:42Z","title":"Jointly Explicit and Implicit Cross-Modal Interaction Network for\n  Anterior Chamber Inflammation Diagnosis","summary":"  Uveitis demands the precise diagnosis of anterior chamber inflammation (ACI)\nfor optimal treatment. However, current diagnostic methods only rely on a\nlimited single-modal disease perspective, which leads to poor performance. In\nthis paper, we investigate a promising yet challenging way to fuse multimodal\ndata for ACI diagnosis. Notably, existing fusion paradigms focus on empowering\nimplicit modality interactions (i.e., self-attention and its variants), but\nneglect to inject explicit modality interactions, especially from clinical\nknowledge and imaging property. To this end, we propose a jointly Explicit and\nimplicit Cross-Modal Interaction Network (EiCI-Net) for Anterior Chamber\nInflammation Diagnosis that uses anterior segment optical coherence tomography\n(AS-OCT) images, slit-lamp images, and clinical data jointly. Specifically, we\nfirst develop CNN-Based Encoders and Tabular Processing Module (TPM) to extract\nefficient feature representations in different modalities. Then, we devise an\nExplicit Cross-Modal Interaction Module (ECIM) to generate attention maps as a\nkind of explicit clinical knowledge based on the tabular feature maps, then\nintegrated them into the slit-lamp feature maps, allowing the CNN-Based Encoder\nto focus on more effective informativeness of the slit-lamp images. After that,\nthe Implicit Cross-Modal Interaction Module (ICIM), a transformer-based\nnetwork, further implicitly enhances modality interactions. Finally, we\nconstruct a considerable real-world dataset from our collaborative hospital and\nconduct sufficient experiments to demonstrate the superior performance of our\nproposed EiCI-Net compared with the state-of-the-art classification methods in\nvarious metrics.\n","authors":["Qian Shao","Ye Dai","Haochao Ying","Kan Xu","Jinhong Wang","Wei Chi","Jian Wu"],"pdf_url":"https://arxiv.org/pdf/2312.06171v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.06252v5","updated":"2023-12-19T12:31:45Z","published":"2023-05-10T15:33:15Z","title":"Embedded Feature Similarity Optimization with Specific Parameter\n  Initialization for 2D/3D Medical Image Registration","summary":"  We present a novel deep learning-based framework: Embedded Feature Similarity\nOptimization with Specific Parameter Initialization (SOPI) for 2D/3D medical\nimage registration which is a most challenging problem due to the difficulty\nsuch as dimensional mismatch, heavy computation load and lack of golden\nevaluation standard. The framework we design includes a parameter specification\nmodule to efficiently choose initialization pose parameter and a\nfine-registration module to align images. The proposed framework takes\nextracting multi-scale features into consideration using a novel composite\nconnection encoder with special training techniques. We compare the method with\nboth learning-based methods and optimization-based methods on a in-house\nCT/X-ray dataset as well as simulated data to further evaluate performance. Our\nexperiments demonstrate that the method in this paper has improved the\nregistration performance, and thereby outperforms the existing methods in terms\nof accuracy and running time. We also show the potential of the proposed method\nas an initial pose estimator. The code is available at\nhttps://github.com/m1nhengChen/SOPI\n","authors":["Minheng Chen","Zhirun Zhang","Shuheng Gu","Youyong Kong"],"pdf_url":"https://arxiv.org/pdf/2305.06252v5.pdf","comment":"14 pages, 5 figures, accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.12102v1","updated":"2023-12-19T12:26:57Z","published":"2023-12-19T12:26:57Z","title":"I-CEE: Tailoring Explanations of Image Classifications Models to User\n  Expertise","summary":"  Effectively explaining decisions of black-box machine learning models is\ncritical to responsible deployment of AI systems that rely on them. Recognizing\ntheir importance, the field of explainable AI (XAI) provides several techniques\nto generate these explanations. Yet, there is relatively little emphasis on the\nuser (the explainee) in this growing body of work and most XAI techniques\ngenerate \"one-size-fits-all\" explanations. To bridge this gap and achieve a\nstep closer towards human-centered XAI, we present I-CEE, a framework that\nprovides Image Classification Explanations tailored to User Expertise. Informed\nby existing work, I-CEE explains the decisions of image classification models\nby providing the user with an informative subset of training data (i.e.,\nexample images), corresponding local explanations, and model decisions.\nHowever, unlike prior work, I-CEE models the informativeness of the example\nimages to depend on user expertise, resulting in different examples for\ndifferent users. We posit that by tailoring the example set to user expertise,\nI-CEE can better facilitate users' understanding and simulatability of the\nmodel. To evaluate our approach, we conduct detailed experiments in both\nsimulation and with human participants (N = 100) on multiple datasets.\nExperiments with simulated users show that I-CEE improves users' ability to\naccurately predict the model's decisions (simulatability) compared to\nbaselines, providing promising preliminary results. Experiments with human\nparticipants demonstrate that our method significantly improves user\nsimulatability accuracy, highlighting the importance of human-centered XAI\n","authors":["Yao Rong","Peizhu Qian","Vaibhav Unhelkar","Enkelejda Kasneci"],"pdf_url":"https://arxiv.org/pdf/2312.12102v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10798v2","updated":"2023-12-19T12:22:45Z","published":"2023-12-17T19:22:39Z","title":"Land use/land cover classification of fused Sentinel-1 and Sentinel-2\n  imageries using ensembles of Random Forests","summary":"  The study explores the synergistic combination of Synthetic Aperture Radar\n(SAR) and Visible-Near Infrared-Short Wave Infrared (VNIR-SWIR) imageries for\nland use/land cover (LULC) classification. Image fusion, employing Bayesian\nfusion, merges SAR texture bands with VNIR-SWIR imageries. The research aims to\ninvestigate the impact of this fusion on LULC classification. Despite the\npopularity of random forests for supervised classification, their limitations,\nsuch as suboptimal performance with fewer features and accuracy stagnation, are\naddressed. To overcome these issues, ensembles of random forests (RFE) are\ncreated, introducing random rotations using the Forest-RC algorithm. Three\nrotation approaches: principal component analysis (PCA), sparse random rotation\n(SRP) matrix, and complete random rotation (CRP) matrix are employed.\nSentinel-1 SAR data and Sentinel-2 VNIR-SWIR data from the IIT-Kanpur region\nconstitute the training datasets, including SAR, SAR with texture, VNIR-SWIR,\nVNIR-SWIR with texture, and fused VNIR-SWIR with texture. The study evaluates\nclassifier efficacy, explores the impact of SAR and VNIR-SWIR fusion on\nclassification, and significantly enhances the execution speed of Bayesian\nfusion code. The SRP-based RFE outperforms other ensembles for the first two\ndatasets, yielding average overall kappa values of 61.80% and 68.18%, while the\nCRP-based RFE excels for the last three datasets with average overall kappa\nvalues of 95.99%, 96.93%, and 96.30%. The fourth dataset achieves the highest\noverall kappa of 96.93%. Furthermore, incorporating texture with SAR bands\nresults in a maximum overall kappa increment of 10.00%, while adding texture to\nVNIR-SWIR bands yields a maximum increment of approximately 3.45%.\n","authors":["Shivam Pande"],"pdf_url":"https://arxiv.org/pdf/2312.10798v2.pdf","comment":"Thesis for Master of Technology. Created: July 2018. Total pages 124"},{"id":"http://arxiv.org/abs/2312.12098v1","updated":"2023-12-19T12:21:09Z","published":"2023-12-19T12:21:09Z","title":"Domain Generalization in LiDAR Semantic Segmentation Leveraged by\n  Density Discriminative Feature Embedding","summary":"  While significant progress has been achieved in LiDAR-based perception,\ndomain generalization continues to present challenges, often resulting in\nreduced performance when encountering unfamiliar datasets due to domain\ndiscrepancies. One of the primary hurdles stems from the variability of LiDAR\nsensors, leading to inconsistencies in point cloud density distribution. Such\ninconsistencies can undermine the effectiveness of perception models. We\naddress this challenge by introducing a new approach that acknowledges a\nfundamental characteristic of LiDAR: the variation in point density due to the\ndistance from the LiDAR to the scene, and the number of beams relative to the\nfield of view. Understanding this, we view each LiDAR's point cloud at various\ndistances as having distinct density distributions, which can be consistent\nacross different LiDAR models. With this insight, we propose the Density\nDiscriminative Feature Embedding (DDFE) module, crafted to specifically extract\nfeatures related to density while ensuring domain invariance across different\nLiDAR sensors. In addition, we introduce a straightforward but effective\ndensity augmentation technique, designed to broaden the density spectrum and\nenhance the capabilities of the DDFE. The proposed DDFE stands out as a\nversatile and lightweight domain generalization module. It can be seamlessly\nintegrated into various 3D backbone networks, consistently outperforming\nexisting state-of-the-art domain generalization approaches. We commit to\nreleasing the source code publicly to foster community collaboration and\nadvancement.\n","authors":["Jaeyeul Kim","Jungwan Woo","Jeonghoon Kim","Sunghoon Im"],"pdf_url":"https://arxiv.org/pdf/2312.12098v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2312.05281v2","updated":"2023-12-19T12:20:22Z","published":"2023-12-08T10:27:47Z","title":"X2-Softmax: Margin Adaptive Loss Function for Face Recognition","summary":"  Learning the discriminative features of different faces is an important task\nin face recognition. By extracting face features in neural networks, it becomes\neasy to measure the similarity of different face images, which makes face\nrecognition possible. To enhance the neural network's face feature\nseparability, incorporating an angular margin during training is common\npractice. State-of-the-art loss functions CosFace and ArcFace apply fixed\nmargins between weights of classes to enhance the inter-class separation of\nface features. Since the distribution of samples in the training set is\nimbalanced, similarities between different identities are unequal. Therefore,\nusing an inappropriately fixed angular margin may lead to the problem that the\nmodel is difficult to converge or the face features are not discriminative\nenough. It is more in line with our intuition that the margins are angular\nadaptive, which could increase with the angles between classes growing. In this\npaper, we propose a new angular margin loss named X2-Softmax. X2-Softmax loss\nhas adaptive angular margins, which provide the margin that increases with the\nangle between different classes growing. The angular adaptive margin ensures\nmodel flexibility and effectively improves the effect of face recognition. We\nhave trained the neural network with X2-Softmax loss on the MS1Mv3 dataset and\ntested it on several evaluation benchmarks to demonstrate the effectiveness and\nsuperiority of our loss function.\n","authors":["Jiamu Xu","Xiaoxiang Liu","Xinyuan Zhang","Yain-Whar Si","Xiaofan Li","Zheng Shi","Ke Wang","Xueyuan Gong"],"pdf_url":"https://arxiv.org/pdf/2312.05281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12096v1","updated":"2023-12-19T12:19:20Z","published":"2023-12-19T12:19:20Z","title":"DLCA-Recon: Dynamic Loose Clothing Avatar Reconstruction from Monocular\n  Videos","summary":"  Reconstructing a dynamic human with loose clothing is an important but\ndifficult task. To address this challenge, we propose a method named DLCA-Recon\nto create human avatars from monocular videos. The distance from loose clothing\nto the underlying body rapidly changes in every frame when the human freely\nmoves and acts. Previous methods lack effective geometric initialization and\nconstraints for guiding the optimization of deformation to explain this\ndramatic change, resulting in the discontinuous and incomplete reconstruction\nsurface. To model the deformation more accurately, we propose to initialize an\nestimated 3D clothed human in the canonical space, as it is easier for\ndeformation fields to learn from the clothed human than from SMPL. With both\nrepresentations of explicit mesh and implicit SDF, we utilize the physical\nconnection information between consecutive frames and propose a dynamic\ndeformation field (DDF) to optimize deformation fields. DDF accounts for\ncontributive forces on loose clothing to enhance the interpretability of\ndeformations and effectively capture the free movement of loose clothing.\nMoreover, we propagate SMPL skinning weights to each individual and refine pose\nand skinning weights during the optimization to improve skinning\ntransformation. Based on more reasonable initialization and DDF, we can\nsimulate real-world physics more accurately. Extensive experiments on public\nand our own datasets validate that our method can produce superior results for\nhumans with loose clothing compared to the SOTA methods.\n","authors":["Chunjie Luo","Fei Luo","Yusen Wang","Enxu Zhao","Chunxia Xiao"],"pdf_url":"https://arxiv.org/pdf/2312.12096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12090v1","updated":"2023-12-19T12:10:12Z","published":"2023-12-19T12:10:12Z","title":"GazeMoDiff: Gaze-guided Diffusion Model for Stochastic Human Motion\n  Prediction","summary":"  Human motion prediction is important for virtual reality (VR) applications,\ne.g., for realistic avatar animation. Existing methods have synthesised body\nmotion only from observed past motion, despite the fact that human gaze is\nknown to correlate strongly with body movements and is readily available in\nrecent VR headsets. We present GazeMoDiff -- a novel gaze-guided denoising\ndiffusion model to generate stochastic human motions. Our method first uses a\ngraph attention network to learn the spatio-temporal correlations between eye\ngaze and human movements and to fuse them into cross-modal gaze-motion\nfeatures. These cross-modal features are injected into a noise prediction\nnetwork via a cross-attention mechanism and progressively denoised to generate\nrealistic human full-body motions. Experimental results on the MoGaze and GIMO\ndatasets demonstrate that our method outperforms the state-of-the-art methods\nby a large margin in terms of average displacement error (15.03% on MoGaze and\n9.20% on GIMO). We further conducted an online user study to compare our method\nwith state-of-the-art methods and the responses from 23 participants validate\nthat the motions generated by our method are more realistic than those from\nother methods. Taken together, our work makes a first important step towards\ngaze-guided stochastic human motion prediction and guides future work on this\nimportant topic in VR research.\n","authors":["Haodong Yan","Zhiming Hu","Syn Schmitt","Andreas Bulling"],"pdf_url":"https://arxiv.org/pdf/2312.12090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12080v1","updated":"2023-12-19T11:57:54Z","published":"2023-12-19T11:57:54Z","title":"Learning Subject-Aware Cropping by Outpainting Professional Photos","summary":"  How to frame (or crop) a photo often depends on the image subject and its\ncontext; e.g., a human portrait. Recent works have defined the subject-aware\nimage cropping task as a nuanced and practical version of image cropping. We\npropose a weakly-supervised approach (GenCrop) to learn what makes a\nhigh-quality, subject-aware crop from professional stock images. Unlike\nsupervised prior work, GenCrop requires no new manual annotations beyond the\nexisting stock image collection. The key challenge in learning from this data,\nhowever, is that the images are already cropped and we do not know what regions\nwere removed. Our insight is combine a library of stock images with a modern,\npre-trained text-to-image diffusion model. The stock image collection provides\ndiversity and its images serve as pseudo-labels for a good crop, while the\ntext-image diffusion model is used to out-paint (i.e., outward inpainting)\nrealistic uncropped images. Using this procedure, we are able to automatically\ngenerate a large dataset of cropped-uncropped training pairs to train a\ncropping model. Despite being weakly-supervised, GenCrop is competitive with\nstate-of-the-art supervised methods and significantly better than comparable\nweakly-supervised baselines on quantitative and qualitative evaluation metrics.\n","authors":["James Hong","Lu Yuan","Michaël Gharbi","Matthew Fisher","Kayvon Fatahalian"],"pdf_url":"https://arxiv.org/pdf/2312.12080v1.pdf","comment":"AAAI 24. Extended version with supplemental materials"},{"id":"http://arxiv.org/abs/2312.10439v2","updated":"2023-12-19T11:43:07Z","published":"2023-12-16T13:06:15Z","title":"Simple Image-level Classification Improves Open-vocabulary Object\n  Detection","summary":"  Open-Vocabulary Object Detection (OVOD) aims to detect novel objects beyond a\ngiven set of base categories on which the detection model is trained. Recent\nOVOD methods focus on adapting the image-level pre-trained vision-language\nmodels (VLMs), such as CLIP, to a region-level object detection task via, eg.,\nregion-level knowledge distillation, regional prompt learning, or region-text\npre-training, to expand the detection vocabulary. These methods have\ndemonstrated remarkable performance in recognizing regional visual concepts,\nbut they are weak in exploiting the VLMs' powerful global scene understanding\nability learned from the billion-scale image-level text descriptions. This\nlimits their capability in detecting hard objects of small, blurred, or\noccluded appearance from novel/base categories, whose detection heavily relies\non contextual information. To address this, we propose a novel approach, namely\nSimple Image-level Classification for Context-Aware Detection Scoring\n(SIC-CADS), to leverage the superior global knowledge yielded from CLIP for\ncomplementing the current OVOD models from a global perspective. The core of\nSIC-CADS is a multi-modal multi-label recognition (MLR) module that learns the\nobject co-occurrence-based contextual information from CLIP to recognize all\npossible object categories in the scene. These image-level MLR scores can then\nbe utilized to refine the instance-level detection scores of the current OVOD\nmodels in detecting those hard objects. This is verified by extensive empirical\nresults on two popular benchmarks, OV-LVIS and OV-COCO, which show that\nSIC-CADS achieves significant and consistent improvement when combined with\ndifferent types of OVOD models. Further, SIC-CADS also improves the\ncross-dataset generalization ability on Objects365 and OpenImages. The code is\navailable at https://github.com/mala-lab/SIC-CADS.\n","authors":["Ruohuan Fang","Guansong Pang","Xiao Bai"],"pdf_url":"https://arxiv.org/pdf/2312.10439v2.pdf","comment":"Accepted at AAAI 2024"},{"id":"http://arxiv.org/abs/2306.07915v4","updated":"2023-12-19T11:40:46Z","published":"2023-06-13T17:18:01Z","title":"Image Captioners Are Scalable Vision Learners Too","summary":"  Contrastive pretraining on image-text pairs from the web is one of the most\npopular large-scale pretraining strategies for vision backbones, especially in\nthe context of large multimodal models. At the same time, image captioning on\nthis type of data is commonly considered an inferior pretraining strategy. In\nthis paper, we perform a fair comparison of these two pretraining strategies,\ncarefully matching training data, compute, and model capacity. Using a standard\nencoder-decoder transformer, we find that captioning alone is surprisingly\neffective: on classification tasks, captioning produces vision encoders\ncompetitive with contrastively pretrained encoders, while surpassing them on\nvision & language tasks. We further analyze the effect of the model\narchitecture and scale, as well as the pretraining data on the representation\nquality, and find that captioning exhibits the same or better scaling behavior\nalong these axes. Overall our results show that plain image captioning is a\nmore powerful pretraining strategy than was previously believed.\n","authors":["Michael Tschannen","Manoj Kumar","Andreas Steiner","Xiaohua Zhai","Neil Houlsby","Lucas Beyer"],"pdf_url":"https://arxiv.org/pdf/2306.07915v4.pdf","comment":"Accepted at NeurIPS 2023. v2 adds SugarCrepe results and more\n  ablations, v3 has minor fixes. v4 adds a code link (\n  https://github.com/google-research/big_vision )"},{"id":"http://arxiv.org/abs/2312.12068v1","updated":"2023-12-19T11:36:03Z","published":"2023-12-19T11:36:03Z","title":"PICNN: A Pathway towards Interpretable Convolutional Neural Networks","summary":"  Convolutional Neural Networks (CNNs) have exhibited great performance in\ndiscriminative feature learning for complex visual tasks. Besides\ndiscrimination power, interpretability is another important yet under-explored\nproperty for CNNs. One difficulty in the CNN interpretability is that filters\nand image classes are entangled. In this paper, we introduce a novel pathway to\nalleviate the entanglement between filters and image classes. The proposed\npathway groups the filters in a late conv-layer of CNN into class-specific\nclusters. Clusters and classes are in a one-to-one relationship. Specifically,\nwe use the Bernoulli sampling to generate the filter-cluster assignment matrix\nfrom a learnable filter-class correspondence matrix. To enable end-to-end\noptimization, we develop a novel reparameterization trick for handling the\nnon-differentiable Bernoulli sampling. We evaluate the effectiveness of our\nmethod on ten widely used network architectures (including nine CNNs and a ViT)\nand five benchmark datasets. Experimental results have demonstrated that our\nmethod PICNN (the combination of standard CNNs with our proposed pathway)\nexhibits greater interpretability than standard CNNs while achieving higher or\ncomparable discrimination power.\n","authors":["Wengang Guo","Jiayi Yang","Huilin Yin","Qijun Chen","Wei Ye"],"pdf_url":"https://arxiv.org/pdf/2312.12068v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12064v1","updated":"2023-12-19T11:32:02Z","published":"2023-12-19T11:32:02Z","title":"MPI Planar Correction of Pulse Based ToF Cameras","summary":"  Time-of-Flight (ToF) cameras are becoming popular in a wide span of areas\nranging from consumer-grade electronic devices to safety-critical industrial\nrobots. This is mainly due to their high frame rate, relative good precision\nand the lowered costs. Although ToF cameras are in continuous development,\nespecially pulse-based variants, they still face different problems, including\nspurious noise over the points or multipath inference (MPI). The latter can\ncause deformed surfaces to manifest themselves on curved surfaces instead of\nplanar ones, making standard spatial data preprocessing, such as plane\nextraction, difficult. In this paper, we focus on the MPI reduction problem\nusing Feature Pyramid Networks (FPN) which allow the mitigation of this type of\nartifact for pulse-based ToF cameras. With our end-to-end network, we managed\nto attenuate the MPI effect on planar surfaces using a learning-based method on\nreal ToF data. Both the custom dataset used for our model training as well as\nthe code is available on the author's Github homepage.\n","authors":["Marian-Leontin Pop","Levente Tamas"],"pdf_url":"https://arxiv.org/pdf/2312.12064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12042v1","updated":"2023-12-19T10:55:46Z","published":"2023-12-19T10:55:46Z","title":"Pose2Gaze: Generating Realistic Human Gaze Behaviour from Full-body\n  Poses using an Eye-body Coordination Model","summary":"  While generating realistic body movements, e.g., for avatars in virtual\nreality, is widely studied in computer vision and graphics, the generation of\neye movements that exhibit realistic coordination with the body remains\nunder-explored. We first report a comprehensive analysis of the coordination of\nhuman eye and full-body movements during everyday activities based on data from\nthe MoGaze and GIMO datasets. We show that eye gaze has strong correlations\nwith head directions and also full-body motions and there exists a noticeable\ntime delay between body and eye movements. Inspired by the analyses, we then\npresent Pose2Gaze -- a novel eye-body coordination model that first uses a\nconvolutional neural network and a spatio-temporal graph convolutional neural\nnetwork to extract features from head directions and full-body poses\nrespectively and then applies a convolutional neural network to generate\nrealistic eye movements. We compare our method with state-of-the-art methods\nthat predict eye gaze only from head movements for three different generation\ntasks and demonstrate that Pose2Gaze significantly outperforms these baselines\non both datasets with an average improvement of 26.4% and 21.6% in mean angular\nerror, respectively. Our findings underline the significant potential of\ncross-modal human gaze behaviour analysis and modelling.\n","authors":["Zhiming Hu","Jiahui Xu","Syn Schmitt","Andreas Bulling"],"pdf_url":"https://arxiv.org/pdf/2312.12042v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09783v3","updated":"2023-12-19T10:36:41Z","published":"2023-12-15T13:36:54Z","title":"Keep the Faith: Faithful Explanations in Convolutional Neural Networks\n  for Case-Based Reasoning","summary":"  Explaining predictions of black-box neural networks is crucial when applied\nto decision-critical tasks. Thus, attribution maps are commonly used to\nidentify important image regions, despite prior work showing that humans prefer\nexplanations based on similar examples. To this end, ProtoPNet learns a set of\nclass-representative feature vectors (prototypes) for case-based reasoning.\nDuring inference, similarities of latent features to prototypes are linearly\nclassified to form predictions and attribution maps are provided to explain the\nsimilarity. In this work, we evaluate whether architectures for case-based\nreasoning fulfill established axioms required for faithful explanations using\nthe example of ProtoPNet. We show that such architectures allow the extraction\nof faithful explanations. However, we prove that the attribution maps used to\nexplain the similarities violate the axioms. We propose a new procedure to\nextract explanations for trained ProtoPNets, named ProtoPFaith. Conceptually,\nthese explanations are Shapley values, calculated on the similarity scores of\neach prototype. They allow to faithfully answer which prototypes are present in\nan unseen image and quantify each pixel's contribution to that presence,\nthereby complying with all axioms. The theoretical violations of ProtoPNet\nmanifest in our experiments on three datasets (CUB-200-2011, Stanford Dogs,\nRSNA) and five architectures (ConvNet, ResNet, ResNet50, WideResNet50,\nResNeXt50). Our experiments show a qualitative difference between the\nexplanations given by ProtoPNet and ProtoPFaith. Additionally, we quantify the\nexplanations with the Area Over the Perturbation Curve, on which ProtoPFaith\noutperforms ProtoPNet on all experiments by a factor $>10^3$.\n","authors":["Tom Nuno Wolf","Fabian Bongratz","Anne-Marie Rickmann","Sebastian Pölsterl","Christian Wachinger"],"pdf_url":"https://arxiv.org/pdf/2312.09783v3.pdf","comment":"To be published in proceedings of AAAI Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2312.11315v2","updated":"2023-12-19T10:31:08Z","published":"2023-12-18T16:10:18Z","title":"CaRe-CNN: Cascading Refinement CNN for Myocardial Infarct Segmentation\n  with Microvascular Obstructions","summary":"  Late gadolinium enhanced (LGE) magnetic resonance (MR) imaging is widely\nestablished to assess the viability of myocardial tissue of patients after\nacute myocardial infarction (MI). We propose the Cascading Refinement CNN\n(CaRe-CNN), which is a fully 3D, end-to-end trained, 3-stage CNN cascade that\nexploits the hierarchical structure of such labeled cardiac data. Throughout\nthe three stages of the cascade, the label definition changes and CaRe-CNN\nlearns to gradually refine its intermediate predictions accordingly.\nFurthermore, to obtain more consistent qualitative predictions, we propose a\nseries of post-processing steps that take anatomical constraints into account.\nOur CaRe-CNN was submitted to the FIMH 2023 MYOSAIQ challenge, where it ranked\nsecond out of 18 participating teams. CaRe-CNN showed great improvements most\nnotably when segmenting the difficult but clinically most relevant myocardial\ninfarct tissue (MIT) as well as microvascular obstructions (MVO). When\ncomputing the average scores over all labels, our method obtained the best\nscore in eight out of ten metrics. Thus, accurate cardiac segmentation after\nacute MI via our CaRe-CNN allows generating patient-specific models of the\nheart serving as an important step towards personalized medicine.\n","authors":["Franz Thaler","Matthias A. F. Gsell","Gernot Plank","Martin Urschler"],"pdf_url":"https://arxiv.org/pdf/2312.11315v2.pdf","comment":"Accepted at VISIGRAPP 2024, 12 pages"},{"id":"http://arxiv.org/abs/2312.12030v1","updated":"2023-12-19T10:30:31Z","published":"2023-12-19T10:30:31Z","title":"Towards Accurate Guided Diffusion Sampling through Symplectic Adjoint\n  Method","summary":"  Training-free guided sampling in diffusion models leverages off-the-shelf\npre-trained networks, such as an aesthetic evaluation model, to guide the\ngeneration process. Current training-free guided sampling algorithms obtain the\nguidance energy function based on a one-step estimate of the clean image.\nHowever, since the off-the-shelf pre-trained networks are trained on clean\nimages, the one-step estimation procedure of the clean image may be inaccurate,\nespecially in the early stages of the generation process in diffusion models.\nThis causes the guidance in the early time steps to be inaccurate. To overcome\nthis problem, we propose Symplectic Adjoint Guidance (SAG), which calculates\nthe gradient guidance in two inner stages. Firstly, SAG estimates the clean\nimage via $n$ function calls, where $n$ serves as a flexible hyperparameter\nthat can be tailored to meet specific image quality requirements. Secondly, SAG\nuses the symplectic adjoint method to obtain the gradients accurately and\nefficiently in terms of the memory requirements. Extensive experiments\ndemonstrate that SAG generates images with higher qualities compared to the\nbaselines in both guided image and video generation tasks.\n","authors":["Jiachun Pan","Hanshu Yan","Jun Hao Liew","Jiashi Feng","Vincent Y. F. Tan"],"pdf_url":"https://arxiv.org/pdf/2312.12030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12028v1","updated":"2023-12-19T10:29:29Z","published":"2023-12-19T10:29:29Z","title":"EyePreserve: Identity-Preserving Iris Synthesis","summary":"  Synthesis of same-identity biometric iris images, both for existing and\nnon-existing identities while preserving the identity across a wide range of\npupil sizes, is complex due to intricate iris muscle constriction mechanism,\nrequiring a precise model of iris non-linear texture deformations to be\nembedded into the synthesis pipeline. This paper presents the first method of\nfully data-driven, identity-preserving, pupil size-varying s ynthesis of iris\nimages. This approach is capable of synthesizing images of irises with\ndifferent pupil sizes representing non-existing identities as well as\nnon-linearly deforming the texture of iris images of existing subjects given\nthe segmentation mask of the target iris image. Iris recognition experiments\nsuggest that the proposed deformation model not only preserves the identity\nwhen changing the pupil size but offers better similarity between same-identity\niris samples with significant differences in pupil size, compared to\nstate-of-the-art linear and non-linear (bio-mechanical-based) iris deformation\nmodels. Two immediate applications of the proposed approach are: (a) synthesis\nof, or enhancement of the existing biometric datasets for iris recognition,\nmimicking those acquired with iris sensors, and (b) helping forensic human\nexperts in examining iris image pairs with significant differences in pupil\ndilation. Source codes and weights of the models are made available with the\npaper.\n","authors":["Siamul Karim Khan","Patrick Tinsley","Mahsa Mitcheff","Patrick Flynn","Kevin W. Bowyer","Adam Czajka"],"pdf_url":"https://arxiv.org/pdf/2312.12028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12023v1","updated":"2023-12-19T10:19:44Z","published":"2023-12-19T10:19:44Z","title":"Progressive Frequency-Aware Network for Laparoscopic Image Desmoking","summary":"  Laparoscopic surgery offers minimally invasive procedures with better patient\noutcomes, but smoke presence challenges visibility and safety. Existing\nlearning-based methods demand large datasets and high computational resources.\nWe propose the Progressive Frequency-Aware Network (PFAN), a lightweight GAN\nframework for laparoscopic image desmoking, combining the strengths of CNN and\nTransformer for progressive information extraction in the frequency domain.\nPFAN features CNN-based Multi-scale Bottleneck-Inverting (MBI) Blocks for\ncapturing local high-frequency information and Locally-Enhanced Axial Attention\nTransformers (LAT) for efficiently handling global low-frequency information.\nPFAN efficiently desmokes laparoscopic images even with limited training data.\nOur method outperforms state-of-the-art approaches in PSNR, SSIM, CIEDE2000,\nand visual quality on the Cholec80 dataset and retains only 629K parameters.\nOur code and models are made publicly available at:\nhttps://github.com/jlzcode/PFAN.\n","authors":["Jiale Zhang","Wenfeng Huang","Xiangyun Liao","Qiong Wang"],"pdf_url":"https://arxiv.org/pdf/2312.12023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17338v2","updated":"2023-12-19T10:06:08Z","published":"2023-03-30T12:45:46Z","title":"Local region-learning modules for point cloud classification","summary":"  Data organization via forming local regions is an integral part of deep\nlearning networks that process 3D point clouds in a hierarchical manner. At\neach level, the point cloud is sampled to extract representative points and\nthese points are used to be centers of local regions. The organization of local\nregions is of considerable importance since it determines the location and size\nof the receptive field at a particular layer of feature aggregation. In this\npaper, we present two local region-learning modules: Center Shift Module to\ninfer the appropriate shift for each center point, and Radius Update Module to\nalter the radius of each local region. The parameters of the modules are\nlearned through optimizing the loss associated with the particular task within\nan end-to-end network. We present alternatives for these modules through\nvarious ways of modeling the interactions of the features and locations of 3D\npoints in the point cloud. We integrated both modules independently and\ntogether to the PointNet++ and PointCNN object classification architectures,\nand demonstrated that the modules contributed to a significant increase in\nclassification accuracy for the ScanObjectNN data set consisting of scans of\nreal-world objects. Our further experiments on ShapeNet data set showed that\nthe modules are also effective on 3D CAD models.\n","authors":["Kaya Turgut","Helin Dutagaci"],"pdf_url":"https://arxiv.org/pdf/2303.17338v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12000v1","updated":"2023-12-19T09:47:18Z","published":"2023-12-19T09:47:18Z","title":"Diffusing More Objects for Semi-Supervised Domain Adaptation with Less\n  Labeling","summary":"  For object detection, it is possible to view the prediction of bounding boxes\nas a reverse diffusion process. Using a diffusion model, the random bounding\nboxes are iteratively refined in a denoising step, conditioned on the image. We\npropose a stochastic accumulator function that starts each run with random\nbounding boxes and combines the slightly different predictions. We empirically\nverify that this improves detection performance. The improved detections are\nleveraged on unlabelled images as weighted pseudo-labels for semi-supervised\nlearning. We evaluate the method on a challenging out-of-domain test set. Our\nmethod brings significant improvements and is on par with human-selected\npseudo-labels, while not requiring any human involvement.\n","authors":["Leander van den Heuvel","Gertjan Burghouts","David W. Zhang","Gwenn Englebienne","Sabina B. van Rooij"],"pdf_url":"https://arxiv.org/pdf/2312.12000v1.pdf","comment":"4 pages, Workshop on DiffusionModels, NeurIPS 2023"},{"id":"http://arxiv.org/abs/2312.07937v2","updated":"2023-12-19T09:39:58Z","published":"2023-12-13T07:30:19Z","title":"BOTH2Hands: Inferring 3D Hands from Both Text Prompts and Body Dynamics","summary":"  The recently emerging text-to-motion advances have spired numerous attempts\nfor convenient and interactive human motion generation. Yet, existing methods\nare largely limited to generating body motions only without considering the\nrich two-hand motions, let alone handling various conditions like body dynamics\nor texts. To break the data bottleneck, we propose BOTH57M, a novel multi-modal\ndataset for two-hand motion generation. Our dataset includes accurate motion\ntracking for the human body and hands and provides pair-wised finger-level hand\nannotations and body descriptions. We further provide a strong baseline method,\nBOTH2Hands, for the novel task: generating vivid two-hand motions from both\nimplicit body dynamics and explicit text prompts. We first warm up two parallel\nbody-to-hand and text-to-hand diffusion models and then utilize the\ncross-attention transformer for motion blending. Extensive experiments and\ncross-validations demonstrate the effectiveness of our approach and dataset for\ngenerating convincing two-hand motions from the hybrid body-and-textual\nconditions. Our dataset and code will be disseminated to the community for\nfuture research.\n","authors":["Wenqian Zhang","Molin Huang","Yuxuan Zhou","Juze Zhang","Jingyi Yu","Jingya Wang","Lan Xu"],"pdf_url":"https://arxiv.org/pdf/2312.07937v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11994v1","updated":"2023-12-19T09:37:25Z","published":"2023-12-19T09:37:25Z","title":"Optimizing Diffusion Noise Can Serve As Universal Motion Priors","summary":"  We propose Diffusion Noise Optimization (DNO), a new method that effectively\nleverages existing motion diffusion models as motion priors for a wide range of\nmotion-related tasks. Instead of training a task-specific diffusion model for\neach new task, DNO operates by optimizing the diffusion latent noise of an\nexisting pre-trained text-to-motion model. Given the corresponding latent noise\nof a human motion, it propagates the gradient from the target criteria defined\non the motion space through the whole denoising process to update the diffusion\nlatent noise. As a result, DNO supports any use cases where criteria can be\ndefined as a function of motion. In particular, we show that, for motion\nediting and control, DNO outperforms existing methods in both achieving the\nobjective and preserving the motion content. DNO accommodates a diverse range\nof editing modes, including changing trajectory, pose, joint locations, or\navoiding newly added obstacles. In addition, DNO is effective in motion\ndenoising and completion, producing smooth and realistic motion from noisy and\npartial inputs. DNO achieves these results at inference time without the need\nfor model retraining, offering great versatility for any defined reward or loss\nfunction on the motion representation.\n","authors":["Korrawe Karunratanakul","Konpat Preechakul","Emre Aksan","Thabo Beeler","Supasorn Suwajanakorn","Siyu Tang"],"pdf_url":"https://arxiv.org/pdf/2312.11994v1.pdf","comment":"Project page: https://korrawe.github.io/dno-project/"},{"id":"http://arxiv.org/abs/2303.08906v2","updated":"2023-12-19T09:16:31Z","published":"2023-03-15T20:02:54Z","title":"VVS: Video-to-Video Retrieval with Irrelevant Frame Suppression","summary":"  In content-based video retrieval (CBVR), dealing with large-scale\ncollections, efficiency is as important as accuracy; thus, several video-level\nfeature-based studies have actively been conducted. Nevertheless, owing to the\nsevere difficulty of embedding a lengthy and untrimmed video into a single\nfeature, these studies have been insufficient for accurate retrieval compared\nto frame-level feature-based studies. In this paper, we show that appropriate\nsuppression of irrelevant frames can provide insight into the current obstacles\nof the video-level approaches. Furthermore, we propose a Video-to-Video\nSuppression network (VVS) as a solution. VVS is an end-to-end framework that\nconsists of an easy distractor elimination stage to identify which frames to\nremove and a suppression weight generation stage to determine the extent to\nsuppress the remaining frames. This structure is intended to effectively\ndescribe an untrimmed video with varying content and meaningless information.\nIts efficacy is proved via extensive experiments, and we show that our approach\nis not only state-of-the-art in video-level approaches but also has a fast\ninference time despite possessing retrieval capabilities close to those of\nframe-level approaches. Code is available at https://github.com/sejong-rcv/VVS\n","authors":["Won Jo","Geuntaek Lim","Gwangjin Lee","Hyunwoo Kim","Byungsoo Ko","Yukyung Choi"],"pdf_url":"https://arxiv.org/pdf/2303.08906v2.pdf","comment":"AAAI-24"},{"id":"http://arxiv.org/abs/2203.16284v3","updated":"2023-12-19T09:12:35Z","published":"2022-03-30T13:24:04Z","title":"FIRe: Fast Inverse Rendering using Directional and Signed Distance\n  Functions","summary":"  Neural 3D implicit representations learn priors that are useful for diverse\napplications, such as single- or multiple-view 3D reconstruction. A major\ndownside of existing approaches while rendering an image is that they require\nevaluating the network multiple times per camera ray so that the high\ncomputational time forms a bottleneck for downstream applications. We address\nthis problem by introducing a novel neural scene representation that we call\nthe directional distance function (DDF). To this end, we learn a signed\ndistance function (SDF) along with our DDF model to represent a class of\nshapes. Specifically, our DDF is defined on the unit sphere and predicts the\ndistance to the surface along any given direction. Therefore, our DDF allows\nrendering images with just a single network evaluation per camera ray. Based on\nour DDF, we present a novel fast algorithm (FIRe) to reconstruct 3D shapes\ngiven a posed depth map. We evaluate our proposed method on 3D reconstruction\nfrom single-view depth images, where we empirically show that our algorithm\nreconstructs 3D shapes more accurately and it is more than 15 times faster (per\niteration) than competing methods.\n","authors":["Tarun Yenamandra","Ayush Tewari","Nan Yang","Florian Bernard","Christian Theobalt","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2203.16284v3.pdf","comment":"News: Accepted to WACV'24. Project page:\n  https://vision.in.tum.de/research/geometry/fire"},{"id":"http://arxiv.org/abs/2312.11973v1","updated":"2023-12-19T09:11:49Z","published":"2023-12-19T09:11:49Z","title":"Continual Learning: Forget-free Winning Subnetworks for Video\n  Representations","summary":"  Inspired by the Regularized Lottery Ticket Hypothesis (RLTH), which\nhighlights the presence of competitive subnetworks within dense networks for\ncontinual learning tasks, we introduce Winning Subnetworks (WSN). This approach\nutilizes reused weights in dense networks to enhance learning in Task\nIncremental Learning (TIL) scenarios. To mitigate overfitting in Few-Shot Class\nIncremental Learning (FSCIL), we have developed WSN variants referred to as the\nSoft subnetwork (SoftNet). Furthermore, addressing WSN's limitation of sparse\nreused weights in Video Incremental Learning (VIL), we propose the Fourier\nSubneural Operator (FSO). The FSO, operating in Fourier space, adaptively and\ncompactly encodes videos, discovering reusable subnetworks with diverse\nbandwidths. We have applied FSO's Fourier representations to various continual\nlearning contexts, including VIL, TIL, and FSCIL. Our extensive experiments\nacross these scenarios demonstrate FSO's remarkable efficacy in continual\nlearning, significantly enhancing task performance at various convolutional\nrepresentational levels: it boosts performance in the higher layers for TIL and\nFSCIL and the lower layers for VIL.\n","authors":["Haeyong Kang","Jaehong Yoon","Sung Ju Hwang","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2312.11973v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2303.14962,\n  arXiv:2306.11305"},{"id":"http://arxiv.org/abs/2312.11972v1","updated":"2023-12-19T09:09:46Z","published":"2023-12-19T09:09:46Z","title":"Expressive Forecasting of 3D Whole-body Human Motions","summary":"  Human motion forecasting, with the goal of estimating future human behavior\nover a period of time, is a fundamental task in many real-world applications.\nHowever, existing works typically concentrate on predicting the major joints of\nthe human body without considering the delicate movements of the human hands.\nIn practical applications, hand gesture plays an important role in human\ncommunication with the real world, and expresses the primary intention of human\nbeings. In this work, we are the first to formulate a whole-body human pose\nforecasting task, which jointly predicts the future body and hand activities.\nCorrespondingly, we propose a novel Encoding-Alignment-Interaction (EAI)\nframework that aims to predict both coarse (body joints) and fine-grained\n(gestures) activities collaboratively, enabling expressive and\ncross-facilitated forecasting of 3D whole-body human motions. Specifically, our\nmodel involves two key constituents: cross-context alignment (XCA) and\ncross-context interaction (XCI). Considering the heterogeneous information\nwithin the whole-body, XCA aims to align the latent features of various human\ncomponents, while XCI focuses on effectively capturing the context interaction\namong the human components. We conduct extensive experiments on a\nnewly-introduced large-scale benchmark and achieve state-of-the-art\nperformance. The code is public for research purposes at\nhttps://github.com/Dingpx/EAI.\n","authors":["Pengxiang Ding","Qiongjie Cui","Min Zhang","Mengyuan Liu","Haofan Wang","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2312.11972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11967v1","updated":"2023-12-19T09:03:53Z","published":"2023-12-19T09:03:53Z","title":"Context Disentangling and Prototype Inheriting for Robust Visual\n  Grounding","summary":"  Visual grounding (VG) aims to locate a specific target in an image based on a\ngiven language query. The discriminative information from context is important\nfor distinguishing the target from other objects, particularly for the targets\nthat have the same category as others. However, most previous methods\nunderestimate such information. Moreover, they are usually designed for the\nstandard scene (without any novel object), which limits their generalization to\nthe open-vocabulary scene. In this paper, we propose a novel framework with\ncontext disentangling and prototype inheriting for robust visual grounding to\nhandle both scenes. Specifically, the context disentangling disentangles the\nreferent and context features, which achieves better discrimination between\nthem. The prototype inheriting inherits the prototypes discovered from the\ndisentangled visual features by a prototype bank to fully utilize the seen\ndata, especially for the open-vocabulary scene. The fused features, obtained by\nleveraging Hadamard product on disentangled linguistic and visual features of\nprototypes to avoid sharp adjusting the importance between the two types of\nfeatures, are then attached with a special token and feed to a vision\nTransformer encoder for bounding box regression. Extensive experiments are\nconducted on both standard and open-vocabulary scenes. The performance\ncomparisons indicate that our method outperforms the state-of-the-art methods\nin both scenarios. {The code is available at\nhttps://github.com/WayneTomas/TransCP.\n","authors":["Wei Tang","Liang Li","Xuejing Liu","Lu Jin","Jinhui Tang","Zechao Li"],"pdf_url":"https://arxiv.org/pdf/2312.11967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11954v1","updated":"2023-12-19T08:55:00Z","published":"2023-12-19T08:55:00Z","title":"Adversarial AutoMixup","summary":"  Data mixing augmentation has been widely applied to improve the\ngeneralization ability of deep neural networks. Recently, offline data mixing\naugmentation, e.g. handcrafted and saliency information-based mixup, has been\ngradually replaced by automatic mixing approaches. Through minimizing two\nsub-tasks, namely, mixed sample generation and mixup classification in an\nend-to-end way, AutoMix significantly improves accuracy on image classification\ntasks. However, as the optimization objective is consistent for the two\nsub-tasks, this approach is prone to generating consistent instead of diverse\nmixed samples, which results in overfitting for target task training. In this\npaper, we propose AdAutomixup, an adversarial automatic mixup augmentation\napproach that generates challenging samples to train a robust classifier for\nimage classification, by alternatively optimizing the classifier and the mixup\nsample generator. AdAutomixup comprises two modules, a mixed example generator,\nand a target classifier. The mixed sample generator aims to produce hard mixed\nexamples to challenge the target classifier while the target classifier`s aim\nis to learn robust features from hard mixed examples to improve generalization.\nTo prevent the collapse of the inherent meanings of images, we further\nintroduce an exponential moving average (EMA) teacher and cosine similarity to\ntrain AdAutomixup in an end-to-end way. Extensive experiments on seven image\nbenchmarks consistently prove that our approach outperforms the state of the\nart in various classification scenarios.\n","authors":["Huafeng Qin","Xin Jin","Yun Jiang","Mounim A. El-Yacoubi","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2312.11954v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11938v1","updated":"2023-12-19T08:31:30Z","published":"2023-12-19T08:31:30Z","title":"DMT: Comprehensive Distillation with Multiple Self-supervised Teachers","summary":"  Numerous self-supervised learning paradigms, such as contrastive learning and\nmasked image modeling, have been proposed to acquire powerful and general\nrepresentations from unlabeled data. However, these models are commonly\npretrained within their specific framework alone, failing to consider the\ncomplementary nature of visual representations. To tackle this issue, we\nintroduce Comprehensive Distillation with Multiple Self-supervised Teachers\n(DMT) for pretrained model compression, which leverages the strengths of\nmultiple off-the-shelf self-supervised models. Our experimental results on\nprominent benchmark datasets exhibit that the proposed method significantly\nsurpasses state-of-the-art competitors while retaining favorable efficiency\nmetrics. On classification tasks, our DMT framework utilizing three different\nself-supervised ViT-Base teachers enhances the performance of both small/tiny\nmodels and the base model itself. For dense tasks, DMT elevates the AP/mIoU of\nstandard SSL models on MS-COCO and ADE20K datasets by 4.0%.\n","authors":["Yuang Liu","Jing Wang","Qiang Zhou","Fan Wang","Jun Wang","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.11938v1.pdf","comment":"ICASSP 2024"},{"id":"http://arxiv.org/abs/2309.02318v2","updated":"2023-12-19T08:20:43Z","published":"2023-09-05T15:34:37Z","title":"TiAVox: Time-aware Attenuation Voxels for Sparse-view 4D DSA\n  Reconstruction","summary":"  Four-dimensional Digital Subtraction Angiography (4D DSA) plays a critical\nrole in the diagnosis of many medical diseases, such as Arteriovenous\nMalformations (AVM) and Arteriovenous Fistulas (AVF). Despite its significant\napplication value, the reconstruction of 4D DSA demands numerous views to\neffectively model the intricate vessels and radiocontrast flow, thereby\nimplying a significant radiation dose. To address this high radiation issue, we\npropose a Time-aware Attenuation Voxel (TiAVox) approach for sparse-view 4D DSA\nreconstruction, which paves the way for high-quality 4D imaging. Additionally,\n2D and 3D DSA imaging results can be generated from the reconstructed 4D DSA\nimages. TiAVox introduces 4D attenuation voxel grids, which reflect attenuation\nproperties from both spatial and temporal dimensions. It is optimized by\nminimizing discrepancies between the rendered images and sparse 2D DSA images.\nWithout any neural network involved, TiAVox enjoys specific physical\ninterpretability. The parameters of each learnable voxel represent the\nattenuation coefficients. We validated the TiAVox approach on both clinical and\nsimulated datasets, achieving a 31.23 Peak Signal-to-Noise Ratio (PSNR) for\nnovel view synthesis using only 30 views on the clinically sourced dataset,\nwhereas traditional Feldkamp-Davis-Kress methods required 133 views. Similarly,\nwith merely 10 views from the synthetic dataset, TiAVox yielded a PSNR of 34.32\nfor novel view synthesis and 41.40 for 3D reconstruction. We also executed\nablation studies to corroborate the essential components of TiAVox. The code\nwill be publically available.\n","authors":["Zhenghong Zhou","Huangxuan Zhao","Jiemin Fang","Dongqiao Xiang","Lei Chen","Lingxia Wu","Feihong Wu","Wenyu Liu","Chuansheng Zheng","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2309.02318v2.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2312.07266v2","updated":"2023-12-19T08:18:47Z","published":"2023-12-12T13:45:56Z","title":"ProxyDet: Synthesizing Proxy Novel Classes via Classwise Mixup for\n  Open-Vocabulary Object Detection","summary":"  Open-vocabulary object detection (OVOD) aims to recognize novel objects whose\ncategories are not included in the training set. In order to classify these\nunseen classes during training, many OVOD frameworks leverage the zero-shot\ncapability of largely pretrained vision and language models, such as CLIP. To\nfurther improve generalization on the unseen novel classes, several approaches\nproposed to additionally train with pseudo region labeling on the external data\nsources that contain a substantial number of novel category labels beyond the\nexisting training data. Albeit its simplicity, these pseudo-labeling methods\nstill exhibit limited improvement with regard to the truly unseen novel classes\nthat were not pseudo-labeled. In this paper, we present a novel, yet simple\ntechnique that helps generalization on the overall distribution of novel\nclasses. Inspired by our observation that numerous novel classes reside within\nthe convex hull constructed by the base (seen) classes in the CLIP embedding\nspace, we propose to synthesize proxy-novel classes approximating novel classes\nvia linear mixup between a pair of base classes. By training our detector with\nthese synthetic proxy-novel classes, we effectively explore the embedding space\nof novel classes. The experimental results on various OVOD benchmarks such as\nLVIS and COCO demonstrate superior performance on novel classes compared to the\nother state-of-the-art methods. Code is available at\nhttps://github.com/clovaai/ProxyDet.\n","authors":["Joonhyun Jeong","Geondo Park","Jayeon Yoo","Hyungsik Jung","Heesu Kim"],"pdf_url":"https://arxiv.org/pdf/2312.07266v2.pdf","comment":"Accepted in AAAI24"},{"id":"http://arxiv.org/abs/2312.11929v1","updated":"2023-12-19T08:15:22Z","published":"2023-12-19T08:15:22Z","title":"Transformer Network for Multi-Person Tracking and Re-Identification in\n  Unconstrained Environment","summary":"  Multi-object tracking (MOT) has profound applications in a variety of fields,\nincluding surveillance, sports analytics, self-driving, and cooperative\nrobotics. Despite considerable advancements, existing MOT methodologies tend to\nfalter when faced with non-uniform movements, occlusions, and\nappearance-reappearance scenarios of the objects. Recognizing this inadequacy,\nwe put forward an integrated MOT method that not only marries object detection\nand identity linkage within a singular, end-to-end trainable framework but also\nequips the model with the ability to maintain object identity links over long\nperiods of time. Our proposed model, named STMMOT, is built around four key\nmodules: 1) candidate proposal generation, which generates object proposals via\na vision-transformer encoder-decoder architecture that detects the object from\neach frame in the video; 2) scale variant pyramid, a progressive pyramid\nstructure to learn the self-scale and cross-scale similarities in multi-scale\nfeature maps; 3) spatio-temporal memory encoder, extracting the essential\ninformation from the memory associated with each object under tracking; and 4)\nspatio-temporal memory decoder, simultaneously resolving the tasks of object\ndetection and identity association for MOT. Our system leverages a robust\nspatio-temporal memory module that retains extensive historical observations\nand effectively encodes them using an attention-based aggregator. The\nuniqueness of STMMOT lies in representing objects as dynamic query embeddings\nthat are updated continuously, which enables the prediction of object states\nwith attention mechanisms and eradicates the need for post-processing.\n","authors":["Hamza Mukhtar","Muhammad Usman Ghani Khan"],"pdf_url":"https://arxiv.org/pdf/2312.11929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11923v1","updated":"2023-12-19T08:03:19Z","published":"2023-12-19T08:03:19Z","title":"IPAD: Iterative, Parallel, and Diffusion-based Network for Scene Text\n  Recognition","summary":"  Nowadays, scene text recognition has attracted more and more attention due to\nits diverse applications. Most state-of-the-art methods adopt an\nencoder-decoder framework with the attention mechanism, autoregressively\ngenerating text from left to right. Despite the convincing performance, this\nsequential decoding strategy constrains inference speed. Conversely,\nnon-autoregressive models provide faster, simultaneous predictions but often\nsacrifice accuracy. Although utilizing an explicit language model can improve\nperformance, it burdens the computational load. Besides, separating linguistic\nknowledge from vision information may harm the final prediction. In this paper,\nwe propose an alternative solution, using a parallel and iterative decoder that\nadopts an easy-first decoding strategy. Furthermore, we regard text recognition\nas an image-based conditional text generation task and utilize the discrete\ndiffusion strategy, ensuring exhaustive exploration of bidirectional contextual\ninformation. Extensive experiments demonstrate that the proposed approach\nachieves superior results on the benchmark datasets, including both Chinese and\nEnglish text images.\n","authors":["Xiaomeng Yang","Zhi Qiao","Yu Zhou","Weiping Wang"],"pdf_url":"https://arxiv.org/pdf/2312.11923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.08288v2","updated":"2023-12-19T07:50:23Z","published":"2023-08-16T11:20:23Z","title":"Improving Audio-Visual Segmentation with Bidirectional Generation","summary":"  The aim of audio-visual segmentation (AVS) is to precisely differentiate\naudible objects within videos down to the pixel level. Traditional approaches\noften tackle this challenge by combining information from various modalities,\nwhere the contribution of each modality is implicitly or explicitly modeled.\nNevertheless, the interconnections between different modalities tend to be\noverlooked in audio-visual modeling. In this paper, inspired by the human\nability to mentally simulate the sound of an object and its visual appearance,\nwe introduce a bidirectional generation framework. This framework establishes\nrobust correlations between an object's visual characteristics and its\nassociated sound, thereby enhancing the performance of AVS. To achieve this, we\nemploy a visual-to-audio projection component that reconstructs audio features\nfrom object segmentation masks and minimizes reconstruction errors. Moreover,\nrecognizing that many sounds are linked to object movements, we introduce an\nimplicit volumetric motion estimation module to handle temporal dynamics that\nmay be challenging to capture using conventional optical flow methods. To\nshowcase the effectiveness of our approach, we conduct comprehensive\nexperiments and analyses on the widely recognized AVSBench benchmark. As a\nresult, we establish a new state-of-the-art performance level in the AVS\nbenchmark, particularly excelling in the challenging MS3 subset which involves\nsegmenting multiple sound sources. To facilitate reproducibility, we plan to\nrelease both the source code and the pre-trained model.\n","authors":["Dawei Hao","Yuxin Mao","Bowen He","Xiaodong Han","Yuchao Dai","Yiran Zhong"],"pdf_url":"https://arxiv.org/pdf/2308.08288v2.pdf","comment":"AAAI Camera Ready. Dawei Hao and Yuxin Mao contribute equality to\n  this paper. Yiran Zhong is the corresponding author. The code will be\n  released at https://github.com/OpenNLPLab/AVS-bidirectional"},{"id":"http://arxiv.org/abs/2312.10686v2","updated":"2023-12-19T07:49:07Z","published":"2023-12-17T11:11:02Z","title":"Out-of-Distribution Detection in Long-Tailed Recognition with Calibrated\n  Outlier Class Learning","summary":"  Existing out-of-distribution (OOD) methods have shown great success on\nbalanced datasets but become ineffective in long-tailed recognition (LTR)\nscenarios where 1) OOD samples are often wrongly classified into head classes\nand/or 2) tail-class samples are treated as OOD samples. To address these\nissues, current studies fit a prior distribution of auxiliary/pseudo OOD data\nto the long-tailed in-distribution (ID) data. However, it is difficult to\nobtain such an accurate prior distribution given the unknowingness of real OOD\nsamples and heavy class imbalance in LTR. A straightforward solution to avoid\nthe requirement of this prior is to learn an outlier class to encapsulate the\nOOD samples. The main challenge is then to tackle the aforementioned confusion\nbetween OOD samples and head/tail-class samples when learning the outlier\nclass. To this end, we introduce a novel calibrated outlier class learning\n(COCL) approach, in which 1) a debiased large margin learning method is\nintroduced in the outlier class learning to distinguish OOD samples from both\nhead and tail classes in the representation space and 2) an outlier-class-aware\nlogit calibration method is defined to enhance the long-tailed classification\nconfidence. Extensive empirical results on three popular benchmarks CIFAR10-LT,\nCIFAR100-LT, and ImageNet-LT demonstrate that COCL substantially outperforms\nstate-of-the-art OOD detection methods in LTR while being able to improve the\nclassification accuracy on ID data. Code is available at\nhttps://github.com/mala-lab/COCL.\n","authors":["Wenjun Miao","Guansong Pang","Tianqi Li","Xiao Bai","Jin Zheng"],"pdf_url":"https://arxiv.org/pdf/2312.10686v2.pdf","comment":"AAAI2024, with supplementary material"},{"id":"http://arxiv.org/abs/2303.10343v2","updated":"2023-12-19T07:44:31Z","published":"2023-03-18T06:13:30Z","title":"Supervision Interpolation via LossMix: Generalizing Mixup for Object\n  Detection and Beyond","summary":"  The success of data mixing augmentations in image classification tasks has\nbeen well-received. However, these techniques cannot be readily applied to\nobject detection due to challenges such as spatial misalignment,\nforeground/background distinction, and plurality of instances. To tackle these\nissues, we first introduce a novel conceptual framework called Supervision\nInterpolation (SI), which offers a fresh perspective on interpolation-based\naugmentations by relaxing and generalizing Mixup. Based on SI, we propose\nLossMix, a simple yet versatile and effective regularization that enhances the\nperformance and robustness of object detectors and more. Our key insight is\nthat we can effectively regularize the training on mixed data by interpolating\ntheir loss errors instead of ground truth labels. Empirical results on the\nPASCAL VOC and MS COCO datasets demonstrate that LossMix can consistently\noutperform state-of-the-art methods widely adopted for detection. Furthermore,\nby jointly leveraging LossMix with unsupervised domain adaptation, we\nsuccessfully improve existing approaches and set a new state of the art for\ncross-domain object detection.\n","authors":["Thanh Vu","Baochen Sun","Bodi Yuan","Alex Ngai","Yueqi Li","Jan-Michael Frahm"],"pdf_url":"https://arxiv.org/pdf/2303.10343v2.pdf","comment":"AAAI-24 Camera Ready Version, with supplementary material, 15 pages"},{"id":"http://arxiv.org/abs/2312.11911v1","updated":"2023-12-19T07:39:45Z","published":"2023-12-19T07:39:45Z","title":"EVI-SAM: Robust, Real-time, Tightly-coupled Event-Visual-Inertial State\n  Estimation and 3D Dense Mapping","summary":"  Event cameras are bio-inspired, motion-activated sensors that demonstrate\nsubstantial potential in handling challenging situations, such as motion blur\nand high-dynamic range. In this paper, we proposed EVI-SAM to tackle the\nproblem of 6 DoF pose tracking and 3D reconstruction using monocular event\ncamera. A novel event-based hybrid tracking framework is designed to estimate\nthe pose, leveraging the robustness of feature matching and the precision of\ndirect alignment. Specifically, we develop an event-based 2D-2D alignment to\nconstruct the photometric constraint, and tightly integrate it with the\nevent-based reprojection constraint. The mapping module recovers the dense and\ncolorful depth of the scene through the image-guided event-based mapping\nmethod. Subsequently, the appearance, texture, and surface mesh of the 3D scene\ncan be reconstructed by fusing the dense depth map from multiple viewpoints\nusing truncated signed distance function (TSDF) fusion. To the best of our\nknowledge, this is the first non-learning work to realize event-based dense\nmapping. Numerical evaluations are performed on both publicly available and\nself-collected datasets, which qualitatively and quantitatively demonstrate the\nsuperior performance of our method. Our EVI-SAM effectively balances accuracy\nand robustness while maintaining computational efficiency, showcasing superior\npose tracking and dense mapping performance in challenging scenarios. Video\nDemo: https://youtu.be/Nn40U4e5Si8.\n","authors":["Weipeng Guan","Peiyu Chen","Huibin Zhao","Yu Wang","Peng Lu"],"pdf_url":"https://arxiv.org/pdf/2312.11911v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06999v3","updated":"2023-12-19T07:30:25Z","published":"2023-03-13T10:54:52Z","title":"Identifying Label Errors in Object Detection Datasets by Loss Inspection","summary":"  Labeling datasets for supervised object detection is a dull and\ntime-consuming task. Errors can be easily introduced during annotation and\noverlooked during review, yielding inaccurate benchmarks and performance\ndegradation of deep neural networks trained on noisy labels. In this work, we\nfor the first time introduce a benchmark for label error detection methods on\nobject detection datasets as well as a label error detection method and a\nnumber of baselines. We simulate four different types of randomly introduced\nlabel errors on train and test sets of well-labeled object detection datasets.\nFor our label error detection method we assume a two-stage object detector to\nbe given and consider the sum of both stages' classification and regression\nlosses. The losses are computed with respect to the predictions and the noisy\nlabels including simulated label errors, aiming at detecting the latter. We\ncompare our method to three baselines: a naive one without deep learning, the\nobject detector's score and the entropy of the classification softmax\ndistribution. We outperform all baselines and demonstrate that among the\nconsidered methods, ours is the only one that detects label errors of all four\ntypes efficiently. Furthermore, we detect real label errors a) on commonly used\ntest datasets in object detection and b) on a proprietary dataset. In both\ncases we achieve low false positives rates, i.e., we detect label errors with a\nprecision for a) of up to 71.5% and for b) with 97%.\n","authors":["Marius Schubert","Tobias Riedlinger","Karsten Kahl","Daniel Kröll","Sebastian Schoenen","Siniša Šegvić","Matthias Rottmann"],"pdf_url":"https://arxiv.org/pdf/2303.06999v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08206v2","updated":"2023-12-19T07:21:03Z","published":"2023-10-12T10:51:23Z","title":"Long-Tailed Classification Based on Coarse-Grained Leading Forest and\n  Multi-Center Loss","summary":"  Long-tailed (LT) classification is an unavoidable and challenging problem in\nthe real world. Most existing long-tailed classification methods focus only on\nsolving the class-wise imbalance while ignoring the attribute-wise imbalance.\nThe deviation of a classification model is caused by both class-wise and\nattribute-wise imbalance. Due to the fact that attributes are implicit in most\ndatasets and the combination of attributes is complex, attribute-wise imbalance\nis more difficult to handle. For this purpose, we proposed a novel long-tailed\nclassification framework, aiming to build a multi-granularity classification\nmodel by means of invariant feature learning. This method first unsupervisedly\nconstructs Coarse-Grained forest (CLF) to better characterize the distribution\nof attributes within a class. Depending on the distribution of attributes, one\ncan customize suitable sampling strategies to construct different imbalanced\ndatasets. We then introduce multi-center loss (MCL) that aims to gradually\neliminate confusing attributes during feature learning process. The proposed\nframework does not necessarily couple to a specific LT classification model\nstructure and can be integrated with any existing LT method as an independent\ncomponent. Extensive experiments show that our approach achieves\nstate-of-the-art performance on both existing benchmarks ImageNet-GLT and\nMSCOCO-GLT and can improve the performance of existing LT methods. Our codes\nare available on GitHub: \\url{https://github.com/jinyery/cognisance}\n","authors":["Jinye Yang","Ji Xu","Di Wu","Jianhang Tang","Shaobo Li","Guoyin Wang"],"pdf_url":"https://arxiv.org/pdf/2310.08206v2.pdf","comment":"This is another research work to apply leading tree structure along\n  with deep learning architecture, aiming to deal with attribute-wise long-tail\n  distribution within class"},{"id":"http://arxiv.org/abs/2311.15570v2","updated":"2023-12-19T07:12:21Z","published":"2023-11-27T06:38:07Z","title":"UFDA: Universal Federated Domain Adaptation with Practical Assumptions","summary":"  Conventional Federated Domain Adaptation (FDA) approaches usually demand an\nabundance of assumptions, which makes them significantly less feasible for\nreal-world situations and introduces security hazards. This paper relaxes the\nassumptions from previous FDAs and studies a more practical scenario named\nUniversal Federated Domain Adaptation (UFDA). It only requires the black-box\nmodel and the label set information of each source domain, while the label sets\nof different source domains could be inconsistent, and the target-domain label\nset is totally blind. Towards a more effective solution for our newly proposed\nUFDA scenario, we propose a corresponding methodology called Hot-Learning with\nContrastive Label Disambiguation (HCLD). It particularly tackles UFDA's domain\nshifts and category gaps problems by using one-hot outputs from the black-box\nmodels of various source domains. Moreover, to better distinguish the shared\nand unknown classes, we further present a cluster-level strategy named\nMutual-Voting Decision (MVD) to extract robust consensus knowledge across peer\nclasses from both source and target domains. Extensive experiments on three\nbenchmark datasets demonstrate that our method achieves comparable performance\nfor our UFDA scenario with much fewer assumptions, compared to previous\nmethodologies with comprehensive additional assumptions.\n","authors":["Xinhui Liu","Zhenghao Chen","Luping Zhou","Dong Xu","Wei Xi","Gairui Bai","Yihan Zhao","Jizhong Zhao"],"pdf_url":"https://arxiv.org/pdf/2311.15570v2.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2312.04326v2","updated":"2023-12-19T06:50:21Z","published":"2023-12-07T14:37:01Z","title":"iDesigner: A High-Resolution and Complex-Prompt Following Text-to-Image\n  Diffusion Model for Interior Design","summary":"  With the open-sourcing of text-to-image models (T2I) such as stable diffusion\n(SD) and stable diffusion XL (SD-XL), there is an influx of models fine-tuned\nin specific domains based on the open-source SD model, such as in anime,\ncharacter portraits, etc. However, there are few specialized models in certain\ndomains, such as interior design, which is attributed to the complex textual\ndescriptions and detailed visual elements inherent in design, alongside the\nnecessity for adaptable resolution. Therefore, text-to-image models for\ninterior design are required to have outstanding prompt-following capabilities,\nas well as iterative collaboration with design professionals to achieve the\ndesired outcome. In this paper, we collect and optimize text-image data in the\ndesign field and continue training in both English and Chinese on the basis of\nthe open-source CLIP model. We also proposed a fine-tuning strategy with\ncurriculum learning and reinforcement learning from CLIP feedback to enhance\nthe prompt-following capabilities of our approach so as to improve the quality\nof image generation. The experimental results on the collected dataset\ndemonstrate the effectiveness of the proposed approach, which achieves\nimpressive results and outperforms strong baselines.\n","authors":["Ruyi Gan","Xiaojun Wu","Junyu Lu","Yuanhe Tian","Dixiang Zhang","Ziwei Wu","Renliang Sun","Chang Liu","Jiaxing Zhang","Pingjian Zhang","Yan Song"],"pdf_url":"https://arxiv.org/pdf/2312.04326v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11897v1","updated":"2023-12-19T06:42:47Z","published":"2023-12-19T06:42:47Z","title":"Text-Conditioned Resampler For Long Form Video Understanding","summary":"  Videos are highly redundant data source and it is often enough to identify a\nfew key moments to solve any given task. In this paper, we present a\ntext-conditioned video resampler (TCR) module that uses a pre-trained and\nfrozen visual encoder and large language model (LLM) to process long video\nsequences for a task. TCR localises relevant visual features from the video\ngiven a text condition and provides them to a LLM to generate a text response.\nDue to its lightweight design and use of cross-attention, TCR can process more\nthan 100 frames at a time allowing the model to use much longer chunks of video\nthan earlier works. We make the following contributions: (i) we design a\ntransformer-based sampling architecture that can process long videos\nconditioned on a task, together with a training method that enables it to\nbridge pre-trained visual and language models; (ii) we empirically validate its\nefficacy on a wide variety of evaluation tasks, and set a new state-of-the-art\non NextQA, EgoSchema, and the EGO4D-LTA challenge; and (iii) we determine tasks\nwhich require longer video contexts and that can thus be used effectively for\nfurther evaluation of long-range video models.\n","authors":["Bruno Korbar","Yongqin Xian","Alessio Tonioni","Andrew Zisserman","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2312.11897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11894v1","updated":"2023-12-19T06:38:18Z","published":"2023-12-19T06:38:18Z","title":"3D-LFM: Lifting Foundation Model","summary":"  The lifting of 3D structure and camera from 2D landmarks is at the\ncornerstone of the entire discipline of computer vision. Traditional methods\nhave been confined to specific rigid objects, such as those in\nPerspective-n-Point (PnP) problems, but deep learning has expanded our\ncapability to reconstruct a wide range of object classes (e.g. C3PDO and PAUL)\nwith resilience to noise, occlusions, and perspective distortions. All these\ntechniques, however, have been limited by the fundamental need to establish\ncorrespondences across the 3D training data -- significantly limiting their\nutility to applications where one has an abundance of \"in-correspondence\" 3D\ndata. Our approach harnesses the inherent permutation equivariance of\ntransformers to manage varying number of points per 3D data instance,\nwithstands occlusions, and generalizes to unseen categories. We demonstrate\nstate of the art performance across 2D-3D lifting task benchmarks. Since our\napproach can be trained across such a broad class of structures we refer to it\nsimply as a 3D Lifting Foundation Model (3D-LFM) -- the first of its kind.\n","authors":["Mosam Dabhi","Laszlo A. Jeni","Simon Lucey"],"pdf_url":"https://arxiv.org/pdf/2312.11894v1.pdf","comment":"Project page is available at https://3dlfm.github.io"},{"id":"http://arxiv.org/abs/2305.07490v4","updated":"2023-12-19T06:27:45Z","published":"2023-05-12T14:04:30Z","title":"ArtGPT-4: Towards Artistic-understanding Large Vision-Language Models\n  with Enhanced Adapter","summary":"  In recent years, advancements in large language models have been remarkable,\nwith models such as ChatGPT demonstrating exceptional proficiency in diverse\nlinguistic tasks. The pre-training of large models with billions of parameters,\nposes a formidable challenge, primarily due to the scarcity of datasets of a\ncommensurate scale for effective training. Nevertheless, innovative strategies\nhave emerged, including methods to fine-tune these pre-trained models using\nfewer parameters set, as evidenced by models like MiniGPT-4 and LLaVA. Despite\ntheir potential in various domains, these models remain limited in their\nunderstanding of artistic imagery. They have yet to fully grasp the intricate\nnuances of art images or to provide an objective articulation of the emotions\nthey evoke, in a manner akin to human perception. This work introduces\nArtGPT-4, a pioneering large vision-language model tailored to address the\ndeficiencies of contemporary models in artistic comprehension. ArtGPT-4\nunderwent training on image-text pairs utilizing a Tesla A100 device in a mere\n2 hours, with a dataset comprising approximately 0.52M entries. Impressively,\nthe model can render images with an artistic-understanding and convey the\nemotions they inspire, mirroring human interpretation. Additionally, this work\npresents a unique dataset designed to evaluate the efficacy of vision-language\nmodels. In subsequent evaluations, ArtGPT-4 not only achieved state-of-the-art\nperformance on the ArtEmis and ArtEmis-v2.0 datasets but also exceeded the\nestablished benchmarks introduced in This study, lagging behind professional\nartists' descriptions by a negligible 0.15 points on a 6-point scale. The code\nand the pre-trained model are accessible in\nhttps://huggingface.co/Tyrannosaurus/ArtGPT-4.\n","authors":["Zhengqing Yuan","Xinyi Wang","Kun Wang","Lichao Sun","Yanfang Ye"],"pdf_url":"https://arxiv.org/pdf/2305.07490v4.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2312.10737v2","updated":"2023-12-19T06:24:13Z","published":"2023-12-17T14:52:31Z","title":"Traffic Incident Database with Multiple Labels Including Various\n  Perspective Environmental Information","summary":"  A large dataset of annotated traffic accidents is necessary to improve the\naccuracy of traffic accident recognition using deep learning models.\nConventional traffic accident datasets provide annotations on traffic accidents\nand other teacher labels, improving traffic accident recognition performance.\nHowever, the labels annotated in conventional datasets need to be more\ncomprehensive to describe traffic accidents in detail. Therefore, we propose\nV-TIDB, a large-scale traffic accident recognition dataset annotated with\nvarious environmental information as multi-labels. Our proposed dataset aims to\nimprove the performance of traffic accident recognition by annotating ten types\nof environmental information as teacher labels in addition to the presence or\nabsence of traffic accidents. V-TIDB is constructed by collecting many videos\nfrom the Internet and annotating them with appropriate environmental\ninformation. In our experiments, we compare the performance of traffic accident\nrecognition when only labels related to the presence or absence of traffic\naccidents are trained and when environmental information is added as a\nmulti-label. In the second experiment, we compare the performance of the\ntraining with only contact level, which represents the severity of the traffic\naccident, and the performance with environmental information added as a\nmulti-label. The results showed that 6 out of 10 environmental information\nlabels improved the performance of recognizing the presence or absence of\ntraffic accidents. In the experiment on the degree of recognition of traffic\naccidents, the performance of recognition of car wrecks and contacts was\nimproved for all environmental information. These experiments show that V-TIDB\ncan be used to learn traffic accident recognition models that take\nenvironmental information into account in detail and can be used for\nappropriate traffic accident analysis.\n","authors":["Shota Nishiyama","Takuma Saito","Ryo Nakamura","Go Ohtani","Hirokatsu Kataoka","Kensho Hara"],"pdf_url":"https://arxiv.org/pdf/2312.10737v2.pdf","comment":"Conference paper accepted to IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS), 2023 Reason for revision: Corrected\n  due to a missing space between sentences in the preview's abstract, which led\n  to an unintended URL interpretation"},{"id":"http://arxiv.org/abs/2312.11880v1","updated":"2023-12-19T06:13:58Z","published":"2023-12-19T06:13:58Z","title":"Point Cloud Segmentation Using Transfer Learning with RandLA-Net: A Case\n  Study on Urban Areas","summary":"  Urban environments are characterized by complex structures and diverse\nfeatures, making accurate segmentation of point cloud data a challenging task.\nThis paper presents a comprehensive study on the application of RandLA-Net, a\nstate-of-the-art neural network architecture, for the 3D segmentation of\nlarge-scale point cloud data in urban areas. The study focuses on three major\nChinese cities, namely Chengdu, Jiaoda, and Shenzhen, leveraging their unique\ncharacteristics to enhance segmentation performance.\n  To address the limited availability of labeled data for these specific urban\nareas, we employed transfer learning techniques. We transferred the learned\nweights from the Sensat Urban and Toronto 3D datasets to initialize our\nRandLA-Net model. Additionally, we performed class remapping to adapt the model\nto the target urban areas, ensuring accurate segmentation results.\n  The experimental results demonstrate the effectiveness of the proposed\napproach achieving over 80\\% F1 score for each areas in 3D point cloud\nsegmentation. The transfer learning strategy proves to be crucial in overcoming\ndata scarcity issues, providing a robust solution for urban point cloud\nanalysis. The findings contribute to the advancement of point cloud\nsegmentation methods, especially in the context of rapidly evolving Chinese\nurban areas.\n","authors":["Alperen Enes Bayar","Ufuk Uyan","Elif Toprak","Cao Yuheng","Tang Juncheng","Ahmet Alp Kindiroglu"],"pdf_url":"https://arxiv.org/pdf/2312.11880v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.14197v4","updated":"2023-12-19T05:54:13Z","published":"2022-12-29T07:03:29Z","title":"PointVST: Self-Supervised Pre-training for 3D Point Clouds via\n  View-Specific Point-to-Image Translation","summary":"  The past few years have witnessed the great success and prevalence of\nself-supervised representation learning within the language and 2D vision\ncommunities. However, such advancements have not been fully migrated to the\nfield of 3D point cloud learning. Different from existing pre-training\nparadigms designed for deep point cloud feature extractors that fall into the\nscope of generative modeling or contrastive learning, this paper proposes a\ntranslative pre-training framework, namely PointVST, driven by a novel\nself-supervised pretext task of cross-modal translation from 3D point clouds to\ntheir corresponding diverse forms of 2D rendered images. More specifically, we\nbegin with deducing view-conditioned point-wise embeddings through the\ninsertion of the viewpoint indicator, and then adaptively aggregate a\nview-specific global codeword, which can be further fed into subsequent 2D\nconvolutional translation heads for image generation. Extensive experimental\nevaluations on various downstream task scenarios demonstrate that our PointVST\nshows consistent and prominent performance superiority over current\nstate-of-the-art approaches as well as satisfactory domain transfer capability.\nOur code will be publicly available at https://github.com/keeganhk/PointVST.\n","authors":["Qijian Zhang","Junhui Hou"],"pdf_url":"https://arxiv.org/pdf/2212.14197v4.pdf","comment":"Accepted in IEEE TVCG"},{"id":"http://arxiv.org/abs/2312.11872v1","updated":"2023-12-19T05:52:38Z","published":"2023-12-19T05:52:38Z","title":"Beyond Prototypes: Semantic Anchor Regularization for Better\n  Representation Learning","summary":"  One of the ultimate goals of representation learning is to achieve\ncompactness within a class and well-separability between classes. Many\noutstanding metric-based and prototype-based methods following the\nExpectation-Maximization paradigm, have been proposed for this objective.\nHowever, they inevitably introduce biases into the learning process,\nparticularly with long-tail distributed training data. In this paper, we reveal\nthat the class prototype is not necessarily to be derived from training\nfeatures and propose a novel perspective to use pre-defined class anchors\nserving as feature centroid to unidirectionally guide feature learning.\nHowever, the pre-defined anchors may have a large semantic distance from the\npixel features, which prevents them from being directly applied. To address\nthis issue and generate feature centroid independent from feature learning, a\nsimple yet effective Semantic Anchor Regularization (SAR) is proposed. SAR\nensures the interclass separability of semantic anchors in the semantic space\nby employing a classifier-aware auxiliary cross-entropy loss during training\nvia disentanglement learning. By pulling the learned features to these semantic\nanchors, several advantages can be attained: 1) the intra-class compactness and\nnaturally inter-class separability, 2) induced bias or errors from feature\nlearning can be avoided, and 3) robustness to the long-tailed problem. The\nproposed SAR can be used in a plug-and-play manner in the existing models.\nExtensive experiments demonstrate that the SAR performs better than previous\nsophisticated prototype-based methods. The implementation is available at\nhttps://github.com/geyanqi/SAR.\n","authors":["Yanqi Ge","Qiang Nie","Ye Huang","Yong Liu","Chengjie Wang","Feng Zheng","Wen Li","Lixin Duan"],"pdf_url":"https://arxiv.org/pdf/2312.11872v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11681v3","updated":"2023-12-19T05:51:18Z","published":"2023-03-21T08:43:15Z","title":"DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic\n  Segmentation Using Diffusion Models","summary":"  Collecting and annotating images with pixel-wise labels is time-consuming and\nlaborious. In contrast, synthetic data can be freely available using a\ngenerative model (e.g., DALL-E, Stable Diffusion). In this paper, we show that\nit is possible to automatically obtain accurate semantic masks of synthetic\nimages generated by the Off-the-shelf Stable Diffusion model, which uses only\ntext-image pairs during training. Our approach, called DiffuMask, exploits the\npotential of the cross-attention map between text and image, which is natural\nand seamless to extend the text-driven image synthesis to semantic mask\ngeneration. DiffuMask uses text-guided cross-attention information to localize\nclass/word-specific regions, which are combined with practical techniques to\ncreate a novel high-resolution and class-discriminative pixel-wise mask. The\nmethods help to reduce data collection and annotation costs obviously.\nExperiments demonstrate that the existing segmentation methods trained on\nsynthetic data of DiffuMask can achieve a competitive performance over the\ncounterpart of real data (VOC 2012, Cityscapes). For some classes (e.g., bird),\nDiffuMask presents promising performance, close to the stateof-the-art result\nof real data (within 3% mIoU gap). Moreover, in the open-vocabulary\nsegmentation (zero-shot) setting, DiffuMask achieves a new SOTA result on\nUnseen class of VOC 2012. The project website can be found at\nhttps://weijiawu.github.io/DiffusionMask/.\n","authors":["Weijia Wu","Yuzhong Zhao","Mike Zheng Shou","Hong Zhou","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2303.11681v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.10985v2","updated":"2023-12-19T05:51:09Z","published":"2021-12-21T05:07:54Z","title":"Learned ISTA with Error-based Thresholding for Adaptive Sparse Coding","summary":"  Drawing on theoretical insights, we advocate an error-based thresholding\n(EBT) mechanism for learned ISTA (LISTA), which utilizes a function of the\nlayer-wise reconstruction error to suggest a specific threshold for each\nobservation in the shrinkage function of each layer. We show that the proposed\nEBT mechanism well disentangles the learnable parameters in the shrinkage\nfunctions from the reconstruction errors, endowing the obtained models with\nimproved adaptivity to possible data variations. With rigorous analyses, we\nfurther show that the proposed EBT also leads to a faster convergence on the\nbasis of LISTA or its variants, in addition to its higher adaptivity. Extensive\nexperimental results confirm our theoretical analyses and verify the\neffectiveness of our methods.\n","authors":["Ziang Li","Kailun Wu","Yiwen Guo","Changshui Zhang"],"pdf_url":"https://arxiv.org/pdf/2112.10985v2.pdf","comment":"Accepted in ICASSP2024"},{"id":"http://arxiv.org/abs/2312.11867v1","updated":"2023-12-19T05:38:14Z","published":"2023-12-19T05:38:14Z","title":"Point Cloud Part Editing: Segmentation, Generation, Assembly, and\n  Selection","summary":"  Ideal part editing should guarantee the diversity of edited parts, the\nfidelity to the remaining parts, and the quality of the results. However,\nprevious methods do not disentangle each part completely, which means the\nedited parts will affect the others, resulting in poor diversity and fidelity.\nIn addition, some methods lack constraints between parts, which need manual\nselections of edited results to ensure quality. Therefore, we propose a\nfour-stage process for point cloud part editing: Segmentation, Generation,\nAssembly, and Selection. Based on this process, we introduce SGAS, a model for\npart editing that employs two strategies: feature disentanglement and\nconstraint. By independently fitting part-level feature distributions, we\nrealize the feature disentanglement. By explicitly modeling the transformation\nfrom object-level distribution to part-level distributions, we realize the\nfeature constraint. Considerable experiments on different datasets demonstrate\nthe efficiency and effectiveness of SGAS on point cloud part editing. In\naddition, SGAS can be pruned to realize unsupervised part-aware point cloud\ngeneration and achieves state-of-the-art results.\n","authors":["Kaiyi Zhang","Yang Chen","Ximing Yang","Weizhong Zhang","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2312.11867v1.pdf","comment":"9 pages, 7 figures, AAAI 2024"},{"id":"http://arxiv.org/abs/2303.14628v2","updated":"2023-12-19T05:28:13Z","published":"2023-03-26T05:26:30Z","title":"Multi-Frame Self-Supervised Depth Estimation with Multi-Scale Feature\n  Fusion in Dynamic Scenes","summary":"  Multi-frame methods improve monocular depth estimation over single-frame\napproaches by aggregating spatial-temporal information via feature matching.\nHowever, the spatial-temporal feature leads to accuracy degradation in dynamic\nscenes. To enhance the performance, recent methods tend to propose complex\narchitectures for feature matching and dynamic scenes. In this paper, we show\nthat a simple learning framework, together with designed feature augmentation,\nleads to superior performance. (1) A novel dynamic objects detecting method\nwith geometry explainability is proposed. The detected dynamic objects are\nexcluded during training, which guarantees the static environment assumption\nand relieves the accuracy degradation problem of the multi-frame depth\nestimation. (2) Multi-scale feature fusion is proposed for feature matching in\nthe multi-frame depth network, which improves feature matching, especially\nbetween frames with large camera motion. (3) The robust knowledge distillation\nwith a robust teacher network and reliability guarantee is proposed, which\nimproves the multi-frame depth estimation without computation complexity\nincrease during the test. The experiments show that our proposed methods\nachieve great performance improvement on the multi-frame depth estimation.\n","authors":["Jiquan Zhong","Xiaolin Huang","Xiao Yu"],"pdf_url":"https://arxiv.org/pdf/2303.14628v2.pdf","comment":"11 pages, 8 figures, ACM MM'23 accepted"},{"id":"http://arxiv.org/abs/2312.11862v1","updated":"2023-12-19T05:14:31Z","published":"2023-12-19T05:14:31Z","title":"Topo-MLP : A Simplicial Network Without Message Passing","summary":"  Due to their ability to model meaningful higher order relations among a set\nof entities, higher order network models have emerged recently as a powerful\nalternative for graph-based network models which are only capable of modeling\nbinary relationships. Message passing paradigm is still dominantly used to\nlearn representations even for higher order network models. While powerful,\nmessage passing can have disadvantages during inference, particularly when the\nhigher order connectivity information is missing or corrupted. To overcome such\nlimitations, we propose Topo-MLP, a purely MLP-based simplicial neural network\nalgorithm to learn the representation of elements in a simplicial complex\nwithout explicitly relying on message passing. Our framework utilizes a novel\nHigher Order Neighborhood Contrastive (HONC) loss which implicitly incorporates\nthe simplicial structure into representation learning. Our proposed model's\nsimplicity makes it faster during inference. Moreover, we show that our model\nis robust when faced with missing or corrupted connectivity structure.\n","authors":["Karthikeyan Natesan Ramamurthy","Aldo Guzmán-Sáenz","Mustafa Hajij"],"pdf_url":"https://arxiv.org/pdf/2312.11862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11376v2","updated":"2023-12-19T05:08:45Z","published":"2023-12-18T17:39:47Z","title":"CLIM: Contrastive Language-Image Mosaic for Region Representation","summary":"  Detecting objects accurately from a large or open vocabulary necessitates the\nvision-language alignment on region representations. However, learning such a\nregion-text alignment by obtaining high-quality box annotations with text\nlabels or descriptions is expensive and infeasible. In contrast, collecting\nimage-text pairs is simpler but lacks precise object location information to\nassociate regions with texts. In this paper, we propose a novel approach called\nContrastive Language-Image Mosaic (CLIM), which leverages large-scale\nimage-text pairs effectively for aligning region and text representations. CLIM\ncombines multiple images into a mosaicked image and treats each image as a\n`pseudo region'. The feature of each pseudo region is extracted and trained to\nbe similar to the corresponding text embedding while dissimilar from others by\na contrastive loss, enabling the model to learn the region-text alignment\nwithout costly box annotations. As a generally applicable approach, CLIM\nconsistently improves different open-vocabulary object detection methods that\nuse caption supervision. Furthermore, CLIM can effectively enhance the region\nrepresentation of vision-language models, thus providing stronger backbones for\nopen-vocabulary object detectors. Our experimental results demonstrate that\nCLIM improves different baseline open-vocabulary object detectors by a large\nmargin on both OV-COCO and OV-LVIS benchmarks. The code is available at\nhttps://github.com/wusize/CLIM.\n","authors":["Size Wu","Wenwei Zhang","Lumin Xu","Sheng Jin","Wentao Liu","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2312.11376v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11856v1","updated":"2023-12-19T04:55:33Z","published":"2023-12-19T04:55:33Z","title":"Self-supervised Learning for Enhancing Geometrical Modeling in 3D-Aware\n  Generative Adversarial Network","summary":"  3D-aware Generative Adversarial Networks (3D-GANs) currently exhibit\nartifacts in their 3D geometrical modeling, such as mesh imperfections and\nholes. These shortcomings are primarily attributed to the limited availability\nof annotated 3D data, leading to a constrained \"valid latent area\" for\nsatisfactory modeling. To address this, we present a Self-Supervised Learning\n(SSL) technique tailored as an auxiliary loss for any 3D-GAN, designed to\nimprove its 3D geometrical modeling capabilities. Our approach pioneers an\ninversion technique for 3D-GANs, integrating an encoder that performs adaptive\nspatially-varying range operations. Utilizing this inversion, we introduce the\nCyclic Generative Constraint (CGC), aiming to densify the valid latent space.\nThe CGC operates via augmented local latent vectors that maintain the same\ngeometric form, and it imposes constraints on the cycle path outputs,\nspecifically the generator-encoder-generator sequence. This SSL methodology\nseamlessly integrates with the inherent GAN loss, ensuring the integrity of\npre-existing 3D-GAN architectures without necessitating alterations. We\nvalidate our approach with comprehensive experiments across various datasets\nand architectures, underscoring its efficacy. Our project website:\nhttps://3dgan-ssl.github.io\n","authors":["Jiarong Guo","Xiaogang Xu","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2312.11856v1.pdf","comment":"13 pages, 12 figures, 6 tables"},{"id":"http://arxiv.org/abs/2305.10701v2","updated":"2023-12-19T04:41:50Z","published":"2023-05-18T04:28:47Z","title":"Personalization as a Shortcut for Few-Shot Backdoor Attack against\n  Text-to-Image Diffusion Models","summary":"  Although recent personalization methods have democratized high-resolution\nimage synthesis by enabling swift concept acquisition with minimal examples and\nlightweight computation, they also present an exploitable avenue for high\naccessible backdoor attacks. This paper investigates a critical and unexplored\naspect of text-to-image (T2I) diffusion models - their potential vulnerability\nto backdoor attacks via personalization. Our study focuses on a zero-day\nbackdoor vulnerability prevalent in two families of personalization methods,\nepitomized by Textual Inversion and DreamBooth.Compared to traditional backdoor\nattacks, our proposed method can facilitate more precise, efficient, and easily\naccessible attacks with a lower barrier to entry. We provide a comprehensive\nreview of personalization in T2I diffusion models, highlighting the operation\nand exploitation potential of this backdoor vulnerability. To be specific, by\nstudying the prompt processing of Textual Inversion and DreamBooth, we have\ndevised dedicated backdoor attacks according to the different ways of dealing\nwith unseen tokens and analyzed the influence of triggers and concept images on\nthe attack effect. Through comprehensive empirical study, we endorse the\nutilization of the nouveau-token backdoor attack due to its impressive\neffectiveness, stealthiness, and integrity, markedly outperforming the\nlegacy-token backdoor attack.\n","authors":["Yihao Huang","Felix Juefei-Xu","Qing Guo","Jie Zhang","Yutong Wu","Ming Hu","Tianlin Li","Geguang Pu","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2305.10701v2.pdf","comment":"10 pages, accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.11850v1","updated":"2023-12-19T04:35:24Z","published":"2023-12-19T04:35:24Z","title":"GCNext: Towards the Unity of Graph Convolutions for Human Motion\n  Prediction","summary":"  The past few years has witnessed the dominance of Graph Convolutional\nNetworks (GCNs) over human motion prediction.Various styles of graph\nconvolutions have been proposed, with each one meticulously designed and\nincorporated into a carefully-crafted network architecture. This paper breaks\nthe limits of existing knowledge by proposing Universal Graph Convolution\n(UniGC), a novel graph convolution concept that re-conceptualizes different\ngraph convolutions as its special cases. Leveraging UniGC on network-level, we\npropose GCNext, a novel GCN-building paradigm that dynamically determines the\nbest-fitting graph convolutions both sample-wise and layer-wise. GCNext offers\nmultiple use cases, including training a new GCN from scratch or refining a\npreexisting GCN. Experiments on Human3.6M, AMASS, and 3DPW datasets show that,\nby incorporating unique module-to-network designs, GCNext yields up to 9x lower\ncomputational cost than existing GCN methods, on top of achieving\nstate-of-the-art performance.\n","authors":["Xinshun Wang","Qiongjie Cui","Chen Chen","Mengyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2312.11850v1.pdf","comment":"to be published in the 38th Annual AAAI Conference on Artificial\n  Intelligence (AAAI-24)"},{"id":"http://arxiv.org/abs/2312.11849v1","updated":"2023-12-19T04:34:15Z","published":"2023-12-19T04:34:15Z","title":"Active contours driven by local and global intensity fitting energy with\n  application to SAR image segmentation and its fast solvers","summary":"  In this paper, we propose a novel variational active contour model based on\nAubert-Aujol (AA) denoising model, which hybrides geodesic active contour (GAC)\nmodel with active contours without edges (ACWE) model and can be used to\nsegment images corrupted by multiplicative gamma noise. We transform the\nproposed model into classic ROF model by adding a proximity term. Inspired by a\nfast denosing algorithm proposed by Jia-Zhao recently, we propose two fast\nfixed point algorithms to solve SAR image segmentation question. Experimental\nresults for real SAR images show that the proposed image segmentation model can\nefficiently stop the contours at weak or blurred edges, and can automatically\ndetect the exterior and interior boundaries of images with multiplicative gamma\nnoise. The proposed fast fixed point algorithms are robustness to\ninitialization contour, and can further reduce about 15% of the time needed for\nalgorithm proposed by Goldstein-Osher.\n","authors":["Guangming Liu","Qi Liu","Jing Liang","Quanying Sun"],"pdf_url":"https://arxiv.org/pdf/2312.11849v1.pdf","comment":"20 pages,28 figures. arXiv admin note: substantial text overlap with\n  arXiv:2312.08376, arXiv:2312.09365"},{"id":"http://arxiv.org/abs/2310.15646v4","updated":"2023-12-19T04:22:44Z","published":"2023-10-24T09:07:47Z","title":"Mean Teacher DETR with Masked Feature Alignment: A Robust Domain\n  Adaptive Detection Transformer Framework","summary":"  Unsupervised domain adaptation object detection (UDAOD) research on Detection\nTransformer(DETR) mainly focuses on feature alignment and existing methods can\nbe divided into two kinds, each of which has its unresolved issues. One-stage\nfeature alignment methods can easily lead to performance fluctuation and\ntraining stagnation. Two-stage feature alignment method based on mean teacher\ncomprises a pretraining stage followed by a self-training stage, each facing\nproblems in obtaining reliable pretrained model and achieving consistent\nperformance gains. Methods mentioned above have not yet explore how to utilize\nthe third related domain such as target-like domain to assist adaptation. To\naddress these issues, we propose a two-stage framework named MTM, i.e. Mean\nTeacher-DETR with Masked Feature Alignment. In the pretraining stage, we\nutilize labeled target-like images produced by image style transfer to avoid\nperformance fluctuation. In the self-training stage, we leverage unlabeled\ntarget images by pseudo labels based on mean teacher and propose a module\ncalled Object Queries Knowledge Transfer (OQKT) to ensure consistent\nperformance gains of the student model. Most importantly, we propose masked\nfeature alignment methods including Masked Domain Query-based Feature Alignment\n(MDQFA) and Masked Token-wise Feature Alignment (MTWFA) to alleviate domain\nshift in a more robust way, which not only prevent training stagnation and lead\nto a robust pretrained model in the pretraining stage, but also enhance the\nmodel's target performance in the self-training stage. Experiments on three\nchallenging scenarios and a theoretical analysis verify the effectiveness of\nMTM.\n","authors":["Weixi Weng","Chun Yuan"],"pdf_url":"https://arxiv.org/pdf/2310.15646v4.pdf","comment":"AAAI2024"},{"id":"http://arxiv.org/abs/2312.11841v1","updated":"2023-12-19T04:14:11Z","published":"2023-12-19T04:14:11Z","title":"MixRT: Mixed Neural Representations For Real-Time NeRF Rendering","summary":"  Neural Radiance Field (NeRF) has emerged as a leading technique for novel\nview synthesis, owing to its impressive photorealistic reconstruction and\nrendering capability. Nevertheless, achieving real-time NeRF rendering in\nlarge-scale scenes has presented challenges, often leading to the adoption of\neither intricate baked mesh representations with a substantial number of\ntriangles or resource-intensive ray marching in baked representations. We\nchallenge these conventions, observing that high-quality geometry, represented\nby meshes with substantial triangles, is not necessary for achieving\nphotorealistic rendering quality. Consequently, we propose MixRT, a novel NeRF\nrepresentation that includes a low-quality mesh, a view-dependent displacement\nmap, and a compressed NeRF model. This design effectively harnesses the\ncapabilities of existing graphics hardware, thus enabling real-time NeRF\nrendering on edge devices. Leveraging a highly-optimized WebGL-based rendering\nframework, our proposed MixRT attains real-time rendering speeds on edge\ndevices (over 30 FPS at a resolution of 1280 x 720 on a MacBook M1 Pro laptop),\nbetter rendering quality (0.2 PSNR higher in indoor scenes of the Unbounded-360\ndatasets), and a smaller storage size (less than 80% compared to\nstate-of-the-art methods).\n","authors":["Chaojian Li","Bichen Wu","Peter Vajda"," Yingyan"," Lin"],"pdf_url":"https://arxiv.org/pdf/2312.11841v1.pdf","comment":"Accepted by 3DV'24. Project Page: https://licj15.github.io/MixRT/"},{"id":"http://arxiv.org/abs/2312.11837v1","updated":"2023-12-19T04:09:05Z","published":"2023-12-19T04:09:05Z","title":"Regulating Intermediate 3D Features for Vision-Centric Autonomous\n  Driving","summary":"  Multi-camera perception tasks have gained significant attention in the field\nof autonomous driving. However, existing frameworks based on Lift-Splat-Shoot\n(LSS) in the multi-camera setting cannot produce suitable dense 3D features due\nto the projection nature and uncontrollable densification process. To resolve\nthis problem, we propose to regulate intermediate dense 3D features with the\nhelp of volume rendering. Specifically, we employ volume rendering to process\nthe dense 3D features to obtain corresponding 2D features (e.g., depth maps,\nsemantic maps), which are supervised by associated labels in the training. This\nmanner regulates the generation of dense 3D features on the feature level,\nproviding appropriate dense and unified features for multiple perception tasks.\nTherefore, our approach is termed Vampire, stands for \"Volume rendering As\nMulti-camera Perception Intermediate feature REgulator\". Experimental results\non the Occ3D and nuScenes datasets demonstrate that Vampire facilitates\nfine-grained and appropriate extraction of dense 3D features, and is\ncompetitive with existing SOTA methods across diverse downstream perception\ntasks like 3D occupancy prediction, LiDAR segmentation and 3D objection\ndetection, while utilizing moderate GPU resources. We provide a video\ndemonstration in the supplementary materials and Codes are available at\ngithub.com/cskkxjk/Vampire.\n","authors":["Junkai Xu","Liang Peng","Haoran Cheng","Linxuan Xia","Qi Zhou","Dan Deng","Wei Qian","Wenxiao Wang","Deng Cai"],"pdf_url":"https://arxiv.org/pdf/2312.11837v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.00634v2","updated":"2023-12-19T03:49:48Z","published":"2023-12-01T14:54:44Z","title":"A Recent Survey of Vision Transformers for Medical Image Segmentation","summary":"  Medical image segmentation plays a crucial role in various healthcare\napplications, enabling accurate diagnosis, treatment planning, and disease\nmonitoring. Traditionally, convolutional neural networks (CNNs) dominated this\ndomain, excelling at local feature extraction. However, their limitations in\ncapturing long-range dependencies across image regions pose challenges for\nsegmenting complex, interconnected structures often encountered in medical\ndata. In recent years, Vision Transformers (ViTs) have emerged as a promising\ntechnique for addressing the challenges in medical image segmentation. Their\nmulti-scale attention mechanism enables effective modeling of long-range\ndependencies between distant structures, crucial for segmenting organs or\nlesions spanning the image. Additionally, ViTs' ability to discern subtle\npattern heterogeneity allows for the precise delineation of intricate\nboundaries and edges, a critical aspect of accurate medical image segmentation.\nHowever, they do lack image-related inductive bias and translational\ninvariance, potentially impacting their performance. Recently, researchers have\ncome up with various ViT-based approaches that incorporate CNNs in their\narchitectures, known as Hybrid Vision Transformers (HVTs) to capture local\ncorrelation in addition to the global information in the images. This survey\npaper provides a detailed review of the recent advancements in ViTs and HVTs\nfor medical image segmentation. Along with the categorization of ViT and\nHVT-based medical image segmentation approaches, we also present a detailed\noverview of their real-time applications in several medical image modalities.\nThis survey may serve as a valuable resource for researchers, healthcare\npractitioners, and students in understanding the state-of-the-art approaches\nfor ViT-based medical image segmentation.\n","authors":["Asifullah Khan","Zunaira Rauf","Abdul Rehman Khan","Saima Rathore","Saddam Hussain Khan","Najmus Saher Shah","Umair Farooq","Hifsa Asif","Aqsa Asif","Umme Zahoora","Rafi Ullah Khalil","Suleman Qamar","Umme Hani Asif","Faiza Babar Khan","Abdul Majid","Jeonghwan Gwak"],"pdf_url":"https://arxiv.org/pdf/2312.00634v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07594v2","updated":"2023-12-19T03:44:25Z","published":"2023-11-10T09:51:24Z","title":"How to Bridge the Gap between Modalities: A Comprehensive Survey on\n  Multimodal Large Language Model","summary":"  This review paper explores Multimodal Large Language Models (MLLMs), which\nintegrate Large Language Models (LLMs) like GPT-4 to handle multimodal data\nsuch as text and vision. MLLMs demonstrate capabilities like generating image\nnarratives and answering image-based questions, bridging the gap towards\nreal-world human-computer interactions and hinting at a potential pathway to\nartificial general intelligence. However, MLLMs still face challenges in\nprocessing the semantic gap in multimodality, which may lead to erroneous\ngeneration, posing potential risks to society. Choosing the appropriate\nmodality alignment method is crucial, as improper methods might require more\nparameters with limited performance improvement. This paper aims to explore\nmodality alignment methods for LLMs and their existing capabilities.\nImplementing modality alignment allows LLMs to address environmental issues and\nenhance accessibility. The study surveys existing modal alignment methods in\nMLLMs into four groups: (1) Multimodal Converters that change data into\nsomething LLMs can understand; (2) Multimodal Perceivers to improve how LLMs\nperceive different types of data; (3) Tools Assistance for changing data into\none common format, usually text; and (4) Data-Driven methods that teach LLMs to\nunderstand specific types of data in a dataset. This field is still in a phase\nof exploration and experimentation, and we will organize and update various\nexisting research methods for multimodal information alignment.\n","authors":["Shezheng Song","Xiaopeng Li","Shasha Li","Shan Zhao","Jie Yu","Jun Ma","Xiaoguang Mao","Weimin Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.07594v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11829v1","updated":"2023-12-19T03:39:56Z","published":"2023-12-19T03:39:56Z","title":"RadOcc: Learning Cross-Modality Occupancy Knowledge through Rendering\n  Assisted Distillation","summary":"  3D occupancy prediction is an emerging task that aims to estimate the\noccupancy states and semantics of 3D scenes using multi-view images. However,\nimage-based scene perception encounters significant challenges in achieving\naccurate prediction due to the absence of geometric priors. In this paper, we\naddress this issue by exploring cross-modal knowledge distillation in this\ntask, i.e., we leverage a stronger multi-modal model to guide the visual model\nduring training. In practice, we observe that directly applying features or\nlogits alignment, proposed and widely used in bird's-eyeview (BEV) perception,\ndoes not yield satisfactory results. To overcome this problem, we introduce\nRadOcc, a Rendering assisted distillation paradigm for 3D Occupancy prediction.\nBy employing differentiable volume rendering, we generate depth and semantic\nmaps in perspective views and propose two novel consistency criteria between\nthe rendered outputs of teacher and student models. Specifically, the depth\nconsistency loss aligns the termination distributions of the rendered rays,\nwhile the semantic consistency loss mimics the intra-segment similarity guided\nby vision foundation models (VLMs). Experimental results on the nuScenes\ndataset demonstrate the effectiveness of our proposed method in improving\nvarious 3D occupancy prediction approaches, e.g., our proposed methodology\nenhances our baseline by 2.2% in the metric of mIoU and achieves 50% in Occ3D\nbenchmark.\n","authors":["Haiming Zhang","Xu Yan","Dongfeng Bai","Jiantao Gao","Pan Wang","Bingbing Liu","Shuguang Cui","Zhen Li"],"pdf_url":"https://arxiv.org/pdf/2312.11829v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2310.04780v5","updated":"2023-12-19T03:39:00Z","published":"2023-10-07T11:45:33Z","title":"IPMix: Label-Preserving Data Augmentation Method for Training Robust\n  Classifiers","summary":"  Data augmentation has been proven effective for training high-accuracy\nconvolutional neural network classifiers by preventing overfitting. However,\nbuilding deep neural networks in real-world scenarios requires not only high\naccuracy on clean data but also robustness when data distributions shift. While\nprior methods have proposed that there is a trade-off between accuracy and\nrobustness, we propose IPMix, a simple data augmentation approach to improve\nrobustness without hurting clean accuracy. IPMix integrates three levels of\ndata augmentation (image-level, patch-level, and pixel-level) into a coherent\nand label-preserving technique to increase the diversity of training data with\nlimited computational overhead. To further improve the robustness, IPMix\nintroduces structural complexity at different levels to generate more diverse\nimages and adopts the random mixing method for multi-scale information fusion.\nExperiments demonstrate that IPMix outperforms state-of-the-art corruption\nrobustness on CIFAR-C and ImageNet-C. In addition, we show that IPMix also\nsignificantly improves the other safety measures, including robustness to\nadversarial perturbations, calibration, prediction consistency, and anomaly\ndetection, achieving state-of-the-art or comparable results on several\nbenchmarks, including ImageNet-R, ImageNet-A, and ImageNet-O.\n","authors":["Zhenglin Huang","Xianan Bao","Na Zhang","Qingqi Zhang","Xiaomei Tu","Biao Wu","Xi Yang"],"pdf_url":"https://arxiv.org/pdf/2310.04780v5.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2312.11826v1","updated":"2023-12-19T03:32:10Z","published":"2023-12-19T03:32:10Z","title":"Decoupled Textual Embeddings for Customized Image Generation","summary":"  Customized text-to-image generation, which aims to learn user-specified\nconcepts with a few images, has drawn significant attention recently. However,\nexisting methods usually suffer from overfitting issues and entangle the\nsubject-unrelated information (e.g., background and pose) with the learned\nconcept, limiting the potential to compose concept into new scenes. To address\nthese issues, we propose the DETEX, a novel approach that learns the\ndisentangled concept embedding for flexible customized text-to-image\ngeneration. Unlike conventional methods that learn a single concept embedding\nfrom the given images, our DETEX represents each image using multiple word\nembeddings during training, i.e., a learnable image-shared subject embedding\nand several image-specific subject-unrelated embeddings. To decouple irrelevant\nattributes (i.e., background and pose) from the subject embedding, we further\npresent several attribute mappers that encode each image as several\nimage-specific subject-unrelated embeddings. To encourage these unrelated\nembeddings to capture the irrelevant information, we incorporate them with\ncorresponding attribute words and propose a joint training strategy to\nfacilitate the disentanglement. During inference, we only use the subject\nembedding for image generation, while selectively using image-specific\nembeddings to retain image-specified attributes. Extensive experiments\ndemonstrate that the subject embedding obtained by our method can faithfully\nrepresent the target concept, while showing superior editability compared to\nthe state-of-the-art methods. Our code will be made published available.\n","authors":["Yufei Cai","Yuxiang Wei","Zhilong Ji","Jinfeng Bai","Hu Han","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2312.11826v1.pdf","comment":"16 pages, 16 figures"},{"id":"http://arxiv.org/abs/2312.11816v1","updated":"2023-12-19T03:15:50Z","published":"2023-12-19T03:15:50Z","title":"A Dual-way Enhanced Framework from Text Matching Point of View for\n  Multimodal Entity Linking","summary":"  Multimodal Entity Linking (MEL) aims at linking ambiguous mentions with\nmultimodal information to entity in Knowledge Graph (KG) such as Wikipedia,\nwhich plays a key role in many applications. However, existing methods suffer\nfrom shortcomings, including modality impurity such as noise in raw image and\nambiguous textual entity representation, which puts obstacles to MEL. We\nformulate multimodal entity linking as a neural text matching problem where\neach multimodal information (text and image) is treated as a query, and the\nmodel learns the mapping from each query to the relevant entity from candidate\nentities. This paper introduces a dual-way enhanced (DWE) framework for MEL:\n(1) our model refines queries with multimodal data and addresses semantic gaps\nusing cross-modal enhancers between text and image information. Besides, DWE\ninnovatively leverages fine-grained image attributes, including facial\ncharacteristic and scene feature, to enhance and refine visual features. (2)By\nusing Wikipedia descriptions, DWE enriches entity semantics and obtains more\ncomprehensive textual representation, which reduces between textual\nrepresentation and the entities in KG. Extensive experiments on three public\nbenchmarks demonstrate that our method achieves state-of-the-art (SOTA)\nperformance, indicating the superiority of our model. The code is released on\nhttps://github.com/season1blue/DWE\n","authors":["Shezheng Song","Shan Zhao","Chengyu Wang","Tianwei Yan","Shasha Li","Xiaoguang Mao","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2312.11816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10422v2","updated":"2023-12-19T03:12:59Z","published":"2023-12-16T11:31:34Z","title":"Learning Dense Correspondence for NeRF-Based Face Reenactment","summary":"  Face reenactment is challenging due to the need to establish dense\ncorrespondence between various face representations for motion transfer. Recent\nstudies have utilized Neural Radiance Field (NeRF) as fundamental\nrepresentation, which further enhanced the performance of multi-view face\nreenactment in photo-realism and 3D consistency. However, establishing dense\ncorrespondence between different face NeRFs is non-trivial, because implicit\nrepresentations lack ground-truth correspondence annotations like mesh-based 3D\nparametric models (e.g., 3DMM) with index-aligned vertexes. Although aligning\n3DMM space with NeRF-based face representations can realize motion control, it\nis sub-optimal for their limited face-only modeling and low identity fidelity.\nTherefore, we are inspired to ask: Can we learn the dense correspondence\nbetween different NeRF-based face representations without a 3D parametric model\nprior? To address this challenge, we propose a novel framework, which adopts\ntri-planes as fundamental NeRF representation and decomposes face tri-planes\ninto three components: canonical tri-planes, identity deformations, and motion.\nIn terms of motion control, our key contribution is proposing a Plane\nDictionary (PlaneDict) module, which efficiently maps the motion conditions to\na linear weighted addition of learnable orthogonal plane bases. To the best of\nour knowledge, our framework is the first method that achieves one-shot\nmulti-view face reenactment without a 3D parametric model prior. Extensive\nexperiments demonstrate that we produce better results in fine-grained motion\ncontrol and identity preservation than previous methods.\n","authors":["Songlin Yang","Wei Wang","Yushi Lan","Xiangyu Fan","Bo Peng","Lei Yang","Jing Dong"],"pdf_url":"https://arxiv.org/pdf/2312.10422v2.pdf","comment":"Accepted by Proceedings of the AAAI Conference on Artificial\n  Intelligence, 2024"},{"id":"http://arxiv.org/abs/2312.11812v1","updated":"2023-12-19T03:01:31Z","published":"2023-12-19T03:01:31Z","title":"Advancements and Challenges in Arabic Optical Character Recognition: A\n  Comprehensive Survey","summary":"  Optical character recognition (OCR) is a vital process that involves the\nextraction of handwritten or printed text from scanned or printed images,\nconverting it into a format that can be understood and processed by machines.\nThis enables further data processing activities such as searching and editing.\nThe automatic extraction of text through OCR plays a crucial role in digitizing\ndocuments, enhancing productivity, improving accessibility, and preserving\nhistorical records. This paper seeks to offer an exhaustive review of\ncontemporary applications, methodologies, and challenges associated with Arabic\nOptical Character Recognition (OCR). A thorough analysis is conducted on\nprevailing techniques utilized throughout the OCR process, with a dedicated\neffort to discern the most efficacious approaches that demonstrate enhanced\noutcomes. To ensure a thorough evaluation, a meticulous keyword-search\nmethodology is adopted, encompassing a comprehensive analysis of articles\nrelevant to Arabic OCR, including both backward and forward citation reviews.\nIn addition to presenting cutting-edge techniques and methods, this paper\ncritically identifies research gaps within the realm of Arabic OCR. By\nhighlighting these gaps, we shed light on potential areas for future\nexploration and development, thereby guiding researchers toward promising\navenues in the field of Arabic OCR. The outcomes of this study provide valuable\ninsights for researchers, practitioners, and stakeholders involved in Arabic\nOCR, ultimately fostering advancements in the field and facilitating the\ncreation of more accurate and efficient OCR systems for the Arabic language.\n","authors":["Mahmoud SalahEldin Kasem","Mohamed Mahmoud","Hyun-Soo Kang"],"pdf_url":"https://arxiv.org/pdf/2312.11812v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07021v2","updated":"2023-12-19T02:46:50Z","published":"2023-12-12T07:15:17Z","title":"Transferring Modality-Aware Pedestrian Attentive Learning for\n  Visible-Infrared Person Re-identification","summary":"  Visible-infrared person re-identification (VI-ReID) aims to search the same\npedestrian of interest across visible and infrared modalities. Existing models\nmainly focus on compensating for modality-specific information to reduce\nmodality variation. However, these methods often lead to a higher computational\noverhead and may introduce interfering information when generating the\ncorresponding images or features. To address this issue, it is critical to\nleverage pedestrian-attentive features and learn modality-complete and\n-consistent representation. In this paper, a novel Transferring Modality-Aware\nPedestrian Attentive Learning (TMPA) model is proposed, focusing on the\npedestrian regions to efficiently compensate for missing modality-specific\nfeatures. Specifically, we propose a region-based data augmentation module\nPedMix to enhance pedestrian region coherence by mixing the corresponding\nregions from different modalities. A lightweight hybrid compensation module,\ni.e., the Modality Feature Transfer (MFT), is devised to integrate cross\nattention and convolution networks to fully explore the discriminative\nmodality-complete features with minimal computational overhead. Extensive\nexperiments conducted on the benchmark SYSU-MM01 and RegDB datasets\ndemonstrated the effectiveness of our proposed TMPA model.\n","authors":["Yuwei Guo","Wenhao Zhang","Licheng Jiao","Shuang Wang","Shuo Wang","Fang Liu"],"pdf_url":"https://arxiv.org/pdf/2312.07021v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11805v1","updated":"2023-12-19T02:39:27Z","published":"2023-12-19T02:39:27Z","title":"Gemini: A Family of Highly Capable Multimodal Models","summary":"  This report introduces a new family of multimodal models, Gemini, that\nexhibit remarkable capabilities across image, audio, video, and text\nunderstanding. The Gemini family consists of Ultra, Pro, and Nano sizes,\nsuitable for applications ranging from complex reasoning tasks to on-device\nmemory-constrained use-cases. Evaluation on a broad range of benchmarks shows\nthat our most-capable Gemini Ultra model advances the state of the art in 30 of\n32 of these benchmarks - notably being the first model to achieve human-expert\nperformance on the well-studied exam benchmark MMLU, and improving the state of\nthe art in every one of the 20 multimodal benchmarks we examined. We believe\nthat the new capabilities of Gemini models in cross-modal reasoning and\nlanguage understanding will enable a wide variety of use cases and we discuss\nour approach toward deploying them responsibly to users.\n","authors":[" Gemini Team","Rohan Anil","Sebastian Borgeaud","Yonghui Wu","Jean-Baptiste Alayrac","Jiahui Yu","Radu Soricut","Johan Schalkwyk","Andrew M. Dai","Anja Hauth","Katie Millican","David Silver","Slav Petrov","Melvin Johnson","Ioannis Antonoglou","Julian Schrittwieser","Amelia Glaese","Jilin Chen","Emily Pitler","Timothy Lillicrap","Angeliki Lazaridou","Orhan Firat","James Molloy","Michael Isard","Paul R. Barham","Tom Hennigan","Benjamin Lee","Fabio Viola","Malcolm Reynolds","Yuanzhong Xu","Ryan Doherty","Eli Collins","Clemens Meyer","Eliza Rutherford","Erica Moreira","Kareem Ayoub","Megha Goel","George Tucker","Enrique Piqueras","Maxim Krikun","Iain Barr","Nikolay Savinov","Ivo Danihelka","Becca Roelofs","Anaïs White","Anders Andreassen","Tamara von Glehn","Lakshman Yagati","Mehran Kazemi","Lucas Gonzalez","Misha Khalman","Jakub Sygnowski","Alexandre Frechette","Charlotte Smith","Laura Culp","Lev Proleev","Yi Luan","Xi Chen","James Lottes","Nathan Schucher","Federico Lebron","Alban Rrustemi","Natalie Clay","Phil Crone","Tomas Kocisky","Jeffrey Zhao","Bartek Perz","Dian Yu","Heidi Howard","Adam Bloniarz","Jack W. Rae","Han Lu","Laurent Sifre","Marcello Maggioni","Fred Alcober","Dan Garrette","Megan Barnes","Shantanu Thakoor","Jacob Austin","Gabriel Barth-Maron","William Wong","Rishabh Joshi","Rahma Chaabouni","Deeni Fatiha","Arun Ahuja","Ruibo Liu","Yunxuan Li","Sarah Cogan","Jeremy Chen","Chao Jia","Chenjie Gu","Qiao Zhang","Jordan Grimstad","Ale Jakse Hartman","Martin Chadwick","Gaurav Singh Tomar","Xavier Garcia","Evan Senter","Emanuel Taropa","Thanumalayan Sankaranarayana Pillai","Jacob Devlin","Michael Laskin","Diego de Las Casas","Dasha Valter","Connie Tao","Lorenzo Blanco","Adrià Puigdomènech Badia","David Reitter","Mianna Chen","Jenny Brennan","Clara Rivera","Sergey Brin","Shariq Iqbal","Gabriela Surita","Jane Labanowski","Abhi Rao","Stephanie Winkler","Emilio Parisotto","Yiming Gu","Kate Olszewska","Yujing Zhang","Ravi Addanki","Antoine Miech","Annie Louis","Laurent El Shafey","Denis Teplyashin","Geoff Brown","Elliot Catt","Nithya Attaluri","Jan Balaguer","Jackie Xiang","Pidong Wang","Zoe Ashwood","Anton Briukhov","Albert Webson","Sanjay Ganapathy","Smit Sanghavi","Ajay Kannan","Ming-Wei Chang","Axel Stjerngren","Josip Djolonga","Yuting Sun","Ankur Bapna","Matthew Aitchison","Pedram Pejman","Henryk Michalewski","Tianhe Yu","Cindy Wang","Juliette Love","Junwhan Ahn","Dawn Bloxwich","Kehang Han","Peter Humphreys","Thibault Sellam","James Bradbury","Varun Godbole","Sina Samangooei","Bogdan Damoc","Alex Kaskasoli","Sébastien M. R. Arnold","Vijay Vasudevan","Shubham Agrawal","Jason Riesa","Dmitry Lepikhin","Richard Tanburn","Srivatsan Srinivasan","Hyeontaek Lim","Sarah Hodkinson","Pranav Shyam","Johan Ferret","Steven Hand","Ankush Garg","Tom Le Paine","Jian Li","Yujia Li","Minh Giang","Alexander Neitz","Zaheer Abbas","Sarah York","Machel Reid","Elizabeth Cole","Aakanksha Chowdhery","Dipanjan Das","Dominika Rogozińska","Vitaly Nikolaev","Pablo Sprechmann","Zachary Nado","Lukas Zilka","Flavien Prost","Luheng He","Marianne Monteiro","Gaurav Mishra","Chris Welty","Josh Newlan","Dawei Jia","Miltiadis Allamanis","Clara Huiyi Hu","Raoul de Liedekerke","Justin Gilmer","Carl Saroufim","Shruti Rijhwani","Shaobo Hou","Disha Shrivastava","Anirudh Baddepudi","Alex Goldin","Adnan Ozturel","Albin Cassirer","Yunhan Xu","Daniel Sohn","Devendra Sachan","Reinald Kim Amplayo","Craig Swanson","Dessie Petrova","Shashi Narayan","Arthur Guez","Siddhartha Brahma","Jessica Landon","Miteyan Patel","Ruizhe Zhao","Kevin Villela","Luyu Wang","Wenhao Jia","Matthew Rahtz","Mai Giménez","Legg Yeung","Hanzhao Lin","James Keeling","Petko Georgiev","Diana Mincu","Boxi Wu","Salem Haykal","Rachel Saputro","Kiran Vodrahalli","James Qin","Zeynep Cankara","Abhanshu Sharma","Nick Fernando","Will Hawkins","Behnam Neyshabur","Solomon Kim","Adrian Hutter","Priyanka Agrawal","Alex Castro-Ros","George van den Driessche","Tao Wang","Fan Yang","Shuo-yiin Chang","Paul Komarek","Ross McIlroy","Mario Lučić","Guodong Zhang","Wael Farhan","Michael Sharman","Paul Natsev","Paul Michel","Yong Cheng","Yamini Bansal","Siyuan Qiao","Kris Cao","Siamak Shakeri","Christina Butterfield","Justin Chung","Paul Kishan Rubenstein","Shivani Agrawal","Arthur Mensch","Kedar Soparkar","Karel Lenc","Timothy Chung","Aedan Pope","Loren Maggiore","Jackie Kay","Priya Jhakra","Shibo Wang","Joshua Maynez","Mary Phuong","Taylor Tobin","Andrea Tacchetti","Maja Trebacz","Kevin Robinson","Yash Katariya","Sebastian Riedel","Paige Bailey","Kefan Xiao","Nimesh Ghelani","Lora Aroyo","Ambrose Slone","Neil Houlsby","Xuehan Xiong","Zhen Yang","Elena Gribovskaya","Jonas Adler","Mateo Wirth","Lisa Lee","Music Li","Thais Kagohara","Jay Pavagadhi","Sophie Bridgers","Anna Bortsova","Sanjay Ghemawat","Zafarali Ahmed","Tianqi Liu","Richard Powell","Vijay Bolina","Mariko Iinuma","Polina Zablotskaia","James Besley","Da-Woon Chung","Timothy Dozat","Ramona Comanescu","Xiance Si","Jeremy Greer","Guolong Su","Martin Polacek","Raphaël Lopez Kaufman","Simon Tokumine","Hexiang Hu","Elena Buchatskaya","Yingjie Miao","Mohamed Elhawaty","Aditya Siddhant","Nenad Tomasev","Jinwei Xing","Christina Greer","Helen Miller","Shereen Ashraf","Aurko Roy","Zizhao Zhang","Ada Ma","Angelos Filos","Milos Besta","Rory Blevins","Ted Klimenko","Chih-Kuan Yeh","Soravit Changpinyo","Jiaqi Mu","Oscar Chang","Mantas Pajarskas","Carrie Muir","Vered Cohen","Charline Le Lan","Krishna Haridasan","Amit Marathe","Steven Hansen","Sholto Douglas","Rajkumar Samuel","Mingqiu Wang","Sophia Austin","Chang Lan","Jiepu Jiang","Justin Chiu","Jaime Alonso Lorenzo","Lars Lowe Sjösund","Sébastien Cevey","Zach Gleicher","Thi Avrahami","Anudhyan Boral","Hansa Srinivasan","Vittorio Selo","Rhys May","Konstantinos Aisopos","Léonard Hussenot","Livio Baldini Soares","Kate Baumli","Michael B. Chang","Adrià Recasens","Ben Caine","Alexander Pritzel","Filip Pavetic","Fabio Pardo","Anita Gergely","Justin Frye","Vinay Ramasesh","Dan Horgan","Kartikeya Badola","Nora Kassner","Subhrajit Roy","Ethan Dyer","Víctor Campos","Alex Tomala","Yunhao Tang","Dalia El Badawy","Elspeth White","Basil Mustafa","Oran Lang","Abhishek Jindal","Sharad Vikram","Zhitao Gong","Sergi Caelles","Ross Hemsley","Gregory Thornton","Fangxiaoyu Feng","Wojciech Stokowiec","Ce Zheng","Phoebe Thacker","Çağlar Ünlü","Zhishuai Zhang","Mohammad Saleh","James Svensson","Max Bileschi","Piyush Patil","Ankesh Anand","Roman Ring","Katerina Tsihlas","Arpi Vezer","Marco Selvi","Toby Shevlane","Mikel Rodriguez","Tom Kwiatkowski","Samira Daruki","Keran Rong","Allan Dafoe","Nicholas FitzGerald","Keren Gu-Lemberg","Mina Khan","Lisa Anne Hendricks","Marie Pellat","Vladimir Feinberg","James Cobon-Kerr","Tara Sainath","Maribeth Rauh","Sayed Hadi Hashemi","Richard Ives","Yana Hasson","YaGuang Li","Eric Noland","Yuan Cao","Nathan Byrd","Le Hou","Qingze Wang","Thibault Sottiaux","Michela Paganini","Jean-Baptiste Lespiau","Alexandre Moufarek","Samer Hassan","Kaushik Shivakumar","Joost van Amersfoort","Amol Mandhane","Pratik Joshi","Anirudh Goyal","Matthew Tung","Andrew Brock","Hannah Sheahan","Vedant Misra","Cheng Li","Nemanja Rakićević","Mostafa Dehghani","Fangyu Liu","Sid Mittal","Junhyuk Oh","Seb Noury","Eren Sezener","Fantine Huot","Matthew Lamm","Nicola De Cao","Charlie Chen","Gamaleldin Elsayed","Ed Chi","Mahdis Mahdieh","Ian Tenney","Nan Hua","Ivan Petrychenko","Patrick Kane","Dylan Scandinaro","Rishub Jain","Jonathan Uesato","Romina Datta","Adam Sadovsky","Oskar Bunyan","Dominik Rabiej","Shimu Wu","John Zhang","Gautam Vasudevan","Edouard Leurent","Mahmoud Alnahlawi","Ionut Georgescu","Nan Wei","Ivy Zheng","Betty Chan","Pam G Rabinovitch","Piotr Stanczyk","Ye Zhang","David Steiner","Subhajit Naskar","Michael Azzam","Matthew Johnson","Adam Paszke","Chung-Cheng Chiu","Jaume Sanchez Elias","Afroz Mohiuddin","Faizan Muhammad","Jin Miao","Andrew Lee","Nino Vieillard","Sahitya Potluri","Jane Park","Elnaz Davoodi","Jiageng Zhang","Jeff Stanway","Drew Garmon","Abhijit Karmarkar","Zhe Dong","Jong Lee","Aviral Kumar","Luowei Zhou","Jonathan Evens","William Isaac","Zhe Chen","Johnson Jia","Anselm Levskaya","Zhenkai Zhu","Chris Gorgolewski","Peter Grabowski","Yu Mao","Alberto Magni","Kaisheng Yao","Javier Snaider","Norman Casagrande","Paul Suganthan","Evan Palmer","Geoffrey Irving","Edward Loper","Manaal Faruqui","Isha Arkatkar","Nanxin Chen","Izhak Shafran","Michael Fink","Alfonso Castaño","Irene Giannoumis","Wooyeol Kim","Mikołaj Rybiński","Ashwin Sreevatsa","Jennifer Prendki","David Soergel","Adrian Goedeckemeyer","Willi Gierke","Mohsen Jafari","Meenu Gaba","Jeremy Wiesner","Diana Gage Wright","Yawen Wei","Harsha Vashisht","Yana Kulizhskaya","Jay Hoover","Maigo Le","Lu Li","Chimezie Iwuanyanwu","Lu Liu","Kevin Ramirez","Andrey Khorlin","Albert Cui","Tian LIN","Marin Georgiev","Marcus Wu","Ricardo Aguilar","Keith Pallo","Abhishek Chakladar","Alena Repina","Xihui Wu","Tom van der Weide","Priya Ponnapalli","Caroline Kaplan","Jiri Simsa","Shuangfeng Li","Olivier Dousse","Fan Yang","Jeff Piper","Nathan Ie","Minnie Lui","Rama Pasumarthi","Nathan Lintz","Anitha Vijayakumar","Lam Nguyen Thiet","Daniel Andor","Pedro Valenzuela","Cosmin Paduraru","Daiyi Peng","Katherine Lee","Shuyuan Zhang","Somer Greene","Duc Dung Nguyen","Paula Kurylowicz","Sarmishta Velury","Sebastian Krause","Cassidy Hardin","Lucas Dixon","Lili Janzer","Kiam Choo","Ziqiang Feng","Biao Zhang","Achintya Singhal","Tejasi Latkar","Mingyang Zhang","Quoc Le","Elena Allica Abellan","Dayou Du","Dan McKinnon","Natasha Antropova","Tolga Bolukbasi","Orgad Keller","David Reid","Daniel Finchelstein","Maria Abi Raad","Remi Crocker","Peter Hawkins","Robert Dadashi","Colin Gaffney","Sid Lall","Ken Franko","Egor Filonov","Anna Bulanova","Rémi Leblond","Vikas Yadav","Shirley Chung","Harry Askham","Luis C. Cobo","Kelvin Xu","Felix Fischer","Jun Xu","Christina Sorokin","Chris Alberti","Chu-Cheng Lin","Colin Evans","Hao Zhou","Alek Dimitriev","Hannah Forbes","Dylan Banarse","Zora Tung","Jeremiah Liu","Mark Omernick","Colton Bishop","Chintu Kumar","Rachel Sterneck","Ryan Foley","Rohan Jain","Swaroop Mishra","Jiawei Xia","Taylor Bos","Geoffrey Cideron","Ehsan Amid","Francesco Piccinno","Xingyu Wang","Praseem Banzal","Petru Gurita","Hila Noga","Premal Shah","Daniel J. Mankowitz","Alex Polozov","Nate Kushman","Victoria Krakovna","Sasha Brown","MohammadHossein Bateni","Dennis Duan","Vlad Firoiu","Meghana Thotakuri","Tom Natan","Anhad Mohananey","Matthieu Geist","Sidharth Mudgal","Sertan Girgin","Hui Li","Jiayu Ye","Ofir Roval","Reiko Tojo","Michael Kwong","James Lee-Thorp","Christopher Yew","Quan Yuan","Sumit Bagri","Danila Sinopalnikov","Sabela Ramos","John Mellor","Abhishek Sharma","Aliaksei Severyn","Jonathan Lai","Kathy Wu","Heng-Tze Cheng","David Miller","Nicolas Sonnerat","Denis Vnukov","Rory Greig","Jennifer Beattie","Emily Caveness","Libin Bai","Julian Eisenschlos","Alex Korchemniy","Tomy Tsai","Mimi Jasarevic","Weize Kong","Phuong Dao","Zeyu Zheng","Frederick Liu","Fan Yang","Rui Zhu","Mark Geller","Tian Huey Teh","Jason Sanmiya","Evgeny Gladchenko","Nejc Trdin","Andrei Sozanschi","Daniel Toyama","Evan Rosen","Sasan Tavakkol","Linting Xue","Chen Elkind","Oliver Woodman","John Carpenter","George Papamakarios","Rupert Kemp","Sushant Kafle","Tanya Grunina","Rishika Sinha","Alice Talbert","Abhimanyu Goyal","Diane Wu","Denese Owusu-Afriyie","Cosmo Du","Chloe Thornton","Jordi Pont-Tuset","Pradyumna Narayana","Jing Li","Sabaer Fatehi","John Wieting","Omar Ajmeri","Benigno Uria","Tao Zhu","Yeongil Ko","Laura Knight","Amélie Héliou","Ning Niu","Shane Gu","Chenxi Pang","Dustin Tran","Yeqing Li","Nir Levine","Ariel Stolovich","Norbert Kalb","Rebeca Santamaria-Fernandez","Sonam Goenka","Wenny Yustalim","Robin Strudel","Ali Elqursh","Balaji Lakshminarayanan","Charlie Deck","Shyam Upadhyay","Hyo Lee","Mike Dusenberry","Zonglin Li","Xuezhi Wang","Kyle Levin","Raphael Hoffmann","Dan Holtmann-Rice","Olivier Bachem","Summer Yue","Sho Arora","Eric Malmi","Daniil Mirylenka","Qijun Tan","Christy Koh","Soheil Hassas Yeganeh","Siim Põder","Steven Zheng","Francesco Pongetti","Mukarram Tariq","Yanhua Sun","Lucian Ionita","Mojtaba Seyedhosseini","Pouya Tafti","Ragha Kotikalapudi","Zhiyu Liu","Anmol Gulati","Jasmine Liu","Xinyu Ye","Bart Chrzaszcz","Lily Wang","Nikhil Sethi","Tianrun Li","Ben Brown","Shreya Singh","Wei Fan","Aaron Parisi","Joe Stanton","Chenkai Kuang","Vinod Koverkathu","Christopher A. Choquette-Choo","Yunjie Li","TJ Lu","Abe Ittycheriah","Prakash Shroff","Pei Sun","Mani Varadarajan","Sanaz Bahargam","Rob Willoughby","David Gaddy","Ishita Dasgupta","Guillaume Desjardins","Marco Cornero","Brona Robenek","Bhavishya Mittal","Ben Albrecht","Ashish Shenoy","Fedor Moiseev","Henrik Jacobsson","Alireza Ghaffarkhah","Morgane Rivière","Alanna Walton","Clément Crepy","Alicia Parrish","Yuan Liu","Zongwei Zhou","Clement Farabet","Carey Radebaugh","Praveen Srinivasan","Claudia van der Salm","Andreas Fidjeland","Salvatore Scellato","Eri Latorre-Chimoto","Hanna Klimczak-Plucińska","David Bridson","Dario de Cesare","Tom Hudson","Piermaria Mendolicchio","Lexi Walker","Alex Morris","Ivo Penchev","Matthew Mauger","Alexey Guseynov","Alison Reid","Seth Odoom","Lucia Loher","Victor Cotruta","Madhavi Yenugula","Dominik Grewe","Anastasia Petrushkina","Tom Duerig","Antonio Sanchez","Steve Yadlowsky","Amy Shen","Amir Globerson","Adam Kurzrok","Lynette Webb","Sahil Dua","Dong Li","Preethi Lahoti","Surya Bhupatiraju","Dan Hurt","Haroon Qureshi","Ananth Agarwal","Tomer Shani","Matan Eyal","Anuj Khare","Shreyas Rammohan Belle","Lei Wang","Chetan Tekur","Mihir Sanjay Kale","Jinliang Wei","Ruoxin Sang","Brennan Saeta","Tyler Liechty","Yi Sun","Yao Zhao","Stephan Lee","Pandu Nayak","Doug Fritz","Manish Reddy Vuyyuru","John Aslanides","Nidhi Vyas","Martin Wicke","Xiao Ma","Taylan Bilal","Evgenii Eltyshev","Daniel Balle","Nina Martin","Hardie Cate","James Manyika","Keyvan Amiri","Yelin Kim","Xi Xiong","Kai Kang","Florian Luisier","Nilesh Tripuraneni","David Madras","Mandy Guo","Austin Waters","Oliver Wang","Joshua Ainslie","Jason Baldridge","Han Zhang","Garima Pruthi","Jakob Bauer","Feng Yang","Riham Mansour","Jason Gelman","Yang Xu","George Polovets","Ji Liu","Honglong Cai","Warren Chen","XiangHai Sheng","Emily Xue","Sherjil Ozair","Adams Yu","Christof Angermueller","Xiaowei Li","Weiren Wang","Julia Wiesinger","Emmanouil Koukoumidis","Yuan Tian","Anand Iyer","Madhu Gurumurthy","Mark Goldenson","Parashar Shah","MK Blake","Hongkun Yu","Anthony Urbanowicz","Jennimaria Palomaki","Chrisantha Fernando","Kevin Brooks","Ken Durden","Harsh Mehta","Nikola Momchev","Elahe Rahimtoroghi","Maria Georgaki","Amit Raul","Sebastian Ruder","Morgan Redshaw","Jinhyuk Lee","Komal Jalan","Dinghua Li","Ginger Perng","Blake Hechtman","Parker Schuh","Milad Nasr","Mia Chen","Kieran Milan","Vladimir Mikulik","Trevor Strohman","Juliana Franco","Tim Green","Demis Hassabis","Koray Kavukcuoglu","Jeffrey Dean","Oriol Vinyals"],"pdf_url":"https://arxiv.org/pdf/2312.11805v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11793v1","updated":"2023-12-19T02:09:38Z","published":"2023-12-19T02:09:38Z","title":"An effective image copy-move forgery detection using entropy image","summary":"  Image forensics has become increasingly important in our daily lives. As a\nfundamental type of forgeries, Copy-Move Forgery Detection (CMFD) has received\nsignificant attention in the academic community. Keypoint-based algorithms,\nparticularly those based on SIFT, have achieved good results in CMFD. However,\nthe most of keypoint detection algorithms often fail to generate sufficient\nmatches when tampered patches are present in smooth areas. To tackle this\nproblem, we introduce entropy images to determine the coordinates and scales of\nkeypoints, resulting significantly increasing the number of keypoints.\nFurthermore, we develop an entropy level clustering algorithm to avoid\nincreased matching complexity caused by non-ideal distribution of grayscale\nvalues in keypoints. Experimental results demonstrate that our algorithm\nachieves a good balance between performance and time efficiency.\n","authors":["Zhaowei Lu","Li Jiang"],"pdf_url":"https://arxiv.org/pdf/2312.11793v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10300v2","updated":"2023-12-19T02:04:18Z","published":"2023-12-16T03:17:30Z","title":"Shot2Story20K: A New Benchmark for Comprehensive Understanding of\n  Multi-shot Videos","summary":"  A short clip of video may contain progression of multiple events and an\ninteresting story line. A human need to capture both the event in every shot\nand associate them together to understand the story behind it. In this work, we\npresent a new multi-shot video understanding benchmark Shot2Story20K with\ndetailed shot-level captions and comprehensive video summaries. To facilitate\nbetter semantic understanding of videos, we provide captions for both visual\nsignals and human narrations. We design several distinct tasks including\nsingle-shot video and narration captioning, multi-shot video summarization, and\nvideo retrieval with shot descriptions. Preliminary experiments show some\nchallenges to generate a long and comprehensive video summary. Nevertheless,\nthe generated imperfect summaries can already significantly boost the\nperformance of existing video understanding tasks such as video\nquestion-answering, promoting an under-explored setting of video understanding\nwith detailed summaries.\n","authors":["Mingfei Han","Linjie Yang","Xiaojun Chang","Heng Wang"],"pdf_url":"https://arxiv.org/pdf/2312.10300v2.pdf","comment":"See https://mingfei.info/shot2story for updates and more information"},{"id":"http://arxiv.org/abs/2306.12681v3","updated":"2023-12-19T02:03:44Z","published":"2023-06-22T05:55:53Z","title":"One at a Time: Progressive Multi-step Volumetric Probability Learning\n  for Reliable 3D Scene Perception","summary":"  Numerous studies have investigated the pivotal role of reliable 3D volume\nrepresentation in scene perception tasks, such as multi-view stereo (MVS) and\nsemantic scene completion (SSC). They typically construct 3D probability\nvolumes directly with geometric correspondence, attempting to fully address the\nscene perception tasks in a single forward pass. However, such a single-step\nsolution makes it hard to learn accurate and convincing volumetric probability,\nespecially in challenging regions like unexpected occlusions and complicated\nlight reflections. Therefore, this paper proposes to decompose the complicated\n3D volume representation learning into a sequence of generative steps to\nfacilitate fine and reliable scene perception. Considering the recent advances\nachieved by strong generative diffusion models, we introduce a multi-step\nlearning framework, dubbed as VPD, dedicated to progressively refining the\nVolumetric Probability in a Diffusion process. Extensive experiments are\nconducted on scene perception tasks including multi-view stereo (MVS) and\nsemantic scene completion (SSC), to validate the efficacy of our method in\nlearning reliable volumetric representations. Notably, for the SSC task, our\nwork stands out as the first to surpass LiDAR-based methods on the\nSemanticKITTI dataset.\n","authors":["Bohan Li","Yasheng Sun","Jingxin Dong","Zheng Zhu","Jinming Liu","Xin Jin","Wenjun Zeng"],"pdf_url":"https://arxiv.org/pdf/2306.12681v3.pdf","comment":"AAAI2024"},{"id":"http://arxiv.org/abs/2312.10088v2","updated":"2023-12-19T01:44:13Z","published":"2023-12-13T05:32:52Z","title":"On Robustness to Missing Video for Audiovisual Speech Recognition","summary":"  It has been shown that learning audiovisual features can lead to improved\nspeech recognition performance over audio-only features, especially for noisy\nspeech. However, in many common applications, the visual features are partially\nor entirely missing, e.g.~the speaker might move off screen. Multi-modal models\nneed to be robust: missing video frames should not degrade the performance of\nan audiovisual model to be worse than that of a single-modality audio-only\nmodel. While there have been many attempts at building robust models, there is\nlittle consensus on how robustness should be evaluated. To address this, we\nintroduce a framework that allows claims about robustness to be evaluated in a\nprecise and testable way. We also conduct a systematic empirical study of the\nrobustness of common audiovisual speech recognition architectures on a range of\nacoustic noise conditions and test suites. Finally, we show that an\narchitecture-agnostic solution based on cascades can consistently achieve\nrobustness to missing video, even in settings where existing techniques for\nrobustness like dropout fall short.\n","authors":["Oscar Chang","Otavio Braga","Hank Liao","Dmitriy Serdyuk","Olivier Siohan"],"pdf_url":"https://arxiv.org/pdf/2312.10088v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11782v1","updated":"2023-12-19T01:33:46Z","published":"2023-12-19T01:33:46Z","title":"Learning Object State Changes in Videos: An Open-World Perspective","summary":"  Object State Changes (OSCs) are pivotal for video understanding. While humans\ncan effortlessly generalize OSC understanding from familiar to unknown objects,\ncurrent approaches are confined to a closed vocabulary. Addressing this gap, we\nintroduce a novel open-world formulation for the video OSC problem. The goal is\nto temporally localize the three stages of an OSC -- the object's initial\nstate, its transitioning state, and its end state -- whether or not the object\nhas been observed during training. Towards this end, we develop VidOSC, a\nholistic learning approach that: (1) leverages text and vision-language models\nfor supervisory signals to obviate manually labeling OSC training data, and (2)\nabstracts fine-grained shared state representations from objects to enhance\ngeneralization. Furthermore, we present HowToChange, the first open-world\nbenchmark for video OSC localization, which offers an order of magnitude\nincrease in the label space and annotation volume compared to the best existing\nbenchmark. Experimental results demonstrate the efficacy of our approach, in\nboth traditional closed-world and open-world scenarios.\n","authors":["Zihui Xue","Kumar Ashutosh","Kristen Grauman"],"pdf_url":"https://arxiv.org/pdf/2312.11782v1.pdf","comment":"Project website: https://vision.cs.utexas.edu/projects/VidOSC/"},{"id":"http://arxiv.org/abs/2207.14513v2","updated":"2023-12-19T01:31:29Z","published":"2022-07-29T07:21:15Z","title":"Uncertainty-Driven Action Quality Assessment","summary":"  Automatic action quality assessment (AQA) has attracted increasing attention\ndue to its wide applications. However, most existing AQA methods employ\ndeterministic models to predict the final score for each action, while\noverlooking the subjectivity and diversity among expert judges during the\nscoring process. In this paper, we propose a novel probabilistic model, named\nUncertainty-Driven AQA (UD-AQA), to utilize and capture the diversity among\nmultiple judge scores. Specifically, we design a Conditional Variational\nAuto-Encoder (CVAE)-based module to encode the uncertainty in expert\nassessment, where multiple judge scores can be produced by sampling latent\nfeatures from the learned latent space multiple times. To further utilize the\nuncertainty, we generate the estimation of uncertainty for each prediction,\nwhich is employed to re-weight AQA regression loss, effectively reducing the\ninfluence of uncertain samples during training. Moreover, we further design an\nuncertainty-guided training strategy to dynamically adjust the learning order\nof the samples from low uncertainty to high uncertainty. The experiments show\nthat our proposed method achieves competitive results on three benchmarks\nincluding the Olympic events MTL-AQA and FineDiving, and the surgical skill\nJIGSAWS datasets.\n","authors":["Caixia Zhou","Yaping Huang","Haibin Ling"],"pdf_url":"https://arxiv.org/pdf/2207.14513v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11775v1","updated":"2023-12-19T01:10:11Z","published":"2023-12-19T01:10:11Z","title":"Towards SAMBA: Segment Anything Model for Brain Tumor Segmentation in\n  Sub-Sharan African Populations","summary":"  Gliomas, the most prevalent primary brain tumors, require precise\nsegmentation for diagnosis and treatment planning. However, this task poses\nsignificant challenges, particularly in the African population, were limited\naccess to high-quality imaging data hampers algorithm performance. In this\nstudy, we propose an innovative approach combining the Segment Anything Model\n(SAM) and a voting network for multi-modal glioma segmentation. By fine-tuning\nSAM with bounding box-guided prompts (SAMBA), we adapt the model to the\ncomplexities of African datasets. Our ensemble strategy, utilizing multiple\nmodalities and views, produces a robust consensus segmentation, addressing\nintra-tumoral heterogeneity. Although the low quality of scans presents\ndifficulties, our methodology has the potential to profoundly impact clinical\npractice in resource-limited settings such as Africa, improving treatment\ndecisions and advancing neuro-oncology research. Furthermore, successful\napplication to other brain tumor types and lesions in the future holds promise\nfor a broader transformation in neurological imaging, improving healthcare\noutcomes across all settings. This study was conducted on the Brain Tumor\nSegmentation (BraTS) Challenge Africa (BraTS-Africa) dataset, which provides a\nvaluable resource for addressing challenges specific to resource-limited\nsettings, particularly the African population, and facilitating the development\nof effective and more generalizable segmentation algorithms. To illustrate our\napproach's potential, our experiments on the BraTS-Africa dataset yielded\ncompelling results, with SAM attaining a Dice coefficient of 86.6 for binary\nsegmentation and 60.4 for multi-class segmentation.\n","authors":["Mohannad Barakat","Noha Magdy","Jjuuko George William","Ethel Phiri","Raymond Confidence","Dong Zhang","Udunna C Anazodo"],"pdf_url":"https://arxiv.org/pdf/2312.11775v1.pdf","comment":"13 pages, 6 figures, 2 tables"},{"id":"http://arxiv.org/abs/2312.11774v1","updated":"2023-12-19T01:09:49Z","published":"2023-12-19T01:09:49Z","title":"Text-Image Conditioned Diffusion for Consistent Text-to-3D Generation","summary":"  By lifting the pre-trained 2D diffusion models into Neural Radiance Fields\n(NeRFs), text-to-3D generation methods have made great progress. Many\nstate-of-the-art approaches usually apply score distillation sampling (SDS) to\noptimize the NeRF representations, which supervises the NeRF optimization with\npre-trained text-conditioned 2D diffusion models such as Imagen. However, the\nsupervision signal provided by such pre-trained diffusion models only depends\non text prompts and does not constrain the multi-view consistency. To inject\nthe cross-view consistency into diffusion priors, some recent works finetune\nthe 2D diffusion model with multi-view data, but still lack fine-grained view\ncoherence. To tackle this challenge, we incorporate multi-view image conditions\ninto the supervision signal of NeRF optimization, which explicitly enforces\nfine-grained view consistency. With such stronger supervision, our proposed\ntext-to-3D method effectively mitigates the generation of floaters (due to\nexcessive densities) and completely empty spaces (due to insufficient\ndensities). Our quantitative evaluations on the T$^3$Bench dataset demonstrate\nthat our method achieves state-of-the-art performance over existing text-to-3D\nmethods. We will make the code publicly available.\n","authors":["Yuze He","Yushi Bai","Matthieu Lin","Jenny Sheng","Yubin Hu","Qi Wang","Yu-Hui Wen","Yong-Jin Liu"],"pdf_url":"https://arxiv.org/pdf/2312.11774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11772v1","updated":"2023-12-19T01:07:36Z","published":"2023-12-19T01:07:36Z","title":"CAManim: Animating end-to-end network activation maps","summary":"  Deep neural networks have been widely adopted in numerous domains due to\ntheir high performance and accessibility to developers and application-specific\nend-users. Fundamental to image-based applications is the development of\nConvolutional Neural Networks (CNNs), which possess the ability to\nautomatically extract features from data. However, comprehending these complex\nmodels and their learned representations, which typically comprise millions of\nparameters and numerous layers, remains a challenge for both developers and\nend-users. This challenge arises due to the absence of interpretable and\ntransparent tools to make sense of black-box models. There exists a growing\nbody of Explainable Artificial Intelligence (XAI) literature, including a\ncollection of methods denoted Class Activation Maps (CAMs), that seek to\ndemystify what representations the model learns from the data, how it informs a\ngiven prediction, and why it, at times, performs poorly in certain tasks. We\npropose a novel XAI visualization method denoted CAManim that seeks to\nsimultaneously broaden and focus end-user understanding of CNN predictions by\nanimating the CAM-based network activation maps through all layers, effectively\ndepicting from end-to-end how a model progressively arrives at the final layer\nactivation. Herein, we demonstrate that CAManim works with any CAM-based method\nand various CNN architectures. Beyond qualitative model assessments, we\nadditionally propose a novel quantitative assessment that expands upon the\nRemove and Debias (ROAD) metric, pairing the qualitative end-to-end network\nvisual explanations assessment with our novel quantitative \"yellow brick ROAD\"\nassessment (ybROAD). This builds upon prior research to address the increasing\ndemand for interpretable, robust, and transparent model assessment methodology,\nultimately improving an end-user's trust in a given model's predictions.\n","authors":["Emily Kaczmarek","Olivier X. Miguel","Alexa C. Bowie","Robin Ducharme","Alysha L. J. Dingwall-Harvey","Steven Hawken","Christine M. Armour","Mark C. Walker","Kevin Dick"],"pdf_url":"https://arxiv.org/pdf/2312.11772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11770v1","updated":"2023-12-19T01:03:19Z","published":"2023-12-19T01:03:19Z","title":"Bridging the Gap: Generalising State-of-the-Art U-Net Models to\n  Sub-Saharan African Populations","summary":"  A critical challenge for tumour segmentation models is the ability to adapt\nto diverse clinical settings, particularly when applied to poor-quality\nneuroimaging data. The uncertainty surrounding this adaptation stems from the\nlack of representative datasets, leaving top-performing models without exposure\nto common artifacts found in MRI data throughout Sub-Saharan Africa (SSA). We\nreplicated a framework that secured the 2nd position in the 2022 BraTS\ncompetition to investigate the impact of dataset composition on model\nperformance and pursued four distinct approaches through training a model with:\n1) BraTS-Africa data only (train_SSA, N=60), 2) BraTS-Adult Glioma data only\n(train_GLI, N=1251), 3) both datasets together (train_ALL, N=1311), and 4)\nthrough further training the train_GLI model with BraTS-Africa data\n(train_ftSSA). Notably, training on a smaller low-quality dataset alone\n(train_SSA) yielded subpar results, and training on a larger high-quality\ndataset alone (train_GLI) struggled to delineate oedematous tissue in the\nlow-quality validation set. The most promising approach (train_ftSSA) involved\npre-training a model on high-quality neuroimages and then fine-tuning it on the\nsmaller, low-quality dataset. This approach outperformed the others, ranking\nsecond in the MICCAI BraTS Africa global challenge external testing phase.\nThese findings underscore the significance of larger sample sizes and broad\nexposure to data in improving segmentation performance. Furthermore, we\ndemonstrated that there is potential for improving such models by fine-tuning\nthem with a wider range of data locally.\n","authors":["Alyssa R. Amod","Alexandra Smith","Pearly Joubert","Confidence Raymond","Dong Zhang","Udunna C. Anazodo","Dodzi Motchon","Tinashe E. M. Mutsvangwa","Sébastien Quetin"],"pdf_url":"https://arxiv.org/pdf/2312.11770v1.pdf","comment":"14 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2312.11763v1","updated":"2023-12-19T00:17:34Z","published":"2023-12-19T00:17:34Z","title":"ADMM-MM Algorithm for General Tensor Decomposition","summary":"  In this paper, we propose a new unified optimization algorithm for general\ntensor decomposition which is formulated as an inverse problem for low-rank\ntensors in the general linear observation models. The proposed algorithm\nsupports three basic loss functions ($\\ell_2$-loss, $\\ell_1$-loss and KL\ndivergence) and various low-rank tensor decomposition models (CP, Tucker, TT,\nand TR decompositions). We derive the optimization algorithm based on\nhierarchical combination of the alternating direction method of multiplier\n(ADMM) and majorization-minimization (MM). We show that wide-range applications\ncan be solved by the proposed algorithm, and can be easily extended to any\nestablished tensor decomposition models in a {plug-and-play} manner.\n","authors":["Manabu Mukai","Hidekata Hontani","Tatsuya Yokota"],"pdf_url":"https://arxiv.org/pdf/2312.11763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.12015v2","updated":"2023-12-19T00:07:16Z","published":"2023-05-19T21:59:23Z","title":"Inventing art styles with no artistic training data","summary":"  We propose two procedures to create painting styles using models trained only\non natural images, providing objective proof that the model is not plagiarizing\nhuman art styles. In the first procedure we use the inductive bias from the\nartistic medium to achieve creative expression. Abstraction is achieved by\nusing a reconstruction loss. The second procedure uses an additional natural\nimage as inspiration to create a new style. These two procedures make it\npossible to invent new painting styles with no artistic training data. We\nbelieve that our approach can help pave the way for the ethical employment of\ngenerative AI in art, without infringing upon the originality of human\ncreators.\n","authors":["Nilin Abrahamsen","Jiahao Yao"],"pdf_url":"https://arxiv.org/pdf/2305.12015v2.pdf","comment":"updated title"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2312.12430v1","updated":"2023-12-19T18:56:52Z","published":"2023-12-19T18:56:52Z","title":"Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP","summary":"  We introduce Efficient Title Reranker via Broadcasting Query Encoder, a novel\ntitle reranking technique to achieve efficient title reranking 20x-40x faster\nthan vanilla passage reranker. However, one of the challenges with the training\nof Efficient Title Reranker is the instability. Analyzing the issue, we found\nsome very difficult ground truths might act as noisy labels causing accuracy to\ndrop as well as some extreme values in model probability output causing nan. To\naddress these issues, we introduce the Sigmoid Trick, a novel technique that\nreduces the gradient update of both cases resulting in better retrieval\nefficacy. Experiments showed the effectiveness of ETR and sigmoid trick as we\nachieved four state-of-the-art positions on the kilt knowledge benchmark.\n","authors":["Ziyi Chen","Heyi Tao","Daqian Zuo","Jize Jiang","Yang Jun","Yuxiang Wei"],"pdf_url":"https://arxiv.org/pdf/2312.12430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12162v1","updated":"2023-12-19T13:51:48Z","published":"2023-12-19T13:51:48Z","title":"PEPT: Expert Finding Meets Personalized Pre-training","summary":"  Finding appropriate experts is essential in Community Question Answering\n(CQA) platforms as it enables the effective routing of questions to potential\nusers who can provide relevant answers. The key is to personalized learning\nexpert representations based on their historical answered questions, and\naccurately matching them with target questions. There have been some\npreliminary works exploring the usability of PLMs in expert finding, such as\npre-training expert or question representations. However, these models usually\nlearn pure text representations of experts from histories, disregarding\npersonalized and fine-grained expert modeling. For alleviating this, we present\na personalized pre-training and fine-tuning paradigm, which could effectively\nlearn expert interest and expertise simultaneously. Specifically, in our\npre-training framework, we integrate historical answered questions of one\nexpert with one target question, and regard it as a candidate aware\nexpert-level input unit. Then, we fuse expert IDs into the pre-training for\nguiding the model to model personalized expert representations, which can help\ncapture the unique characteristics and expertise of each individual expert.\nAdditionally, in our pre-training task, we design: 1) a question-level masked\nlanguage model task to learn the relatedness between histories, enabling the\nmodeling of question-level expert interest; 2) a vote-oriented task to capture\nquestion-level expert expertise by predicting the vote score the expert would\nreceive. Through our pre-training framework and tasks, our approach could\nholistically learn expert representations including interests and expertise.\nOur method has been extensively evaluated on six real-world CQA datasets, and\nthe experimental results consistently demonstrate the superiority of our\napproach over competitive baseline methods.\n","authors":["Qiyao Peng","Hongtao Liu","Hongyan Xu","Yinghui Wang","Wenjun Wang"],"pdf_url":"https://arxiv.org/pdf/2312.12162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.02327v2","updated":"2023-12-19T13:06:54Z","published":"2022-01-07T04:55:45Z","title":"On the Effectiveness of Sampled Softmax Loss for Item Recommendation","summary":"  The learning objective plays a fundamental role to build a recommender\nsystem. Most methods routinely adopt either pointwise or pairwise loss to train\nthe model parameters, while rarely pay attention to softmax loss due to its\ncomputational complexity when scaling up to large datasets or intractability\nfor streaming data. The sampled softmax (SSM) loss emerges as an efficient\nsubstitute for softmax loss. Its special case, InfoNCE loss, has been widely\nused in self-supervised learning and exhibited remarkable performance for\ncontrastive learning. Nonetheless, limited recommendation work uses the SSM\nloss as the learning objective. Worse still, none of them explores its\nproperties thoroughly and answers ``Does SSM loss suit for item\nrecommendation?'' and ``What are the conceptual advantages of SSM loss, as\ncompared with the prevalent losses?'', to the best of our knowledge.\n  In this work, we aim to offer a better understanding of SSM for item\nrecommendation. Specifically, we first theoretically reveal three\nmodel-agnostic advantages: (1) mitigating popularity bias; (2) mining hard\nnegative samples; and (3) maximizing the ranking metric. However, based on our\nempirical studies, we recognize that the default choice of cosine similarity\nfunction in SSM limits its ability in learning the magnitudes of representation\nvectors. As such, the combinations of SSM with the models that also fall short\nin adjusting magnitudes may result in poor representations. One step further,\nwe provide mathematical proof that message passing schemes in graph convolution\nnetworks can adjust representation magnitude according to node degree, which\nnaturally compensates for the shortcoming of SSM. Extensive experiments on four\nbenchmark datasets justify our analyses, demonstrating the superiority of SSM\nfor item recommendation. Our implementations are available in both TensorFlow\nand PyTorch.\n","authors":["Jiancan Wu","Xiang Wang","Xingyu Gao","Jiawei Chen","Hongcheng Fu","Tianyu Qiu"],"pdf_url":"https://arxiv.org/pdf/2201.02327v2.pdf","comment":"Accepted by TOIS"},{"id":"http://arxiv.org/abs/2312.12111v1","updated":"2023-12-19T12:33:38Z","published":"2023-12-19T12:33:38Z","title":"Designing and Evaluating General-Purpose User Representations Based on\n  Behavioral Logs from a Measurement Process Perspective: A Case Study with\n  Snapchat","summary":"  In human-computer interaction, understanding user behaviors and tailoring\nsystems accordingly is pivotal. To this end, general-purpose user\nrepresentation learning based on behavior logs is emerging as a powerful tool\nin user modeling, offering adaptability to various downstream tasks such as\nitem recommendations and ad conversion prediction, without the need to\nfine-tune the upstream user model. While this methodology has shown promise in\ncontexts like search engines and e-commerce platforms, its fit for instant\nmessaging apps, a cornerstone of modern digital communication, remains largely\nuncharted. These apps, with their distinct interaction patterns, data\nstructures, and user expectations, necessitate specialized attention. We\nexplore this user modeling approach with Snapchat data as a case study.\nFurthermore, we introduce a novel design and evaluation framework rooted in the\nprinciples of the Measurement Process Framework from social science research\nmethodology. Using this new framework, we design a Transformer-based user model\nthat can produce high-quality general-purpose user representations for instant\nmessaging platforms like Snapchat.\n","authors":["Qixiang Fang","Zhihan Zhou","Francesco Barbieri","Yozen Liu","Leonardo Neves","Dong Nguyen","Daniel L. Oberski","Maarten W. Bos","Ron Dotsch"],"pdf_url":"https://arxiv.org/pdf/2312.12111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12100v1","updated":"2023-12-19T12:22:40Z","published":"2023-12-19T12:22:40Z","title":"VITA: 'Carefully Chosen and Weighted Less' Is Better in Medication\n  Recommendation","summary":"  We address the medication recommendation problem, which aims to recommend\neffective medications for a patient's current visit by utilizing information\n(e.g., diagnoses and procedures) given at the patient's current and past\nvisits. While there exist a number of recommender systems designed for this\nproblem, we point out that they are challenged in accurately capturing the\nrelation (spec., the degree of relevance) between the current and each of the\npast visits for the patient when obtaining her current health status, which is\nthe basis for recommending medications. To address this limitation, we propose\na novel medication recommendation framework, named VITA, based on the following\ntwo novel ideas: (1) relevant-Visit selectIon; (2) Target-aware Attention.\nThrough extensive experiments using real-world datasets, we demonstrate the\nsuperiority of VITA (spec., up to 5.56% higher accuracy, in terms of Jaccard,\nthan the best competitor) and the effectiveness of its two core ideas. The code\nis available at https://github.com/jhheo0123/VITA.\n","authors":["Taeri Kim","Jiho Heo","Hongil Kim","Kijung Shin","Sang-Wook Kim"],"pdf_url":"https://arxiv.org/pdf/2312.12100v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.11229v2","updated":"2023-12-19T08:01:16Z","published":"2023-12-18T14:23:47Z","title":"CaseGNN: Graph Neural Networks for Legal Case Retrieval with\n  Text-Attributed Graphs","summary":"  Legal case retrieval is an information retrieval task in the legal domain,\nwhich aims to retrieve relevant cases with a given query case. Recent research\nof legal case retrieval mainly relies on traditional bag-of-words models and\nlanguage models. Although these methods have achieved significant improvement\nin retrieval accuracy, there are still two challenges: (1) Legal structural\ninformation neglect. Previous neural legal case retrieval models mostly encode\nthe unstructured raw text of case into a case representation, which causes the\nlack of important legal structural information in a case and leads to poor case\nrepresentation; (2) Lengthy legal text limitation. When using the powerful\nBERT-based models, there is a limit of input text lengths, which inevitably\nrequires to shorten the input via truncation or division with a loss of legal\ncontext information. In this paper, a graph neural networks-based legal case\nretrieval model, CaseGNN, is developed to tackle these challenges. To\neffectively utilise the legal structural information during encoding, a case is\nfirstly converted into a Text-Attributed Case Graph (TACG), followed by a\ndesigned Edge Graph Attention Layer and a readout function to obtain the case\ngraph representation. The CaseGNN model is optimised with a carefully designed\ncontrastive loss with easy and hard negative sampling. Since the text\nattributes in the case graph come from individual sentences, the restriction of\nusing language models is further avoided without losing the legal context.\nExtensive experiments have been conducted on two benchmarks from COLIEE 2022\nand COLIEE 2023, which demonstrate that CaseGNN outperforms other\nstate-of-the-art legal case retrieval methods. The code has been released on\nhttps://github.com/yanran-tang/CaseGNN.\n","authors":["Yanran Tang","Ruihong Qiu","Yilun Liu","Xue Li","Zi Huang"],"pdf_url":"https://arxiv.org/pdf/2312.11229v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11518v2","updated":"2023-12-19T07:40:45Z","published":"2023-09-19T09:17:07Z","title":"Ad-load Balancing via Off-policy Learning in a Content Marketplace","summary":"  Ad-load balancing is a critical challenge in online advertising systems,\nparticularly in the context of social media platforms, where the goal is to\nmaximize user engagement and revenue while maintaining a satisfactory user\nexperience. This requires the optimization of conflicting objectives, such as\nuser satisfaction and ads revenue. Traditional approaches to ad-load balancing\nrely on static allocation policies, which fail to adapt to changing user\npreferences and contextual factors. In this paper, we present an approach that\nleverages off-policy learning and evaluation from logged bandit feedback. We\nstart by presenting a motivating analysis of the ad-load balancing problem,\nhighlighting the conflicting objectives between user satisfaction and ads\nrevenue. We emphasize the nuances that arise due to user heterogeneity and the\ndependence on the user's position within a session. Based on this analysis, we\ndefine the problem as determining the optimal ad-load for a particular feed\nfetch. To tackle this problem, we propose an off-policy learning framework that\nleverages unbiased estimators such as Inverse Propensity Scoring (IPS) and\nDoubly Robust (DR) to learn and estimate the policy values using offline\ncollected stochastic data. We present insights from online A/B experiments\ndeployed at scale across over 80 million users generating over 200 million\nsessions, where we find statistically significant improvements in both user\nsatisfaction metrics and ads revenue for the platform.\n","authors":["Hitesh Sagtani","Madan Jhawar","Rishabh Mehrotra","Olivier Jeunen"],"pdf_url":"https://arxiv.org/pdf/2309.11518v2.pdf","comment":"Early version presented at the CONSEQUENCES '23 workshop at RecSys\n  '23, final version appearing at WSDM '24"},{"id":"http://arxiv.org/abs/2311.16751v2","updated":"2023-12-19T01:00:12Z","published":"2023-11-28T12:50:40Z","title":"MultiCBR: Multi-view Contrastive Learning for Bundle Recommendation","summary":"  Bundle recommendation seeks to recommend a bundle of related items to users\nto improve both user experience and the profits of platform. Existing bundle\nrecommendation models have progressed from capturing only user-bundle\ninteractions to the modeling of multiple relations among users, bundles and\nitems. CrossCBR, in particular, incorporates cross-view contrastive learning\ninto a two-view preference learning framework, significantly improving SOTA\nperformance. It does, however, have two limitations: 1) the two-view\nformulation does not fully exploit all the heterogeneous relations among users,\nbundles and items; and 2) the \"early contrast and late fusion\" framework is\nless effective in capturing user preference and difficult to generalize to\nmultiple views. In this paper, we present MultiCBR, a novel Multi-view\nContrastive learning framework for Bundle Recommendation. First, we devise a\nmulti-view representation learning framework capable of capturing all the\nuser-bundle, user-item and bundle-item relations, especially better utilizing\nthe bundle-item affiliations to enhance sparse bundles' representations.\nSecond, we innovatively adopt an \"early fusion and late contrast\" design that\nfirst fuses the multi-view representations before performing self-supervised\ncontrastive learning. In comparison to existing approaches, our framework\nreverses the order of fusion and contrast, introducing the following\nadvantages: 1)our framework is capable of modeling both cross-view and ego-view\npreferences, allowing us to achieve enhanced user preference modeling; and 2)\ninstead of requiring quadratic number of cross-view contrastive losses, we only\nrequire two self-supervised contrastive losses, resulting in minimal extra\ncosts. Experimental results on three public datasets indicate that our method\noutperforms SOTA methods.\n","authors":["Yunshan Ma","Yingzhi He","Xiang Wang","Yinwei Wei","Xiaoyu Du","Yuyangzi Fu","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2311.16751v2.pdf","comment":"fix a typo in Table 2, i.e., the R@20 and N@20 of LightGCL are\n  updated"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2311.18826v3","updated":"2023-12-19T18:59:34Z","published":"2023-11-30T18:59:05Z","title":"Geometry-Aware Normalizing Wasserstein Flows for Optimal Causal\n  Inference","summary":"  This manuscript enriches the framework of continuous normalizing flows (CNFs)\nwithin causal inference, primarily to augment the geometric properties of\nparametric submodels used in targeted maximum likelihood estimation (TMLE). By\nintroducing an innovative application of CNFs, we construct a refined series of\nparametric submodels that enable a directed interpolation between the prior\ndistribution $p_0$ and the empirical distribution $p_1$. This proposed\nmethodology serves to optimize the semiparametric efficiency bound in causal\ninference by orchestrating CNFs to align with Wasserstein gradient flows. Our\napproach not only endeavors to minimize the mean squared error in the\nestimation but also imbues the estimators with geometric sophistication,\nthereby enhancing robustness against misspecification. This robustness is\ncrucial, as it alleviates the dependence on the standard $n^{\\frac{1}{4}}$ rate\nfor a doubly-robust perturbation direction in TMLE. By incorporating robust\noptimization principles and differential geometry into the estimators, the\ndeveloped geometry-aware CNFs represent a significant advancement in the\npursuit of doubly robust causal inference.\n","authors":["Kaiwen Hou"],"pdf_url":"https://arxiv.org/pdf/2311.18826v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12433v1","updated":"2023-12-19T18:58:40Z","published":"2023-12-19T18:58:40Z","title":"Tracking Any Object Amodally","summary":"  Amodal perception, the ability to comprehend complete object structures from\npartial visibility, is a fundamental skill, even for infants. Its significance\nextends to applications like autonomous driving, where a clear understanding of\nheavily occluded objects is essential. However, modern detection and tracking\nalgorithms often overlook this critical capability, perhaps due to the\nprevalence of modal annotations in most datasets. To address the scarcity of\namodal data, we introduce the TAO-Amodal benchmark, featuring 880 diverse\ncategories in thousands of video sequences. Our dataset includes amodal and\nmodal bounding boxes for visible and occluded objects, including objects that\nare partially out-of-frame. To enhance amodal tracking with object permanence,\nwe leverage a lightweight plug-in module, the amodal expander, to transform\nstandard, modal trackers into amodal ones through fine-tuning on a few hundred\nvideo sequences with data augmentation. We achieve a 3.3\\% and 1.6\\%\nimprovement on the detection and tracking of occluded objects on TAO-Amodal.\nWhen evaluated on people, our method produces dramatic improvements of 2x\ncompared to state-of-the-art modal baselines.\n","authors":["Cheng-Yen Hsieh","Tarasha Khurana","Achal Dave","Deva Ramanan"],"pdf_url":"https://arxiv.org/pdf/2312.12433v1.pdf","comment":"Project Page: https://tao-amodal.github.io"},{"id":"http://arxiv.org/abs/2312.12430v1","updated":"2023-12-19T18:56:52Z","published":"2023-12-19T18:56:52Z","title":"Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP","summary":"  We introduce Efficient Title Reranker via Broadcasting Query Encoder, a novel\ntitle reranking technique to achieve efficient title reranking 20x-40x faster\nthan vanilla passage reranker. However, one of the challenges with the training\nof Efficient Title Reranker is the instability. Analyzing the issue, we found\nsome very difficult ground truths might act as noisy labels causing accuracy to\ndrop as well as some extreme values in model probability output causing nan. To\naddress these issues, we introduce the Sigmoid Trick, a novel technique that\nreduces the gradient update of both cases resulting in better retrieval\nefficacy. Experiments showed the effectiveness of ETR and sigmoid trick as we\nachieved four state-of-the-art positions on the kilt knowledge benchmark.\n","authors":["Ziyi Chen","Heyi Tao","Daqian Zuo","Jize Jiang","Yang Jun","Yuxiang Wei"],"pdf_url":"https://arxiv.org/pdf/2312.12430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12416v1","updated":"2023-12-19T18:47:30Z","published":"2023-12-19T18:47:30Z","title":"Prompting Hard or Hardly Prompting: Prompt Inversion for Text-to-Image\n  Diffusion Models","summary":"  The quality of the prompts provided to text-to-image diffusion models\ndetermines how faithful the generated content is to the user's intent, often\nrequiring `prompt engineering'. To harness visual concepts from target images\nwithout prompt engineering, current approaches largely rely on embedding\ninversion by optimizing and then mapping them to pseudo-tokens. However,\nworking with such high-dimensional vector representations is challenging\nbecause they lack semantics and interpretability, and only allow simple vector\noperations when using them. Instead, this work focuses on inverting the\ndiffusion model to obtain interpretable language prompts directly. The\nchallenge of doing this lies in the fact that the resulting optimization\nproblem is fundamentally discrete and the space of prompts is exponentially\nlarge; this makes using standard optimization techniques, such as stochastic\ngradient descent, difficult. To this end, we utilize a delayed projection\nscheme to optimize for prompts representative of the vocabulary space in the\nmodel. Further, we leverage the findings that different timesteps of the\ndiffusion process cater to different levels of detail in an image. The later,\nnoisy, timesteps of the forward diffusion process correspond to the semantic\ninformation, and therefore, prompt inversion in this range provides tokens\nrepresentative of the image semantics. We show that our approach can identify\nsemantically interpretable and meaningful prompts for a target image which can\nbe used to synthesize diverse images with similar content. We further\nillustrate the application of the optimized prompts in evolutionary image\ngeneration and concept removal.\n","authors":["Shweta Mahajan","Tanzila Rahman","Kwang Moo Yi","Leonid Sigal"],"pdf_url":"https://arxiv.org/pdf/2312.12416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08645v2","updated":"2023-12-19T18:46:19Z","published":"2022-12-16T18:39:32Z","title":"Efficient Conditionally Invariant Representation Learning","summary":"  We introduce the Conditional Independence Regression CovariancE (CIRCE), a\nmeasure of conditional independence for multivariate continuous-valued\nvariables. CIRCE applies as a regularizer in settings where we wish to learn\nneural features $\\varphi(X)$ of data $X$ to estimate a target $Y$, while being\nconditionally independent of a distractor $Z$ given $Y$. Both $Z$ and $Y$ are\nassumed to be continuous-valued but relatively low dimensional, whereas $X$ and\nits features may be complex and high dimensional. Relevant settings include\ndomain-invariant learning, fairness, and causal learning. The procedure\nrequires just a single ridge regression from $Y$ to kernelized features of $Z$,\nwhich can be done in advance. It is then only necessary to enforce independence\nof $\\varphi(X)$ from residuals of this regression, which is possible with\nattractive estimation properties and consistency guarantees. By contrast,\nearlier measures of conditional feature dependence require multiple regressions\nfor each step of feature learning, resulting in more severe bias and variance,\nand greater computational cost. When sufficiently rich features are used, we\nestablish that CIRCE is zero if and only if $\\varphi(X) \\perp \\!\\!\\! \\perp Z\n\\mid Y$. In experiments, we show superior performance to previous methods on\nchallenging benchmarks, including learning conditionally invariant image\nfeatures.\n","authors":["Roman Pogodin","Namrata Deka","Yazhe Li","Danica J. Sutherland","Victor Veitch","Arthur Gretton"],"pdf_url":"https://arxiv.org/pdf/2212.08645v2.pdf","comment":"ICLR 2023"},{"id":"http://arxiv.org/abs/2308.13304v2","updated":"2023-12-19T18:45:10Z","published":"2023-08-25T11:04:35Z","title":"Rapid Artefact Removal and H&E-Stained Tissue Segmentation","summary":"  We present an innovative method for rapidly segmenting hematoxylin and eosin\n(H&E)-stained tissue in whole-slide images (WSIs) that eliminates a wide range\nof undesirable artefacts such as pen marks and scanning artefacts. Our method\ninvolves taking a single-channel representation of a lowmagnification RGB\noverview of the WSI in which the pixel values are bimodally distributed such\nthat H&E-stained tissue is easily distinguished from both background and a wide\nvariety of artefacts. We demonstrate our method on 30 WSIs prepared from a wide\nrange of institutions and WSI digital scanners, each containing substantial\nartefacts, and compare it to segmentations provided by Otsu thresholding and\nHistolab tissue segmentation and pen filtering tools. We found that our method\nsegmented the tissue and fully removed all artefacts in 29 out of 30 WSIs,\nwhereas Otsu thresholding failed to remove any artefacts, and the Histolab pen\nfiltering tools only partially removed the pen marks. The beauty of our\napproach lies in its simplicity: manipulating RGB colour space and using Otsu\nthresholding allows for the segmentation of H&E-stained tissue and the rapid\nremoval of artefacts without the need for machine learning or parameter tuning.\n","authors":["B. A. Schreiber","J. Denholm","F. Jaeckle","M. J. Arends","K. M. Branson","C. -B. Schönlieb","E. J. Soilleux"],"pdf_url":"https://arxiv.org/pdf/2308.13304v2.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2312.12400v1","updated":"2023-12-19T18:35:33Z","published":"2023-12-19T18:35:33Z","title":"New classes of the greedy-applicable arm feature distributions in the\n  sparse linear bandit problem","summary":"  We consider the sparse contextual bandit problem where arm feature affects\nreward through the inner product of sparse parameters. Recent studies have\ndeveloped sparsity-agnostic algorithms based on the greedy arm selection\npolicy. However, the analysis of these algorithms requires strong assumptions\non the arm feature distribution to ensure that the greedily selected samples\nare sufficiently diverse; One of the most common assumptions, relaxed symmetry,\nimposes approximate origin-symmetry on the distribution, which cannot allow\ndistributions that has origin-asymmetric support. In this paper, we show that\nthe greedy algorithm is applicable to a wider range of the arm feature\ndistributions from two aspects. Firstly, we show that a mixture distribution\nthat has a greedy-applicable component is also greedy-applicable. Second, we\npropose new distribution classes, related to Gaussian mixture, discrete, and\nradial distribution, for which the sample diversity is guaranteed. The proposed\nclasses can describe distributions with origin-asymmetric support and, in\nconjunction with the first claim, provide theoretical guarantees of the greedy\npolicy for a very wide range of the arm feature distributions.\n","authors":["Koji Ichikawa","Shinji Ito","Daisuke Hatano","Hanna Sumita","Takuro Fukunaga","Naonori Kakimura","Ken-ichi Kawarabayashi"],"pdf_url":"https://arxiv.org/pdf/2312.12400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.08830v2","updated":"2023-12-19T18:31:20Z","published":"2023-01-20T23:55:30Z","title":"Finding Nash equilibria by minimizing approximate exploitability with\n  learned best responses","summary":"  There has been substantial progress on finding game-theoretic equilibria.\nMost of that work has focused on games with finite, discrete action spaces.\nHowever, many games involving space, time, money, and other fine-grained\nquantities have continuous action spaces (or are best modeled as such). We\nstudy the problem of finding an approximate Nash equilibrium of games with\ncontinuous action sets. The standard measure of closeness to Nash equilibrium\nis exploitability, which measures how much players can benefit from\nunilaterally changing their strategy. We propose two new methods that minimize\nan approximation of the exploitability with respect to the strategy profile.\nThe first method uses a learned best-response function, which takes the current\nstrategy profile as input and returns candidate best responses for each player.\nThe strategy profile and best-response functions are trained simultaneously,\nwith the former trying to minimize exploitability while the latter tries to\nmaximize it. The second method maintains an ensemble of candidate best\nresponses for each player. In each iteration, the best-performing elements of\neach ensemble are used to update the current strategy profile. The strategy\nprofile and best-response ensembles are simultaneously trained to minimize and\nmaximize the approximate exploitability, respectively. We evaluate our methods\non various continuous games, showing that they outperform prior methods.\n","authors":["Carlos Martin","Tuomas Sandholm"],"pdf_url":"https://arxiv.org/pdf/2301.08830v2.pdf","comment":"arXiv admin note: text overlap with arXiv:1611.01673 by other authors"},{"id":"http://arxiv.org/abs/2309.15188v3","updated":"2023-12-19T18:25:08Z","published":"2023-09-26T18:49:30Z","title":"ICML 2023 Topological Deep Learning Challenge : Design and Results","summary":"  This paper presents the computational challenge on topological deep learning\nthat was hosted within the ICML 2023 Workshop on Topology and Geometry in\nMachine Learning. The competition asked participants to provide open-source\nimplementations of topological neural networks from the literature by\ncontributing to the python packages TopoNetX (data processing) and TopoModelX\n(deep learning). The challenge attracted twenty-eight qualifying submissions in\nits two-month duration. This paper describes the design of the challenge and\nsummarizes its main findings.\n","authors":["Mathilde Papillon","Mustafa Hajij","Helen Jenne","Johan Mathe","Audun Myers","Theodore Papamarkou","Ghada Zamzmi","Tolga Birdal","Tamal Dey","Tim Doster","Tegan Emerson","Gurusankar Gopalakrishnan","Devendra Govil","Aldo Guzmán-Sáenz","Henry Kvinge","Neal Livesay","Soham Mukherjee","Shreyas N. Samaga","Karthikeyan Natesan Ramamurthy","Maneel Reddy Karri","Paul Rosen","Sophia Sanborn","Robin Walters","Jens Agerberg","Sadrodin Barikbin","Claudio Battiloro","Gleb Bazhenov","Guillermo Bernardez","Aiden Brent","Sergio Escalera","Simone Fiorellino","Dmitrii Gavrilev","Mohammed Hassanin","Paul Häusner","Odin Hoff Gardaa","Abdelwahed Khamis","Manuel Lecha","German Magai","Tatiana Malygina","Rubén Ballester","Kalyan Nadimpalli","Alexander Nikitin","Abraham Rabinowitz","Alessandro Salatiello","Simone Scardapane","Luca Scofano","Suraj Singh","Jens Sjölund","Pavel Snopov","Indro Spinelli","Lev Telyatnikov","Lucia Testa","Maosheng Yang","Yixiao Yue","Olga Zaghen","Ali Zia","Nina Miolane"],"pdf_url":"https://arxiv.org/pdf/2309.15188v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12369v1","updated":"2023-12-19T18:00:15Z","published":"2023-12-19T18:00:15Z","title":"Chasing Fairness in Graphs: A GNN Architecture Perspective","summary":"  There has been significant progress in improving the performance of graph\nneural networks (GNNs) through enhancements in graph data, model architecture\ndesign, and training strategies. For fairness in graphs, recent studies achieve\nfair representations and predictions through either graph data pre-processing\n(e.g., node feature masking, and topology rewiring) or fair training strategies\n(e.g., regularization, adversarial debiasing, and fair contrastive learning).\nHow to achieve fairness in graphs from the model architecture perspective is\nless explored. More importantly, GNNs exhibit worse fairness performance\ncompared to multilayer perception since their model architecture (i.e.,\nneighbor aggregation) amplifies biases. To this end, we aim to achieve fairness\nvia a new GNN architecture. We propose \\textsf{F}air \\textsf{M}essage\n\\textsf{P}assing (FMP) designed within a unified optimization framework for\nGNNs. Notably, FMP \\textit{explicitly} renders sensitive attribute usage in\n\\textit{forward propagation} for node classification task using cross-entropy\nloss without data pre-processing. In FMP, the aggregation is first adopted to\nutilize neighbors' information and then the bias mitigation step explicitly\npushes demographic group node presentation centers together. In this way, FMP\nscheme can aggregate useful information from neighbors and mitigate bias to\nachieve better fairness and prediction tradeoff performance. Experiments on\nnode classification tasks demonstrate that the proposed FMP outperforms several\nbaselines in terms of fairness and accuracy on three real-world datasets. The\ncode is available in {\\url{https://github.com/zhimengj0326/FMP}}.\n","authors":["Zhimeng Jiang","Xiaotian Han","Chao Fan","Zirui Liu","Na Zou","Ali Mostafavi","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2312.12369v1.pdf","comment":"Accepted by AAAI Conference on Artificial Intelligence (AAAI) 2024.\n  arXiv admin note: substantial text overlap with arXiv:2202.04187"},{"id":"http://arxiv.org/abs/2312.05571v2","updated":"2023-12-19T17:48:20Z","published":"2023-12-09T13:20:49Z","title":"Frugal LMs Trained to Invoke Symbolic Solvers Achieve\n  Parameter-Efficient Arithmetic Reasoning","summary":"  Large Language Models (LLM) exhibit zero-shot mathematical reasoning capacity\nas a behavior emergent with scale, commonly manifesting as chain-of-thoughts\n(CoT) reasoning. However, multiple empirical findings suggest that this prowess\nis exclusive to LLMs with exorbitant sizes (beyond 50 billion parameters).\nMeanwhile, educational neuroscientists suggest that symbolic algebraic\nmanipulation be introduced around the same time as arithmetic word problems to\nmodularize language-to-formulation, symbolic manipulation of the formulation,\nand endgame arithmetic. In this paper, we start with the hypothesis that much\nsmaller LMs, which are weak at multi-step reasoning, can achieve reasonable\narithmetic reasoning if arithmetic word problems are posed as a\nformalize-then-solve task. In our architecture, which we call SYRELM, the LM\nserves the role of a translator to map natural language arithmetic questions\ninto a formal language (FL) description. A symbolic solver then evaluates the\nFL expression to obtain the answer. A small frozen LM, equipped with an\nefficient low-rank adapter, is capable of generating FL expressions that\nincorporate natural language descriptions of the arithmetic problem (e.g.,\nvariable names and their purposes, formal expressions combining variables,\netc.). We adopt policy-gradient reinforcement learning to train the adapted LM,\ninformed by the non-differentiable symbolic solver. This marks a sharp\ndeparture from the recent development in tool-augmented LLMs, in which the\nexternal tools (e.g., calculator, Web search, etc.) are essentially detached\nfrom the learning phase of the LM. SYRELM shows massive improvements (e.g.,\n+30.65 absolute point improvement in accuracy on the SVAMP dataset using GPT-J\n6B model) over base LMs, while keeping our testbed easy to diagnose, interpret\nand within reach of most researchers.\n","authors":["Subhabrata Dutta","Joykirat Singh","Ishan Pandey","Sunny Manchanda","Soumen Chakrabarti","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2312.05571v2.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2205.15677v4","updated":"2023-12-19T17:42:05Z","published":"2022-05-31T10:35:55Z","title":"Augmentation-Aware Self-Supervision for Data-Efficient GAN Training","summary":"  Training generative adversarial networks (GANs) with limited data is\nchallenging because the discriminator is prone to overfitting. Previously\nproposed differentiable augmentation demonstrates improved data efficiency of\ntraining GANs. However, the augmentation implicitly introduces undesired\ninvariance to augmentation for the discriminator since it ignores the change of\nsemantics in the label space caused by data transformation, which may limit the\nrepresentation learning ability of the discriminator and ultimately affect the\ngenerative modeling performance of the generator. To mitigate the negative\nimpact of invariance while inheriting the benefits of data augmentation, we\npropose a novel augmentation-aware self-supervised discriminator that predicts\nthe augmentation parameter of the augmented data. Particularly, the prediction\ntargets of real data and generated data are required to be distinguished since\nthey are different during training. We further encourage the generator to\nadversarially learn from the self-supervised discriminator by generating\naugmentation-predictable real and not fake data. This formulation connects the\nlearning objective of the generator and the arithmetic $-$ harmonic mean\ndivergence under certain assumptions. We compare our method with\nstate-of-the-art (SOTA) methods using the class-conditional BigGAN and\nunconditional StyleGAN2 architectures on data-limited CIFAR-10, CIFAR-100,\nFFHQ, LSUN-Cat, and five low-shot datasets. Experimental results demonstrate\nsignificant improvements of our method over SOTA methods in training\ndata-efficient GANs.\n","authors":["Liang Hou","Qi Cao","Yige Yuan","Songtao Zhao","Chongyang Ma","Siyuan Pan","Pengfei Wan","Zhongyuan Wang","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2205.15677v4.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2312.12357v1","updated":"2023-12-19T17:38:26Z","published":"2023-12-19T17:38:26Z","title":"Modeling non-linear Effects with Neural Networks in Relational Event\n  Models","summary":"  Dynamic networks offer an insight of how relational systems evolve. However,\nmodeling these networks efficiently remains a challenge, primarily due to\ncomputational constraints, especially as the number of observed events grows.\nThis paper addresses this issue by introducing the Deep Relational Event\nAdditive Model (DREAM) as a solution to the computational challenges presented\nby modeling non-linear effects in Relational Event Models (REMs). DREAM relies\non Neural Additive Models to model non-linear effects, allowing each effect to\nbe captured by an independent neural network. By strategically trading\ncomputational complexity for improved memory management and leveraging the\ncomputational capabilities of Graphic Processor Units (GPUs), DREAM efficiently\ncaptures complex non-linear relationships within data. This approach\ndemonstrates the capability of DREAM in modeling dynamic networks and scaling\nto larger networks. Comparisons with traditional REM approaches showcase DREAM\nsuperior computational efficiency. The model potential is further demonstrated\nby an examination of the patent citation network, which contains nearly 8\nmillion nodes and 100 million events.\n","authors":["Edoardo Filippi-Mazzola","Ernst C. Wit"],"pdf_url":"https://arxiv.org/pdf/2312.12357v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.16222v2","updated":"2023-12-19T17:19:49Z","published":"2022-10-28T15:56:55Z","title":"Improving Lipschitz-Constrained Neural Networks by Learning Activation\n  Functions","summary":"  Lipschitz-constrained neural networks have several advantages over\nunconstrained ones and can be applied to a variety of problems, making them a\ntopic of attention in the deep learning community. Unfortunately, it has been\nshown both theoretically and empirically that they perform poorly when equipped\nwith ReLU activation functions. By contrast, neural networks with learnable\n1-Lipschitz linear splines are known to be more expressive. In this paper, we\nshow that such networks correspond to global optima of a constrained functional\noptimization problem that consists of the training of a neural network composed\nof 1-Lipschitz linear layers and 1-Lipschitz freeform activation functions with\nsecond-order total-variation regularization. Further, we propose an efficient\nmethod to train these neural networks. Our numerical experiments show that our\ntrained networks compare favorably with existing 1-Lipschitz neural\narchitectures.\n","authors":["Stanislas Ducotterd","Alexis Goujon","Pakshal Bohra","Dimitris Perdios","Sebastian Neumayer","Michael Unser"],"pdf_url":"https://arxiv.org/pdf/2210.16222v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12345v1","updated":"2023-12-19T17:17:52Z","published":"2023-12-19T17:17:52Z","title":"On the Effectiveness of Retrieval, Alignment, and Replay in Manipulation","summary":"  Imitation learning with visual observations is notoriously inefficient when\naddressed with end-to-end behavioural cloning methods. In this paper, we\nexplore an alternative paradigm which decomposes reasoning into three phases.\nFirst, a retrieval phase, which informs the robot what it can do with an\nobject. Second, an alignment phase, which informs the robot where to interact\nwith the object. And third, a replay phase, which informs the robot how to\ninteract with the object. Through a series of real-world experiments on\neveryday tasks, such as grasping, pouring, and inserting objects, we show that\nthis decomposition brings unprecedented learning efficiency, and effective\ninter- and intra-class generalisation. Videos are available at\nhttps://www.robot-learning.uk/retrieval-alignment-replay.\n","authors":["Norman Di Palo","Edward Johns"],"pdf_url":"https://arxiv.org/pdf/2312.12345v1.pdf","comment":"Published in IEEE Robotics and Automation Letters (RA-L). (Accepted\n  December 2023)"},{"id":"http://arxiv.org/abs/2312.12339v1","updated":"2023-12-19T17:12:35Z","published":"2023-12-19T17:12:35Z","title":"Value Explicit Pretraining for Goal-Based Transfer Learning","summary":"  We propose a method that allows for learning task-agnostic representations\nbased on value function estimates from a sequence of observations where the\nlast frame corresponds to a goal. These representations would learn to relate\nstates across different tasks, based on the temporal distance to the goal\nstate, irrespective of the appearance changes and dynamics. This method could\nbe used to transfer learnt policies/skills to unseen related tasks.\n","authors":["Kiran Lekkala","Henghui Bao","Sumedh Sontakke","Laurent Itti"],"pdf_url":"https://arxiv.org/pdf/2312.12339v1.pdf","comment":"Accepted at CoRL 2023 Workshop on PRL"},{"id":"http://arxiv.org/abs/2311.05587v4","updated":"2023-12-19T17:07:19Z","published":"2023-11-09T18:47:33Z","title":"Bayesian Methods for Media Mix Modelling with shape and funnel effects","summary":"  In recent years, significant progress in generative AI has highlighted the\nimportant role of physics-inspired models that utilize advanced mathematical\nconcepts based on fundamental physics principles to enhance artificial\nintelligence capabilities. Among these models, those based on diffusion\nequations have greatly improved image quality. This study aims to explore the\npotential uses of Maxwell-Boltzmann equation, which forms the basis of the\nkinetic theory of gases, and the Michaelis-Menten model in Marketing Mix\nModelling (MMM) applications. We propose incorporating these equations into\nHierarchical Bayesian models to analyse consumer behaviour in the context of\nadvertising. These equation sets excel in accurately describing the random\ndynamics in complex systems like social interactions and consumer-advertising\ninteractions.\n","authors":["Javier Marin"],"pdf_url":"https://arxiv.org/pdf/2311.05587v4.pdf","comment":"Rev. 4, December 2023"},{"id":"http://arxiv.org/abs/2312.08528v2","updated":"2023-12-19T17:07:02Z","published":"2023-12-13T21:34:30Z","title":"auto-sktime: Automated Time Series Forecasting","summary":"  In today's data-driven landscape, time series forecasting is pivotal in\ndecision-making across various sectors. Yet, the proliferation of more diverse\ntime series data, coupled with the expanding landscape of available forecasting\nmethods, poses significant challenges for forecasters. To meet the growing\ndemand for efficient forecasting, we introduce auto-sktime, a novel framework\nfor automated time series forecasting. The proposed framework uses the power of\nautomated machine learning (AutoML) techniques to automate the creation of the\nentire forecasting pipeline. The framework employs Bayesian optimization, to\nautomatically construct pipelines from statistical, machine learning (ML) and\ndeep neural network (DNN) models. Furthermore, we propose three essential\nimprovements to adapt AutoML to time series data: First, pipeline templates to\naccount for the different supported forecasting models. Second, a novel\nwarm-starting technique to start the optimization from prior optimization runs.\nThird, we adapt multi-fidelity optimizations to make them applicable to a\nsearch space containing statistical, ML and DNN models. Experimental results on\n64 diverse real-world time series datasets demonstrate the effectiveness and\nefficiency of the framework, outperforming traditional methods while requiring\nminimal human involvement.\n","authors":["Marc-André Zöller","Marius Lindauer","Marco F. Huber"],"pdf_url":"https://arxiv.org/pdf/2312.08528v2.pdf","comment":"Submitted to AISTATS 2024"},{"id":"http://arxiv.org/abs/2211.08494v2","updated":"2023-12-19T17:04:59Z","published":"2022-11-15T20:47:14Z","title":"Who Reviews The Reviewers? A Multi-Level Jury Problem","summary":"  We consider the problem of determining a binary ground truth using advice\nfrom a group of independent reviewers (experts) who express their guess about a\nground truth correctly with some independent probability (competence). In this\nsetting, when all reviewers are competent (competence greater than one-half),\nthe Condorcet Jury Theorem tells us that adding more reviewers increases the\noverall accuracy, and if all competences are known, then there exists an\noptimal weighting of the reviewers. However, in practical settings, reviewers\nmay be noisy or incompetent, i.e., competence below half, and the number of\nexperts may be small, so the asymptotic Condorcet Jury Theorem is not\npractically relevant. In such cases we explore appointing one or more chairs\n(judges) who determine the weight of each reviewer for aggregation, creating\nmultiple levels. However, these chairs may be unable to correctly identify the\ncompetence of the reviewers they oversee, and therefore unable to compute the\noptimal weighting. We give conditions when a set of chairs is able to weight\nthe reviewers optimally, and depending on the competence distribution of the\nagents, give results about when it is better to have more chairs or more\nreviewers. Through numerical simulations we show that in some cases it is\nbetter to have more chairs, but in many cases it is better to have more\nreviewers.\n","authors":["Ben Abramowitz","Omer Lev","Nicholas Mattei"],"pdf_url":"https://arxiv.org/pdf/2211.08494v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12337v1","updated":"2023-12-19T17:03:50Z","published":"2023-12-19T17:03:50Z","title":"pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable\n  Generalizable 3D Reconstruction","summary":"  We introduce pixelSplat, a feed-forward model that learns to reconstruct 3D\nradiance fields parameterized by 3D Gaussian primitives from pairs of images.\nOur model features real-time and memory-efficient rendering for scalable\ntraining as well as fast 3D reconstruction at inference time. To overcome local\nminima inherent to sparse and locally supported representations, we predict a\ndense probability distribution over 3D and sample Gaussian means from that\nprobability distribution. We make this sampling operation differentiable via a\nreparameterization trick, allowing us to back-propagate gradients through the\nGaussian splatting representation. We benchmark our method on wide-baseline\nnovel view synthesis on the real-world RealEstate10k and ACID datasets, where\nwe outperform state-of-the-art light field transformers and accelerate\nrendering by 2.5 orders of magnitude while reconstructing an interpretable and\neditable 3D radiance field.\n","authors":["David Charatan","Sizhe Li","Andrea Tagliasacchi","Vincent Sitzmann"],"pdf_url":"https://arxiv.org/pdf/2312.12337v1.pdf","comment":"Project page: https://pixelsplat.github.io/"},{"id":"http://arxiv.org/abs/2312.12321v1","updated":"2023-12-19T16:47:12Z","published":"2023-12-19T16:47:12Z","title":"Bypassing the Safety Training of Open-Source LLMs with Priming Attacks","summary":"  With the recent surge in popularity of LLMs has come an ever-increasing need\nfor LLM safety training. In this paper, we show that SOTA open-source LLMs are\nvulnerable to simple, optimization-free attacks we refer to as $\\textit{priming\nattacks}$, which are easy to execute and effectively bypass alignment from\nsafety training. Our proposed attack improves the Attack Success Rate on\nHarmful Behaviors, as measured by Llama Guard, by up to $3.3\\times$ compared to\nbaselines. Source code and data are available at\nhttps://github.com/uiuc-focal-lab/llm-priming-attacks .\n","authors":["Jason Vega","Isha Chaudhary","Changming Xu","Gagandeep Singh"],"pdf_url":"https://arxiv.org/pdf/2312.12321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12318v1","updated":"2023-12-19T16:43:17Z","published":"2023-12-19T16:43:17Z","title":"An Alternate View on Optimal Filtering in an RKHS","summary":"  Kernel Adaptive Filtering (KAF) are mathematically principled methods which\nsearch for a function in a Reproducing Kernel Hilbert Space. While they work\nwell for tasks such as time series prediction and system identification they\nare plagued by a linear relationship between number of training samples and\nmodel size, hampering their use on the very large data sets common in today's\ndata saturated world. Previous methods try to solve this issue by\nsparsification. We describe a novel view of optimal filtering which may provide\na route towards solutions in a RKHS which do not necessarily have this linear\ngrowth in model size. We do this by defining a RKHS in which the time structure\nof a stochastic process is still present. Using correntropy [11], an extension\nof the idea of a covariance function, we create a time based functional which\ndescribes some potentially nonlinear desired mapping function. This form of a\nsolution may provide a fruitful line of research for creating more efficient\nrepresentations of functionals in a RKHS, while theoretically providing\ncomputational complexity in the test set similar to Wiener solution.\n","authors":["Benjamin Colburn","Jose C. Principe","Luis G. Sanchez Giraldo"],"pdf_url":"https://arxiv.org/pdf/2312.12318v1.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2312.12315v1","updated":"2023-12-19T16:39:32Z","published":"2023-12-19T16:39:32Z","title":"Celestial Machine Learning: Discovering the Planarity, Heliocentricity,\n  and Orbital Equation of Mars with AI Feynman","summary":"  Can a machine or algorithm discover or learn the elliptical orbit of Mars\nfrom astronomical sightings alone? Johannes Kepler required two paradigm shifts\nto discover his First Law regarding the elliptical orbit of Mars. Firstly, a\nshift from the geocentric to the heliocentric frame of reference. Secondly, the\nreduction of the orbit of Mars from a three- to a two-dimensional space. We\nextend AI Feynman, a physics-inspired tool for symbolic regression, to discover\nthe heliocentricity and planarity of Mars' orbit and emulate his discovery of\nKepler's first law.\n","authors":["Zi-Yu Khoo","Gokul Rajiv","Abel Yang","Jonathan Sze Choong Low","Stéphane Bressan"],"pdf_url":"https://arxiv.org/pdf/2312.12315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09775v2","updated":"2023-12-19T16:35:56Z","published":"2023-12-15T13:28:42Z","title":"A Comparative Evaluation of Additive Separability Tests for\n  Physics-Informed Machine Learning","summary":"  Many functions characterising physical systems are additively separable. This\nis the case, for instance, of mechanical Hamiltonian functions in physics,\npopulation growth equations in biology, and consumer preference and utility\nfunctions in economics. We consider the scenario in which a surrogate of a\nfunction is to be tested for additive separability. The detection that the\nsurrogate is additively separable can be leveraged to improve further learning.\nHence, it is beneficial to have the ability to test for such separability in\nsurrogates. The mathematical approach is to test if the mixed partial\nderivative of the surrogate is zero; or empirically, lower than a threshold. We\npresent and comparatively and empirically evaluate the eight methods to compute\nthe mixed partial derivative of a surrogate function.\n","authors":["Zi-Yu Khoo","Jonathan Sze Choong Low","Stéphane Bressan"],"pdf_url":"https://arxiv.org/pdf/2312.09775v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.12756v2","updated":"2023-12-19T16:17:04Z","published":"2022-09-21T08:28:43Z","title":"FAL-CUR: Fair Active Learning using Uncertainty and Representativeness\n  on Fair Clustering","summary":"  Active Learning (AL) techniques have proven to be highly effective in\nreducing data labeling costs across a range of machine learning tasks.\nNevertheless, one known challenge of these methods is their potential to\nintroduce unfairness towards sensitive attributes. Although recent approaches\nhave focused on enhancing fairness in AL, they tend to reduce the model's\naccuracy. To address this issue, we propose a novel strategy, named Fair Active\nLearning using fair Clustering, Uncertainty, and Representativeness (FAL-CUR),\nto improve fairness in AL. FAL-CUR tackles the fairness problem in AL by\ncombining fair clustering with an acquisition function that determines which\nsamples to query based on their uncertainty and representativeness scores. We\nevaluate the performance of FAL-CUR on four real-world datasets, and the\nresults demonstrate that FAL-CUR achieves a 15% - 20% improvement in fairness\ncompared to the best state-of-the-art method in terms of equalized odds while\nmaintaining stable accuracy scores. Furthermore, an ablation study highlights\nthe crucial roles of fair clustering in preserving fairness and the acquisition\nfunction in stabilizing the accuracy performance.\n","authors":["Ricky Fajri","Akrati Saxena","Yulong Pei","Mykola Pechenizkiy"],"pdf_url":"https://arxiv.org/pdf/2209.12756v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14743v5","updated":"2023-12-19T16:05:51Z","published":"2023-11-21T18:41:26Z","title":"A Baseline Analysis of Reward Models' Ability To Accurately Analyze\n  Foundation Models Under Distribution Shift","summary":"  Foundation models, specifically Large Language Models (LLM's), have lately\ngained wide-spread attention and adoption. Reinforcement Learning with Human\nFeedback (RLHF) involves training a reward model to capture desired behaviors,\nwhich is then used to align LLM's. These reward models are additionally used at\ninference-time to estimate LLM responses' adherence to those desired behaviors.\nHowever, there is little work measuring how robust these reward models are to\ndistribution shifts. In this work, we evaluate how reward model performance -\nmeasured via accuracy and calibration (i.e. alignment between accuracy and\nconfidence) - is affected by distribution shift. We show novel calibration\npatterns and accuracy drops due to OOD prompts and responses, and that the\nreward model is more sensitive to shifts in responses than prompts.\nAdditionally, we adapt an OOD detection technique commonly used in\nclassification to the reward model setting to detect these distribution shifts\nin prompts and responses.\n","authors":["Will LeVine","Ben Pikus","Tony Chen","Sean Hendryx"],"pdf_url":"https://arxiv.org/pdf/2311.14743v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12276v1","updated":"2023-12-19T15:57:37Z","published":"2023-12-19T15:57:37Z","title":"Prompt-based Domain Discrimination for Multi-source Time Series Domain\n  Adaptation","summary":"  Time series domain adaptation stands as a pivotal and intricate challenge\nwith diverse applications, including but not limited to human activity\nrecognition, sleep stage classification, and machine fault diagnosis. Despite\nthe numerous domain adaptation techniques proposed to tackle this complex\nproblem, their primary focus has been on the common representations of time\nseries data. This concentration might inadvertently lead to the oversight of\nvaluable domain-specific information originating from different source domains.\nTo bridge this gap, we introduce POND, a novel prompt-based deep learning model\ndesigned explicitly for multi-source time series domain adaptation. POND is\ntailored to address significant challenges, notably: 1) The unavailability of a\nquantitative relationship between meta-data information and time series\ndistributions, and 2) The dearth of exploration into extracting domain-specific\nmeta-data information. In this paper, we present an instance-level prompt\ngenerator and a fidelity loss mechanism to facilitate the faithful learning of\nmeta-data information. Additionally, we propose a domain discrimination\ntechnique to discern domain-specific meta-data information from multiple source\ndomains. Our approach involves a simple yet effective meta-learning algorithm\nto optimize the objective efficiently. Furthermore, we augment the model's\nperformance by incorporating the Mixture of Expert (MoE) technique. The\nefficacy and robustness of our proposed POND model are extensively validated\nthrough experiments across 50 scenarios encompassing five datasets, which\ndemonstrates that our proposed POND model outperforms the state-of-the-art\nmethods by up to $66\\%$ on the F1-score.\n","authors":["Junxiang Wang","Guangji Bai","Wei Cheng","Zhengzhang Chen","Liang Zhao","Haifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2312.12276v1.pdf","comment":"Undergoing work"},{"id":"http://arxiv.org/abs/2312.12275v1","updated":"2023-12-19T15:56:30Z","published":"2023-12-19T15:56:30Z","title":"Emergence of In-Context Reinforcement Learning from Noise Distillation","summary":"  In-Context Reinforcement Learning is an emerging field with great potential\nfor advancing Artificial Intelligence. Its core capability lies in generalizing\nto unseen tasks through interaction with the environment. To master these\ncapabilities, an agent must be trained on specifically curated data that\nincludes a policy improvement that an algorithm seeks to extract and then apply\nin context in the environment. However, for numerous tasks, training RL agents\nmay be unfeasible, while obtaining human demonstrations can be relatively easy.\nAdditionally, it is rare to be given the optimal policy, typically, only\nsuboptimal demonstrations are available. We propose $AD^{\\epsilon}$, a method\nthat leverages demonstrations without policy improvement and enables multi-task\nin-context learning in the presence of a suboptimal demonstrator. This is\nachieved by artificially creating a history of incremental improvement, wherein\nnoise is systematically introduced into the demonstrator's policy.\nConsequently, each successive transition illustrates a marginally better\ntrajectory than the previous one. Our approach was tested on the Dark Room and\nDark Key-to-Door environments, resulting in over a $\\textbf{2}$x improvement\ncompared to the best available policy in the data.\n","authors":["Ilya Zisman","Vladislav Kurenkov","Alexander Nikulin","Viacheslav Sinii","Sergey Kolesnikov"],"pdf_url":"https://arxiv.org/pdf/2312.12275v1.pdf","comment":"Preprint, work in progress"},{"id":"http://arxiv.org/abs/2312.10237v2","updated":"2023-12-19T15:44:40Z","published":"2023-12-15T22:09:04Z","title":"Vertical Federated Alzheimer's Detection on Multimodal Data","summary":"  In the era of rapidly advancing medical technologies, the segmentation of\nmedical data has become inevitable, necessitating the development of privacy\npreserving machine learning algorithms that can train on distributed data.\nConsolidating sensitive medical data is not always an option particularly due\nto the stringent privacy regulations imposed by the Health Insurance\nPortability and Accountability Act (HIPAA). In this paper, we introduce a HIPAA\ncompliant framework that can train from distributed data. We then propose a\nmultimodal vertical federated model for Alzheimer's Disease (AD) detection, a\nserious neurodegenerative condition that can cause dementia, severely impairing\nbrain function and hindering simple tasks, especially without preventative\ncare. This vertical federated model offers a distributed architecture that\nenables collaborative learning across diverse sources of medical data while\nrespecting privacy constraints imposed by HIPAA. It is also able to leverage\nmultiple modalities of data, enhancing the robustness and accuracy of AD\ndetection. Our proposed model not only contributes to the advancement of\nfederated learning techniques but also holds promise for overcoming the hurdles\nposed by data segmentation in medical research. By using vertical federated\nlearning, this research strives to provide a framework that enables healthcare\ninstitutions to harness the collective intelligence embedded in their\ndistributed datasets without compromising patient privacy.\n","authors":["Paul K. Mandal"],"pdf_url":"https://arxiv.org/pdf/2312.10237v2.pdf","comment":"14 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2312.12258v1","updated":"2023-12-19T15:43:50Z","published":"2023-12-19T15:43:50Z","title":"Inferring the relationship between soil temperature and the normalized\n  difference vegetation index with machine learning","summary":"  Changes in climate can greatly affect the phenology of plants, which can have\nimportant feedback effects, such as altering the carbon cycle. These\nphenological feedback effects are often induced by a shift in the start or end\ndates of the growing season of plants. The normalized difference vegetation\nindex (NDVI) serves as a straightforward indicator for assessing the presence\nof green vegetation and can also provide an estimation of the plants' growing\nseason. In this study, we investigated the effect of soil temperature on the\ntiming of the start of the season (SOS), timing of the peak of the season\n(POS), and the maximum annual NDVI value (PEAK) in subarctic grassland\necosystems between 2014 and 2019. We also explored the impact of other\nmeteorological variables, including air temperature, precipitation, and\nirradiance, on the inter-annual variation in vegetation phenology. Using\nmachine learning (ML) techniques and SHapley Additive exPlanations (SHAP)\nvalues, we analyzed the relative importance and contribution of each variable\nto the phenological predictions. Our results reveal a significant relationship\nbetween soil temperature and SOS and POS, indicating that higher soil\ntemperatures lead to an earlier start and peak of the growing season. However,\nthe Peak NDVI values showed just a slight increase with higher soil\ntemperatures. The analysis of other meteorological variables demonstrated their\nimpacts on the inter-annual variation of the vegetation phenology. Ultimately,\nthis study contributes to our knowledge of the relationships between soil\ntemperature, meteorological variables, and vegetation phenology, providing\nvaluable insights for predicting vegetation phenology characteristics and\nmanaging subarctic grasslands in the face of climate change. Additionally, this\nwork provides a solid foundation for future ML-based vegetation phenology\nstudies.\n","authors":["Steven Mortier","Amir Hamedpour","Bart Bussmann","Ruth Phoebe Tchana Wandji","Steven Latré","Bjarni D. Sigurdsson","Tom De Schepper","Tim Verdonck"],"pdf_url":"https://arxiv.org/pdf/2312.12258v1.pdf","comment":"31 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2312.12255v1","updated":"2023-12-19T15:39:09Z","published":"2023-12-19T15:39:09Z","title":"TaskFlex Solver for Multi-Agent Pursuit via Automatic Curriculum\n  Learning","summary":"  This paper addresses the problem of multi-agent pursuit, where slow pursuers\ncooperate to capture fast evaders in a confined environment with obstacles.\nExisting heuristic algorithms often lack expressive coordination strategies and\nare highly sensitive to task conditions, requiring extensive hyperparameter\ntuning. In contrast, reinforcement learning (RL) has been applied to this\nproblem and is capable of obtaining cooperative pursuit strategies. However,\nRL-based methods face challenges in training for complex scenarios due to the\nvast amount of training data and limited adaptability to varying task\nconditions, such as different scene sizes, varying numbers and speeds of\nobstacles, and flexible speed ratios of the evader to the pursuer. In this\nwork, we combine RL and curriculum learning to introduce a flexible solver for\nmultiagent pursuit problems, named TaskFlex Solver (TFS), which is capable of\nsolving multi-agent pursuit problems with diverse and dynamically changing task\nconditions in both 2-dimensional and 3-dimensional scenarios. TFS utilizes a\ncurriculum learning method that constructs task distributions based on training\nprogress, enhancing training efficiency and final performance. Our algorithm\nconsists of two main components: the Task Evaluator, which evaluates task\nsuccess rates and selects tasks of moderate difficulty to maintain a curriculum\narchive, and the Task Sampler, which constructs training distributions by\nsampling tasks from the curriculum archive to maximize policy improvement.\nExperiments show that TFS produces much stronger performance than baselines and\nachieves close to 100% capture rates in both 2-dimensional and 3-dimensional\nmulti-agent pursuit problems with diverse and dynamically changing scenes. The\nproject website is at https://sites.google.com/view/tfs-2023.\n","authors":["Jiayu Chen","Guosheng Li","Chao Yu","Xinyi Yang","Botian Xu","Huazhong Yang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2312.12255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.06154v3","updated":"2023-12-19T15:37:22Z","published":"2023-06-09T14:49:20Z","title":"HypLL: The Hyperbolic Learning Library","summary":"  Deep learning in hyperbolic space is quickly gaining traction in the fields\nof machine learning, multimedia, and computer vision. Deep networks commonly\noperate in Euclidean space, implicitly assuming that data lies on regular\ngrids. Recent advances have shown that hyperbolic geometry provides a viable\nalternative foundation for deep learning, especially when data is hierarchical\nin nature and when working with few embedding dimensions. Currently however, no\naccessible open-source library exists to build hyperbolic network modules akin\nto well-known deep learning libraries. We present HypLL, the Hyperbolic\nLearning Library to bring the progress on hyperbolic deep learning together.\nHypLL is built on top of PyTorch, with an emphasis in its design for\nease-of-use, in order to attract a broad audience towards this new and\nopen-ended research direction. The code is available at:\nhttps://github.com/maxvanspengler/hyperbolic_learning_library.\n","authors":["Max van Spengler","Philipp Wirth","Pascal Mettes"],"pdf_url":"https://arxiv.org/pdf/2306.06154v3.pdf","comment":"ACM Multimedia Open-Source Software Competition 2023"},{"id":"http://arxiv.org/abs/2312.12246v1","updated":"2023-12-19T15:30:10Z","published":"2023-12-19T15:30:10Z","title":"MDD-UNet: Domain Adaptation for Medical Image Segmentation with\n  Theoretical Guarantees, a Proof of Concept","summary":"  The current state-of-the art techniques for image segmentation are often\nbased on U-Net architectures, a U-shaped encoder-decoder networks with skip\nconnections. Despite the powerful performance, the architecture often does not\nperform well when used on data which has different characteristics than the\ndata it was trained on. Many techniques for improving performance in the\npresence of domain shift have been developed, however typically only have loose\nconnections to the theory of domain adaption. In this work, we propose an\nunsupervised domain adaptation framework for U-Nets with theoretical guarantees\nbased on the Margin Disparity Discrepancy [1] called the MDD-UNet. We evaluate\nthe proposed technique on the task of hippocampus segmentation, and find that\nthe MDD-UNet is able to learn features which are domain-invariant with no\nknowledge about the labels in the target domain. The MDD-UNet improves\nperformance over the standard U-Net on 11 out of 12 combinations of datasets.\nThis work serves as a proof of concept by demonstrating an improvement on the\nU-Net in it's standard form without modern enhancements, which opens up a new\navenue of studying domain adaptation for models with very large hypothesis\nspaces from both methodological and practical perspectives. Code is available\nat https://github.com/asbjrnmunk/mdd-unet.\n","authors":["Asbjørn Munk","Ao Ma","Mads Nielsen"],"pdf_url":"https://arxiv.org/pdf/2312.12246v1.pdf","comment":"Published at NLDL 2024"},{"id":"http://arxiv.org/abs/2307.05152v2","updated":"2023-12-19T15:24:58Z","published":"2023-07-11T10:17:57Z","title":"Fast Neural Network Inference on FPGAs for Triggering on Long-Lived\n  Particles at Colliders","summary":"  Experimental particle physics demands a sophisticated trigger and acquisition\nsystem capable to efficiently retain the collisions of interest for further\ninvestigation. Heterogeneous computing with the employment of FPGA cards may\nemerge as a trending technology for the triggering strategy of the upcoming\nhigh-luminosity program of the Large Hadron Collider at CERN. In this context,\nwe present two machine-learning algorithms for selecting events where neutral\nlong-lived particles decay within the detector volume studying their accuracy\nand inference time when accelerated on commercially available Xilinx FPGA\naccelerator cards. The inference time is also confronted with a CPU- and\nGPU-based hardware setup. The proposed new algorithms are proven efficient for\nthe considered benchmark physics scenario and their accuracy is found to not\ndegrade when accelerated on the FPGA cards. The results indicate that all\ntested architectures fit within the latency requirements of a second-level\ntrigger farm and that exploiting accelerator technologies for real-time\nprocessing of particle-physics collisions is a promising research field that\ndeserves additional investigations, in particular with machine-learning models\nwith a large number of trainable parameters.\n","authors":["Andrea Coccaro","Francesco Armando Di Bello","Stefano Giagu","Lucrezia Rambelli","Nicola Stocchetti"],"pdf_url":"https://arxiv.org/pdf/2307.05152v2.pdf","comment":"12 pages, 10 figures, 2 tables"},{"id":"http://arxiv.org/abs/2312.12237v1","updated":"2023-12-19T15:22:37Z","published":"2023-12-19T15:22:37Z","title":"Roll With the Punches: Expansion and Shrinkage of Soft Label Selection\n  for Semi-supervised Fine-Grained Learning","summary":"  While semi-supervised learning (SSL) has yielded promising results, the more\nrealistic SSL scenario remains to be explored, in which the unlabeled data\nexhibits extremely high recognition difficulty, e.g., fine-grained visual\nclassification in the context of SSL (SS-FGVC). The increased recognition\ndifficulty on fine-grained unlabeled data spells disaster for pseudo-labeling\naccuracy, resulting in poor performance of the SSL model. To tackle this\nchallenge, we propose Soft Label Selection with Confidence-Aware Clustering\nbased on Class Transition Tracking (SoC) by reconstructing the pseudo-label\nselection process by jointly optimizing Expansion Objective and Shrinkage\nObjective, which is based on a soft label manner. Respectively, the former\nobjective encourages soft labels to absorb more candidate classes to ensure the\nattendance of ground-truth class, while the latter encourages soft labels to\nreject more noisy classes, which is theoretically proved to be equivalent to\nentropy minimization. In comparisons with various state-of-the-art methods, our\napproach demonstrates its superior performance in SS-FGVC. Checkpoints and\nsource code are available at https://github.com/NJUyued/SoC4SS-FGVC.\n","authors":["Yue Duan","Zhen Zhao","Lei Qi","Luping Zhou","Lei Wang","Yinghuan Shi"],"pdf_url":"https://arxiv.org/pdf/2312.12237v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2305.14978v2","updated":"2023-12-19T15:21:24Z","published":"2023-05-24T10:13:13Z","title":"Probabilistic Exponential Integrators","summary":"  Probabilistic solvers provide a flexible and efficient framework for\nsimulation, uncertainty quantification, and inference in dynamical systems.\nHowever, like standard solvers, they suffer performance penalties for certain\nstiff systems, where small steps are required not for reasons of numerical\naccuracy but for the sake of stability. This issue is greatly alleviated in\nsemi-linear problems by the probabilistic exponential integrators developed in\nthis paper. By including the fast, linear dynamics in the prior, we arrive at a\nclass of probabilistic integrators with favorable properties. Namely, they are\nproven to be L-stable, and in a certain case reduce to a classic exponential\nintegrator -- with the added benefit of providing a probabilistic account of\nthe numerical error. The method is also generalized to arbitrary non-linear\nsystems by imposing piece-wise semi-linearity on the prior via Jacobians of the\nvector field at the previous estimates, resulting in probabilistic exponential\nRosenbrock methods. We evaluate the proposed methods on multiple stiff\ndifferential equations and demonstrate their improved stability and efficiency\nover established probabilistic solvers. The present contribution thus expands\nthe range of problems that can be effectively tackled within probabilistic\nnumerics.\n","authors":["Nathanael Bosch","Philipp Hennig","Filip Tronarp"],"pdf_url":"https://arxiv.org/pdf/2305.14978v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12236v1","updated":"2023-12-19T15:20:27Z","published":"2023-12-19T15:20:27Z","title":"Generalization Analysis of Machine Learning Algorithms via the\n  Worst-Case Data-Generating Probability Measure","summary":"  In this paper, the worst-case probability measure over the data is introduced\nas a tool for characterizing the generalization capabilities of machine\nlearning algorithms. More specifically, the worst-case probability measure is a\nGibbs probability measure and the unique solution to the maximization of the\nexpected loss under a relative entropy constraint with respect to a reference\nprobability measure. Fundamental generalization metrics, such as the\nsensitivity of the expected loss, the sensitivity of the empirical risk, and\nthe generalization gap are shown to have closed-form expressions involving the\nworst-case data-generating probability measure. Existing results for the Gibbs\nalgorithm, such as characterizing the generalization gap as a sum of mutual\ninformation and lautum information, up to a constant factor, are recovered. A\nnovel parallel is established between the worst-case data-generating\nprobability measure and the Gibbs algorithm. Specifically, the Gibbs\nprobability measure is identified as a fundamental commonality of the model\nspace and the data space for machine learning algorithms.\n","authors":["Xinying Zou","Samir M. Perlaza","Iñaki Esnaola","Eitan Altman"],"pdf_url":"https://arxiv.org/pdf/2312.12236v1.pdf","comment":"To appear in the Proceedings of the AAAI Conference on Artificial\n  Intelligence (7 + 2 pages)"},{"id":"http://arxiv.org/abs/2312.12230v1","updated":"2023-12-19T15:15:52Z","published":"2023-12-19T15:15:52Z","title":"It's All in the Mix: Wasserstein Machine Learning with Mixed Features","summary":"  Problem definition: The recent advent of data-driven and end-to-end\ndecision-making across different areas of operations management has led to an\never closer integration of prediction models from machine learning and\noptimization models from operations research. A key challenge in this context\nis the presence of estimation errors in the prediction models, which tend to be\namplified by the subsequent optimization model -- a phenomenon that is often\nreferred to as the Optimizer's Curse or the Error-Maximization Effect of\nOptimization.\n  Methodology/results: A contemporary approach to combat such estimation errors\nis offered by distributionally robust problem formulations that consider all\ndata-generating distributions close to the empirical distribution derived from\nhistorical samples, where `closeness' is determined by the Wasserstein\ndistance. While those techniques show significant promise in problems where all\ninput features are continuous, they scale exponentially when binary and/or\ncategorical features are present. This paper demonstrates that such\nmixed-feature problems can indeed be solved in polynomial time. We present a\npractically efficient algorithm to solve mixed-feature problems, and we compare\nour method against alternative techniques both theoretically and empirically on\nstandard benchmark instances.\n  Managerial implications: Data-driven operations management problems often\ninvolve prediction models with discrete features. We develop and analyze a\nmethodology that faithfully accounts for the presence of discrete features, and\nwe demonstrate that our approach can significantly outperform existing methods\nthat are agnostic to the presence of discrete features, both theoretically and\nacross standard benchmark instances.\n","authors":["Reza Belbasi","Aras Selvi","Wolfram Wiesemann"],"pdf_url":"https://arxiv.org/pdf/2312.12230v1.pdf","comment":"48 pages (31 main + proofs), 7 tables, 2 colored plots, an early\n  version appeared in NeurIPS 2022 main track (arXiv 2205.13501)"},{"id":"http://arxiv.org/abs/2305.14160v4","updated":"2023-12-19T15:13:52Z","published":"2023-05-23T15:26:20Z","title":"Label Words are Anchors: An Information Flow Perspective for\n  Understanding In-Context Learning","summary":"  In-context learning (ICL) emerges as a promising capability of large language\nmodels (LLMs) by providing them with demonstration examples to perform diverse\ntasks. However, the underlying mechanism of how LLMs learn from the provided\ncontext remains under-explored. In this paper, we investigate the working\nmechanism of ICL through an information flow lens. Our findings reveal that\nlabel words in the demonstration examples function as anchors: (1) semantic\ninformation aggregates into label word representations during the shallow\ncomputation layers' processing; (2) the consolidated information in label words\nserves as a reference for LLMs' final predictions. Based on these insights, we\nintroduce an anchor re-weighting method to improve ICL performance, a\ndemonstration compression technique to expedite inference, and an analysis\nframework for diagnosing ICL errors in GPT2-XL. The promising applications of\nour findings again validate the uncovered ICL working mechanism and pave the\nway for future studies.\n","authors":["Lean Wang","Lei Li","Damai Dai","Deli Chen","Hao Zhou","Fandong Meng","Jie Zhou","Xu Sun"],"pdf_url":"https://arxiv.org/pdf/2305.14160v4.pdf","comment":"Accepted by EMNLP 2023"},{"id":"http://arxiv.org/abs/2312.12226v1","updated":"2023-12-19T15:12:39Z","published":"2023-12-19T15:12:39Z","title":"On the Parameterization of Second-Order Optimization Effective Towards\n  the Infinite Width","summary":"  Second-order optimization has been developed to accelerate the training of\ndeep neural networks and it is being applied to increasingly larger-scale\nmodels. In this study, towards training on further larger scales, we identify a\nspecific parameterization for second-order optimization that promotes feature\nlearning in a stable manner even if the network width increases significantly.\nInspired by a maximal update parameterization, we consider a one-step update of\nthe gradient and reveal the appropriate scales of hyperparameters including\nrandom initialization, learning rates, and damping terms. Our approach covers\ntwo major second-order optimization algorithms, K-FAC and Shampoo, and we\ndemonstrate that our parameterization achieves higher generalization\nperformance in feature learning. In particular, it enables us to transfer the\nhyperparameters across models with different widths.\n","authors":["Satoki Ishikawa","Ryo Karakida"],"pdf_url":"https://arxiv.org/pdf/2312.12226v1.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2305.17205v2","updated":"2023-12-19T15:12:37Z","published":"2023-05-26T18:53:35Z","title":"Ghost Noise for Regularizing Deep Neural Networks","summary":"  Batch Normalization (BN) is widely used to stabilize the optimization process\nand improve the test performance of deep neural networks. The regularization\neffect of BN depends on the batch size and explicitly using smaller batch sizes\nwith Batch Normalization, a method known as Ghost Batch Normalization (GBN),\nhas been found to improve generalization in many settings. We investigate the\neffectiveness of GBN by disentangling the induced ``Ghost Noise'' from\nnormalization and quantitatively analyzing the distribution of noise as well as\nits impact on model performance. Inspired by our analysis, we propose a new\nregularization technique called Ghost Noise Injection (GNI) that imitates the\nnoise in GBN without incurring the detrimental train-test discrepancy effects\nof small batch training. We experimentally show that GNI can provide a greater\ngeneralization benefit than GBN. Ghost Noise Injection can also be beneficial\nin otherwise non-noisy settings such as layer-normalized networks, providing\nadditional evidence of the usefulness of Ghost Noise in Batch Normalization as\na regularizer.\n","authors":["Atli Kosson","Dongyang Fan","Martin Jaggi"],"pdf_url":"https://arxiv.org/pdf/2305.17205v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12223v1","updated":"2023-12-19T15:11:46Z","published":"2023-12-19T15:11:46Z","title":"Self-Supervised Detection of Perfect and Partial Input-Dependent\n  Symmetries","summary":"  Group equivariance ensures consistent responses to group transformations of\nthe input, leading to more robust models and enhanced generalization\ncapabilities. However, this property can lead to overly constrained models if\nthe symmetries considered in the group differ from those observed in data.\nWhile common methods address this by determining the appropriate level of\nsymmetry at the dataset level, they are limited to supervised settings and\nignore scenarios in which multiple levels of symmetry co-exist in the same\ndataset. For instance, pictures of cars and planes exhibit different levels of\nrotation, yet both are included in the CIFAR-10 dataset. In this paper, we\npropose a method able to detect the level of symmetry of each input without the\nneed for labels. To this end, we derive a sufficient and necessary condition to\nlearn the distribution of symmetries in the data. Using the learned\ndistribution, we generate pseudo-labels that allow us to learn the levels of\nsymmetry of each input in a self-supervised manner. We validate the\neffectiveness of our approach on synthetic datasets with different per-class\nlevels of symmetries e.g. MNISTMultiple, in which digits are uniformly rotated\nwithin a class-dependent interval. We demonstrate that our method can be used\nfor practical applications such as the generation of standardized datasets in\nwhich the symmetries are not present, as well as the detection of\nout-of-distribution symmetries during inference. By doing so, both the\ngeneralization and robustness of non-equivariant models can be improved. Our\ncode is publicly available at https://github.com/aurban0/ssl-sym.\n","authors":["Alonso Urbano","David W. Romero"],"pdf_url":"https://arxiv.org/pdf/2312.12223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12216v1","updated":"2023-12-19T15:05:52Z","published":"2023-12-19T15:05:52Z","title":"Sharing is CAIRing: Characterizing Principles and Assessing Properties\n  of Universal Privacy Evaluation for Synthetic Tabular Data","summary":"  Data sharing is a necessity for innovative progress in many domains,\nespecially in healthcare. However, the ability to share data is hindered by\nregulations protecting the privacy of natural persons. Synthetic tabular data\nprovide a promising solution to address data sharing difficulties but does not\ninherently guarantee privacy. Still, there is a lack of agreement on\nappropriate methods for assessing the privacy-preserving capabilities of\nsynthetic data, making it difficult to compare results across studies. To the\nbest of our knowledge, this is the first work to identify properties that\nconstitute good universal privacy evaluation metrics for synthetic tabular\ndata. The goal of such metrics is to enable comparability across studies and to\nallow non-technical stakeholders to understand how privacy is protected. We\nidentify four principles for the assessment of metrics: Comparability,\nApplicability, Interpretability, and Representativeness (CAIR). To quantify and\nrank the degree to which evaluation metrics conform to the CAIR principles, we\ndesign a rubric using a scale of 1-4. Each of the four properties is scored on\nfour parameters, yielding 16 total dimensions. We study the applicability and\nusefulness of the CAIR principles and rubric by assessing a selection of\nmetrics popular in other studies. The results provide granular insights into\nthe strengths and weaknesses of existing metrics that not only rank the metrics\nbut highlight areas of potential improvements. We expect that the CAIR\nprinciples will foster agreement among researchers and organizations on which\nuniversal privacy evaluation metrics are appropriate for synthetic tabular\ndata.\n","authors":["Tobias Hyrup","Anton Danholt Lautrup","Arthur Zimek","Peter Schneider-Kamp"],"pdf_url":"https://arxiv.org/pdf/2312.12216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10090v4","updated":"2023-12-19T14:55:15Z","published":"2023-11-16T18:58:43Z","title":"JaxMARL: Multi-Agent RL Environments in JAX","summary":"  Benchmarks play an important role in the development of machine learning\nalgorithms. For example, research in reinforcement learning (RL) has been\nheavily influenced by available environments and benchmarks. However, RL\nenvironments are traditionally run on the CPU, limiting their scalability with\ntypical academic compute. Recent advancements in JAX have enabled the wider use\nof hardware acceleration to overcome these computational hurdles, enabling\nmassively parallel RL training pipelines and environments. This is particularly\nuseful for multi-agent reinforcement learning (MARL) research. First of all,\nmultiple agents must be considered at each environment step, adding\ncomputational burden, and secondly, the sample complexity is increased due to\nnon-stationarity, decentralised partial observability, or other MARL\nchallenges. In this paper, we present JaxMARL, the first open-source code base\nthat combines ease-of-use with GPU enabled efficiency, and supports a large\nnumber of commonly used MARL environments as well as popular baseline\nalgorithms. When considering wall clock time, our experiments show that per-run\nour JAX-based training pipeline is up to 12500x faster than existing\napproaches. This enables efficient and thorough evaluations, with the potential\nto alleviate the evaluation crisis of the field. We also introduce and\nbenchmark SMAX, a vectorised, simplified version of the popular StarCraft\nMulti-Agent Challenge, which removes the need to run the StarCraft II game\nengine. This not only enables GPU acceleration, but also provides a more\nflexible MARL environment, unlocking the potential for self-play,\nmeta-learning, and other future applications in MARL. We provide code at\nhttps://github.com/flairox/jaxmarl.\n","authors":["Alexander Rutherford","Benjamin Ellis","Matteo Gallici","Jonathan Cook","Andrei Lupu","Gardar Ingvarsson","Timon Willi","Akbir Khan","Christian Schroeder de Witt","Alexandra Souly","Saptarashmi Bandyopadhyay","Mikayel Samvelyan","Minqi Jiang","Robert Tjarko Lange","Shimon Whiteson","Bruno Lacerda","Nick Hawes","Tim Rocktaschel","Chris Lu","Jakob Nicolaus Foerster"],"pdf_url":"https://arxiv.org/pdf/2311.10090v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12206v1","updated":"2023-12-19T14:44:26Z","published":"2023-12-19T14:44:26Z","title":"Identification of Causal Structure in the Presence of Missing Data with\n  Additive Noise Model","summary":"  Missing data are an unavoidable complication frequently encountered in many\ncausal discovery tasks. When a missing process depends on the missing values\nthemselves (known as self-masking missingness), the recovery of the joint\ndistribution becomes unattainable, and detecting the presence of such\nself-masking missingness remains a perplexing challenge. Consequently, due to\nthe inability to reconstruct the original distribution and to discern the\nunderlying missingness mechanism, simply applying existing causal discovery\nmethods would lead to wrong conclusions. In this work, we found that the recent\nadvances additive noise model has the potential for learning causal structure\nunder the existence of the self-masking missingness. With this observation, we\naim to investigate the identification problem of learning causal structure from\nmissing data under an additive noise model with different missingness\nmechanisms, where the `no self-masking missingness' assumption can be\neliminated appropriately. Specifically, we first elegantly extend the scope of\nidentifiability of causal skeleton to the case with weak self-masking\nmissingness (i.e., no other variable could be the cause of self-masking\nindicators except itself). We further provide the sufficient and necessary\nidentification conditions of the causal direction under additive noise model\nand show that the causal structure can be identified up to an IN-equivalent\npattern. We finally propose a practical algorithm based on the above\ntheoretical results on learning the causal skeleton and causal direction.\nExtensive experiments on synthetic and real data demonstrate the efficiency and\neffectiveness of the proposed algorithms.\n","authors":["Jie Qiao","Zhengming Chen","Jianhua Yu","Ruichu Cai","Zhifeng Hao"],"pdf_url":"https://arxiv.org/pdf/2312.12206v1.pdf","comment":"Accepted by AAAI-2024"},{"id":"http://arxiv.org/abs/2210.01905v3","updated":"2023-12-19T14:39:40Z","published":"2022-10-04T20:56:24Z","title":"Polar Encoding: A Simple Baseline Approach for Classification with\n  Missing Values","summary":"  We propose polar encoding, a representation of categorical and numerical\n$[0,1]$-valued attributes with missing values to be used in a classification\ncontext. We argue that this is a good baseline approach, because it can be used\nwith any classification algorithm, preserves missingness information, is very\nsimple to apply and offers good performance. In particular, unlike the existing\nmissing-indicator approach, it does not require imputation, ensures that\nmissing values are equidistant from non-missing values, and lets decision tree\nalgorithms choose how to split missing values, thereby providing a practical\nrealisation of the \"missingness incorporated in attributes\" (MIA) proposal.\nFurthermore, we show that categorical and $[0,1]$-valued attributes can be\nviewed as special cases of a single attribute type, corresponding to the\nclassical concept of barycentric coordinates, and that this offers a natural\ninterpretation of polar encoding as a fuzzified form of one-hot encoding. With\nan experiment based on twenty real-life datasets with missing values, we show\nthat, in terms of the resulting classification performance, polar encoding\nperforms better than the state-of-the-art strategies \\e{multiple imputation by\nchained equations} (MICE) and \\e{multiple imputation with denoising\nautoencoders} (MIDAS) and -- depending on the classifier -- about as well or\nbetter than mean/mode imputation with missing-indicators.\n","authors":["Oliver Urs Lenz","Daniel Peralta","Chris Cornelis"],"pdf_url":"https://arxiv.org/pdf/2210.01905v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.06009v2","updated":"2023-12-19T14:29:53Z","published":"2022-06-13T09:55:04Z","title":"Relative Policy-Transition Optimization for Fast Policy Transfer","summary":"  We consider the problem of policy transfer between two Markov Decision\nProcesses (MDPs). We introduce a lemma based on existing theoretical results in\nreinforcement learning to measure the relativity gap between two arbitrary\nMDPs, that is the difference between any two cumulative expected returns\ndefined on different policies and environment dynamics. Based on this lemma, we\npropose two new algorithms referred to as Relative Policy Optimization (RPO)\nand Relative Transition Optimization (RTO), which offer fast policy transfer\nand dynamics modelling, respectively. RPO transfers the policy evaluated in one\nenvironment to maximize the return in another, while RTO updates the\nparameterized dynamics model to reduce the gap between the dynamics of the two\nenvironments. Integrating the two algorithms results in the complete Relative\nPolicy-Transition Optimization (RPTO) algorithm, in which the policy interacts\nwith the two environments simultaneously, such that data collections from two\nenvironments, policy and transition updates are completed in one closed loop to\nform a principled learning framework for policy transfer. We demonstrate the\neffectiveness of RPTO on a set of MuJoCo continuous control tasks by creating\npolicy transfer problems via variant dynamics.\n","authors":["Jiawei Xu","Cheng Zhou","Yizheng Zhang","Baoxiang Wang","Lei Han"],"pdf_url":"https://arxiv.org/pdf/2206.06009v2.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.12193v1","updated":"2023-12-19T14:27:26Z","published":"2023-12-19T14:27:26Z","title":"Gaussian process learning of nonlinear dynamics","summary":"  One of the pivotal tasks in scientific machine learning is to represent\nunderlying dynamical systems from time series data. Many methods for such\ndynamics learning explicitly require the derivatives of state data, which are\nnot directly available and can be approximated conventionally by finite\ndifferences. However, the discrete approximations of time derivatives may\nresult in a poor estimation when state data are scarce and/or corrupted by\nnoise, thus compromising the predictiveness of the learned dynamical models. To\novercome this technical hurdle, we propose a new method that learns nonlinear\ndynamics through a Bayesian inference of characterizing model parameters. This\nmethod leverages a Gaussian process representation of states, and constructs a\nlikelihood function using the correlation between state data and their\nderivatives, yet prevents explicit evaluations of time derivatives. Through a\nBayesian scheme, a probabilistic estimate of the model parameters is given by\nthe posterior distribution, and thus a quantification is facilitated for\nuncertainties from noisy state data and the learning process. Specifically, we\nwill discuss the applicability of the proposed method to two typical scenarios\nfor dynamical systems: parameter identification and estimation with an affine\nstructure of the system, and nonlinear parametric approximation without prior\nknowledge.\n","authors":["Dongwei Ye","Mengwu Guo"],"pdf_url":"https://arxiv.org/pdf/2312.12193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16454v3","updated":"2023-12-19T14:27:21Z","published":"2023-03-29T04:43:03Z","title":"Conductivity Imaging from Internal Measurements with Mixed Least-Squares\n  Deep Neural Networks","summary":"  In this work we develop a novel approach using deep neural networks to\nreconstruct the conductivity distribution in elliptic problems from one\nmeasurement of the solution over the whole domain. The approach is based on a\nmixed reformulation of the governing equation and utilizes the standard\nleast-squares objective, with deep neural networks as ansatz functions to\napproximate the conductivity and flux simultaneously. We provide a thorough\nanalysis of the deep neural network approximations of the conductivity for both\ncontinuous and empirical losses, including rigorous error estimates that are\nexplicit in terms of the noise level, various penalty parameters and neural\nnetwork architectural parameters (depth, width and parameter bound). We also\nprovide multiple numerical experiments in two- and multi-dimensions to\nillustrate distinct features of the approach, e.g., excellent stability with\nrespect to data noise and capability of solving high-dimensional problems.\n","authors":["Bangti Jin","Xiyao Li","Qimeng Quan","Zhi Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.16454v3.pdf","comment":"corrected a few typos"},{"id":"http://arxiv.org/abs/2312.12191v1","updated":"2023-12-19T14:26:23Z","published":"2023-12-19T14:26:23Z","title":"CUDC: A Curiosity-Driven Unsupervised Data Collection Method with\n  Adaptive Temporal Distances for Offline Reinforcement Learning","summary":"  Offline reinforcement learning (RL) aims to learn an effective policy from a\npre-collected dataset. Most existing works are to develop sophisticated\nlearning algorithms, with less emphasis on improving the data collection\nprocess. Moreover, it is even challenging to extend the single-task setting and\ncollect a task-agnostic dataset that allows an agent to perform multiple\ndownstream tasks. In this paper, we propose a Curiosity-driven Unsupervised\nData Collection (CUDC) method to expand feature space using adaptive temporal\ndistances for task-agnostic data collection and ultimately improve learning\nefficiency and capabilities for multi-task offline RL. To achieve this, CUDC\nestimates the probability of the k-step future states being reachable from the\ncurrent states, and adapts how many steps into the future that the dynamics\nmodel should predict. With this adaptive reachability mechanism in place, the\nfeature representation can be diversified, and the agent can navigate itself to\ncollect higher-quality data with curiosity. Empirically, CUDC surpasses\nexisting unsupervised methods in efficiency and learning performance in various\ndownstream offline RL tasks of the DeepMind control suite.\n","authors":["Chenyu Sun","Hangwei Qian","Chunyan Miao"],"pdf_url":"https://arxiv.org/pdf/2312.12191v1.pdf","comment":"Accepted at AAAI-24"},{"id":"http://arxiv.org/abs/2312.12190v1","updated":"2023-12-19T14:25:41Z","published":"2023-12-19T14:25:41Z","title":"Decentralised and collaborative machine learning framework for IoT","summary":"  Decentralised machine learning has recently been proposed as a potential\nsolution to the security issues of the canonical federated learning approach.\nIn this paper, we propose a decentralised and collaborative machine learning\nframework specially oriented to resource-constrained devices, usual in IoT\ndeployments. With this aim we propose the following construction blocks. First,\nan incremental learning algorithm based on prototypes that was specifically\nimplemented to work in low-performance computing elements. Second, two\nrandom-based protocols to exchange the local models among the computing\nelements in the network. Finally, two algorithmics approaches for prediction\nand prototype creation. This proposal was compared to a typical centralized\nincremental learning approach in terms of accuracy, training time and\nrobustness with very promising results.\n","authors":["Martín González-Soto","Rebeca P. Díaz-Redondo","Manuel Fernández-Veiga","Bruno Rodríguez-Castro","Ana Fernández-Vilas"],"pdf_url":"https://arxiv.org/pdf/2312.12190v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12183v1","updated":"2023-12-19T14:15:20Z","published":"2023-12-19T14:15:20Z","title":"Poincaré Differential Privacy for Hierarchy-aware Graph Embedding","summary":"  Hierarchy is an important and commonly observed topological property in\nreal-world graphs that indicate the relationships between supervisors and\nsubordinates or the organizational behavior of human groups. As hierarchy is\nintroduced as a new inductive bias into the Graph Neural Networks (GNNs) in\nvarious tasks, it implies latent topological relations for attackers to improve\ntheir inference attack performance, leading to serious privacy leakage issues.\nIn addition, existing privacy-preserving frameworks suffer from reduced\nprotection ability in hierarchical propagation due to the deficiency of\nadaptive upper-bound estimation of the hierarchical perturbation boundary. It\nis of great urgency to effectively leverage the hierarchical property of data\nwhile satisfying privacy guarantees. To solve the problem, we propose the\nPoincar\\'e Differential Privacy framework, named PoinDP, to protect the\nhierarchy-aware graph embedding based on hyperbolic geometry. Specifically,\nPoinDP first learns the hierarchy weights for each entity based on the\nPoincar\\'e model in hyperbolic space. Then, the Personalized Hierarchy-aware\nSensitivity is designed to measure the sensitivity of the hierarchical\nstructure and adaptively allocate the privacy protection strength. Besides, the\nHyperbolic Gaussian Mechanism (HGM) is proposed to extend the Gaussian\nmechanism in Euclidean space to hyperbolic space to realize random\nperturbations that satisfy differential privacy under the hyperbolic space\nmetric. Extensive experiment results on five real-world datasets demonstrate\nthe proposed PoinDP's advantages of effective privacy protection while\nmaintaining good performance on the node classification task.\n","authors":["Yuecen Wei","Haonan Yuan","Xingcheng Fu","Qingyun Sun","Hao Peng","Xianxian Li","Chunming Hu"],"pdf_url":"https://arxiv.org/pdf/2312.12183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17658v4","updated":"2023-12-19T14:14:44Z","published":"2023-10-18T15:24:34Z","title":"Is Channel Independent strategy optimal for Time Series Forecasting?","summary":"  There has been an emergence of various models for long-term time series\nforecasting. Recent studies have demonstrated that a single linear layer, using\nChannel Dependent (CD) or Channel Independent (CI) modeling, can even\noutperform a large number of sophisticated models. However, current research\nprimarily considers CD and CI as two complementary yet mutually exclusive\napproaches, unable to harness these two extremes simultaneously. And it is also\na challenging issue that both CD and CI are static strategies that cannot be\ndetermined to be optimal for a specific dataset without extensive experiments.\nIn this paper, we reconsider whether the current CI strategy is the best\nsolution for time series forecasting. First, we propose a simple yet effective\nstrategy called CSC, which stands for $\\mathbf{C}$hannel\n$\\mathbf{S}$elf-$\\mathbf{C}$lustering strategy, for linear models. Our Channel\nSelf-Clustering (CSC) enhances CI strategy's performance improvements while\nreducing parameter size, for exmpale by over 10 times on electricity dataset,\nand significantly cutting training time. Second, we further propose Channel\nRearrangement (CR), a method for deep models inspired by the self-clustering.\nCR attains competitive performance against baselines. Finally, we also discuss\nwhether it is best to forecast the future values using the historical values of\nthe same channel as inputs. We hope our findings and methods could inspire new\nsolutions beyond CD/CI.\n","authors":["Yuan Peiwen","Zhu Changsheng"],"pdf_url":"https://arxiv.org/pdf/2310.17658v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.15289v4","updated":"2023-12-19T14:14:22Z","published":"2023-09-26T21:56:03Z","title":"SEPT: Towards Efficient Scene Representation Learning for Motion\n  Prediction","summary":"  Motion prediction is crucial for autonomous vehicles to operate safely in\ncomplex traffic environments. Extracting effective spatiotemporal relationships\namong traffic elements is key to accurate forecasting. Inspired by the\nsuccessful practice of pretrained large language models, this paper presents\nSEPT, a modeling framework that leverages self-supervised learning to develop\npowerful spatiotemporal understanding for complex traffic scenes. Specifically,\nour approach involves three masking-reconstruction modeling tasks on scene\ninputs including agents' trajectories and road network, pretraining the scene\nencoder to capture kinematics within trajectory, spatial structure of road\nnetwork, and interactions among roads and agents. The pretrained encoder is\nthen finetuned on the downstream forecasting task. Extensive experiments\ndemonstrate that SEPT, without elaborate architectural design or manual feature\nengineering, achieves state-of-the-art performance on the Argoverse 1 and\nArgoverse 2 motion forecasting benchmarks, outperforming previous methods on\nall main metrics by a large margin.\n","authors":["Zhiqian Lan","Yuxuan Jiang","Yao Mu","Chen Chen","Shengbo Eben Li"],"pdf_url":"https://arxiv.org/pdf/2309.15289v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14901v2","updated":"2023-12-19T14:12:04Z","published":"2023-05-24T08:55:08Z","title":"Chain-of-Questions Training with Latent Answers for Robust Multistep\n  Question Answering","summary":"  We train a language model (LM) to robustly answer multistep questions by\ngenerating and answering sub-questions. We propose Chain-of-Questions, a\nframework that trains a model to generate sub-questions and sub-answers one at\na time by leveraging human annotated question decomposition meaning\nrepresentation (QDMR). The key technical challenge is that QDMR only contains\nsub-questions but not answers to those sub-questions, so we treat sub-answers\nas latent variables and optimize them using a novel dynamic mixture of Hard-EM\nand MAPO. Chain-of-Questions greatly outperforms strong neuro-symbolic methods\nby 9.0 F1 on DROP contrast set, and outperforms GPT-3.5 by 24.3 F1 on HOTPOTQA\nadversarial set, thus demonstrating the effectiveness and robustness of our\nframework.\n","authors":["Wang Zhu","Jesse Thomason","Robin Jia"],"pdf_url":"https://arxiv.org/pdf/2305.14901v2.pdf","comment":"Accepted by the EMNLP 2023"},{"id":"http://arxiv.org/abs/2312.10130v2","updated":"2023-12-19T14:08:10Z","published":"2023-12-15T15:53:10Z","title":"Improving new physics searches with diffusion models for event\n  observables and jet constituents","summary":"  We introduce a new technique called Drapes to enhance the sensitivity in\nsearches for new physics at the LHC. By training diffusion models on side-band\ndata, we show how background templates for the signal region can be generated\neither directly from noise, or by partially applying the diffusion process to\nexisting data. In the partial diffusion case, data can be drawn from side-band\nregions, with the inverse diffusion performed for new target conditional\nvalues, or from the signal region, preserving the distribution over the\nconditional property that defines the signal region. We apply this technique to\nthe hunt for resonances using the LHCO di-jet dataset, and achieve\nstate-of-the-art performance for background template generation using high\nlevel input features. We also show how Drapes can be applied to low level\ninputs with jet constituents, reducing the model dependence on the choice of\ninput observables. Using jet constituents we can further improve sensitivity to\nthe signal process, but observe a loss in performance where the signal\nsignificance before applying any selection is below 4$\\sigma$.\n","authors":["Debajyoti Sengupta","Matthew Leigh","John Andrew Raine","Samuel Klein","Tobias Golling"],"pdf_url":"https://arxiv.org/pdf/2312.10130v2.pdf","comment":"34 pages, 19 figures"},{"id":"http://arxiv.org/abs/2312.10194v2","updated":"2023-12-19T14:02:20Z","published":"2023-12-15T20:41:09Z","title":"Pareto Envelope Augmented with Reinforcement Learning: Multi-objective\n  reinforcement learning-based approach for Large-Scale Constrained Pressurized\n  Water Reactor optimization","summary":"  A novel method, the Pareto Envelope Augmented with Reinforcement Learning\n(PEARL), has been developed to address the challenges posed by multi-objective\nproblems, particularly in the field of engineering where the evaluation of\ncandidate solutions can be time-consuming. PEARL distinguishes itself from\ntraditional policy-based multi-objective Reinforcement Learning methods by\nlearning a single policy, eliminating the need for multiple neural networks to\nindependently solve simpler sub-problems. Several versions inspired from deep\nlearning and evolutionary techniques have been crafted, catering to both\nunconstrained and constrained problem domains. Curriculum Learning is harnessed\nto effectively manage constraints in these versions. PEARL's performance is\nfirst evaluated on classical multi-objective benchmarks. Additionally, it is\ntested on two practical PWR core Loading Pattern optimization problems to\nshowcase its real-world applicability. The first problem involves optimizing\nthe Cycle length and the rod-integrated peaking factor as the primary\nobjectives, while the second problem incorporates the mean average enrichment\nas an additional objective. Furthermore, PEARL addresses three types of\nconstraints related to boron concentration, peak pin burnup, and peak pin\npower. The results are systematically compared against a conventional approach,\nthe Non-dominated Sorting Genetic Algorithm. Notably, PEARL, specifically the\nPEARL-NdS variant, efficiently uncovers a Pareto front without necessitating\nadditional efforts from the algorithm designer, as opposed to a single\noptimization with scaled objectives. It also outperforms the classical approach\nacross multiple performance metrics, including the Hyper-volume.\n","authors":["Paul Seurin","Koroush Shirvan"],"pdf_url":"https://arxiv.org/pdf/2312.10194v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.04690v2","updated":"2023-12-19T13:41:47Z","published":"2023-08-09T03:56:07Z","title":"Finite Element Operator Network for Solving Parametric PDEs","summary":"  Partial differential equations (PDEs) underlie our understanding and\nprediction of natural phenomena across numerous fields, including physics,\nengineering, and finance. However, solving parametric PDEs is a complex task\nthat necessitates efficient numerical methods. In this paper, we propose a\nnovel approach for solving parametric PDEs using a Finite Element Operator\nNetwork (FEONet). Our proposed method leverages the power of deep learning in\nconjunction with traditional numerical methods, specifically the finite element\nmethod, to solve parametric PDEs in the absence of any paired input-output\ntraining data. We performed various experiments on several benchmark problems\nand confirmed that our approach has demonstrated excellent performance across\nvarious settings and environments, proving its versatility in terms of\naccuracy, generalization, and computational flexibility. Our FEONet framework\nshows potential for application in various fields where PDEs play a crucial\nrole in modeling complex domains with diverse boundary conditions and singular\nbehavior. Furthermore, we provide theoretical convergence analysis to support\nour approach, utilizing finite element approximation in numerical analysis.\n","authors":["Jae Yong Lee","Seungchan Ko","Youngjoon Hong"],"pdf_url":"https://arxiv.org/pdf/2308.04690v2.pdf","comment":"23 pages, 11 figures"},{"id":"http://arxiv.org/abs/2305.13030v4","updated":"2023-12-19T13:29:33Z","published":"2023-05-22T13:33:37Z","title":"Adaptive action supervision in reinforcement learning from real-world\n  multi-agent demonstrations","summary":"  Modeling of real-world biological multi-agents is a fundamental problem in\nvarious scientific and engineering fields. Reinforcement learning (RL) is a\npowerful framework to generate flexible and diverse behaviors in cyberspace;\nhowever, when modeling real-world biological multi-agents, there is a domain\ngap between behaviors in the source (i.e., real-world data) and the target\n(i.e., cyberspace for RL), and the source environment parameters are usually\nunknown. In this paper, we propose a method for adaptive action supervision in\nRL from real-world demonstrations in multi-agent scenarios. We adopt an\napproach that combines RL and supervised learning by selecting actions of\ndemonstrations in RL based on the minimum distance of dynamic time warping for\nutilizing the information of the unknown source dynamics. This approach can be\neasily applied to many existing neural network architectures and provide us\nwith an RL model balanced between reproducibility as imitation and\ngeneralization ability to obtain rewards in cyberspace. In the experiments,\nusing chase-and-escape and football tasks with the different dynamics between\nthe unknown source and target environments, we show that our approach achieved\na balance between the reproducibility and the generalization ability compared\nwith the baselines. In particular, we used the tracking data of professional\nfootball players as expert demonstrations in football and show successful\nperformances despite the larger gap between behaviors in the source and target\nenvironments than the chase-and-escape task.\n","authors":["Keisuke Fujii","Kazushi Tsutsui","Atom Scott","Hiroshi Nakahara","Naoya Takeishi","Yoshinobu Kawahara"],"pdf_url":"https://arxiv.org/pdf/2305.13030v4.pdf","comment":"14 pages, 5 figures, accepted in ICAART 2024 Oral"},{"id":"http://arxiv.org/abs/2312.12145v1","updated":"2023-12-19T13:28:34Z","published":"2023-12-19T13:28:34Z","title":"OVD-Explorer:Optimism Should Not Be the Sole Pursuit of Exploration in\n  Noisy Environments","summary":"  In reinforcement learning, the optimism in the face of uncertainty (OFU) is a\nmainstream principle for directing exploration towards less explored areas,\ncharacterized by higher uncertainty. However, in the presence of environmental\nstochasticity (noise), purely optimistic exploration may lead to excessive\nprobing of high-noise areas, consequently impeding exploration efficiency.\nHence, in exploring noisy environments, while optimism-driven exploration\nserves as a foundation, prudent attention to alleviating unnecessary\nover-exploration in high-noise areas becomes beneficial. In this work, we\npropose Optimistic Value Distribution Explorer (OVD-Explorer) to achieve a\nnoise-aware optimistic exploration for continuous control. OVD-Explorer\nproposes a new measurement of the policy's exploration ability considering\nnoise in optimistic perspectives, and leverages gradient ascent to drive\nexploration. Practically, OVD-Explorer can be easily integrated with continuous\ncontrol RL algorithms. Extensive evaluations on the MuJoCo and GridChaos tasks\ndemonstrate the superiority of OVD-Explorer in achieving noise-aware optimistic\nexploration.\n","authors":["Jinyi Liu","Zhi Wang","Yan Zheng","Jianye Hao","Chenjia Bai","Junjie Ye","Zhen Wang","Haiyin Piao","Yang Sun"],"pdf_url":"https://arxiv.org/pdf/2312.12145v1.pdf","comment":"Accepted by AAAI 2024, with appendix"},{"id":"http://arxiv.org/abs/2312.12141v1","updated":"2023-12-19T13:23:18Z","published":"2023-12-19T13:23:18Z","title":"Exploring the Residual Stream of Transformers","summary":"  Transformer-based models have achieved great breakthroughs in recent years.\nHowever, there are many significant questions that have not been answered in\nthe field of explaining the reason why the models have powerful outputs. We do\nnot know how to locate the models' important parameters storing the knowledge\nfor predicting the next word, and whether these parameters are stored on the\nsame layer/module or different ones. Moreover, we do not understand the\nmechanism to merge the knowledge into the final embedding for next word\nprediction. In this paper, we explore the residual stream of transformers to\nincrease the interpretability. We find the mechanism behind residual connection\nis a direct addition function on before-softmax values, so the probabilities of\ntokens with larger before-softmax values will increase. Moreover, we prove that\nusing log probability increase as contribution scores is reasonable, and based\non this we can locate important parameters. Besides, we propose a method to\nanalyze how previous layers affect upper layers by comparing the inner\nproducts. The experimental results and case study show that our research can\nincrease the interpretability of transformer-based models. We will release our\ncode on https://github.com/zepingyu0512/residualstream.\n","authors":["Zeping Yu","Kailai Yang","Zhiwei Liu","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2312.12141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12137v1","updated":"2023-12-19T13:17:43Z","published":"2023-12-19T13:17:43Z","title":"Best Arm Identification with Fixed Budget: A Large Deviation Perspective","summary":"  We consider the problem of identifying the best arm in stochastic Multi-Armed\nBandits (MABs) using a fixed sampling budget. Characterizing the minimal\ninstance-specific error probability for this problem constitutes one of the\nimportant remaining open problems in MABs. When arms are selected using a\nstatic sampling strategy, the error probability decays exponentially with the\nnumber of samples at a rate that can be explicitly derived via Large Deviation\ntechniques. Analyzing the performance of algorithms with adaptive sampling\nstrategies is however much more challenging. In this paper, we establish a\nconnection between the Large Deviation Principle (LDP) satisfied by the\nempirical proportions of arm draws and that satisfied by the empirical arm\nrewards. This connection holds for any adaptive algorithm, and is leveraged (i)\nto improve error probability upper bounds of some existing algorithms, such as\nthe celebrated \\sr (Successive Rejects) algorithm \\citep{audibert2010best}, and\n(ii) to devise and analyze new algorithms. In particular, we present \\sred\n(Continuous Rejects), a truly adaptive algorithm that can reject arms in {\\it\nany} round based on the observed empirical gaps between the rewards of various\narms. Applying our Large Deviation results, we prove that \\sred enjoys better\nperformance guarantees than existing algorithms, including \\sr. Extensive\nnumerical experiments confirm this observation.\n","authors":["Po-An Wang","Ruo-Chun Tzeng","Alexandre Proutiere"],"pdf_url":"https://arxiv.org/pdf/2312.12137v1.pdf","comment":"This work has been published in NeurIPS 2023"},{"id":"http://arxiv.org/abs/2312.12135v1","updated":"2023-12-19T13:14:52Z","published":"2023-12-19T13:14:52Z","title":"Object Detection for Automated Coronary Artery Using Deep Learning","summary":"  In the era of digital medicine, medical imaging serves as a widespread\ntechnique for early disease detection, with a substantial volume of images\nbeing generated and stored daily in electronic patient records. X-ray\nangiography imaging is a standard and one of the most common methods for\nrapidly diagnosing coronary artery diseases. The notable achievements of recent\ndeep learning algorithms align with the increased use of electronic health\nrecords and diagnostic imaging. Deep neural networks, leveraging abundant data,\nadvanced algorithms, and powerful computational capabilities, prove highly\neffective in the analysis and interpretation of images. In this context, Object\ndetection methods have become a promising approach, particularly through\nconvolutional neural networks (CNN), streamlining medical image analysis by\neliminating manual feature extraction. This allows for direct feature\nextraction from images, ensuring high accuracy in results. Therefore, in our\npaper, we utilized the object detection method on X-ray angiography images to\nprecisely identify the location of coronary artery stenosis. As a result, this\nmodel enables automatic and real-time detection of stenosis locations,\nassisting in the crucial and sensitive decision-making process for healthcare\nprofessionals.\n","authors":["Hadis Keshavarz","Hossein Sadr"],"pdf_url":"https://arxiv.org/pdf/2312.12135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10418v2","updated":"2023-12-19T13:11:49Z","published":"2023-12-16T11:13:40Z","title":"Fractional Deep Reinforcement Learning for Age-Minimal Mobile Edge\n  Computing","summary":"  Mobile edge computing (MEC) is a promising paradigm for real-time\napplications with intensive computational needs (e.g., autonomous driving), as\nit can reduce the processing delay. In this work, we focus on the timeliness of\ncomputational-intensive updates, measured by Age-ofInformation (AoI), and study\nhow to jointly optimize the task updating and offloading policies for AoI with\nfractional form. Specifically, we consider edge load dynamics and formulate a\ntask scheduling problem to minimize the expected time-average AoI. The\nuncertain edge load dynamics, the nature of the fractional objective, and\nhybrid continuous-discrete action space (due to the joint optimization) make\nthis problem challenging and existing approaches not directly applicable. To\nthis end, we propose a fractional reinforcement learning(RL) framework and\nprove its convergence. We further design a model-free fractional deep RL (DRL)\nalgorithm, where each device makes scheduling decisions with the hybrid action\nspace without knowing the system dynamics and decisions of other devices.\nExperimental results show that our proposed algorithms reduce the average AoI\nby up to 57.6% compared with several non-fractional benchmarks.\n","authors":["Lyudong Jin","Ming Tang","Meng Zhang","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2312.10418v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12133v1","updated":"2023-12-19T13:11:35Z","published":"2023-12-19T13:11:35Z","title":"Object-Aware Domain Generalization for Object Detection","summary":"  Single-domain generalization (S-DG) aims to generalize a model to unseen\nenvironments with a single-source domain. However, most S-DG approaches have\nbeen conducted in the field of classification. When these approaches are\napplied to object detection, the semantic features of some objects can be\ndamaged, which can lead to imprecise object localization and misclassification.\nTo address these problems, we propose an object-aware domain generalization\n(OA-DG) method for single-domain generalization in object detection. Our method\nconsists of data augmentation and training strategy, which are called OA-Mix\nand OA-Loss, respectively. OA-Mix generates multi-domain data with multi-level\ntransformation and object-aware mixing strategy. OA-Loss enables models to\nlearn domain-invariant representations for objects and backgrounds from the\noriginal and OA-Mixed images. Our proposed method outperforms state-of-the-art\nworks on standard benchmarks. Our code is available at\nhttps://github.com/WoojuLee24/OA-DG.\n","authors":["Wooju Lee","Dasol Hong","Hyungtae Lim","Hyun Myung"],"pdf_url":"https://arxiv.org/pdf/2312.12133v1.pdf","comment":"Accepted by AAAI-24. The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2310.00757v2","updated":"2023-12-19T13:10:23Z","published":"2023-10-01T18:27:59Z","title":"Mind the Gap: Federated Learning Broadens Domain Generalization in\n  Diagnostic AI Models","summary":"  Developing robust artificial intelligence (AI) models that generalize well to\nunseen datasets is challenging and usually requires large and variable\ndatasets, preferably from multiple institutions. In federated learning (FL), a\nmodel is trained collaboratively at numerous sites that hold local datasets\nwithout exchanging them. So far, the impact of training strategy, i.e., local\nversus collaborative, on the diagnostic on-domain and off-domain performance of\nAI models interpreting chest radiographs has not been assessed. Consequently,\nusing 610,000 chest radiographs from five institutions across the globe, we\nassessed diagnostic performance as a function of training strategy (i.e., local\nvs. collaborative), network architecture (i.e., convolutional vs.\ntransformer-based), generalization performance (i.e., on-domain vs.\noff-domain), imaging finding (i.e., cardiomegaly, pleural effusion, pneumonia,\natelectasis, consolidation, pneumothorax, and no abnormality), dataset size\n(i.e., from n=18,000 to 213,921 radiographs), and dataset diversity. Large\ndatasets not only showed minimal performance gains with FL but, in some\ninstances, even exhibited decreases. In contrast, smaller datasets revealed\nmarked improvements. Thus, on-domain performance was mainly driven by training\ndata size. However, off-domain performance leaned more on training diversity.\nWhen trained collaboratively across diverse external institutions, AI models\nconsistently surpassed models trained locally for off-domain tasks, emphasizing\nFL's potential in leveraging data diversity. In conclusion, FL can bolster\ndiagnostic privacy, reproducibility, and off-domain reliability of AI models\nand, potentially, optimize healthcare outcomes.\n","authors":["Soroosh Tayebi Arasteh","Christiane Kuhl","Marwin-Jonathan Saehn","Peter Isfort","Daniel Truhn","Sven Nebelung"],"pdf_url":"https://arxiv.org/pdf/2310.00757v2.pdf","comment":"Published in Nature Scientific Reports"},{"id":"http://arxiv.org/abs/2302.04977v3","updated":"2023-12-19T13:05:06Z","published":"2023-02-09T23:34:17Z","title":"Mithridates: Auditing and Boosting Backdoor Resistance of Machine\n  Learning Pipelines","summary":"  Machine learning (ML) models trained on data from potentially untrusted\nsources are vulnerable to poisoning. A small, maliciously crafted subset of the\ntraining inputs can cause the model to learn a \"backdoor\" task (e.g.,\nmisclassify inputs with a certain feature) in addition to its main task. Recent\nresearch proposed many hypothetical backdoor attacks whose efficacy heavily\ndepends on the configuration and training hyperparameters of the target model.\n  Given the variety of potential backdoor attacks, ML engineers who are not\nsecurity experts have no way to measure how vulnerable their current training\npipelines are, nor do they have a practical way to compare training\nconfigurations so as to pick the more resistant ones. Deploying a defense\nrequires evaluating and choosing from among dozens of research papers and\nre-engineering the training pipeline.\n  In this paper, we aim to provide ML engineers with pragmatic tools to audit\nthe backdoor resistance of their training pipelines and to compare different\ntraining configurations, to help choose one that best balances accuracy and\nsecurity.\n  First, we propose a universal, attack-agnostic resistance metric based on the\nminimum number of training inputs that must be compromised before the model\nlearns any backdoor.\n  Second, we design, implement, and evaluate Mithridates a multi-stage approach\nthat integrates backdoor resistance into the training-configuration search. ML\ndevelopers already rely on hyperparameter search to find configurations that\nmaximize the model's accuracy. Mithridates extends this standard tool to\nbalance accuracy and resistance without disruptive changes to the training\npipeline. We show that hyperparameters found by Mithridates increase resistance\nto multiple types of backdoor attacks by 3-5x with only a slight impact on\naccuracy. We also discuss extensions to AutoML and federated learning.\n","authors":["Eugene Bagdasaryan","Vitaly Shmatikov"],"pdf_url":"https://arxiv.org/pdf/2302.04977v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.05428v2","updated":"2023-12-19T13:00:21Z","published":"2023-01-25T03:21:42Z","title":"STERLING: Synergistic Representation Learning on Bipartite Graphs","summary":"  A fundamental challenge of bipartite graph representation learning is how to\nextract informative node embeddings. Self-Supervised Learning (SSL) is a\npromising paradigm to address this challenge. Most recent bipartite graph SSL\nmethods are based on contrastive learning which learns embeddings by\ndiscriminating positive and negative node pairs. Contrastive learning usually\nrequires a large number of negative node pairs, which could lead to\ncomputational burden and semantic errors. In this paper, we introduce a novel\nsynergistic representation learning model (STERLING) to learn node embeddings\nwithout negative node pairs. STERLING preserves the unique local and global\nsynergies in bipartite graphs. The local synergies are captured by maximizing\nthe similarity of the inter-type and intra-type positive node pairs, and the\nglobal synergies are captured by maximizing the mutual information of\nco-clusters. Theoretical analysis demonstrates that STERLING could improve the\nconnectivity between different node types in the embedding space. Extensive\nempirical evaluation on various benchmark datasets and tasks demonstrates the\neffectiveness of STERLING for extracting node embeddings.\n","authors":["Baoyu Jing","Yuchen Yan","Kaize Ding","Chanyoung Park","Yada Zhu","Huan Liu","Hanghang Tong"],"pdf_url":"https://arxiv.org/pdf/2302.05428v2.pdf","comment":"Accepted by AAAI'2024"},{"id":"http://arxiv.org/abs/2304.05805v2","updated":"2023-12-19T12:59:42Z","published":"2023-04-12T12:29:58Z","title":"GDP nowcasting with artificial neural networks: How much does long-term\n  memory matter?","summary":"  In our study, we apply artificial neural networks (ANNs) to nowcast quarterly\nGDP growth for the U.S. economy. Using the monthly FRED-MD database, we compare\nthe nowcasting performance of five different ANN architectures: the multilayer\nperceptron (MLP), the one-dimensional convolutional neural network (1D CNN),\nthe Elman recurrent neural network (RNN), the long short-term memory network\n(LSTM), and the gated recurrent unit (GRU). The empirical analysis presents the\nresults from two distinctively different evaluation periods. The first (2012:Q1\n-- 2019:Q4) is characterized by balanced economic growth, while the second\n(2012:Q1 -- 2022:Q4) also includes periods of the COVID-19 recession. According\nto our results, longer input sequences result in more accurate nowcasts in\nperiods of balanced economic growth. However, this effect ceases above a\nrelatively low threshold value of around six quarters (eighteen months). During\nperiods of economic turbulence (e.g., during the COVID-19 recession), longer\ninput sequences do not help the models' predictive performance; instead, they\nseem to weaken their generalization capability. Combined results from the two\nevaluation periods indicate that architectural features enabling for long-term\nmemory do not result in more accurate nowcasts. On the other hand, the 1D CNN\nhas proved to be a highly suitable model for GDP nowcasting. The network has\nshown good nowcasting performance among the competitors during the first\nevaluation period and achieved the overall best accuracy during the second\nevaluation period. Consequently, first in the literature, we propose the\napplication of the 1D CNN for economic nowcasting.\n","authors":["Kristóf Németh","Dániel Hadházi"],"pdf_url":"https://arxiv.org/pdf/2304.05805v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2106.08901 by other authors"},{"id":"http://arxiv.org/abs/2312.12123v1","updated":"2023-12-19T12:56:56Z","published":"2023-12-19T12:56:56Z","title":"Probabilistic Prediction of Longitudinal Trajectory Considering Driving\n  Heterogeneity with Interpretability","summary":"  Automated vehicles are envisioned to navigate safely in complex mixed-traffic\nscenarios alongside human-driven vehicles. To promise a high degree of safety,\naccurately predicting the maneuvers of surrounding vehicles and their future\npositions is a critical task and attracts much attention. However, most\nexisting studies focused on reasoning about positional information based on\nobjective historical trajectories without fully considering the heterogeneity\nof driving behaviors. Therefore, this study proposes a trajectory prediction\nframework that combines Mixture Density Networks (MDN) and considers the\ndriving heterogeneity to provide probabilistic and personalized predictions.\nSpecifically, based on a certain length of historical trajectory data, the\nsituation-specific driving preferences of each driver are identified, where key\ndriving behavior feature vectors are extracted to characterize heterogeneity in\ndriving behavior among different drivers. With the inputs of the short-term\nhistorical trajectory data and key driving behavior feature vectors, a\nprobabilistic LSTMMD-DBV model combined with LSTM-based encoder-decoder\nnetworks and MDN layers is utilized to carry out personalized predictions.\nFinally, the SHapley Additive exPlanations (SHAP) method is employed to\ninterpret the trained model for predictions. The proposed framework is tested\nbased on a wide-range vehicle trajectory dataset. The results indicate that the\nproposed model can generate probabilistic future trajectories with remarkably\nimproved predictions compared to existing benchmark models. Moreover, the\nresults confirm that the additional input of driving behavior feature vectors\nrepresenting the heterogeneity of driving behavior could provide more\ninformation and thus contribute to improving the prediction accuracy.\n","authors":["Shuli Wang","Kun Gao","Lanfang Zhang","Yang Liu","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2312.12123v1.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2312.12115v1","updated":"2023-12-19T12:46:22Z","published":"2023-12-19T12:46:22Z","title":"Shaping Up SHAP: Enhancing Stability through Layer-Wise Neighbor\n  Selection","summary":"  Machine learning techniques, such as deep learning and ensemble methods, are\nwidely used in various domains due to their ability to handle complex\nreal-world tasks. However, their black-box nature has raised multiple concerns\nabout the fairness, trustworthiness, and transparency of computer-assisted\ndecision-making. This has led to the emergence of local post-hoc explainability\nmethods, which offer explanations for individual decisions made by black-box\nalgorithms. Among these methods, Kernel SHAP is widely used due to its\nmodel-agnostic nature and its well-founded theoretical framework. Despite these\nstrengths, Kernel SHAP suffers from high instability: different executions of\nthe method with the same inputs can lead to significantly different\nexplanations, which diminishes the utility of post-hoc explainability. The\ncontribution of this paper is two-fold. On the one hand, we show that Kernel\nSHAP's instability is caused by its stochastic neighbor selection procedure,\nwhich we adapt to achieve full stability without compromising explanation\nfidelity. On the other hand, we show that by restricting the neighbors\ngeneration to perturbations of size 1 -- which we call the coalitions of Layer\n1 -- we obtain a novel feature-attribution method that is fully stable,\nefficient to compute, and still meaningful.\n","authors":["Gwladys Kelodjou","Laurence Rozé","Véronique Masson","Luis Galárraga","Romaric Gaudel","Maurice Tchuente","Alexandre Termier"],"pdf_url":"https://arxiv.org/pdf/2312.12115v1.pdf","comment":"To appear in AAAI-24"},{"id":"http://arxiv.org/abs/2303.16532v2","updated":"2023-12-19T12:43:34Z","published":"2023-03-29T08:39:36Z","title":"Futures Quantitative Investment with Heterogeneous Continual Graph\n  Neural Network","summary":"  This study aims to address the challenges of futures price prediction in\nhigh-frequency trading (HFT) by proposing a continuous learning factor\npredictor based on graph neural networks. The model integrates multi-factor\npricing theories with real-time market dynamics, effectively bypassing the\nlimitations of existing methods that lack financial theory guidance and ignore\nvarious trend signals and their interactions. We propose three heterogeneous\ntasks, including price moving average regression, price gap regression and\nchange-point detection to trace the short-, intermediate-, and long-term trend\nfactors present in the data. In addition, this study also considers the\ncross-sectional correlation characteristics of future contracts, where prices\nof different futures often show strong dynamic correlations. Each variable\n(future contract) depends not only on its historical values (temporal) but also\non the observation of other variables (cross-sectional). To capture these\ndynamic relationships more accurately, we resort to the spatio-temporal graph\nneural network (STGNN) to enhance the predictive power of the model. The model\nemploys a continuous learning strategy to simultaneously consider these tasks\n(factors). Additionally, due to the heterogeneity of the tasks, we propose to\ncalculate parameter importance with mutual information between original\nobservations and the extracted features to mitigate the catastrophic forgetting\n(CF) problem. Empirical tests on 49 commodity futures in China's futures market\ndemonstrate that the proposed model outperforms other state-of-the-art models\nin terms of prediction accuracy. Not only does this research promote the\nintegration of financial theory and deep learning, but it also provides a\nscientific basis for actual trading decisions.\n","authors":["Min Hu","Zhizhong Tan","Bin Liu","Guosheng Yin"],"pdf_url":"https://arxiv.org/pdf/2303.16532v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12113v1","updated":"2023-12-19T12:36:39Z","published":"2023-12-19T12:36:39Z","title":"Variational Mode Decomposition-Based Nonstationary Coherent Structure\n  Analysis for Spatiotemporal Data","summary":"  The modal analysis techniques face difficulties in handling nonstationary\nphenomena. This paper presents a variational mode decomposition-based\nnonstationary coherent structure (VMD-NCS) analysis that enables the extraction\nand analysis of coherent structures in case of nonstationary phenomena from\nhigh-dimensional spatiotemporal data. The VMD-NCS analysis decomposes the input\nspatiotemporal data into intrinsic coherent structures (ICSs) that represent\nnonstationary spatiotemporal patterns and exhibit coherence in both the spatial\nand temporal directions. Furthermore, unlike many conventional modal analysis\ntechniques, the proposed method accounts for the temporal changes in the\nspatial distribution with time. The performance of the VMD-NCS analysis was\nvalidated based on the transient growth phenomena in the flow around a\ncylinder. It was confirmed that the temporal changes in the spatial\ndistribution, depicting the transient growth of vortex shedding where\nfluctuations arising in the far-wake region gradually approach the near-wake\nregion, were represented as a single ICS. Further, in the analysis of the\nquasi-periodic flow field around a pitching airfoil, the temporal changes in\nthe spatial distribution and the amplitude of vortex shedding behind the\nairfoil, influenced by the pitching motion of the airfoil, were captured as a\nsingle ICS. Additionally, the impact of two parameters, adjusting the number of\nICSs ($K$) and the penalty factor related to the temporal coherence ($\\alpha$),\nwas investigated. The results revealed that $K$ has a significant impact on the\nVMD-NCS analysis results. In the case of a relatively high $K$, the VMD-NCS\nanalysis tends to extract more periodic spatiotemporal patterns resembling the\nresults of dynamic mode decomposition, whereas in the case of a small $K$, the\nanalysis tends to extract more nonstationary spatiotemporal patterns.\n","authors":["Yuya Ohmichi"],"pdf_url":"https://arxiv.org/pdf/2312.12113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12112v1","updated":"2023-12-19T12:34:46Z","published":"2023-12-19T12:34:46Z","title":"Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation\n  in ultra low-data regimes","summary":"  Machine Learning (ML) in low-data settings remains an underappreciated yet\ncrucial problem. This challenge is pronounced in low-to-middle income countries\nwhere access to large datasets is often limited or even absent. Hence, data\naugmentation methods to increase the sample size of datasets needed for ML are\nkey to unlocking the transformative potential of ML in data-deprived regions\nand domains. Unfortunately, the limited training set constrains traditional\ntabular synthetic data generators in their ability to generate a large and\ndiverse augmented dataset needed for ML tasks. To address this technical\nchallenge, we introduce CLLM, which leverages the prior knowledge of Large\nLanguage Models (LLMs) for data augmentation in the low-data regime. While\ndiverse, not all the data generated by LLMs will help increase utility for a\ndownstream task, as for any generative model. Consequently, we introduce a\nprincipled curation process, leveraging learning dynamics, coupled with\nconfidence and uncertainty metrics, to obtain a high-quality dataset.\nEmpirically, on multiple real-world datasets, we demonstrate the superior\nperformance of LLMs in the low-data regime compared to conventional generators.\nWe further show our curation mechanism improves the downstream performance for\nall generators, including LLMs. Additionally, we provide insights and\nunderstanding into the LLM generation and curation mechanism, shedding light on\nthe features that enable them to output high-quality augmented datasets. CLLM\npaves the way for wider usage of ML in data scarce domains and regions, by\nallying the strengths of LLMs with a robust data-centric approach.\n","authors":["Nabeel Seedat","Nicolas Huynh","Boris van Breugel","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2312.12112v1.pdf","comment":"*Seedat & Huynh contributed equally"},{"id":"http://arxiv.org/abs/2310.18313v2","updated":"2023-12-19T12:27:58Z","published":"2023-10-27T17:59:51Z","title":"FP8-LM: Training FP8 Large Language Models","summary":"  In this paper, we explore FP8 low-bit data formats for efficient training of\nlarge language models (LLMs). Our key insight is that most variables, such as\ngradients and optimizer states, in LLM training can employ low-precision data\nformats without compromising model accuracy and requiring no changes to\nhyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision\nframework for training LLMs. This framework offers three levels of FP8\nutilization to streamline mixed-precision and distributed parallel training for\nLLMs. It gradually incorporates 8-bit gradients, optimizer states, and\ndistributed learning in an incremental manner. Experiment results show that,\nduring the training of GPT-175B model on H100 GPU platform, our FP8\nmixed-precision training framework not only achieved a remarkable 39% reduction\nin real memory usage but also ran 75% faster than the widely adopted BF16\nframework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer\nEngine by 37%. This largely reduces the training costs for large foundation\nmodels. Furthermore, our FP8 mixed-precision training methodology is generic.\nIt can be seamlessly applied to other tasks such as LLM instruction tuning and\nreinforcement learning with human feedback, offering savings in fine-tuning\nexpenses. Our FP8 low-precision training framework is open-sourced at\n{https://github.com/Azure/MS-AMP}{aka.ms/MS.AMP}.\n","authors":["Houwen Peng","Kan Wu","Yixuan Wei","Guoshuai Zhao","Yuxiang Yang","Ze Liu","Yifan Xiong","Ziyue Yang","Bolin Ni","Jingcheng Hu","Ruihang Li","Miaosen Zhang","Chen Li","Jia Ning","Ruizhe Wang","Zheng Zhang","Shuguang Liu","Joe Chau","Han Hu","Peng Cheng"],"pdf_url":"https://arxiv.org/pdf/2310.18313v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12102v1","updated":"2023-12-19T12:26:57Z","published":"2023-12-19T12:26:57Z","title":"I-CEE: Tailoring Explanations of Image Classifications Models to User\n  Expertise","summary":"  Effectively explaining decisions of black-box machine learning models is\ncritical to responsible deployment of AI systems that rely on them. Recognizing\ntheir importance, the field of explainable AI (XAI) provides several techniques\nto generate these explanations. Yet, there is relatively little emphasis on the\nuser (the explainee) in this growing body of work and most XAI techniques\ngenerate \"one-size-fits-all\" explanations. To bridge this gap and achieve a\nstep closer towards human-centered XAI, we present I-CEE, a framework that\nprovides Image Classification Explanations tailored to User Expertise. Informed\nby existing work, I-CEE explains the decisions of image classification models\nby providing the user with an informative subset of training data (i.e.,\nexample images), corresponding local explanations, and model decisions.\nHowever, unlike prior work, I-CEE models the informativeness of the example\nimages to depend on user expertise, resulting in different examples for\ndifferent users. We posit that by tailoring the example set to user expertise,\nI-CEE can better facilitate users' understanding and simulatability of the\nmodel. To evaluate our approach, we conduct detailed experiments in both\nsimulation and with human participants (N = 100) on multiple datasets.\nExperiments with simulated users show that I-CEE improves users' ability to\naccurately predict the model's decisions (simulatability) compared to\nbaselines, providing promising preliminary results. Experiments with human\nparticipants demonstrate that our method significantly improves user\nsimulatability accuracy, highlighting the importance of human-centered XAI\n","authors":["Yao Rong","Peizhu Qian","Vaibhav Unhelkar","Enkelejda Kasneci"],"pdf_url":"https://arxiv.org/pdf/2312.12102v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06453v2","updated":"2023-12-19T12:13:25Z","published":"2023-09-12T08:16:58Z","title":"Narrowing the Gap between Supervised and Unsupervised Sentence\n  Representation Learning with Large Language Model","summary":"  Sentence Representation Learning (SRL) is a fundamental task in Natural\nLanguage Processing (NLP), with the Contrastive Learning of Sentence Embeddings\n(CSE) being the mainstream technique due to its superior performance. An\nintriguing phenomenon in CSE is the significant performance gap between\nsupervised and unsupervised methods, with their only difference lying in the\ntraining data. Previous works attribute this performance gap to differences in\ntwo representation properties (alignment and uniformity). However, since\nalignment and uniformity only measure the results, they fail to answer \"What\naspects of the training data contribute to the performance gap?\" and \"How can\nthe performance gap be narrowed?\", In this paper, we conduct empirical\nexperiments to answer these \"What\" and \"How\" questions. We first answer the\n\"What\" question by thoroughly comparing the behavior of supervised and\nunsupervised CSE during their respective training processes. From the\ncomparison, we identify the similarity pattern as a key factor to the\nperformance gap, and introduce a metric, called Relative Fitting Difficulty\n(RFD), to measure the complexity of the similarity pattern. Then, based on the\ninsights gained from the \"What\" question, we tackle the \"How\" question by\nincreasing the pattern complexity of the training data. We achieve this by\nleveraging the In-Context Learning (ICL) capability of the Large Language Model\n(LLM) to generate data that simulates complex patterns. By utilizing the\nhierarchical patterns in the LLM-generated data, we effectively narrow the gap\nbetween supervised and unsupervised CSE. We release our codes and appendix at\nhttps://github.com/BDBC-KG-NLP/NGCSE.\n","authors":["Mingxin Li","Richong Zhang","Zhijie Nie","Yongyi Mao"],"pdf_url":"https://arxiv.org/pdf/2309.06453v2.pdf","comment":"Accepted at AAAI24"},{"id":"http://arxiv.org/abs/2303.16737v2","updated":"2023-12-19T11:55:14Z","published":"2023-03-29T14:41:03Z","title":"Multi-Agent Reinforcement Learning with Action Masking for UAV-enabled\n  Mobile Communications","summary":"  Unmanned Aerial Vehicles (UAVs) are increasingly used as aerial base stations\nto provide ad hoc communications infrastructure. Building upon prior research\nefforts which consider either static nodes, 2D trajectories or single UAV\nsystems, this paper focuses on the use of multiple UAVs for providing wireless\ncommunication to mobile users in the absence of terrestrial communications\ninfrastructure. In particular, we jointly optimize UAV 3D trajectory and NOMA\npower allocation to maximize system throughput. Firstly, a weighted\nK-means-based clustering algorithm establishes UAV-user associations at regular\nintervals. The efficacy of training a novel Shared Deep Q-Network (SDQN) with\naction masking is then explored. Unlike training each UAV separately using DQN,\nthe SDQN reduces training time by using the experiences of multiple UAVs\ninstead of a single agent. We also show that SDQN can be used to train a\nmulti-agent system with differing action spaces. Simulation results confirm\nthat: 1) training a shared DQN outperforms a conventional DQN in terms of\nmaximum system throughput (+20%) and training time (-10%); 2) it can converge\nfor agents with different action spaces, yielding a 9% increase in throughput\ncompared to mutual learning algorithms; and 3) combining NOMA with an SDQN\narchitecture enables the network to achieve a better sum rate compared with\nexisting baseline schemes.\n","authors":["Danish Rizvi","David Boyle"],"pdf_url":"https://arxiv.org/pdf/2303.16737v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12068v1","updated":"2023-12-19T11:36:03Z","published":"2023-12-19T11:36:03Z","title":"PICNN: A Pathway towards Interpretable Convolutional Neural Networks","summary":"  Convolutional Neural Networks (CNNs) have exhibited great performance in\ndiscriminative feature learning for complex visual tasks. Besides\ndiscrimination power, interpretability is another important yet under-explored\nproperty for CNNs. One difficulty in the CNN interpretability is that filters\nand image classes are entangled. In this paper, we introduce a novel pathway to\nalleviate the entanglement between filters and image classes. The proposed\npathway groups the filters in a late conv-layer of CNN into class-specific\nclusters. Clusters and classes are in a one-to-one relationship. Specifically,\nwe use the Bernoulli sampling to generate the filter-cluster assignment matrix\nfrom a learnable filter-class correspondence matrix. To enable end-to-end\noptimization, we develop a novel reparameterization trick for handling the\nnon-differentiable Bernoulli sampling. We evaluate the effectiveness of our\nmethod on ten widely used network architectures (including nine CNNs and a ViT)\nand five benchmark datasets. Experimental results have demonstrated that our\nmethod PICNN (the combination of standard CNNs with our proposed pathway)\nexhibits greater interpretability than standard CNNs while achieving higher or\ncomparable discrimination power.\n","authors":["Wengang Guo","Jiayi Yang","Huilin Yin","Qijun Chen","Wei Ye"],"pdf_url":"https://arxiv.org/pdf/2312.12068v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12067v1","updated":"2023-12-19T11:34:10Z","published":"2023-12-19T11:34:10Z","title":"Optimistic Policy Gradient in Multi-Player Markov Games with a Single\n  Controller: Convergence Beyond the Minty Property","summary":"  Policy gradient methods enjoy strong practical performance in numerous tasks\nin reinforcement learning. Their theoretical understanding in multiagent\nsettings, however, remains limited, especially beyond two-player competitive\nand potential Markov games. In this paper, we develop a new framework to\ncharacterize optimistic policy gradient methods in multi-player Markov games\nwith a single controller. Specifically, under the further assumption that the\ngame exhibits an equilibrium collapse, in that the marginals of coarse\ncorrelated equilibria (CCE) induce Nash equilibria (NE), we show convergence to\nstationary $\\epsilon$-NE in $O(1/\\epsilon^2)$ iterations, where $O(\\cdot)$\nsuppresses polynomial factors in the natural parameters of the game. Such an\nequilibrium collapse is well-known to manifest itself in two-player zero-sum\nMarkov games, but also occurs even in a class of multi-player Markov games with\nseparable interactions, as established by recent work. As a result, we bypass\nknown complexity barriers for computing stationary NE when either of our\nassumptions fails. Our approach relies on a natural generalization of the\nclassical Minty property that we introduce, which we anticipate to have further\napplications beyond Markov games.\n","authors":["Ioannis Anagnostides","Ioannis Panageas","Gabriele Farina","Tuomas Sandholm"],"pdf_url":"https://arxiv.org/pdf/2312.12067v1.pdf","comment":"To appear at AAAI 2024"},{"id":"http://arxiv.org/abs/2312.12065v1","updated":"2023-12-19T11:33:18Z","published":"2023-12-19T11:33:18Z","title":"PPO-Clip Attains Global Optimality: Towards Deeper Understandings of\n  Clipping","summary":"  Proximal Policy Optimization algorithm employing a clipped surrogate\nobjective (PPO-Clip) is a prominent exemplar of the policy optimization\nmethods. However, despite its remarkable empirical success, PPO-Clip lacks\ntheoretical substantiation to date. In this paper, we contribute to the field\nby establishing the first global convergence results of a PPO-Clip variant in\nboth tabular and neural function approximation settings. Our findings highlight\nthe $O(1/\\sqrt{T})$ min-iterate convergence rate specifically in the context of\nneural function approximation. We tackle the inherent challenges in analyzing\nPPO-Clip through three central concepts: (i) We introduce a generalized version\nof the PPO-Clip objective, illuminated by its connection with the hinge loss.\n(ii) Employing entropic mirror descent, we establish asymptotic convergence for\ntabular PPO-Clip with direct policy parameterization. (iii) Inspired by the\ntabular analysis, we streamline convergence analysis by introducing a two-step\npolicy improvement approach. This decouples policy search from complex neural\npolicy parameterization using a regression-based update scheme. Furthermore, we\ngain deeper insights into the efficacy of PPO-Clip by interpreting these\ngeneralized objectives. Our theoretical findings also mark the first\ncharacterization of the influence of the clipping mechanism on PPO-Clip\nconvergence. Importantly, the clipping range affects only the pre-constant of\nthe convergence rate.\n","authors":["Nai-Chieh Huang","Ping-Chun Hsieh","Kuo-Hao Ho","I-Chen Wu"],"pdf_url":"https://arxiv.org/pdf/2312.12065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12050v1","updated":"2023-12-19T11:14:37Z","published":"2023-12-19T11:14:37Z","title":"Extension of the Dip-test Repertoire -- Efficient and Differentiable\n  p-value Calculation for Clustering","summary":"  Over the last decade, the Dip-test of unimodality has gained increasing\ninterest in the data mining community as it is a parameter-free statistical\ntest that reliably rates the modality in one-dimensional samples. It returns a\nso called Dip-value and a corresponding probability for the sample's\nunimodality (Dip-p-value). These two values share a sigmoidal relationship.\nHowever, the specific transformation is dependent on the sample size. Many\nDip-based clustering algorithms use bootstrapped look-up tables translating\nDip- to Dip-p-values for a certain limited amount of sample sizes. We propose a\nspecifically designed sigmoid function as a substitute for these\nstate-of-the-art look-up tables. This accelerates computation and provides an\napproximation of the Dip- to Dip-p-value transformation for every single sample\nsize. Further, it is differentiable and can therefore easily be integrated in\nlearning schemes using gradient descent. We showcase this by exploiting our\nfunction in a novel subspace clustering algorithm called Dip'n'Sub. We\nhighlight in extensive experiments the various benefits of our proposal.\n","authors":["Lena G. M. Bauer","Collin Leiber","Christian Böhm","Claudia Plant"],"pdf_url":"https://arxiv.org/pdf/2312.12050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12049v1","updated":"2023-12-19T11:11:03Z","published":"2023-12-19T11:11:03Z","title":"EncryIP: A Practical Encryption-Based Framework for Model Intellectual\n  Property Protection","summary":"  In the rapidly growing digital economy, protecting intellectual property (IP)\nassociated with digital products has become increasingly important. Within this\ncontext, machine learning (ML) models, being highly valuable digital assets,\nhave gained significant attention for IP protection. This paper introduces a\npractical encryption-based framework called \\textit{EncryIP}, which seamlessly\nintegrates a public-key encryption scheme into the model learning process. This\napproach enables the protected model to generate randomized and confused\nlabels, ensuring that only individuals with accurate secret keys, signifying\nauthorized users, can decrypt and reveal authentic labels. Importantly, the\nproposed framework not only facilitates the protected model to multiple\nauthorized users without requiring repetitive training of the original ML model\nwith IP protection methods but also maintains the model's performance without\ncompromising its accuracy. Compared to existing methods like watermark-based,\ntrigger-based, and passport-based approaches, \\textit{EncryIP} demonstrates\nsuperior effectiveness in both training protected models and efficiently\ndetecting the unauthorized spread of ML models.\n","authors":["Xin Mu","Yu Wang","Zhengan Huang","Junzuo Lai","Yehong Zhang","Hui Wang","Yue Yu"],"pdf_url":"https://arxiv.org/pdf/2312.12049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.16058v2","updated":"2023-12-19T11:00:18Z","published":"2022-10-28T11:11:04Z","title":"Goal Exploration Augmentation via Pre-trained Skills for Sparse-Reward\n  Long-Horizon Goal-Conditioned Reinforcement Learning","summary":"  Reinforcement learning (RL) often struggles to accomplish a sparse-reward\nlong-horizon task in a complex environment. Goal-conditioned reinforcement\nlearning (GCRL) has been employed to tackle this difficult problem via a\ncurriculum of easy-to-reach sub-goals. In GCRL, exploring novel sub-goals is\nessential for the agent to ultimately find the pathway to the desired goal. How\nto explore novel sub-goals efficiently is one of the most challenging issues in\nGCRL. Several goal exploration methods have been proposed to address this issue\nbut still struggle to find the desired goals efficiently. In this paper, we\npropose a novel learning objective by optimizing the entropy of both achieved\nand new goals to be explored for more efficient goal exploration in sub-goal\nselection based GCRL. To optimize this objective, we first explore and exploit\nthe frequently occurring goal-transition patterns mined in the environments\nsimilar to the current task to compose skills via skill learning. Then, the\npretrained skills are applied in goal exploration. Evaluation on a variety of\nspare-reward long-horizon benchmark tasks suggests that incorporating our\nmethod into several state-of-the-art GCRL baselines significantly boosts their\nexploration efficiency while improving or maintaining their performance. The\nsource code is available at: https://github.com/GEAPS/GEAPS.\n","authors":["Lisheng Wu","Ke Chen"],"pdf_url":"https://arxiv.org/pdf/2210.16058v2.pdf","comment":"Accepted for publication in Machine Learning (Springer): 35 pages, 15\n  figures"},{"id":"http://arxiv.org/abs/2312.12044v1","updated":"2023-12-19T10:57:12Z","published":"2023-12-19T10:57:12Z","title":"XLand-MiniGrid: Scalable Meta-Reinforcement Learning Environments in JAX","summary":"  We present XLand-MiniGrid, a suite of tools and grid-world environments for\nmeta-reinforcement learning research inspired by the diversity and depth of\nXLand and the simplicity and minimalism of MiniGrid. XLand-Minigrid is written\nin JAX, designed to be highly scalable, and can potentially run on GPU or TPU\naccelerators, democratizing large-scale experimentation with limited resources.\nTo demonstrate the generality of our library, we have implemented some\nwell-known single-task environments as well as new meta-learning environments\ncapable of generating $10^8$ distinct tasks. We have empirically shown that the\nproposed environments can scale up to $2^{13}$ parallel instances on the GPU,\nreaching tens of millions of steps per second.\n","authors":["Alexander Nikulin","Vladislav Kurenkov","Ilya Zisman","Artem Agarkov","Viacheslav Sinii","Sergey Kolesnikov"],"pdf_url":"https://arxiv.org/pdf/2312.12044v1.pdf","comment":"NeurIPS 2023, Workshop, Source code:\n  https://github.com/corl-team/xland-minigrid"},{"id":"http://arxiv.org/abs/2306.01843v3","updated":"2023-12-19T10:43:16Z","published":"2023-06-02T18:03:03Z","title":"Lifting Architectural Constraints of Injective Flows","summary":"  Normalizing Flows explicitly maximize a full-dimensional likelihood on the\ntraining data. However, real data is typically only supported on a\nlower-dimensional manifold leading the model to expend significant compute on\nmodeling noise. Injective Flows fix this by jointly learning a manifold and the\ndistribution on it. So far, they have been limited by restrictive architectures\nand/or high computational cost. We lift both constraints by a new efficient\nestimator for the maximum likelihood loss, compatible with free-form bottleneck\narchitectures. We further show that naively learning both the data manifold and\nthe distribution on it can lead to divergent solutions, and use this insight to\nmotivate a stable maximum likelihood training objective. We perform extensive\nexperiments on toy, tabular and image data, demonstrating the competitive\nperformance of the resulting model.\n","authors":["Peter Sorrenson","Felix Draxler","Armand Rousselot","Sander Hummerich","Lea Zimmermann","Ullrich Köthe"],"pdf_url":"https://arxiv.org/pdf/2306.01843v3.pdf","comment":"Resubmission of previous work: title and abstract have been changed\n  and new content has been added"},{"id":"http://arxiv.org/abs/2312.09783v3","updated":"2023-12-19T10:36:41Z","published":"2023-12-15T13:36:54Z","title":"Keep the Faith: Faithful Explanations in Convolutional Neural Networks\n  for Case-Based Reasoning","summary":"  Explaining predictions of black-box neural networks is crucial when applied\nto decision-critical tasks. Thus, attribution maps are commonly used to\nidentify important image regions, despite prior work showing that humans prefer\nexplanations based on similar examples. To this end, ProtoPNet learns a set of\nclass-representative feature vectors (prototypes) for case-based reasoning.\nDuring inference, similarities of latent features to prototypes are linearly\nclassified to form predictions and attribution maps are provided to explain the\nsimilarity. In this work, we evaluate whether architectures for case-based\nreasoning fulfill established axioms required for faithful explanations using\nthe example of ProtoPNet. We show that such architectures allow the extraction\nof faithful explanations. However, we prove that the attribution maps used to\nexplain the similarities violate the axioms. We propose a new procedure to\nextract explanations for trained ProtoPNets, named ProtoPFaith. Conceptually,\nthese explanations are Shapley values, calculated on the similarity scores of\neach prototype. They allow to faithfully answer which prototypes are present in\nan unseen image and quantify each pixel's contribution to that presence,\nthereby complying with all axioms. The theoretical violations of ProtoPNet\nmanifest in our experiments on three datasets (CUB-200-2011, Stanford Dogs,\nRSNA) and five architectures (ConvNet, ResNet, ResNet50, WideResNet50,\nResNeXt50). Our experiments show a qualitative difference between the\nexplanations given by ProtoPNet and ProtoPFaith. Additionally, we quantify the\nexplanations with the Area Over the Perturbation Curve, on which ProtoPFaith\noutperforms ProtoPNet on all experiments by a factor $>10^3$.\n","authors":["Tom Nuno Wolf","Fabian Bongratz","Anne-Marie Rickmann","Sebastian Pölsterl","Christian Wachinger"],"pdf_url":"https://arxiv.org/pdf/2312.09783v3.pdf","comment":"To be published in proceedings of AAAI Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2312.11315v2","updated":"2023-12-19T10:31:08Z","published":"2023-12-18T16:10:18Z","title":"CaRe-CNN: Cascading Refinement CNN for Myocardial Infarct Segmentation\n  with Microvascular Obstructions","summary":"  Late gadolinium enhanced (LGE) magnetic resonance (MR) imaging is widely\nestablished to assess the viability of myocardial tissue of patients after\nacute myocardial infarction (MI). We propose the Cascading Refinement CNN\n(CaRe-CNN), which is a fully 3D, end-to-end trained, 3-stage CNN cascade that\nexploits the hierarchical structure of such labeled cardiac data. Throughout\nthe three stages of the cascade, the label definition changes and CaRe-CNN\nlearns to gradually refine its intermediate predictions accordingly.\nFurthermore, to obtain more consistent qualitative predictions, we propose a\nseries of post-processing steps that take anatomical constraints into account.\nOur CaRe-CNN was submitted to the FIMH 2023 MYOSAIQ challenge, where it ranked\nsecond out of 18 participating teams. CaRe-CNN showed great improvements most\nnotably when segmenting the difficult but clinically most relevant myocardial\ninfarct tissue (MIT) as well as microvascular obstructions (MVO). When\ncomputing the average scores over all labels, our method obtained the best\nscore in eight out of ten metrics. Thus, accurate cardiac segmentation after\nacute MI via our CaRe-CNN allows generating patient-specific models of the\nheart serving as an important step towards personalized medicine.\n","authors":["Franz Thaler","Matthias A. F. Gsell","Gernot Plank","Martin Urschler"],"pdf_url":"https://arxiv.org/pdf/2312.11315v2.pdf","comment":"Accepted at VISIGRAPP 2024, 12 pages"},{"id":"http://arxiv.org/abs/2312.12028v1","updated":"2023-12-19T10:29:29Z","published":"2023-12-19T10:29:29Z","title":"EyePreserve: Identity-Preserving Iris Synthesis","summary":"  Synthesis of same-identity biometric iris images, both for existing and\nnon-existing identities while preserving the identity across a wide range of\npupil sizes, is complex due to intricate iris muscle constriction mechanism,\nrequiring a precise model of iris non-linear texture deformations to be\nembedded into the synthesis pipeline. This paper presents the first method of\nfully data-driven, identity-preserving, pupil size-varying s ynthesis of iris\nimages. This approach is capable of synthesizing images of irises with\ndifferent pupil sizes representing non-existing identities as well as\nnon-linearly deforming the texture of iris images of existing subjects given\nthe segmentation mask of the target iris image. Iris recognition experiments\nsuggest that the proposed deformation model not only preserves the identity\nwhen changing the pupil size but offers better similarity between same-identity\niris samples with significant differences in pupil size, compared to\nstate-of-the-art linear and non-linear (bio-mechanical-based) iris deformation\nmodels. Two immediate applications of the proposed approach are: (a) synthesis\nof, or enhancement of the existing biometric datasets for iris recognition,\nmimicking those acquired with iris sensors, and (b) helping forensic human\nexperts in examining iris image pairs with significant differences in pupil\ndilation. Source codes and weights of the models are made available with the\npaper.\n","authors":["Siamul Karim Khan","Patrick Tinsley","Mahsa Mitcheff","Patrick Flynn","Kevin W. Bowyer","Adam Czajka"],"pdf_url":"https://arxiv.org/pdf/2312.12028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12022v1","updated":"2023-12-19T10:18:57Z","published":"2023-12-19T10:18:57Z","title":"LightGCNet: A Lightweight Geometric Constructive Neural Network for\n  Data-Driven Soft sensors","summary":"  Data-driven soft sensors provide a potentially cost-effective and more\naccurate modeling approach to measure difficult-to-measure indices in\nindustrial processes compared to mechanistic approaches. Artificial\nintelligence (AI) techniques, such as deep learning, have become a popular soft\nsensors modeling approach in the area of machine learning and big data.\nHowever, soft sensors models based deep learning potentially lead to complex\nmodel structures and excessive training time. In addition, industrial processes\noften rely on distributed control systems (DCS) characterized by resource\nconstraints. Herein, guided by spatial geometric, a lightweight geometric\nconstructive neural network, namely LightGCNet, is proposed, which utilizes\ncompact angle constraint to assign the hidden parameters from dynamic\nintervals. At the same time, a node pool strategy and spatial geometric\nrelationships are used to visualize and optimize the process of assigning\nhidden parameters, enhancing interpretability. In addition, the universal\napproximation property of LightGCNet is proved by spatial geometric analysis.\nTwo versions algorithmic implementations of LightGCNet are presented in this\narticle. Simulation results concerning both benchmark datasets and the ore\ngrinding process indicate remarkable merits of LightGCNet in terms of small\nnetwork size, fast learning speed, and sound generalization.\n","authors":["Jing Nan","Yan Qin","Wei Dai","Chau Yuen"],"pdf_url":"https://arxiv.org/pdf/2312.12022v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2307.00185"},{"id":"http://arxiv.org/abs/2310.05161v4","updated":"2023-12-19T10:13:33Z","published":"2023-10-08T13:36:05Z","title":"Recurrent Neural Language Models as Probabilistic Finite-state Automata","summary":"  Studying language models (LMs) in terms of well-understood formalisms allows\nus to precisely characterize their abilities and limitations. Previous work has\ninvestigated the representational capacity of recurrent neural network (RNN)\nLMs in terms of their capacity to recognize unweighted formal languages.\nHowever, LMs do not describe unweighted formal languages -- rather, they define\n\\emph{probability distributions} over strings. In this work, we study what\nclasses of such probability distributions RNN LMs can represent, which allows\nus to make more direct statements about their capabilities. We show that simple\nRNNs are equivalent to a subclass of probabilistic finite-state automata, and\ncan thus model a strict subset of probability distributions expressible by\nfinite-state models. Furthermore, we study the space complexity of representing\nfinite-state LMs with RNNs. We show that, to represent an arbitrary\ndeterministic finite-state LM with $N$ states over an alphabet $\\alphabet$, an\nRNN requires $\\Omega\\left(N |\\Sigma|\\right)$ neurons. These results present a\nfirst step towards characterizing the classes of distributions RNN LMs can\nrepresent and thus help us understand their capabilities and limitations.\n","authors":["Anej Svete","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2310.05161v4.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2312.12009v1","updated":"2023-12-19T09:58:54Z","published":"2023-12-19T09:58:54Z","title":"Active Preference Inference using Language Models and Probabilistic\n  Reasoning","summary":"  Actively inferring user preferences, for example by asking good questions, is\nimportant for any human-facing decision-making system. Active inference allows\nsuch systems to adapt and personalize themselves to nuanced individual\npreferences. To enable this ability for instruction-tuned large language models\n(LLMs), one may prompt them to ask users questions to infer their preferences,\ntransforming the language models into more robust, interactive systems.\nHowever, out of the box, these models are not efficient at extracting\npreferences: the questions they generate are not informative, requiring a high\nnumber of user interactions and impeding the usability of the downstream\nsystem. In this work, we introduce an inference-time algorithm that helps LLMs\nquickly infer preferences by using more informative questions. Our algorithm\nuses a probabilistic model whose conditional distributions are defined by\nprompting an LLM, and returns questions that optimize expected entropy and\nexpected model change. Results in a simplified interactive web shopping setting\nwith real product items show that an LLM equipped with our entropy reduction\nalgorithm outperforms baselines with the same underlying LLM on task\nperformance while using fewer user interactions.\n","authors":["Top Piriyakulkij","Volodymyr Kuleshov","Kevin Ellis"],"pdf_url":"https://arxiv.org/pdf/2312.12009v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12003v1","updated":"2023-12-19T09:51:02Z","published":"2023-12-19T09:51:02Z","title":"Modelling and characterization of fine Particulate Matter dynamics in\n  Bujumbura using low cost sensors","summary":"  Air pollution is a result of multiple sources including both natural and\nanthropogenic activities. The rapid urbanization of the cities such as\nBujumbura economic capital of Burundi, is one of these factors. The very first\ncharacterization of the spatio-temporal variability of PM2.5 in Bujumbura and\nthe forecasting of PM2.5 concentration have been conducted in this paper using\ndata collected during a year, from august 2022 to august 2023, by low cost\nsensors installed in Bujumbura city. For each commune, an hourly, daily and\nseasonal analysis were carried out and the results showed that the mass\nconcentrations of PM2.5 in the three municipalities differ from one commune to\nanother. The average hourly and annual PM2.5 concentrations exceed the World\nHealth Organization standards. The range is between 28.3 and 35.0 microgram/m3\n. In order to make prediction of PM2.5 concentration, an investigation of RNN\nwith Long Short Term Memory (LSTM) has been undertaken.\n","authors":["Egide Ndamuzi","Rachel Akimana","Paterne Gahungu","Elie Bimenyimana"],"pdf_url":"https://arxiv.org/pdf/2312.12003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.01190v3","updated":"2023-12-19T09:45:36Z","published":"2023-02-02T16:16:25Z","title":"On the Efficacy of Differentially Private Few-shot Image Classification","summary":"  There has been significant recent progress in training differentially private\n(DP) models which achieve accuracy that approaches the best non-private models.\nThese DP models are typically pretrained on large public datasets and then\nfine-tuned on private downstream datasets that are relatively large and similar\nin distribution to the pretraining data. However, in many applications\nincluding personalization and federated learning, it is crucial to perform well\n(i) in the few-shot setting, as obtaining large amounts of labeled data may be\nproblematic; and (ii) on datasets from a wide variety of domains for use in\nvarious specialist settings. To understand under which conditions few-shot DP\ncan be effective, we perform an exhaustive set of experiments that reveals how\nthe accuracy and vulnerability to attack of few-shot DP image classification\nmodels are affected as the number of shots per class, privacy level, model\narchitecture, downstream dataset, and subset of learnable parameters in the\nmodel vary. We show that to achieve DP accuracy on par with non-private models,\nthe shots per class must be increased as the privacy level increases. We also\nshow that learning parameter-efficient FiLM adapters under DP is competitive\nwith learning just the final classifier layer or learning all of the network\nparameters. Finally, we evaluate DP federated learning systems and establish\nstate-of-the-art performance on the challenging FLAIR benchmark.\n","authors":["Marlon Tobaben","Aliaksandra Shysheya","John Bronskill","Andrew Paverd","Shruti Tople","Santiago Zanella-Beguelin","Richard E Turner","Antti Honkela"],"pdf_url":"https://arxiv.org/pdf/2302.01190v3.pdf","comment":"49 pages, 24 figures; published in TMLR 12/2023\n  https://openreview.net/forum?id=hFsr59Imzm"},{"id":"http://arxiv.org/abs/2305.16901v2","updated":"2023-12-19T09:41:25Z","published":"2023-05-26T13:14:05Z","title":"Generalizing Adam to Manifolds for Efficiently Training Transformers","summary":"  One of the primary reasons behind the success of neural networks has been the\nemergence of an array of new, highly-successful optimizers, perhaps most\nimportantly the Adam optimizer. It is wiedely used for training neural\nnetworks, yet notoriously hard to interpret. Lacking a clear physical\nintuition, Adam is difficult to generalize to manifolds. Some attempts have\nbeen made to directly apply parts of the Adam algorithm to manifolds or to find\nan underlying structure, but a full generalization has remained elusive. In\nthis work a new approach is presented that leverages the special structure of\nthe manifolds which are relevant for optimization of neural networks, such as\nthe Stiefel manifold, the symplectic Stiefel manifold, the Grassmann manifold\nand the symplectic Grassmann manifold: all of these are homogeneous spaces and\nas such admit a global tangent space representation. This global tangent space\nrepresentation is used to perform all of the steps in the Adam optimizer. The\nresulting algorithm is then applied to train a transformer for which\northogonality constraints are enforced up to machine precision and we observe\nsignificant speed-ups in the training process. Optimization of neural networks\nwhere they weights do not lie on a manifold is identified as a special case of\nthe presented framkework. This allows for a flexible implementation in which\nthe learning rate is adapted simultaneously for all parameters, irrespective of\nwhether they are an element of a general manifold or a vector space.\n","authors":["Benedikt Brantner"],"pdf_url":"https://arxiv.org/pdf/2305.16901v2.pdf","comment":"19 pages, 4 figures, was presented at Enumath2023"},{"id":"http://arxiv.org/abs/2210.07780v3","updated":"2023-12-19T09:31:06Z","published":"2022-10-14T13:09:11Z","title":"Federated Best Arm Identification with Heterogeneous Clients","summary":"  We study best arm identification in a federated multi-armed bandit setting\nwith a central server and multiple clients, when each client has access to a\n{\\em subset} of arms and each arm yields independent Gaussian observations. The\ngoal is to identify the best arm of each client subject to an upper bound on\nthe error probability; here, the best arm is one that has the largest {\\em\naverage} value of the means averaged across all clients having access to the\narm. Our interest is in the asymptotics as the error probability vanishes. We\nprovide an asymptotic lower bound on the growth rate of the expected stopping\ntime of any algorithm. Furthermore, we show that for any algorithm whose upper\nbound on the expected stopping time matches with the lower bound up to a\nmultiplicative constant ({\\em almost-optimal} algorithm), the ratio of any two\nconsecutive communication time instants must be {\\em bounded}, a result that is\nof independent interest. We thereby infer that an algorithm can communicate no\nmore sparsely than at exponential time instants in order to be almost-optimal.\nFor the class of almost-optimal algorithms, we present the first-of-its-kind\nasymptotic lower bound on the expected number of {\\em communication rounds}\nuntil stoppage. We propose a novel algorithm that communicates at exponential\ntime instants, and demonstrate that it is asymptotically almost-optimal.\n","authors":["Zhirui Chen","P. N. Karthik","Vincent Y. F. Tan","Yeow Meng Chee"],"pdf_url":"https://arxiv.org/pdf/2210.07780v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.09010v4","updated":"2023-12-19T09:18:30Z","published":"2022-12-18T04:44:38Z","title":"Risk-Sensitive Reinforcement Learning with Exponential Criteria","summary":"  While reinforcement learning has shown experimental success in a number of\napplications, it is known to be sensitive to noise and perturbations in the\nparameters of the system, leading to high variance in the total reward amongst\ndifferent episodes in slightly different environments. To introduce robustness,\nas well as sample efficiency, risk-sensitive reinforcement learning methods are\nbeing thoroughly studied. In this work, we provide a definition of robust\nreinforcement learning policies and formulate a risk-sensitive reinforcement\nlearning problem to approximate them, by solving an optimization problem with\nrespect to a modified objective based on exponential criteria. In particular,\nwe study a model-free risk-sensitive variation of the widely-used Monte Carlo\nPolicy Gradient algorithm and introduce a novel risk-sensitive online\nActor-Critic algorithm based on solving a multiplicative Bellman equation using\nstochastic approximation updates. Analytical results suggest that the use of\nexponential criteria generalizes commonly used ad-hoc regularization\napproaches, improves sample efficiency, and introduces robustness with respect\nto perturbations in the model parameters and the environment. The\nimplementation, performance, and robustness properties of the proposed methods\nare evaluated in simulated experiments.\n","authors":["Erfaun Noorani","Christos Mavridis","John Baras"],"pdf_url":"https://arxiv.org/pdf/2212.09010v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11976v1","updated":"2023-12-19T09:18:12Z","published":"2023-12-19T09:18:12Z","title":"When Model Meets New Normals: Test-time Adaptation for Unsupervised\n  Time-series Anomaly Detection","summary":"  Time-series anomaly detection deals with the problem of detecting anomalous\ntimesteps by learning normality from the sequence of observations. However, the\nconcept of normality evolves over time, leading to a \"new normal problem\",\nwhere the distribution of normality can be changed due to the distribution\nshifts between training and test data. This paper highlights the prevalence of\nthe new normal problem in unsupervised time-series anomaly detection studies.\nTo tackle this issue, we propose a simple yet effective test-time adaptation\nstrategy based on trend estimation and a self-supervised approach to learning\nnew normalities during inference. Extensive experiments on real-world\nbenchmarks demonstrate that incorporating the proposed strategy into the\nanomaly detector consistently improves the model's performance compared to the\nbaselines, leading to robustness to the distribution shifts.\n","authors":["Dongmin Kim","Sunghyun Park","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2312.11976v1.pdf","comment":"Accepted to AAAI 2024, 17 pages, https://github.com/carrtesy/M2N2"},{"id":"http://arxiv.org/abs/2312.11973v1","updated":"2023-12-19T09:11:49Z","published":"2023-12-19T09:11:49Z","title":"Continual Learning: Forget-free Winning Subnetworks for Video\n  Representations","summary":"  Inspired by the Regularized Lottery Ticket Hypothesis (RLTH), which\nhighlights the presence of competitive subnetworks within dense networks for\ncontinual learning tasks, we introduce Winning Subnetworks (WSN). This approach\nutilizes reused weights in dense networks to enhance learning in Task\nIncremental Learning (TIL) scenarios. To mitigate overfitting in Few-Shot Class\nIncremental Learning (FSCIL), we have developed WSN variants referred to as the\nSoft subnetwork (SoftNet). Furthermore, addressing WSN's limitation of sparse\nreused weights in Video Incremental Learning (VIL), we propose the Fourier\nSubneural Operator (FSO). The FSO, operating in Fourier space, adaptively and\ncompactly encodes videos, discovering reusable subnetworks with diverse\nbandwidths. We have applied FSO's Fourier representations to various continual\nlearning contexts, including VIL, TIL, and FSCIL. Our extensive experiments\nacross these scenarios demonstrate FSO's remarkable efficacy in continual\nlearning, significantly enhancing task performance at various convolutional\nrepresentational levels: it boosts performance in the higher layers for TIL and\nFSCIL and the lower layers for VIL.\n","authors":["Haeyong Kang","Jaehong Yoon","Sung Ju Hwang","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2312.11973v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2303.14962,\n  arXiv:2306.11305"},{"id":"http://arxiv.org/abs/2207.08012v5","updated":"2023-12-19T09:05:55Z","published":"2022-07-16T20:37:46Z","title":"Meta-Referential Games to Learn Compositional Learning Behaviours","summary":"  Human beings use compositionality to generalise from past experiences to\nnovel experiences. We assume a separation of our experiences into fundamental\natomic components that can be recombined in novel ways to support our ability\nto engage with novel experiences. We frame this as the ability to learn to\ngeneralise compositionally, and we will refer to behaviours making use of this\nability as compositional learning behaviours (CLBs). A central problem to\nlearning CLBs is the resolution of a binding problem (BP). While it is another\nfeat of intelligence that human beings perform with ease, it is not the case\nfor state-of-the-art artificial agents. Thus, in order to build artificial\nagents able to collaborate with human beings, we propose to develop a novel\nbenchmark to investigate agents' abilities to exhibit CLBs by solving a\ndomain-agnostic version of the BP. We take inspiration from the language\nemergence and grounding framework of referential games and propose a\nmeta-learning extension of referential games, entitled Meta-Referential Games,\nand use this framework to build our benchmark, the Symbolic Behaviour Benchmark\n(S2B). We provide baseline results and error analysis showing that our\nbenchmark is a compelling challenge that we hope will spur the research\ncommunity towards developing more capable artificial agents.\n","authors":["Kevin Denamganaï","Sondess Missaoui","James Alfred Walker"],"pdf_url":"https://arxiv.org/pdf/2207.08012v5.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2312.11969v1","updated":"2023-12-19T09:04:26Z","published":"2023-12-19T09:04:26Z","title":"GroupMixNorm Layer for Learning Fair Models","summary":"  Recent research has identified discriminatory behavior of automated\nprediction algorithms towards groups identified on specific protected\nattributes (e.g., gender, ethnicity, age group, etc.). When deployed in\nreal-world scenarios, such techniques may demonstrate biased predictions\nresulting in unfair outcomes. Recent literature has witnessed algorithms for\nmitigating such biased behavior mostly by adding convex surrogates of fairness\nmetrics such as demographic parity or equalized odds in the loss function,\nwhich are often not easy to estimate. This research proposes a novel\nin-processing based GroupMixNorm layer for mitigating bias from deep learning\nmodels. The GroupMixNorm layer probabilistically mixes group-level feature\nstatistics of samples across different groups based on the protected attribute.\nThe proposed method improves upon several fairness metrics with minimal impact\non overall accuracy. Analysis on benchmark tabular and image datasets\ndemonstrates the efficacy of the proposed method in achieving state-of-the-art\nperformance. Further, the experimental analysis also suggests the robustness of\nthe GroupMixNorm layer against new protected attributes during inference and\nits utility in eliminating bias from a pre-trained network.\n","authors":["Anubha Pandey","Aditi Rai","Maneet Singh","Deepak Bhatt","Tanmoy Bhowmik"],"pdf_url":"https://arxiv.org/pdf/2312.11969v1.pdf","comment":"12 pages, 6 figures, Pacific-Asia Conference on Knowledge Discovery\n  and Data Mining (PAKDD) 2023"},{"id":"http://arxiv.org/abs/2210.15657v3","updated":"2023-12-19T08:59:26Z","published":"2022-10-25T10:20:27Z","title":"Detecting fake accounts through Generative Adversarial Network in online\n  social media","summary":"  Online social media is integral to human life, facilitating messaging,\ninformation sharing, and confidential communication while preserving privacy.\nPlatforms like Twitter, Instagram, and Facebook exemplify this phenomenon.\nHowever, users face challenges due to network anomalies, often stemming from\nmalicious activities such as identity theft for financial gain or harm. This\npaper proposes a novel method using user similarity measures and the Generative\nAdversarial Network (GAN) algorithm to identify fake user accounts in the\nTwitter dataset. Despite the problem's complexity, the method achieves an AUC\nrate of 80\\% in classifying and detecting fake accounts. Notably, the study\nbuilds on previous research, highlighting advancements and insights into the\nevolving landscape of anomaly detection in online social networks.\n","authors":["Jinus Bordbar","Mohammadreza Mohammadrezaie","Saman Ardalan","Mohammad Ebrahim Shiri"],"pdf_url":"https://arxiv.org/pdf/2210.15657v3.pdf","comment":"need more investigation on the paper"},{"id":"http://arxiv.org/abs/2212.01071v4","updated":"2023-12-19T08:58:50Z","published":"2022-12-02T10:22:18Z","title":"Fake detection in imbalance dataset by Semi-supervised learning with GAN","summary":"  As social media continues to grow rapidly, the prevalence of harassment on\nthese platforms has also increased. This has piqued the interest of researchers\nin the field of fake detection. Social media data, often forms complex graphs\nwith numerous nodes, posing several challenges. These challenges and\nlimitations include dealing with a significant amount of irrelevant features in\nmatrices and addressing issues such as high data dispersion and an imbalanced\nclass distribution within the dataset. To overcome these challenges and\nlimitations, researchers have employed auto-encoders and a combination of\nsemi-supervised learning with a GAN algorithm, referred to as SGAN. Our\nproposed method utilizes auto-encoders for feature extraction and incorporates\nSGAN. By leveraging an unlabeled dataset, the unsupervised layer of SGAN\ncompensates for the limited availability of labeled data, making efficient use\nof the limited number of labeled instances. Multiple evaluation metrics were\nemployed, including the Confusion Matrix and the ROC curve. The dataset was\ndivided into training and testing sets, with 100 labeled samples for training\nand 1,000 samples for testing. The novelty of our research lies in applying\nSGAN to address the issue of imbalanced datasets in fake account detection. By\noptimizing the use of a smaller number of labeled instances and reducing the\nneed for extensive computational power, our method offers a more efficient\nsolution. Additionally, our study contributes to the field by achieving an 81%\naccuracy in detecting fake accounts using only 100 labeled samples. This\ndemonstrates the potential of SGAN as a powerful tool for handling minority\nclasses and addressing big data challenges in fake account detection.\n","authors":["Jinus Bordbar","Saman Ardalan","Mohammadreza Mohammadrezaie","Zahra Ghasemi"],"pdf_url":"https://arxiv.org/pdf/2212.01071v4.pdf","comment":"need more investigation on results"},{"id":"http://arxiv.org/abs/2312.11952v1","updated":"2023-12-19T08:53:00Z","published":"2023-12-19T08:53:00Z","title":"Automatic Parameter Selection for Non-Redundant Clustering","summary":"  High-dimensional datasets often contain multiple meaningful clusterings in\ndifferent subspaces. For example, objects can be clustered either by color,\nweight, or size, revealing different interpretations of the given dataset. A\nvariety of approaches are able to identify such non-redundant clusterings.\nHowever, most of these methods require the user to specify the expected number\nof subspaces and clusters for each subspace. Stating these values is a\nnon-trivial problem and usually requires detailed knowledge of the input\ndataset. In this paper, we propose a framework that utilizes the Minimum\nDescription Length Principle (MDL) to detect the number of subspaces and\nclusters per subspace automatically. We describe an efficient procedure that\ngreedily searches the parameter space by splitting and merging subspaces and\nclusters within subspaces. Additionally, an encoding strategy is introduced\nthat allows us to detect outliers in each subspace. Extensive experiments show\nthat our approach is highly competitive to state-of-the-art methods.\n","authors":["Collin Leiber","Dominik Mautz","Claudia Plant","Christian Böhm"],"pdf_url":"https://arxiv.org/pdf/2312.11952v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.01242v2","updated":"2023-12-19T08:52:02Z","published":"2023-02-02T17:24:43Z","title":"Neuro-Symbolic Continual Learning: Knowledge, Reasoning Shortcuts and\n  Concept Rehearsal","summary":"  We introduce Neuro-Symbolic Continual Learning, where a model has to solve a\nsequence of neuro-symbolic tasks, that is, it has to map sub-symbolic inputs to\nhigh-level concepts and compute predictions by reasoning consistently with\nprior knowledge. Our key observation is that neuro-symbolic tasks, although\ndifferent, often share concepts whose semantics remains stable over time.\nTraditional approaches fall short: existing continual strategies ignore\nknowledge altogether, while stock neuro-symbolic architectures suffer from\ncatastrophic forgetting. We show that leveraging prior knowledge by combining\nneuro-symbolic architectures with continual strategies does help avoid\ncatastrophic forgetting, but also that doing so can yield models affected by\nreasoning shortcuts. These undermine the semantics of the acquired concepts,\neven when detailed prior knowledge is provided upfront and inference is exact,\nand in turn continual performance. To overcome these issues, we introduce COOL,\na COncept-level cOntinual Learning strategy tailored for neuro-symbolic\ncontinual problems that acquires high-quality concepts and remembers them over\ntime. Our experiments on three novel benchmarks highlights how COOL attains\nsustained high performance on neuro-symbolic continual learning tasks in which\nother strategies fail.\n","authors":["Emanuele Marconato","Gianpaolo Bontempo","Elisa Ficarra","Simone Calderara","Andrea Passerini","Stefano Teso"],"pdf_url":"https://arxiv.org/pdf/2302.01242v2.pdf","comment":"40th International Conference on Machine Learning (ICML 2023)"},{"id":"http://arxiv.org/abs/2010.10258v3","updated":"2023-12-19T08:45:50Z","published":"2020-10-19T03:01:33Z","title":"Hierarchical Autoregressive Modeling for Neural Video Compression","summary":"  Recent work by Marino et al. (2020) showed improved performance in sequential\ndensity estimation by combining masked autoregressive flows with hierarchical\nlatent variable models. We draw a connection between such autoregressive\ngenerative models and the task of lossy video compression. Specifically, we\nview recent neural video compression methods (Lu et al., 2019; Yang et al.,\n2020b; Agustssonet al., 2020) as instances of a generalized stochastic temporal\nautoregressive transform, and propose avenues for enhancement based on this\ninsight. Comprehensive evaluations on large-scale video data show improved\nrate-distortion performance over both state-of-the-art neural and conventional\nvideo compression methods.\n","authors":["Ruihan Yang","Yibo Yang","Joseph Marino","Stephan Mandt"],"pdf_url":"https://arxiv.org/pdf/2010.10258v3.pdf","comment":"Published as a conference paper at ICLR 2021"},{"id":"http://arxiv.org/abs/2308.12681v2","updated":"2023-12-19T08:43:57Z","published":"2023-08-24T09:40:37Z","title":"LR-XFL: Logical Reasoning-based Explainable Federated Learning","summary":"  Federated learning (FL) is an emerging approach for training machine learning\nmodels collaboratively while preserving data privacy. The need for privacy\nprotection makes it difficult for FL models to achieve global transparency and\nexplainability. To address this limitation, we incorporate logic-based\nexplanations into FL by proposing the Logical Reasoning-based eXplainable\nFederated Learning (LR-XFL) approach. Under LR-XFL, FL clients create local\nlogic rules based on their local data and send them, along with model updates,\nto the FL server. The FL server connects the local logic rules through a proper\nlogical connector that is derived based on properties of client data, without\nrequiring access to the raw data. In addition, the server also aggregates the\nlocal model updates with weight values determined by the quality of the\nclients' local data as reflected by their uploaded logic rules. The results\nshow that LR-XFL outperforms the most relevant baseline by 1.19%, 5.81% and\n5.41% in terms of classification accuracy, rule accuracy and rule fidelity,\nrespectively. The explicit rule evaluation and expression under LR-XFL enable\nhuman experts to validate and correct the rules on the server side, hence\nimproving the global FL model's robustness to errors. It has the potential to\nenhance the transparency of FL models for areas like healthcare and finance\nwhere both data privacy and explainability are important.\n","authors":["Yanci Zhang","Han Yu"],"pdf_url":"https://arxiv.org/pdf/2308.12681v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11939v1","updated":"2023-12-19T08:38:03Z","published":"2023-12-19T08:38:03Z","title":"Time-Series Contrastive Learning against False Negatives and Class\n  Imbalance","summary":"  As an exemplary self-supervised approach for representation learning,\ntime-series contrastive learning has exhibited remarkable advancements in\ncontemporary research. While recent contrastive learning strategies have\nfocused on how to construct appropriate positives and negatives, in this study,\nwe conduct theoretical analysis and find they have overlooked the fundamental\nissues: false negatives and class imbalance inherent in the InfoNCE loss-based\nframework. Therefore, we introduce a straightforward modification grounded in\nthe SimCLR framework, universally adaptable to models engaged in the instance\ndiscrimination task. By constructing instance graphs to facilitate interactive\nlearning among instances, we emulate supervised contrastive learning via the\nmultiple-instances discrimination task, mitigating the harmful impact of false\nnegatives. Moreover, leveraging the graph structure and few-labeled data, we\nperform semi-supervised consistency classification and enhance the\nrepresentative ability of minority classes. We compared our method with the\nmost popular time-series contrastive learning methods on four real-world\ntime-series datasets and demonstrated our significant advantages in overall\nperformance.\n","authors":["Xiyuan Jin","Jing Wang","Lei Liu","Youfang Lin"],"pdf_url":"https://arxiv.org/pdf/2312.11939v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09844v2","updated":"2023-12-19T08:27:44Z","published":"2023-12-15T14:49:41Z","title":"Small Dataset, Big Gains: Enhancing Reinforcement Learning by Offline\n  Pre-Training with Model Based Augmentation","summary":"  Offline reinforcement learning leverages pre-collected datasets of\ntransitions to train policies. It can serve as effective initialization for\nonline algorithms, enhancing sample efficiency and speeding up convergence.\nHowever, when such datasets are limited in size and quality, offline\npre-training can produce sub-optimal policies and lead to degraded online\nreinforcement learning performance. In this paper we propose a model-based data\naugmentation strategy to maximize the benefits of offline reinforcement\nlearning pre-training and reduce the scale of data needed to be effective. Our\napproach leverages a world model of the environment trained on the offline\ndataset to augment states during offline pre-training. We evaluate our approach\non a variety of MuJoCo robotic tasks and our results show it can jump-start\nonline fine-tuning and substantially reduce - in some cases by an order of\nmagnitude - the required number of environment interactions.\n","authors":["Girolamo Macaluso","Alessandro Sestini","Andrew D. Bagdanov"],"pdf_url":"https://arxiv.org/pdf/2312.09844v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11934v1","updated":"2023-12-19T08:20:19Z","published":"2023-12-19T08:20:19Z","title":"Identification of Causal Structure with Latent Variables Based on Higher\n  Order Cumulants","summary":"  Causal discovery with latent variables is a crucial but challenging task.\nDespite the emergence of numerous methods aimed at addressing this challenge,\nthey are not fully identified to the structure that two observed variables are\ninfluenced by one latent variable and there might be a directed edge in\nbetween. Interestingly, we notice that this structure can be identified through\nthe utilization of higher-order cumulants. By leveraging the higher-order\ncumulants of non-Gaussian data, we provide an analytical solution for\nestimating the causal coefficients or their ratios. With the estimated (ratios\nof) causal coefficients, we propose a novel approach to identify the existence\nof a causal edge between two observed variables subject to latent variable\ninfluence. In case when such a causal edge exits, we introduce an asymmetry\ncriterion to determine the causal direction. The experimental results\ndemonstrate the effectiveness of our proposed method.\n","authors":["Wei Chen","Zhiyi Huang","Ruichu Cai","Zhifeng Hao","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.11934v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.11933v1","updated":"2023-12-19T08:20:09Z","published":"2023-12-19T08:20:09Z","title":"Dynamic Frequency Domain Graph Convolutional Network for Traffic\n  Forecasting","summary":"  Complex spatial dependencies in transportation networks make traffic\nprediction extremely challenging. Much existing work is devoted to learning\ndynamic graph structures among sensors, and the strategy of mining spatial\ndependencies from traffic data, known as data-driven, tends to be an intuitive\nand effective approach. However, Time-Shift of traffic patterns and noise\ninduced by random factors hinder data-driven spatial dependence modeling. In\nthis paper, we propose a novel dynamic frequency domain graph convolution\nnetwork (DFDGCN) to capture spatial dependencies. Specifically, we mitigate the\neffects of time-shift by Fourier transform, and introduce the identity\nembedding of sensors and time embedding when capturing data for graph learning\nsince traffic data with noise is not entirely reliable. The graph is combined\nwith static predefined and self-adaptive graphs during graph convolution to\npredict future traffic data through classical causal convolutions. Extensive\nexperiments on four real-world datasets demonstrate that our model is effective\nand outperforms the baselines.\n","authors":["Yujie Li","Zezhi Shao","Yongjun Xu","Qiang Qiu","Zhaogang Cao","Fei Wang"],"pdf_url":"https://arxiv.org/pdf/2312.11933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11929v1","updated":"2023-12-19T08:15:22Z","published":"2023-12-19T08:15:22Z","title":"Transformer Network for Multi-Person Tracking and Re-Identification in\n  Unconstrained Environment","summary":"  Multi-object tracking (MOT) has profound applications in a variety of fields,\nincluding surveillance, sports analytics, self-driving, and cooperative\nrobotics. Despite considerable advancements, existing MOT methodologies tend to\nfalter when faced with non-uniform movements, occlusions, and\nappearance-reappearance scenarios of the objects. Recognizing this inadequacy,\nwe put forward an integrated MOT method that not only marries object detection\nand identity linkage within a singular, end-to-end trainable framework but also\nequips the model with the ability to maintain object identity links over long\nperiods of time. Our proposed model, named STMMOT, is built around four key\nmodules: 1) candidate proposal generation, which generates object proposals via\na vision-transformer encoder-decoder architecture that detects the object from\neach frame in the video; 2) scale variant pyramid, a progressive pyramid\nstructure to learn the self-scale and cross-scale similarities in multi-scale\nfeature maps; 3) spatio-temporal memory encoder, extracting the essential\ninformation from the memory associated with each object under tracking; and 4)\nspatio-temporal memory decoder, simultaneously resolving the tasks of object\ndetection and identity association for MOT. Our system leverages a robust\nspatio-temporal memory module that retains extensive historical observations\nand effectively encodes them using an attention-based aggregator. The\nuniqueness of STMMOT lies in representing objects as dynamic query embeddings\nthat are updated continuously, which enables the prediction of object states\nwith attention mechanisms and eradicates the need for post-processing.\n","authors":["Hamza Mukhtar","Muhammad Usman Ghani Khan"],"pdf_url":"https://arxiv.org/pdf/2312.11929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10276v2","updated":"2023-12-19T08:12:51Z","published":"2023-12-16T00:50:17Z","title":"Asymmetric Norms to Approximate the Minimum Action Distance","summary":"  This paper presents a state representation for reward-free Markov decision\nprocesses. The idea is to learn, in a self-supervised manner, an embedding\nspace where distances between pairs of embedded states correspond to the\nminimum number of actions needed to transition between them. Unlike previous\nmethods, our approach incorporates an asymmetric norm parametrization, enabling\naccurate approximations of minimum action distances in environments with\ninherent asymmetry. We show how this representation can be leveraged to learn\ngoal-conditioned policies, providing a notion of similarity between states and\ngoals and a useful heuristic distance to guide planning. To validate our\napproach, we conduct empirical experiments on both symmetric and asymmetric\nenvironments. Our results show that our asymmetric norm parametrization\nperforms comparably to symmetric norms in symmetric environments and surpasses\nsymmetric norms in asymmetric environments.\n","authors":["Lorenzo Steccanella","Anders Jonsson"],"pdf_url":"https://arxiv.org/pdf/2312.10276v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11927v1","updated":"2023-12-19T08:09:36Z","published":"2023-12-19T08:09:36Z","title":"Empowering Dual-Level Graph Self-Supervised Pretraining with Motif\n  Discovery","summary":"  While self-supervised graph pretraining techniques have shown promising\nresults in various domains, their application still experiences challenges of\nlimited topology learning, human knowledge dependency, and incompetent\nmulti-level interactions. To address these issues, we propose a novel solution,\nDual-level Graph self-supervised Pretraining with Motif discovery (DGPM), which\nintroduces a unique dual-level pretraining structure that orchestrates\nnode-level and subgraph-level pretext tasks. Unlike prior approaches, DGPM\nautonomously uncovers significant graph motifs through an edge pooling module,\naligning learned motif similarities with graph kernel-based similarities. A\ncross-matching task enables sophisticated node-motif interactions and novel\nrepresentation learning. Extensive experiments on 15 datasets validate DGPM's\neffectiveness and generalizability, outperforming state-of-the-art methods in\nunsupervised representation learning and transfer learning settings. The\nautonomously discovered motifs demonstrate the potential of DGPM to enhance\nrobustness and interpretability.\n","authors":["Pengwei Yan","Kaisong Song","Zhuoren Jiang","Yangyang Kang","Tianqianjin Lin","Changlong Sun","Xiaozhong Liu"],"pdf_url":"https://arxiv.org/pdf/2312.11927v1.pdf","comment":"14 pages, 6 figures, accepted by AAAI'24"},{"id":"http://arxiv.org/abs/2312.11926v1","updated":"2023-12-19T08:07:41Z","published":"2023-12-19T08:07:41Z","title":"Big Learning Expectation Maximization","summary":"  Mixture models serve as one fundamental tool with versatile applications.\nHowever, their training techniques, like the popular Expectation Maximization\n(EM) algorithm, are notoriously sensitive to parameter initialization and often\nsuffer from bad local optima that could be arbitrarily worse than the optimal.\nTo address the long-lasting bad-local-optima challenge, we draw inspiration\nfrom the recent ground-breaking foundation models and propose to leverage their\nunderlying big learning principle to upgrade the EM. Specifically, we present\nthe Big Learning EM (BigLearn-EM), an EM upgrade that simultaneously performs\njoint, marginal, and orthogonally transformed marginal matchings between data\nand model distributions. Through simulated experiments, we empirically show\nthat the BigLearn-EM is capable of delivering the optimal with high\nprobability; comparisons on benchmark clustering datasets further demonstrate\nits effectiveness and advantages over existing techniques. The code is\navailable at\nhttps://github.com/YulaiCong/Big-Learning-Expectation-Maximization.\n","authors":["Yulai Cong","Sijia Li"],"pdf_url":"https://arxiv.org/pdf/2312.11926v1.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2312.11918v1","updated":"2023-12-19T07:56:25Z","published":"2023-12-19T07:56:25Z","title":"A Case Study in CUDA Kernel Fusion: Implementing FlashAttention-2 on\n  NVIDIA Hopper Architecture using the CUTLASS Library","summary":"  We provide an optimized implementation of the forward pass of\nFlashAttention-2, a popular memory-aware scaled dot-product attention\nalgorithm, as a custom fused CUDA kernel targeting NVIDIA Hopper architecture\nand written using the open-source CUTLASS library. In doing so, we explain the\nchallenges and techniques involved in fusing online-softmax with back-to-back\nGEMM kernels, utilizing the Hopper-specific Tensor Memory Accelerator (TMA) and\nWarpgroup Matrix-Multiply-Accumulate (WGMMA) instructions, defining and\ntransforming CUTLASS Layouts and Tensors, overlapping copy and GEMM operations,\nand choosing optimal tile sizes for the Q, K and V attention matrices while\nbalancing the register pressure and shared memory utilization. In head-to-head\nbenchmarks on a single H100 PCIe GPU for some common choices of\nhyperparameters, we observe 20-50% higher FLOPs/s over a version of\nFlashAttention-2 optimized for last-generation NVIDIA Ampere architecture.\n","authors":["Ganesh Bikshandi","Jay Shah"],"pdf_url":"https://arxiv.org/pdf/2312.11918v1.pdf","comment":"13 pages, comments welcome"},{"id":"http://arxiv.org/abs/2303.10343v2","updated":"2023-12-19T07:44:31Z","published":"2023-03-18T06:13:30Z","title":"Supervision Interpolation via LossMix: Generalizing Mixup for Object\n  Detection and Beyond","summary":"  The success of data mixing augmentations in image classification tasks has\nbeen well-received. However, these techniques cannot be readily applied to\nobject detection due to challenges such as spatial misalignment,\nforeground/background distinction, and plurality of instances. To tackle these\nissues, we first introduce a novel conceptual framework called Supervision\nInterpolation (SI), which offers a fresh perspective on interpolation-based\naugmentations by relaxing and generalizing Mixup. Based on SI, we propose\nLossMix, a simple yet versatile and effective regularization that enhances the\nperformance and robustness of object detectors and more. Our key insight is\nthat we can effectively regularize the training on mixed data by interpolating\ntheir loss errors instead of ground truth labels. Empirical results on the\nPASCAL VOC and MS COCO datasets demonstrate that LossMix can consistently\noutperform state-of-the-art methods widely adopted for detection. Furthermore,\nby jointly leveraging LossMix with unsupervised domain adaptation, we\nsuccessfully improve existing approaches and set a new state of the art for\ncross-domain object detection.\n","authors":["Thanh Vu","Baochen Sun","Bodi Yuan","Alex Ngai","Yueqi Li","Jan-Michael Frahm"],"pdf_url":"https://arxiv.org/pdf/2303.10343v2.pdf","comment":"AAAI-24 Camera Ready Version, with supplementary material, 15 pages"},{"id":"http://arxiv.org/abs/2309.11518v2","updated":"2023-12-19T07:40:45Z","published":"2023-09-19T09:17:07Z","title":"Ad-load Balancing via Off-policy Learning in a Content Marketplace","summary":"  Ad-load balancing is a critical challenge in online advertising systems,\nparticularly in the context of social media platforms, where the goal is to\nmaximize user engagement and revenue while maintaining a satisfactory user\nexperience. This requires the optimization of conflicting objectives, such as\nuser satisfaction and ads revenue. Traditional approaches to ad-load balancing\nrely on static allocation policies, which fail to adapt to changing user\npreferences and contextual factors. In this paper, we present an approach that\nleverages off-policy learning and evaluation from logged bandit feedback. We\nstart by presenting a motivating analysis of the ad-load balancing problem,\nhighlighting the conflicting objectives between user satisfaction and ads\nrevenue. We emphasize the nuances that arise due to user heterogeneity and the\ndependence on the user's position within a session. Based on this analysis, we\ndefine the problem as determining the optimal ad-load for a particular feed\nfetch. To tackle this problem, we propose an off-policy learning framework that\nleverages unbiased estimators such as Inverse Propensity Scoring (IPS) and\nDoubly Robust (DR) to learn and estimate the policy values using offline\ncollected stochastic data. We present insights from online A/B experiments\ndeployed at scale across over 80 million users generating over 200 million\nsessions, where we find statistically significant improvements in both user\nsatisfaction metrics and ads revenue for the platform.\n","authors":["Hitesh Sagtani","Madan Jhawar","Rishabh Mehrotra","Olivier Jeunen"],"pdf_url":"https://arxiv.org/pdf/2309.11518v2.pdf","comment":"Early version presented at the CONSEQUENCES '23 workshop at RecSys\n  '23, final version appearing at WSDM '24"},{"id":"http://arxiv.org/abs/2303.06999v3","updated":"2023-12-19T07:30:25Z","published":"2023-03-13T10:54:52Z","title":"Identifying Label Errors in Object Detection Datasets by Loss Inspection","summary":"  Labeling datasets for supervised object detection is a dull and\ntime-consuming task. Errors can be easily introduced during annotation and\noverlooked during review, yielding inaccurate benchmarks and performance\ndegradation of deep neural networks trained on noisy labels. In this work, we\nfor the first time introduce a benchmark for label error detection methods on\nobject detection datasets as well as a label error detection method and a\nnumber of baselines. We simulate four different types of randomly introduced\nlabel errors on train and test sets of well-labeled object detection datasets.\nFor our label error detection method we assume a two-stage object detector to\nbe given and consider the sum of both stages' classification and regression\nlosses. The losses are computed with respect to the predictions and the noisy\nlabels including simulated label errors, aiming at detecting the latter. We\ncompare our method to three baselines: a naive one without deep learning, the\nobject detector's score and the entropy of the classification softmax\ndistribution. We outperform all baselines and demonstrate that among the\nconsidered methods, ours is the only one that detects label errors of all four\ntypes efficiently. Furthermore, we detect real label errors a) on commonly used\ntest datasets in object detection and b) on a proprietary dataset. In both\ncases we achieve low false positives rates, i.e., we detect label errors with a\nprecision for a) of up to 71.5% and for b) with 97%.\n","authors":["Marius Schubert","Tobias Riedlinger","Karsten Kahl","Daniel Kröll","Sebastian Schoenen","Siniša Šegvić","Matthias Rottmann"],"pdf_url":"https://arxiv.org/pdf/2303.06999v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11905v1","updated":"2023-12-19T07:23:49Z","published":"2023-12-19T07:23:49Z","title":"Convergence Visualizer of Decentralized Federated Distillation with\n  Reduced Communication Costs","summary":"  Federated learning (FL) achieves collaborative learning without the need for\ndata sharing, thus preventing privacy leakage. To extend FL into a fully\ndecentralized algorithm, researchers have applied distributed optimization\nalgorithms to FL by considering machine learning (ML) tasks as parameter\noptimization problems. Conversely, the consensus-based multi-hop federated\ndistillation (CMFD) proposed in the authors' previous work makes neural network\n(NN) models get close with others in a function space rather than in a\nparameter space. Hence, this study solves two unresolved challenges of CMFD:\n(1) communication cost reduction and (2) visualization of model convergence.\nBased on a proposed dynamic communication cost reduction method (DCCR), the\namount of data transferred in a network is reduced; however, with a slight\ndegradation in the prediction accuracy. In addition, a technique for\nvisualizing the distance between the NN models in a function space is also\nproposed. The technique applies a dimensionality reduction technique by\napproximating infinite-dimensional functions as numerical vectors to visualize\nthe trajectory of how the models change by the distributed learning algorithm.\n","authors":["Akihito Taya","Yuuki Nishiyama","Kaoru Sezaki"],"pdf_url":"https://arxiv.org/pdf/2312.11905v1.pdf","comment":"(c) 2023 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2311.15570v2","updated":"2023-12-19T07:12:21Z","published":"2023-11-27T06:38:07Z","title":"UFDA: Universal Federated Domain Adaptation with Practical Assumptions","summary":"  Conventional Federated Domain Adaptation (FDA) approaches usually demand an\nabundance of assumptions, which makes them significantly less feasible for\nreal-world situations and introduces security hazards. This paper relaxes the\nassumptions from previous FDAs and studies a more practical scenario named\nUniversal Federated Domain Adaptation (UFDA). It only requires the black-box\nmodel and the label set information of each source domain, while the label sets\nof different source domains could be inconsistent, and the target-domain label\nset is totally blind. Towards a more effective solution for our newly proposed\nUFDA scenario, we propose a corresponding methodology called Hot-Learning with\nContrastive Label Disambiguation (HCLD). It particularly tackles UFDA's domain\nshifts and category gaps problems by using one-hot outputs from the black-box\nmodels of various source domains. Moreover, to better distinguish the shared\nand unknown classes, we further present a cluster-level strategy named\nMutual-Voting Decision (MVD) to extract robust consensus knowledge across peer\nclasses from both source and target domains. Extensive experiments on three\nbenchmark datasets demonstrate that our method achieves comparable performance\nfor our UFDA scenario with much fewer assumptions, compared to previous\nmethodologies with comprehensive additional assumptions.\n","authors":["Xinhui Liu","Zhenghao Chen","Luping Zhou","Dong Xu","Wei Xi","Gairui Bai","Yihan Zhao","Jizhong Zhao"],"pdf_url":"https://arxiv.org/pdf/2311.15570v2.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2312.11903v1","updated":"2023-12-19T07:06:32Z","published":"2023-12-19T07:06:32Z","title":"Sign Language Conversation Interpretation Using Wearable Sensors and\n  Machine Learning","summary":"  The count of people suffering from various levels of hearing loss reached\n1.57 billion in 2019. This huge number tends to suffer on many personal and\nprofessional levels and strictly needs to be included with the rest of society\nhealthily. This paper presents a proof of concept of an automatic sign language\nrecognition system based on data obtained using a wearable device of 3 flex\nsensors. The system is designed to interpret a selected set of American Sign\nLanguage (ASL) dynamic words by collecting data in sequences of the performed\nsigns and using machine learning methods. The built models achieved\nhigh-quality performances, such as Random Forest with 99% accuracy, Support\nVector Machine (SVM) with 99%, and two K-Nearest Neighbor (KNN) models with\n98%. This indicates many possible paths toward the development of a full-scale\nsystem.\n","authors":["Basma Kalandar","Ziemowit Dworakowski"],"pdf_url":"https://arxiv.org/pdf/2312.11903v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11898v1","updated":"2023-12-19T06:47:22Z","published":"2023-12-19T06:47:22Z","title":"Short-Term Multi-Horizon Line Loss Rate Forecasting of a Distribution\n  Network Using Attention-GCN-LSTM","summary":"  Accurately predicting line loss rates is vital for effective line loss\nmanagement in distribution networks, especially over short-term multi-horizons\nranging from one hour to one week. In this study, we propose\nAttention-GCN-LSTM, a novel method that combines Graph Convolutional Networks\n(GCN), Long Short-Term Memory (LSTM), and a three-level attention mechanism to\naddress this challenge. By capturing spatial and temporal dependencies, our\nmodel enables accurate forecasting of line loss rates across multiple horizons.\nThrough comprehensive evaluation using real-world data from 10KV feeders, our\nAttention-GCN-LSTM model consistently outperforms existing algorithms,\nexhibiting superior performance in terms of prediction accuracy and\nmulti-horizon forecasting. This model holds significant promise for enhancing\nline loss management in distribution networks.\n","authors":["Jie Liu","Yijia Cao","Yong Li","Yixiu Guo","Wei Deng"],"pdf_url":"https://arxiv.org/pdf/2312.11898v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02651v2","updated":"2023-12-19T06:44:03Z","published":"2023-10-04T08:19:04Z","title":"Hire When You Need to: Gradual Participant Recruitment for Auction-based\n  Federated Learning","summary":"  The success of Federated Learning (FL) depends on the quantity and quality of\nthe data owners (DOs) as well as their motivation to join FL model training.\nReputation-based FL participant selection methods have been proposed. However,\nthey still face the challenges of the cold start problem and potential\nselection bias towards highly reputable DOs. Such a bias can result in lower\nreputation DOs being prematurely excluded from future FL training rounds,\nthereby reducing the diversity of training data and the generalizability of the\nresulting models. To address these challenges, we propose the Gradual\nParticipant Selection scheme for Auction-based Federated Learning (GPS-AFL).\nUnlike existing AFL incentive mechanisms which generally assume that all DOs\nrequired for an FL task must be selected in one go, GPS-AFL gradually selects\nthe required DOs over multiple rounds of training as more information is\nrevealed through repeated interactions. It is designed to strike a balance\nbetween cost saving and performance enhancement, while mitigating the drawbacks\nof selection bias in reputation-based FL. Extensive experiments based on\nreal-world datasets demonstrate the significant advantages of GPS-AFL, which\nreduces costs by 33.65% and improved total utility by 2.91%, on average\ncompared to the best-performing state-of-the-art approach.\n","authors":["Xavier Tan","Han Yu"],"pdf_url":"https://arxiv.org/pdf/2310.02651v2.pdf","comment":"9 Pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2310.15516v3","updated":"2023-12-19T06:39:27Z","published":"2023-10-24T04:50:32Z","title":"Graph Attention-based Deep Reinforcement Learning for solving the\n  Chinese Postman Problem with Load-dependent costs","summary":"  Recently, Deep reinforcement learning (DRL) models have shown promising\nresults in solving routing problems. However, most DRL solvers are commonly\nproposed to solve node routing problems, such as the Traveling Salesman Problem\n(TSP). Meanwhile, there has been limited research on applying neural methods to\narc routing problems, such as the Chinese Postman Problem (CPP), since they\noften feature irregular and complex solution spaces compared to TSP. To fill\nthese gaps, this paper proposes a novel DRL framework to address the CPP with\nload-dependent costs (CPP-LC) (Corberan et al., 2018), which is a complex arc\nrouting problem with load constraints. The novelty of our method is two-fold.\nFirst, we formulate the CPP-LC as a Markov Decision Process (MDP) sequential\nmodel. Subsequently, we introduce an autoregressive model based on DRL, namely\nArc-DRL, consisting of an encoder and decoder to address the CPP-LC challenge\neffectively. Such a framework allows the DRL model to work efficiently and\nscalably to arc routing problems. Furthermore, we propose a new bio-inspired\nmeta-heuristic solution based on Evolutionary Algorithm (EA) for CPP-LC.\nExtensive experiments show that Arc-DRL outperforms existing meta-heuristic\nmethods such as Iterative Local Search (ILS) and Variable Neighborhood Search\n(VNS) proposed by (Corberan et al., 2018) on large benchmark datasets for\nCPP-LC regarding both solution quality and running time; while the EA gives the\nbest solution quality with much more running time. We release our C++\nimplementations for metaheuristics such as EA, ILS and VNS along with the code\nfor data generation and our generated data at\nhttps://github.com/HySonLab/Chinese_Postman_Problem\n","authors":["Cong Dao Tran","Truong Son Hy"],"pdf_url":"https://arxiv.org/pdf/2310.15516v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11894v1","updated":"2023-12-19T06:38:18Z","published":"2023-12-19T06:38:18Z","title":"3D-LFM: Lifting Foundation Model","summary":"  The lifting of 3D structure and camera from 2D landmarks is at the\ncornerstone of the entire discipline of computer vision. Traditional methods\nhave been confined to specific rigid objects, such as those in\nPerspective-n-Point (PnP) problems, but deep learning has expanded our\ncapability to reconstruct a wide range of object classes (e.g. C3PDO and PAUL)\nwith resilience to noise, occlusions, and perspective distortions. All these\ntechniques, however, have been limited by the fundamental need to establish\ncorrespondences across the 3D training data -- significantly limiting their\nutility to applications where one has an abundance of \"in-correspondence\" 3D\ndata. Our approach harnesses the inherent permutation equivariance of\ntransformers to manage varying number of points per 3D data instance,\nwithstands occlusions, and generalizes to unseen categories. We demonstrate\nstate of the art performance across 2D-3D lifting task benchmarks. Since our\napproach can be trained across such a broad class of structures we refer to it\nsimply as a 3D Lifting Foundation Model (3D-LFM) -- the first of its kind.\n","authors":["Mosam Dabhi","Laszlo A. Jeni","Simon Lucey"],"pdf_url":"https://arxiv.org/pdf/2312.11894v1.pdf","comment":"Project page is available at https://3dlfm.github.io"},{"id":"http://arxiv.org/abs/2312.11026v2","updated":"2023-12-19T06:32:32Z","published":"2023-12-18T08:59:31Z","title":"MISA: Unveiling the Vulnerabilities in Split Federated Learning","summary":"  \\textit{Federated learning} (FL) and \\textit{split learning} (SL) are\nprevailing distributed paradigms in recent years. They both enable shared\nglobal model training while keeping data localized on users' devices. The\nformer excels in parallel execution capabilities, while the latter enjoys low\ndependence on edge computing resources and strong privacy protection.\n\\textit{Split federated learning} (SFL) combines the strengths of both FL and\nSL, making it one of the most popular distributed architectures. Furthermore, a\nrecent study has claimed that SFL exhibits robustness against poisoning\nattacks, with a fivefold improvement compared to FL in terms of robustness.\n  In this paper, we present a novel poisoning attack known as MISA. It poisons\nboth the top and bottom models, causing a \\textbf{\\underline{misa}}lignment in\nthe global model, ultimately leading to a drastic accuracy collapse. This\nattack unveils the vulnerabilities in SFL, challenging the conventional belief\nthat SFL is robust against poisoning attacks. Extensive experiments demonstrate\nthat our proposed MISA poses a significant threat to the availability of SFL,\nunderscoring the imperative for academia and industry to accord this matter due\nattention.\n","authors":["Wei Wan","Yuxuan Ning","Shengshan Hu","Lulu Xue","Minghui Li","Leo Yu Zhang","Hai Jin"],"pdf_url":"https://arxiv.org/pdf/2312.11026v2.pdf","comment":"This paper has been accepted by the IEEE International Conference on\n  Acoustics, Speech, and Signal Processing (ICASSP 2024)"},{"id":"http://arxiv.org/abs/2312.11891v1","updated":"2023-12-19T06:28:32Z","published":"2023-12-19T06:28:32Z","title":"Hierarchical and Incremental Structural Entropy Minimization for\n  Unsupervised Social Event Detection","summary":"  As a trending approach for social event detection, graph neural network\n(GNN)-based methods enable a fusion of natural language semantics and the\ncomplex social network structural information, thus showing SOTA performance.\nHowever, GNN-based methods can miss useful message correlations. Moreover, they\nrequire manual labeling for training and predetermining the number of events\nfor prediction. In this work, we address social event detection via graph\nstructural entropy (SE) minimization. While keeping the merits of the GNN-based\nmethods, the proposed framework, HISEvent, constructs more informative message\ngraphs, is unsupervised, and does not require the number of events given a\npriori. Specifically, we incrementally explore the graph neighborhoods using\n1-dimensional (1D) SE minimization to supplement the existing message graph\nwith edges between semantically related messages. We then detect events from\nthe message graph by hierarchically minimizing 2-dimensional (2D) SE. Our\nproposed 1D and 2D SE minimization algorithms are customized for social event\ndetection and effectively tackle the efficiency problem of the existing SE\nminimization algorithms. Extensive experiments show that HISEvent consistently\noutperforms GNN-based methods and achieves the new SOTA for social event\ndetection under both closed- and open-set settings while being efficient and\nrobust.\n","authors":["Yuwei Cao","Hao Peng","Zhengtao Yu","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2312.11891v1.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2312.11882v1","updated":"2023-12-19T06:16:13Z","published":"2023-12-19T06:16:13Z","title":"ConsistentEE: A Consistent and Hardness-Guided Early Exiting Method for\n  Accelerating Language Models Inference","summary":"  Early Exiting is one of the most popular methods to achieve efficient\ninference. Current early exiting methods adopt the (weighted) sum of the cross\nentropy loss of all internal classifiers during training, imposing all these\nclassifiers to predict all instances correctly. However, during inference, as\nlong as one internal classifier predicts an instance correctly, it can\naccelerate without losing accuracy. Thus, there is a notable gap between\ntraining and inference. We propose ConsistentEE, an early exiting method that\nis consistent in training and inference. ConsistentEE formulates the early\nexiting process as a reinforcement learning problem. A policy network is added\nto decide whether an instance should exit or continue. The training objective\nof ConsistentEE only require each instance to be predicted correctly by one\ninternal classifier. Additionally, we introduce the concept Memorize Layer to\nmeasure the hardness of an instance. We incorporate memorized layer into reward\nfunction design, which allows ``easy'' instances to focus more on acceleration\nwhile ``hard'' instances to focus more on accuracy. Experimental results show\nthat our method outperforms other baselines on various natural language\nunderstanding and generation tasks.\n","authors":["Ziqian Zeng","Yihuai Hong","Hongliang Dai","Huiping Zhuang","Cen Chen"],"pdf_url":"https://arxiv.org/pdf/2312.11882v1.pdf","comment":"Accepted in AAAI24"},{"id":"http://arxiv.org/abs/2312.11880v1","updated":"2023-12-19T06:13:58Z","published":"2023-12-19T06:13:58Z","title":"Point Cloud Segmentation Using Transfer Learning with RandLA-Net: A Case\n  Study on Urban Areas","summary":"  Urban environments are characterized by complex structures and diverse\nfeatures, making accurate segmentation of point cloud data a challenging task.\nThis paper presents a comprehensive study on the application of RandLA-Net, a\nstate-of-the-art neural network architecture, for the 3D segmentation of\nlarge-scale point cloud data in urban areas. The study focuses on three major\nChinese cities, namely Chengdu, Jiaoda, and Shenzhen, leveraging their unique\ncharacteristics to enhance segmentation performance.\n  To address the limited availability of labeled data for these specific urban\nareas, we employed transfer learning techniques. We transferred the learned\nweights from the Sensat Urban and Toronto 3D datasets to initialize our\nRandLA-Net model. Additionally, we performed class remapping to adapt the model\nto the target urban areas, ensuring accurate segmentation results.\n  The experimental results demonstrate the effectiveness of the proposed\napproach achieving over 80\\% F1 score for each areas in 3D point cloud\nsegmentation. The transfer learning strategy proves to be crucial in overcoming\ndata scarcity issues, providing a robust solution for urban point cloud\nanalysis. The findings contribute to the advancement of point cloud\nsegmentation methods, especially in the context of rapidly evolving Chinese\nurban areas.\n","authors":["Alperen Enes Bayar","Ufuk Uyan","Elif Toprak","Cao Yuheng","Tang Juncheng","Ahmet Alp Kindiroglu"],"pdf_url":"https://arxiv.org/pdf/2312.11880v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11875v1","updated":"2023-12-19T06:06:30Z","published":"2023-12-19T06:06:30Z","title":"Sparse is Enough in Fine-tuning Pre-trained Large Language Model","summary":"  With the prevalence of pre-training-fine-tuning paradigm, how to efficiently\nadapt the pre-trained model to the downstream tasks has been an intriguing\nissue. Parameter-Efficient Fine-Tuning (PEFT) methods have been proposed for\nlow-cost adaptation, including Adapters, Bia-only, and the recently widely used\nLow-Rank Adaptation. Although these methods have demonstrated their\neffectiveness to some extent and have been widely applied, the underlying\nprinciples are still unclear. In this paper, we reveal the transition of loss\nlandscape in the downstream domain from random initialization to pre-trained\ninitialization, that is, from low-amplitude oscillation to high-amplitude\noscillation. The parameter gradients exhibit a property akin to sparsity, where\na small fraction of components dominate the total gradient norm, for instance,\n1% of the components account for 99% of the gradient. This property ensures\nthat the pre-trained model can easily find a flat minimizer which guarantees\nthe model's ability to generalize even with a low number of trainable\nparameters. Based on this, we propose a gradient-based sparse fine-tuning\nalgorithm, named Sparse Increment Fine-Tuning (SIFT), and validate its\neffectiveness on a range of tasks including the GLUE Benchmark and\nInstruction-tuning. The code is accessible at https://github.com/song-wx/SIFT/.\n","authors":["Weixi Song","Zuchao Li","Lefei Zhang","Hai Zhao","Bo Du"],"pdf_url":"https://arxiv.org/pdf/2312.11875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.10985v2","updated":"2023-12-19T05:51:09Z","published":"2021-12-21T05:07:54Z","title":"Learned ISTA with Error-based Thresholding for Adaptive Sparse Coding","summary":"  Drawing on theoretical insights, we advocate an error-based thresholding\n(EBT) mechanism for learned ISTA (LISTA), which utilizes a function of the\nlayer-wise reconstruction error to suggest a specific threshold for each\nobservation in the shrinkage function of each layer. We show that the proposed\nEBT mechanism well disentangles the learnable parameters in the shrinkage\nfunctions from the reconstruction errors, endowing the obtained models with\nimproved adaptivity to possible data variations. With rigorous analyses, we\nfurther show that the proposed EBT also leads to a faster convergence on the\nbasis of LISTA or its variants, in addition to its higher adaptivity. Extensive\nexperimental results confirm our theoretical analyses and verify the\neffectiveness of our methods.\n","authors":["Ziang Li","Kailun Wu","Yiwen Guo","Changshui Zhang"],"pdf_url":"https://arxiv.org/pdf/2112.10985v2.pdf","comment":"Accepted in ICASSP2024"},{"id":"http://arxiv.org/abs/2312.09131v2","updated":"2023-12-19T05:40:27Z","published":"2023-12-14T17:01:58Z","title":"Physics-Informed Neural Network Lyapunov Functions: PDE\n  Characterization, Learning, and Verification","summary":"  We provide a systematic investigation of using physics-informed neural\nnetworks to compute Lyapunov functions. We encode Lyapunov conditions as a\npartial differential equation (PDE) and use this for training neural network\nLyapunov functions. We analyze the analytical properties of the solutions to\nthe Lyapunov and Zubov PDEs. In particular, we show that employing the Zubov\nequation in training neural Lyapunov functions can lead to approximate regions\nof attraction close to the true domain of attraction. We also examine\napproximation errors and the convergence of neural approximations to the unique\nsolution of Zubov's equation. We then provide sufficient conditions for the\nlearned neural Lyapunov functions that can be readily verified by\nsatisfiability modulo theories (SMT) solvers, enabling formal verification of\nboth local stability analysis and region-of-attraction estimates in the large.\nThrough a number of nonlinear examples, ranging from low to high dimensions, we\ndemonstrate that the proposed framework can outperform traditional\nsums-of-squares (SOS) Lyapunov functions obtained using semidefinite\nprogramming (SDP).\n","authors":["Jun Liu","Yiming Meng","Maxwell Fitzsimmons","Ruikun Zhou"],"pdf_url":"https://arxiv.org/pdf/2312.09131v2.pdf","comment":"The current version has been submitted for publication"},{"id":"http://arxiv.org/abs/2311.05144v2","updated":"2023-12-19T05:21:31Z","published":"2023-11-09T04:49:41Z","title":"Counter-Empirical Attacking based on Adversarial Reinforcement Learning\n  for Time-Relevant Scoring System","summary":"  Scoring systems are commonly seen for platforms in the era of big data. From\ncredit scoring systems in financial services to membership scores in E-commerce\nshopping platforms, platform managers use such systems to guide users towards\nthe encouraged activity pattern, and manage resources more effectively and more\nefficiently thereby. To establish such scoring systems, several \"empirical\ncriteria\" are firstly determined, followed by dedicated top-down design for\neach factor of the score, which usually requires enormous effort to adjust and\ntune the scoring function in the new application scenario. What's worse, many\nfresh projects usually have no ground-truth or any experience to evaluate a\nreasonable scoring system, making the designing even harder. To reduce the\neffort of manual adjustment of the scoring function in every new scoring\nsystem, we innovatively study the scoring system from the preset empirical\ncriteria without any ground truth, and propose a novel framework to improve the\nsystem from scratch. In this paper, we propose a \"counter-empirical attacking\"\nmechanism that can generate \"attacking\" behavior traces and try to break the\nempirical rules of the scoring system. Then an adversarial \"enhancer\" is\napplied to evaluate the scoring system and find the improvement strategy. By\ntraining the adversarial learning problem, a proper scoring function can be\nlearned to be robust to the attacking activity traces that are trying to\nviolate the empirical criteria. Extensive experiments have been conducted on\ntwo scoring systems including a shared computing resource platform and a\nfinancial credit system. The experimental results have validated the\neffectiveness of our proposed framework.\n","authors":["Xiangguo Sun","Hong Cheng","Hang Dong","Bo Qiao","Si Qin","Qingwei Lin"],"pdf_url":"https://arxiv.org/pdf/2311.05144v2.pdf","comment":"Accepted by TKDE"},{"id":"http://arxiv.org/abs/2312.11863v1","updated":"2023-12-19T05:17:27Z","published":"2023-12-19T05:17:27Z","title":"Neural Network Approximation for Pessimistic Offline Reinforcement\n  Learning","summary":"  Deep reinforcement learning (RL) has shown remarkable success in specific\noffline decision-making scenarios, yet its theoretical guarantees are still\nunder development. Existing works on offline RL theory primarily emphasize a\nfew trivial settings, such as linear MDP or general function approximation with\nstrong assumptions and independent data, which lack guidance for practical use.\nThe coupling of deep learning and Bellman residuals makes this problem\nchallenging, in addition to the difficulty of data dependence. In this paper,\nwe establish a non-asymptotic estimation error of pessimistic offline RL using\ngeneral neural network approximation with $\\mathcal{C}$-mixing data regarding\nthe structure of networks, the dimension of datasets, and the concentrability\nof data coverage, under mild assumptions. Our result shows that the estimation\nerror consists of two parts: the first converges to zero at a desired rate on\nthe sample size with partially controllable concentrability, and the second\nbecomes negligible if the residual constraint is tight. This result\ndemonstrates the explicit efficiency of deep adversarial offline RL frameworks.\nWe utilize the empirical process tool for $\\mathcal{C}$-mixing sequences and\nthe neural network approximation theory for the H\\\"{o}lder class to achieve\nthis. We also develop methods to bound the Bellman estimation error caused by\nfunction approximation with empirical Bellman constraint perturbations.\nAdditionally, we present a result that lessens the curse of dimensionality\nusing data with low intrinsic dimensionality and function classes with low\ncomplexity. Our estimation provides valuable insights into the development of\ndeep offline RL and guidance for algorithm model design.\n","authors":["Di Wu","Yuling Jiao","Li Shen","Haizhao Yang","Xiliang Lu"],"pdf_url":"https://arxiv.org/pdf/2312.11863v1.pdf","comment":"Full version of the paper accepted to the 38th Annual AAAI Conference\n  on Artificial Intelligence (AAAI 2024)"},{"id":"http://arxiv.org/abs/2312.11862v1","updated":"2023-12-19T05:14:31Z","published":"2023-12-19T05:14:31Z","title":"Topo-MLP : A Simplicial Network Without Message Passing","summary":"  Due to their ability to model meaningful higher order relations among a set\nof entities, higher order network models have emerged recently as a powerful\nalternative for graph-based network models which are only capable of modeling\nbinary relationships. Message passing paradigm is still dominantly used to\nlearn representations even for higher order network models. While powerful,\nmessage passing can have disadvantages during inference, particularly when the\nhigher order connectivity information is missing or corrupted. To overcome such\nlimitations, we propose Topo-MLP, a purely MLP-based simplicial neural network\nalgorithm to learn the representation of elements in a simplicial complex\nwithout explicitly relying on message passing. Our framework utilizes a novel\nHigher Order Neighborhood Contrastive (HONC) loss which implicitly incorporates\nthe simplicial structure into representation learning. Our proposed model's\nsimplicity makes it faster during inference. Moreover, we show that our model\nis robust when faced with missing or corrupted connectivity structure.\n","authors":["Karthikeyan Natesan Ramamurthy","Aldo Guzmán-Sáenz","Mustafa Hajij"],"pdf_url":"https://arxiv.org/pdf/2312.11862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11861v1","updated":"2023-12-19T05:13:16Z","published":"2023-12-19T05:13:16Z","title":"MG-Skip: Random Multi-Gossip Skipping Method for Nonsmooth Distributed\n  Optimization","summary":"  Distributed optimization methods with probabilistic local updates have\nrecently gained attention for their provable ability to communication\nacceleration. Nevertheless, this capability is effective only when the loss\nfunction is smooth and the network is sufficiently well-connected. In this\npaper, we propose the first linear convergent method MG-Skip with probabilistic\nlocal updates for nonsmooth distributed optimization. Without any extra\ncondition for the network connectivity, MG-Skip allows for the multiple-round\ngossip communication to be skipped in most iterations, while its iteration\ncomplexity is $\\mathcal{O}\\left(\\kappa \\log \\frac{1}{\\epsilon}\\right)$ and\ncommunication complexity is only\n$\\mathcal{O}\\left(\\sqrt{\\frac{\\kappa}{(1-\\rho)}} \\log\n\\frac{1}{\\epsilon}\\right)$, where $\\kappa$ is the condition number of the loss\nfunction and $\\rho$ reflects the connectivity of the network topology. To the\nbest of our knowledge, MG-Skip achieves the best communication complexity when\nthe loss function has the smooth (strongly convex)+nonsmooth (convex) composite\nform.\n","authors":["Luyao Guo","Luqing Wang","Xinli Shi","Jinde Cao"],"pdf_url":"https://arxiv.org/pdf/2312.11861v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11858v1","updated":"2023-12-19T04:58:37Z","published":"2023-12-19T04:58:37Z","title":"SimCalib: Graph Neural Network Calibration based on Similarity between\n  Nodes","summary":"  Graph neural networks (GNNs) have exhibited impressive performance in\nmodeling graph data as exemplified in various applications. Recently, the GNN\ncalibration problem has attracted increasing attention, especially in\ncost-sensitive scenarios. Previous work has gained empirical insights on the\nissue, and devised effective approaches for it, but theoretical supports still\nfall short. In this work, we shed light on the relationship between GNN\ncalibration and nodewise similarity via theoretical analysis. A novel\ncalibration framework, named SimCalib, is accordingly proposed to consider\nsimilarity between nodes at global and local levels. At the global level, the\nMahalanobis distance between the current node and class prototypes is\nintegrated to implicitly consider similarity between the current node and all\nnodes in the same class. At the local level, the similarity of node\nrepresentation movement dynamics, quantified by nodewise homophily and relative\ndegree, is considered. Informed about the application of nodewise movement\npatterns in analyzing nodewise behavior on the over-smoothing problem, we\nempirically present a possible relationship between over-smoothing and GNN\ncalibration problem. Experimentally, we discover a correlation between nodewise\nsimilarity and model calibration improvement, in alignment with our theoretical\nresults. Additionally, we conduct extensive experiments investigating different\ndesign factors and demonstrate the effectiveness of our proposed SimCalib\nframework for GNN calibration by achieving state-of-the-art performance on 14\nout of 16 benchmarks.\n","authors":["Boshi Tang","Zhiyong Wu","Xixin Wu","Qiaochu Huang","Jun Chen","Shun Lei","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2312.11858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.13976v3","updated":"2023-12-19T04:44:35Z","published":"2023-08-27T00:31:04Z","title":"Label Denoising through Cross-Model Agreement","summary":"  Learning from corrupted labels is very common in real-world machine-learning\napplications. Memorizing such noisy labels could affect the learning of the\nmodel, leading to sub-optimal performances. In this work, we propose a novel\nframework to learn robust machine-learning models from noisy labels. Through an\nempirical study, we find that different models make relatively similar\npredictions on clean examples, while the predictions on noisy examples vary\nmuch more across different models. Motivated by this observation, we propose\n\\em denoising with cross-model agreement \\em (DeCA) which aims to minimize the\nKL-divergence between the true label distributions parameterized by two machine\nlearning models while maximizing the likelihood of data observation. We employ\nthe proposed DeCA on both the binary label scenario and the multiple label\nscenario. For the binary label scenario, we select implicit feedback\nrecommendation as the downstream task and conduct experiments with four\nstate-of-the-art recommendation models on four datasets. For the multiple-label\nscenario, the downstream application is image classification on two benchmark\ndatasets. Experimental results demonstrate that the proposed methods\nsignificantly improve the model performance compared with normal training and\nother denoising methods on both binary and multiple-label scenarios.\n","authors":["Yu Wang","Xin Xin","Zaiqiao Meng","Joemon Jose","Fuli Feng"],"pdf_url":"https://arxiv.org/pdf/2308.13976v3.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2105.09605"},{"id":"http://arxiv.org/abs/2312.09323v3","updated":"2023-12-19T04:31:21Z","published":"2023-12-07T19:58:37Z","title":"Perspectives on the State and Future of Deep Learning -- 2023","summary":"  The goal of this series is to chronicle opinions and issues in the field of\nmachine learning as they stand today and as they change over time. The plan is\nto host this survey periodically until the AI singularity\npaperclip-frenzy-driven doomsday, keeping an updated list of topical questions\nand interviewing new community members for each edition. In this issue, we\nprobed people's opinions on interpretable AI, the value of benchmarking in\nmodern NLP, the state of progress towards understanding deep learning, and the\nfuture of academia.\n","authors":["Micah Goldblum","Anima Anandkumar","Richard Baraniuk","Tom Goldstein","Kyunghyun Cho","Zachary C Lipton","Melanie Mitchell","Preetum Nakkiran","Max Welling","Andrew Gordon Wilson"],"pdf_url":"https://arxiv.org/pdf/2312.09323v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11846v1","updated":"2023-12-19T04:26:12Z","published":"2023-12-19T04:26:12Z","title":"Initializing Services in Interactive ML Systems for Diverse Users","summary":"  This paper studies ML systems that interactively learn from users across\nmultiple subpopulations with heterogeneous data distributions. The primary\nobjective is to provide specialized services for different user groups while\nalso predicting user preferences. Once the users select a service based on how\nwell the service anticipated their preference, the services subsequently adapt\nand refine themselves based on the user data they accumulate, resulting in an\niterative, alternating minimization process between users and services\n(learning dynamics). Employing such tailored approaches has two main\nchallenges: (i) Unknown user preferences: Typically, data on user preferences\nare unavailable without interaction, and uniform data collection across a large\nand diverse user base can be prohibitively expensive. (ii) Suboptimal Local\nSolutions: The total loss (sum of loss functions across all users and all\nservices) landscape is not convex even if the individual losses on a single\nservice are convex, making it likely for the learning dynamics to get stuck in\nlocal minima. The final outcome of the aforementioned learning dynamics is thus\nstrongly influenced by the initial set of services offered to users, and is not\nguaranteed to be close to the globally optimal outcome. In this work, we\npropose a randomized algorithm to adaptively select very few users to collect\npreference data from, while simultaneously initializing a set of services. We\nprove that under mild assumptions on the loss functions, the expected total\nloss achieved by the algorithm right after initialization is within a factor of\nthe globally optimal total loss with complete user preference data, and this\nfactor scales only logarithmically in the number of services. Our theory is\ncomplemented by experiments on real as well as semi-synthetic datasets.\n","authors":["Avinandan Bose","Mihaela Curmei","Daniel L. Jiang","Jamie Morgenstern","Sarah Dean","Lillian J. Ratliff","Maryam Fazel"],"pdf_url":"https://arxiv.org/pdf/2312.11846v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11835v1","updated":"2023-12-19T04:03:47Z","published":"2023-12-19T04:03:47Z","title":"Provably Convergent Federated Trilevel Learning","summary":"  Trilevel learning, also called trilevel optimization (TLO), has been\nrecognized as a powerful modelling tool for hierarchical decision process and\nwidely applied in many machine learning applications, such as robust neural\narchitecture search, hyperparameter optimization, and domain adaptation.\nTackling TLO problems has presented a great challenge due to their nested\ndecision-making structure. In addition, existing works on TLO face the\nfollowing key challenges: 1) they all focus on the non-distributed setting,\nwhich may lead to privacy breach; 2) they do not offer any non-asymptotic\nconvergence analysis which characterizes how fast an algorithm converges. To\naddress the aforementioned challenges, this paper proposes an asynchronous\nfederated trilevel optimization method to solve TLO problems. The proposed\nmethod utilizes $\\mu$-cuts to construct a hyper-polyhedral approximation for\nthe TLO problem and solve it in an asynchronous manner. We demonstrate that the\nproposed $\\mu$-cuts are applicable to not only convex functions but also a wide\nrange of non-convex functions that meet the $\\mu$-weakly convex assumption.\nFurthermore, we theoretically analyze the non-asymptotic convergence rate for\nthe proposed method by showing its iteration complexity to obtain\n$\\epsilon$-stationary point is upper bounded by\n$\\mathcal{O}(\\frac{1}{\\epsilon^2})$. Extensive experiments on real-world\ndatasets have been conducted to elucidate the superiority of the proposed\nmethod, e.g., it has a faster convergence rate with a maximum acceleration of\napproximately 80$\\%$.\n","authors":["Yang Jiao","Kai Yang","Tiancheng Wu","Chengtao Jian","Jianwei Huang"],"pdf_url":"https://arxiv.org/pdf/2312.11835v1.pdf","comment":"Accepted at AAAI 2024"},{"id":"http://arxiv.org/abs/2312.11834v1","updated":"2023-12-19T04:02:50Z","published":"2023-12-19T04:02:50Z","title":"Multi-agent reinforcement learning using echo-state network and its\n  application to pedestrian dynamics","summary":"  In recent years, simulations of pedestrians using the multi-agent\nreinforcement learning (MARL) have been studied. This study considered the\nroads on a grid-world environment, and implemented pedestrians as MARL agents\nusing an echo-state network and the least squares policy iteration method.\nUnder this environment, the ability of these agents to learn to move forward by\navoiding other agents was investigated. Specifically, we considered two types\nof tasks: the choice between a narrow direct route and a broad detour, and the\nbidirectional pedestrian flow in a corridor. The simulations results indicated\nthat the learning was successful when the density of the agents was not that\nhigh.\n","authors":["Hisato Komatsu"],"pdf_url":"https://arxiv.org/pdf/2312.11834v1.pdf","comment":"19 pages, 10 figures"},{"id":"http://arxiv.org/abs/2004.08697v7","updated":"2023-12-19T03:58:16Z","published":"2020-04-18T20:09:34Z","title":"CausalVAE: Structured Causal Disentanglement in Variational Autoencoder","summary":"  Learning disentanglement aims at finding a low dimensional representation\nwhich consists of multiple explanatory and generative factors of the\nobservational data. The framework of variational autoencoder (VAE) is commonly\nused to disentangle independent factors from observations. However, in real\nscenarios, factors with semantics are not necessarily independent. Instead,\nthere might be an underlying causal structure which renders these factors\ndependent. We thus propose a new VAE based framework named CausalVAE, which\nincludes a Causal Layer to transform independent exogenous factors into causal\nendogenous ones that correspond to causally related concepts in data. We\nfurther analyze the model identifiabitily, showing that the proposed model\nlearned from observations recovers the true one up to a certain degree by\nproviding supervision signals (e.g. feature labels). Experiments are conducted\non various datasets, including synthetic and real word benchmark CelebA.\nResults show that the causal representations learned by CausalVAE are\nsemantically interpretable, and their causal relationship as a Directed Acyclic\nGraph (DAG) is identified with good accuracy. Furthermore, we demonstrate that\nthe proposed CausalVAE model is able to generate counterfactual data through\n\"do-operation\" to the causal factors.\n","authors":["Mengyue Yang","Furui Liu","Zhitang Chen","Xinwei Shen","Jianye Hao","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2004.08697v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11832v1","updated":"2023-12-19T03:48:39Z","published":"2023-12-19T03:48:39Z","title":"The Validity of a Machine Learning-Based Video Game in the Objective\n  Screening of Attention Deficit Hyperactivity Disorder in Children Aged 5 to\n  12 Years","summary":"  Objective: Early identification of ADHD is necessary to provide the\nopportunity for timely treatment. However, screening the symptoms of ADHD on a\nlarge scale is not easy. This study aimed to validate a video game (FishFinder)\nfor the screening of ADHD using objective measurement of the core symptoms of\nthis disorder. Method: The FishFinder measures attention and impulsivity\nthrough in-game performance and evaluates the child's hyperactivity using\nsmartphone motion sensors. This game was tested on 26 children with ADHD and 26\nhealthy children aged 5 to 12 years. A Support Vector Machine was employed to\ndetect children with ADHD. results: This system showed 92.3% accuracy, 90%\nsensitivity, and 93.7% specificity using a combination of in-game and movement\nfeatures. Conclusions: The FishFinder demonstrated a strong ability to identify\nADHD in children. So, this game can be used as an affordable, accessible, and\nenjoyable method for the objective screening of ADHD.\n","authors":["Zeinab Zakani","Hadi Moradi","Sogand Ghasemzadeh","Maryam Riazi","Fatemeh Mortazavi"],"pdf_url":"https://arxiv.org/pdf/2312.11832v1.pdf","comment":"30 pages, 4 figures, 11 tables"},{"id":"http://arxiv.org/abs/2312.11831v1","updated":"2023-12-19T03:45:27Z","published":"2023-12-19T03:45:27Z","title":"Locally-Minimal Probabilistic Explanations","summary":"  Formal abductive explanations offer crucial guarantees of rigor and so are of\ninterest in high-stakes uses of machine learning (ML). One drawback of\nabductive explanations is explanation size, justified by the cognitive limits\nof human decision-makers. Probabilistic abductive explanations (PAXps) address\nthis limitation, but their theoretical and practical complexity makes their\nexact computation most often unrealistic. This paper proposes novel efficient\nalgorithms for the computation of locally-minimal PXAps, which offer\nhigh-quality approximations of PXAps in practice. The experimental results\ndemonstrate the practical efficiency of the proposed algorithms.\n","authors":["Yacine Izza","Kuldeep S. Meel","Joao Marques-Silva"],"pdf_url":"https://arxiv.org/pdf/2312.11831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11651v2","updated":"2023-12-19T03:35:36Z","published":"2023-09-20T21:32:58Z","title":"Drift Control of High-Dimensional RBM: A Computational Method Based on\n  Neural Networks","summary":"  Motivated by applications in queueing theory, we consider a stochastic\ncontrol problem whose state space is the $d$-dimensional positive orthant. The\ncontrolled process $Z$ evolves as a reflected Brownian motion whose covariance\nmatrix is exogenously specified, as are its directions of reflection from the\northant's boundary surfaces. A system manager chooses a drift vector\n$\\theta(t)$ at each time $t$ based on the history of $Z$, and the cost rate at\ntime $t$ depends on both $Z(t)$ and $\\theta(t)$. In our initial problem\nformulation, the objective is to minimize expected discounted cost over an\ninfinite planning horizon, after which we treat the corresponding ergodic\ncontrol problem. Extending earlier work by Han et al. (Proceedings of the\nNational Academy of Sciences, 2018, 8505-8510), we develop and illustrate a\nsimulation-based computational method that relies heavily on deep neural\nnetwork technology. For test problems studied thus far, our method is accurate\nto within a fraction of one percent, and is computationally feasible in\ndimensions up to at least $d=30$.\n","authors":["Baris Ata","J. Michael Harrison","Nian Si"],"pdf_url":"https://arxiv.org/pdf/2309.11651v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.09532v3","updated":"2023-12-19T03:35:13Z","published":"2023-02-19T10:34:08Z","title":"Pseudo Contrastive Learning for Graph-based Semi-supervised Learning","summary":"  Pseudo Labeling is a technique used to improve the performance of\nsemi-supervised Graph Neural Networks (GNNs) by generating additional\npseudo-labels based on confident predictions. However, the quality of generated\npseudo-labels has been a longstanding concern due to the sensitivity of the\nclassification objective with respect to the given labels. To avoid the\nuntrustworthy classification supervision indicating ``a node belongs to a\nspecific class,'' we favor the fault-tolerant contrasting supervision\ndemonstrating ``two nodes do not belong to the same class.'' Thus, the problem\nof generating high-quality pseudo-labels is then transformed into a relaxed\nversion, i.e., identifying reliable negative pairs. To achieve this, we propose\na general framework for GNNs, termed Pseudo Contrastive Learning (PCL). It\nseparates two nodes whose positive and negative pseudo-labels target the same\nclass. To incorporate topological knowledge into learning, we devise a\ntopologically weighted contrastive loss that spends more effort separating\nnegative pairs with smaller topological distances. Experimentally, we apply PCL\nto various GNNs, which consistently outperform their counterparts using other\npopular general techniques on five real-world graphs.\n","authors":["Weigang Lu","Ziyu Guan","Wei Zhao","Yaming Yang","Yuanhai Lv","Lining Xing","Baosheng Yu","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2302.09532v3.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2312.11822v1","updated":"2023-12-19T03:27:38Z","published":"2023-12-19T03:27:38Z","title":"Classification of complex local environments in systems of particle\n  shapes through shape-symmetry encoded data augmentation","summary":"  Detecting and analyzing the local environment is crucial for investigating\nthe dynamical processes of crystal nucleation and shape colloidal particle\nself-assembly. Recent developments in machine learning provide a promising\navenue for better order parameters in complex systems that are challenging to\nstudy using traditional approaches. However, the application of machine\nlearning to self-assembly on systems of particle shapes is still underexplored.\nTo address this gap, we propose a simple, physics-agnostic, yet powerful\napproach that involves training a multilayer perceptron (MLP) as a local\nenvironment classifier for systems of particle shapes, using input features\nsuch as particle distances and orientations. Our MLP classifier is trained in a\nsupervised manner with a shape symmetry-encoded data augmentation technique\nwithout the need for any conventional roto-translations invariant symmetry\nfunctions. We evaluate the performance of our classifiers on four different\nscenarios involving self-assembly of cubic structures, 2-dimensional and\n3-dimensional patchy particle shape systems, hexagonal bipyramids with varying\naspect ratios, and truncated shapes with different degrees of truncation. The\nproposed training process and data augmentation technique are both\nstraightforward and flexible, enabling easy application of the classifier to\nother processes involving particle orientations. Our work thus presents a\nvaluable tool for investigating self-assembly processes on systems of particle\nshapes, with potential applications in structure identification of any\nparticle-based or molecular system where orientations can be defined.\n","authors":[" Shih-Kuang"," Lee","Sun-Ting Tsai","Sharon Glotzer"],"pdf_url":"https://arxiv.org/pdf/2312.11822v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2312.11819v1","updated":"2023-12-19T03:24:55Z","published":"2023-12-19T03:24:55Z","title":"An Adaptive Placement and Parallelism Framework for Accelerating RLHF\n  Training","summary":"  Recently, ChatGPT or InstructGPT like large language models (LLM) has made a\nsignificant impact in the AI world. These models are incredibly versatile,\ncapable of performing language tasks on par or even exceeding the capabilities\nof human experts. Many works have attempted to reproduce the complex\nInstructGPT's RLHF (Reinforcement Learning with Human Feedback) training\npipeline. However, the mainstream distributed RLHF training methods typically\nadopt a fixed model placement strategy, referred to as the Flattening strategy.\nThis strategy treats all four models involved in RLHF as a single entity and\nplaces them on all devices, regardless of their differences. Unfortunately,\nthis strategy exacerbates the generation bottlenecks in the RLHF training and\ndegrades the overall training efficiency. To address these issues, we propose\nan adaptive model placement framework that offers two flexible model placement\nstrategies. These strategies allow for the agile allocation of models across\ndevices in a fine-grained manner. The Interleaving strategy helps reduce memory\nredundancy and communication costs during RLHF training. On the other hand, the\nSeparation strategy improves the throughput of model training by separating the\ntraining and generation stages of the RLHF pipeline. Notably, this framework\nseamlessly integrates with other mainstream techniques for acceleration and\nenables automatic hyperparameter search. Extensive experiments have\ndemonstrated that our Interleaving and Separation strategies can achieve\nnotable improvements up to 11x, compared to the current state-of-the-art (SOTA)\napproaches. These experiments encompassed a wide range of training scenarios,\ninvolving models of varying sizes and devices of different scales. The results\nhighlight the effectiveness and superiority of our approaches in accelerating\nthe training of distributed RLHF.\n","authors":["Youshao Xiao","Weichang Wu","Zhenglei Zhou","Fagui Mao","Shangchun Zhao","Lin Ju","Lei Liang","Xiaolu Zhang","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2312.11819v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2312.12436v1","updated":"2023-12-19T18:59:22Z","published":"2023-12-19T18:59:22Z","title":"A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise","summary":"  The surge of interest towards Multi-modal Large Language Models (MLLMs),\ne.g., GPT-4V(ision) from OpenAI, has marked a significant trend in both\nacademia and industry. They endow Large Language Models (LLMs) with powerful\ncapabilities in visual understanding, enabling them to tackle diverse\nmulti-modal tasks. Very recently, Google released Gemini, its newest and most\ncapable MLLM built from the ground up for multi-modality. In light of the\nsuperior reasoning capabilities, can Gemini challenge GPT-4V's leading position\nin multi-modal learning? In this paper, we present a preliminary exploration of\nGemini Pro's visual understanding proficiency, which comprehensively covers\nfour domains: fundamental perception, advanced cognition, challenging vision\ntasks, and various expert capacities. We compare Gemini Pro with the\nstate-of-the-art GPT-4V to evaluate its upper limits, along with the latest\nopen-sourced MLLM, Sphinx, which reveals the gap between manual efforts and\nblack-box systems. The qualitative samples indicate that, while GPT-4V and\nGemini showcase different answering styles and preferences, they can exhibit\ncomparable visual reasoning capabilities, and Sphinx still trails behind them\nconcerning domain generalizability. Specifically, GPT-4V tends to elaborate\ndetailed explanations and intermediate steps, and Gemini prefers to output a\ndirect and concise answer. The quantitative evaluation on the popular MME\nbenchmark also demonstrates the potential of Gemini to be a strong challenger\nto GPT-4V. Our early investigation of Gemini also observes some common issues\nof MLLMs, indicating that there still remains a considerable distance towards\nartificial general intelligence. Our project for tracking the progress of MLLM\nis released at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\n","authors":["Chaoyou Fu","Renrui Zhang","Haojia Lin","Zihan Wang","Timin Gao","Yongdong Luo","Yubo Huang","Zhengye Zhang","Longtian Qiu","Gaoxiang Ye","Yunhang Shen","Mengdan Zhang","Peixian Chen","Sirui Zhao","Xiawu Zheng","Shaohui Lin","Deqiang Jiang","Di Yin","Peng Gao","Ke Li","Xing Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2312.12436v1.pdf","comment":"Total 120 pages. See our project at\n  https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models"},{"id":"http://arxiv.org/abs/2301.11145v2","updated":"2023-12-19T17:09:04Z","published":"2023-01-26T14:52:30Z","title":"Learning from Mistakes: Self-Regularizing Hierarchical Representations\n  in Point Cloud Semantic Segmentation","summary":"  Recent advances in autonomous robotic technologies have highlighted the\ngrowing need for precise environmental analysis. LiDAR semantic segmentation\nhas gained attention to accomplish fine-grained scene understanding by acting\ndirectly on raw content provided by sensors. Recent solutions showed how\ndifferent learning techniques can be used to improve the performance of the\nmodel, without any architectural or dataset change. Following this trend, we\npresent a coarse-to-fine setup that LEArns from classification mistaKes (LEAK)\nderived from a standard model. First, classes are clustered into macro groups\naccording to mutual prediction errors; then, the learning process is\nregularized by: (1) aligning class-conditional prototypical feature\nrepresentation for both fine and coarse classes, (2) weighting instances with a\nper-class fairness index. Our LEAK approach is very general and can be\nseamlessly applied on top of any segmentation architecture; indeed,\nexperimental results showed that it enables state-of-the-art performances on\ndifferent architectures, datasets and tasks, while ensuring more balanced\nclass-wise results and faster convergence.\n","authors":["Elena Camuffo","Umberto Michieli","Simone Milani"],"pdf_url":"https://arxiv.org/pdf/2301.11145v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10493v2","updated":"2023-12-19T15:55:23Z","published":"2023-12-16T16:14:50Z","title":"Debiasing Multimodal Sarcasm Detection with Contrastive Learning","summary":"  Despite commendable achievements made by existing work, prevailing multimodal\nsarcasm detection studies rely more on textual content over visual information.\nIt unavoidably induces spurious correlations between textual words and labels,\nthereby significantly hindering the models' generalization capability. To\naddress this problem, we define the task of out-of-distribution (OOD)\nmultimodal sarcasm detection, which aims to evaluate models' generalizability\nwhen the word distribution is different in training and testing settings.\nMoreover, we propose a novel debiasing multimodal sarcasm detection framework\nwith contrastive learning, which aims to mitigate the harmful effect of biased\ntextual factors for robust OOD generalization. In particular, we first design\ncounterfactual data augmentation to construct the positive samples with\ndissimilar word biases and negative samples with similar word biases.\nSubsequently, we devise an adapted debiasing contrastive learning mechanism to\nempower the model to learn robust task-relevant features and alleviate the\nadverse effect of biased words. Extensive experiments show the superiority of\nthe proposed framework.\n","authors":["Mengzhao Jia","Can Xie","Liqiang Jing"],"pdf_url":"https://arxiv.org/pdf/2312.10493v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12174v1","updated":"2023-12-19T14:05:15Z","published":"2023-12-19T14:05:15Z","title":"Low-Consumption Partial Transcoding by HEVC","summary":"  A transcoding scheme for the High Efficiency Video Coding (HEVC) is proposed\nthat allows any partial frame modification to be followed by a partial\nre-compression of only the modified areas, while guaranteeing identical\nreconstruction of non-modified areas. To this end, first, syntax elements of\nall Coding Units (CU) in the frame are parsed and decoded according to their\nscan order. Then CUs that are collocated with a replaced area are re-encoded\nwith new content to generate a partial set of new syntax elements. In order to\navoid spatial propagation of the decoding mismatch due to the new content, CUs\non the border of the replaced area are losslessly coded such that\nreconstruction of immediately neighboring CUs in the scan order are protected\nfrom the modification. The proposed method has been implemented on top of the\nHEVC test Model (HM) in All-Intra (AI) coding configuration and experiments\nshow that, depending on the test parameters, it can offer both a bitrate saving\n(up to 4% in terms of BD-BR) and a transcoding acceleration (up to 83%)\ncompared to a full transcoding scheme.\n","authors":["Mohsen Abdoli","Félix Henry","Gordon Clare"],"pdf_url":"https://arxiv.org/pdf/2312.12174v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12150v1","updated":"2023-12-19T13:33:54Z","published":"2023-12-19T13:33:54Z","title":"Comparative Study of Hardware and Software Power Measurements in Video\n  Compression","summary":"  The environmental impact of video streaming services has been discussed as\npart of the strategies towards sustainable information and communication\ntechnologies. A first step towards that is the energy profiling and assessment\nof energy consumption of existing video technologies. This paper presents a\ncomprehensive study of power measurement techniques in video compression,\ncomparing the use of hardware and software power meters. An experimental\nmethodology to ensure reliability of measurements is introduced. Key findings\ndemonstrate the high correlation of hardware and software based energy\nmeasurements for two video codecs across different spatial and temporal\nresolutions at a lower computational overhead.\n","authors":["Angeliki Katsenou","Xinyi Wang","Daniel Schien","David Bull"],"pdf_url":"https://arxiv.org/pdf/2312.12150v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2312.06171v2","updated":"2023-12-19T12:36:47Z","published":"2023-12-11T07:20:42Z","title":"Jointly Explicit and Implicit Cross-Modal Interaction Network for\n  Anterior Chamber Inflammation Diagnosis","summary":"  Uveitis demands the precise diagnosis of anterior chamber inflammation (ACI)\nfor optimal treatment. However, current diagnostic methods only rely on a\nlimited single-modal disease perspective, which leads to poor performance. In\nthis paper, we investigate a promising yet challenging way to fuse multimodal\ndata for ACI diagnosis. Notably, existing fusion paradigms focus on empowering\nimplicit modality interactions (i.e., self-attention and its variants), but\nneglect to inject explicit modality interactions, especially from clinical\nknowledge and imaging property. To this end, we propose a jointly Explicit and\nimplicit Cross-Modal Interaction Network (EiCI-Net) for Anterior Chamber\nInflammation Diagnosis that uses anterior segment optical coherence tomography\n(AS-OCT) images, slit-lamp images, and clinical data jointly. Specifically, we\nfirst develop CNN-Based Encoders and Tabular Processing Module (TPM) to extract\nefficient feature representations in different modalities. Then, we devise an\nExplicit Cross-Modal Interaction Module (ECIM) to generate attention maps as a\nkind of explicit clinical knowledge based on the tabular feature maps, then\nintegrated them into the slit-lamp feature maps, allowing the CNN-Based Encoder\nto focus on more effective informativeness of the slit-lamp images. After that,\nthe Implicit Cross-Modal Interaction Module (ICIM), a transformer-based\nnetwork, further implicitly enhances modality interactions. Finally, we\nconstruct a considerable real-world dataset from our collaborative hospital and\nconduct sufficient experiments to demonstrate the superior performance of our\nproposed EiCI-Net compared with the state-of-the-art classification methods in\nvarious metrics.\n","authors":["Qian Shao","Ye Dai","Haochao Ying","Kan Xu","Jinhong Wang","Wei Chi","Jian Wu"],"pdf_url":"https://arxiv.org/pdf/2312.06171v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07594v2","updated":"2023-12-19T03:44:25Z","published":"2023-11-10T09:51:24Z","title":"How to Bridge the Gap between Modalities: A Comprehensive Survey on\n  Multimodal Large Language Model","summary":"  This review paper explores Multimodal Large Language Models (MLLMs), which\nintegrate Large Language Models (LLMs) like GPT-4 to handle multimodal data\nsuch as text and vision. MLLMs demonstrate capabilities like generating image\nnarratives and answering image-based questions, bridging the gap towards\nreal-world human-computer interactions and hinting at a potential pathway to\nartificial general intelligence. However, MLLMs still face challenges in\nprocessing the semantic gap in multimodality, which may lead to erroneous\ngeneration, posing potential risks to society. Choosing the appropriate\nmodality alignment method is crucial, as improper methods might require more\nparameters with limited performance improvement. This paper aims to explore\nmodality alignment methods for LLMs and their existing capabilities.\nImplementing modality alignment allows LLMs to address environmental issues and\nenhance accessibility. The study surveys existing modal alignment methods in\nMLLMs into four groups: (1) Multimodal Converters that change data into\nsomething LLMs can understand; (2) Multimodal Perceivers to improve how LLMs\nperceive different types of data; (3) Tools Assistance for changing data into\none common format, usually text; and (4) Data-Driven methods that teach LLMs to\nunderstand specific types of data in a dataset. This field is still in a phase\nof exploration and experimentation, and we will organize and update various\nexisting research methods for multimodal information alignment.\n","authors":["Shezheng Song","Xiaopeng Li","Shasha Li","Shan Zhao","Jie Yu","Jun Ma","Xiaoguang Mao","Weimin Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.07594v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11793v1","updated":"2023-12-19T02:09:38Z","published":"2023-12-19T02:09:38Z","title":"An effective image copy-move forgery detection using entropy image","summary":"  Image forensics has become increasingly important in our daily lives. As a\nfundamental type of forgeries, Copy-Move Forgery Detection (CMFD) has received\nsignificant attention in the academic community. Keypoint-based algorithms,\nparticularly those based on SIFT, have achieved good results in CMFD. However,\nthe most of keypoint detection algorithms often fail to generate sufficient\nmatches when tampered patches are present in smooth areas. To tackle this\nproblem, we introduce entropy images to determine the coordinates and scales of\nkeypoints, resulting significantly increasing the number of keypoints.\nFurthermore, we develop an entropy level clustering algorithm to avoid\nincreased matching complexity caused by non-ideal distribution of grayscale\nvalues in keypoints. Experimental results demonstrate that our algorithm\nachieves a good balance between performance and time efficiency.\n","authors":["Zhaowei Lu","Li Jiang"],"pdf_url":"https://arxiv.org/pdf/2312.11793v1.pdf","comment":null}]}}